- file_name: graphrag/__init__.py
  imports: []
  functions: []
- file_name: graphrag/__main__.py
  imports:
  - module: graphrag.cli.main
    name: app
    alias: null
  functions: []
- file_name: graphrag/api/__init__.py
  imports:
  - module: graphrag.api.index
    name: build_index
    alias: null
  - module: graphrag.api.prompt_tune
    name: generate_indexing_prompts
    alias: null
  - module: graphrag.api.query
    name: basic_search
    alias: null
  - module: graphrag.api.query
    name: basic_search_streaming
    alias: null
  - module: graphrag.api.query
    name: drift_search
    alias: null
  - module: graphrag.api.query
    name: drift_search_streaming
    alias: null
  - module: graphrag.api.query
    name: global_search
    alias: null
  - module: graphrag.api.query
    name: global_search_streaming
    alias: null
  - module: graphrag.api.query
    name: local_search
    alias: null
  - module: graphrag.api.query
    name: local_search_streaming
    alias: null
  - module: graphrag.api.query
    name: multi_index_basic_search
    alias: null
  - module: graphrag.api.query
    name: multi_index_drift_search
    alias: null
  - module: graphrag.api.query
    name: multi_index_global_search
    alias: null
  - module: graphrag.api.query
    name: multi_index_local_search
    alias: null
  - module: graphrag.prompt_tune.types
    name: DocSelectionType
    alias: null
  functions: []
- file_name: graphrag/api/index.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: IndexingMethod
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.run_pipeline
    name: run_pipeline
    alias: null
  - module: graphrag.index.run.utils
    name: create_callback_chain
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.index.workflows.factory
    name: PipelineFactory
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  functions:
  - name: build_index
    start_line: 29
    end_line: 96
    code: "async def build_index(\n    config: GraphRagConfig,\n    method: IndexingMethod\
      \ | str = IndexingMethod.Standard,\n    is_update_run: bool = False,\n    memory_profile:\
      \ bool = False,\n    callbacks: list[WorkflowCallbacks] | None = None,\n   \
      \ additional_context: dict[str, Any] | None = None,\n    verbose: bool = False,\n\
      \    input_documents: pd.DataFrame | None = None,\n) -> list[PipelineRunResult]:\n\
      \    \"\"\"Run the pipeline with the given configuration.\n\n    Parameters\n\
      \    ----------\n    config : GraphRagConfig\n        The configuration.\n \
      \   method : IndexingMethod default=IndexingMethod.Standard\n        Styling\
      \ of indexing to perform (full LLM, NLP + LLM, etc.).\n    memory_profile :\
      \ bool\n        Whether to enable memory profiling.\n    callbacks : list[WorkflowCallbacks]\
      \ | None default=None\n        A list of callbacks to register.\n    additional_context\
      \ : dict[str, Any] | None default=None\n        Additional context to pass to\
      \ the pipeline run. This can be accessed in the pipeline state under the 'additional_context'\
      \ key.\n    input_documents : pd.DataFrame | None default=None.\n        Override\
      \ document loading and parsing and supply your own dataframe of documents to\
      \ index.\n\n    Returns\n    -------\n    list[PipelineRunResult]\n        The\
      \ list of pipeline run results\n    \"\"\"\n    init_loggers(config=config,\
      \ verbose=verbose)\n\n    # Create callbacks for pipeline lifecycle events if\
      \ provided\n    workflow_callbacks = (\n        create_callback_chain(callbacks)\
      \ if callbacks else NoopWorkflowCallbacks()\n    )\n\n    outputs: list[PipelineRunResult]\
      \ = []\n\n    if memory_profile:\n        logger.warning(\"New pipeline does\
      \ not yet support memory profiling.\")\n\n    logger.info(\"Initializing indexing\
      \ pipeline...\")\n    # todo: this could propagate out to the cli for better\
      \ clarity, but will be a breaking api change\n    method = _get_method(method,\
      \ is_update_run)\n    pipeline = PipelineFactory.create_pipeline(config, method)\n\
      \n    workflow_callbacks.pipeline_start(pipeline.names())\n\n    async for output\
      \ in run_pipeline(\n        pipeline,\n        config,\n        callbacks=workflow_callbacks,\n\
      \        is_update_run=is_update_run,\n        additional_context=additional_context,\n\
      \        input_documents=input_documents,\n    ):\n        outputs.append(output)\n\
      \        if output.errors and len(output.errors) > 0:\n            logger.error(\"\
      Workflow %s completed with errors\", output.workflow)\n        else:\n     \
      \       logger.info(\"Workflow %s completed successfully\", output.workflow)\n\
      \        logger.debug(str(output.result))\n\n    workflow_callbacks.pipeline_end(outputs)\n\
      \    return outputs"
    signature: "def build_index(\n    config: GraphRagConfig,\n    method: IndexingMethod\
      \ | str = IndexingMethod.Standard,\n    is_update_run: bool = False,\n    memory_profile:\
      \ bool = False,\n    callbacks: list[WorkflowCallbacks] | None = None,\n   \
      \ additional_context: dict[str, Any] | None = None,\n    verbose: bool = False,\n\
      \    input_documents: pd.DataFrame | None = None,\n) -> list[PipelineRunResult]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/index/run/utils.py::create_callback_chain
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: logger.warning
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: graphrag/api/index.py::_get_method
      type: internal
    - target: graphrag/index/workflows/factory.py::PipelineFactory::create_pipeline
      type: external
    - target: workflow_callbacks.pipeline_start
      type: unresolved
    - target: pipeline.names
      type: unresolved
    - target: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
    - target: outputs.append
      type: unresolved
    - target: len
      type: builtin
    - target: logger.error
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: str
      type: builtin
    - target: workflow_callbacks.pipeline_end
      type: unresolved
    visibility: public
    node_id: graphrag/api/index.py::build_index
    called_by: []
  - name: _get_method
    start_line: 99
    end_line: 101
    code: "def _get_method(method: IndexingMethod | str, is_update_run: bool) -> str:\n\
      \    m = method.value if isinstance(method, IndexingMethod) else method\n  \
      \  return f\"{m}-update\" if is_update_run else m"
    signature: 'def _get_method(method: IndexingMethod | str, is_update_run: bool)
      -> str'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    visibility: protected
    node_id: graphrag/api/index.py::_get_method
    called_by:
    - source: graphrag/api/index.py::build_index
      type: internal
- file_name: graphrag/api/prompt_tune.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Annotated
    alias: null
  - module: annotated_types
    name: null
    alias: null
  - module: pydantic
    name: PositiveInt
    alias: null
  - module: pydantic
    name: validate_call
    alias: null
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: MAX_TOKEN_COUNT
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: PROMPT_TUNING_MODEL_ID
    alias: null
  - module: graphrag.prompt_tune.generator.community_report_rating
    name: generate_community_report_rating
    alias: null
  - module: graphrag.prompt_tune.generator.community_report_summarization
    name: create_community_summarization_prompt
    alias: null
  - module: graphrag.prompt_tune.generator.community_reporter_role
    name: generate_community_reporter_role
    alias: null
  - module: graphrag.prompt_tune.generator.domain
    name: generate_domain
    alias: null
  - module: graphrag.prompt_tune.generator.entity_relationship
    name: generate_entity_relationship_examples
    alias: null
  - module: graphrag.prompt_tune.generator.entity_summarization_prompt
    name: create_entity_summarization_prompt
    alias: null
  - module: graphrag.prompt_tune.generator.entity_types
    name: generate_entity_types
    alias: null
  - module: graphrag.prompt_tune.generator.extract_graph_prompt
    name: create_extract_graph_prompt
    alias: null
  - module: graphrag.prompt_tune.generator.language
    name: detect_language
    alias: null
  - module: graphrag.prompt_tune.generator.persona
    name: generate_persona
    alias: null
  - module: graphrag.prompt_tune.loader.input
    name: load_docs_in_chunks
    alias: null
  - module: graphrag.prompt_tune.types
    name: DocSelectionType
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  functions:
  - name: generate_indexing_prompts
    start_line: 56
    end_line: 202
    code: "async def generate_indexing_prompts(\n    config: GraphRagConfig,\n   \
      \ chunk_size: PositiveInt = graphrag_config_defaults.chunks.size,\n    overlap:\
      \ Annotated[\n        int, annotated_types.Gt(-1)\n    ] = graphrag_config_defaults.chunks.overlap,\n\
      \    limit: PositiveInt = 15,\n    selection_method: DocSelectionType = DocSelectionType.RANDOM,\n\
      \    domain: str | None = None,\n    language: str | None = None,\n    max_tokens:\
      \ int = MAX_TOKEN_COUNT,\n    discover_entity_types: bool = True,\n    min_examples_required:\
      \ PositiveInt = 2,\n    n_subset_max: PositiveInt = 300,\n    k: PositiveInt\
      \ = 15,\n    verbose: bool = False,\n) -> tuple[str, str, str]:\n    \"\"\"\
      Generate indexing prompts.\n\n    Parameters\n    ----------\n    - config:\
      \ The GraphRag configuration.\n    - output_path: The path to store the prompts.\n\
      \    - chunk_size: The chunk token size to use for input text units.\n    -\
      \ limit: The limit of chunks to load.\n    - selection_method: The chunk selection\
      \ method.\n    - domain: The domain to map the input documents to.\n    - language:\
      \ The language to use for the prompts.\n    - max_tokens: The maximum number\
      \ of tokens to use on entity extraction prompts\n    - discover_entity_types:\
      \ Generate entity types.\n    - min_examples_required: The minimum number of\
      \ examples required for entity extraction prompts.\n    - n_subset_max: The\
      \ number of text chunks to embed when using auto selection method.\n    - k:\
      \ The number of documents to select when using auto selection method.\n\n  \
      \  Returns\n    -------\n    tuple[str, str, str]: entity extraction prompt,\
      \ entity summarization prompt, community summarization prompt\n    \"\"\"\n\
      \    init_loggers(config=config, verbose=verbose, filename=\"prompt-tuning.log\"\
      )\n\n    # Retrieve documents\n    logger.info(\"Chunking documents...\")\n\
      \    doc_list = await load_docs_in_chunks(\n        config=config,\n       \
      \ limit=limit,\n        select_method=selection_method,\n        logger=logger,\n\
      \        chunk_size=chunk_size,\n        overlap=overlap,\n        n_subset_max=n_subset_max,\n\
      \        k=k,\n    )\n\n    # Create LLM from config\n    # TODO: Expose a way\
      \ to specify Prompt Tuning model ID through config\n    logger.info(\"Retrieving\
      \ language model configuration...\")\n    default_llm_settings = config.get_language_model_config(PROMPT_TUNING_MODEL_ID)\n\
      \n    logger.info(\"Creating language model...\")\n    llm = ModelManager().register_chat(\n\
      \        name=\"prompt_tuning\",\n        model_type=default_llm_settings.type,\n\
      \        config=default_llm_settings,\n        callbacks=NoopWorkflowCallbacks(),\n\
      \        cache=None,\n    )\n\n    if not domain:\n        logger.info(\"Generating\
      \ domain...\")\n        domain = await generate_domain(llm, doc_list)\n\n  \
      \  if not language:\n        logger.info(\"Detecting language...\")\n      \
      \  language = await detect_language(llm, doc_list)\n\n    logger.info(\"Generating\
      \ persona...\")\n    persona = await generate_persona(llm, domain)\n\n    logger.info(\"\
      Generating community report ranking description...\")\n    community_report_ranking\
      \ = await generate_community_report_rating(\n        llm, domain=domain, persona=persona,\
      \ docs=doc_list\n    )\n\n    entity_types = None\n    extract_graph_llm_settings\
      \ = config.get_language_model_config(\n        config.extract_graph.model_id\n\
      \    )\n    if discover_entity_types:\n        logger.info(\"Generating entity\
      \ types...\")\n        entity_types = await generate_entity_types(\n       \
      \     llm,\n            domain=domain,\n            persona=persona,\n     \
      \       docs=doc_list,\n            json_mode=extract_graph_llm_settings.model_supports_json\
      \ or False,\n        )\n\n    logger.info(\"Generating entity relationship examples...\"\
      )\n    examples = await generate_entity_relationship_examples(\n        llm,\n\
      \        persona=persona,\n        entity_types=entity_types,\n        docs=doc_list,\n\
      \        language=language,\n        json_mode=False,  # config.llm.model_supports_json\
      \ should be used, but these prompts are used in non-json mode by the index engine\n\
      \    )\n\n    logger.info(\"Generating entity extraction prompt...\")\n    extract_graph_prompt\
      \ = create_extract_graph_prompt(\n        entity_types=entity_types,\n     \
      \   docs=doc_list,\n        examples=examples,\n        language=language,\n\
      \        json_mode=False,  # config.llm.model_supports_json should be used,\
      \ but these prompts are used in non-json mode by the index engine\n        tokenizer=get_tokenizer(model_config=extract_graph_llm_settings),\n\
      \        max_token_count=max_tokens,\n        min_examples_required=min_examples_required,\n\
      \    )\n\n    logger.info(\"Generating entity summarization prompt...\")\n \
      \   entity_summarization_prompt = create_entity_summarization_prompt(\n    \
      \    persona=persona,\n        language=language,\n    )\n\n    logger.info(\"\
      Generating community reporter role...\")\n    community_reporter_role = await\
      \ generate_community_reporter_role(\n        llm, domain=domain, persona=persona,\
      \ docs=doc_list\n    )\n\n    logger.info(\"Generating community summarization\
      \ prompt...\")\n    community_summarization_prompt = create_community_summarization_prompt(\n\
      \        persona=persona,\n        role=community_reporter_role,\n        report_rating_description=community_report_ranking,\n\
      \        language=language,\n    )\n\n    logger.debug(\"Generated domain: %s\"\
      , domain)\n    logger.debug(\"Detected language: %s\", language)\n    logger.debug(\"\
      Generated persona: %s\", persona)\n\n    return (\n        extract_graph_prompt,\n\
      \        entity_summarization_prompt,\n        community_summarization_prompt,\n\
      \    )"
    signature: "def generate_indexing_prompts(\n    config: GraphRagConfig,\n    chunk_size:\
      \ PositiveInt = graphrag_config_defaults.chunks.size,\n    overlap: Annotated[\n\
      \        int, annotated_types.Gt(-1)\n    ] = graphrag_config_defaults.chunks.overlap,\n\
      \    limit: PositiveInt = 15,\n    selection_method: DocSelectionType = DocSelectionType.RANDOM,\n\
      \    domain: str | None = None,\n    language: str | None = None,\n    max_tokens:\
      \ int = MAX_TOKEN_COUNT,\n    discover_entity_types: bool = True,\n    min_examples_required:\
      \ PositiveInt = 2,\n    n_subset_max: PositiveInt = 300,\n    k: PositiveInt\
      \ = 15,\n    verbose: bool = False,\n) -> tuple[str, str, str]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: annotated_types::Gt
      type: external
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.info
      type: unresolved
    - target: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().register_chat
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: graphrag/prompt_tune/generator/domain.py::generate_domain
      type: internal
    - target: graphrag/prompt_tune/generator/language.py::detect_language
      type: internal
    - target: graphrag/prompt_tune/generator/persona.py::generate_persona
      type: internal
    - target: graphrag/prompt_tune/generator/community_report_rating.py::generate_community_report_rating
      type: internal
    - target: graphrag/prompt_tune/generator/entity_types.py::generate_entity_types
      type: internal
    - target: graphrag/prompt_tune/generator/entity_relationship.py::generate_entity_relationship_examples
      type: internal
    - target: graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/prompt_tune/generator/entity_summarization_prompt.py::create_entity_summarization_prompt
      type: internal
    - target: graphrag/prompt_tune/generator/community_reporter_role.py::generate_community_reporter_role
      type: internal
    - target: graphrag/prompt_tune/generator/community_report_summarization.py::create_community_summarization_prompt
      type: internal
    - target: logger.debug
      type: unresolved
    visibility: public
    node_id: graphrag/api/prompt_tune.py::generate_indexing_prompts
    called_by: []
- file_name: graphrag/api/query.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: pydantic
    name: validate_call
    alias: null
  - module: graphrag.callbacks.noop_query_callbacks
    name: NoopQueryCallbacks
    alias: null
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.config.embeddings
    name: community_full_content_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: entity_description_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: text_unit_text_embedding
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  - module: graphrag.query.factory
    name: get_basic_search_engine
    alias: null
  - module: graphrag.query.factory
    name: get_drift_search_engine
    alias: null
  - module: graphrag.query.factory
    name: get_global_search_engine
    alias: null
  - module: graphrag.query.factory
    name: get_local_search_engine
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_communities
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_covariates
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_entities
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_relationships
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_report_embeddings
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_reports
    alias: null
  - module: graphrag.query.indexer_adapters
    name: read_indexer_text_units
    alias: null
  - module: graphrag.utils.api
    name: get_embedding_store
    alias: null
  - module: graphrag.utils.api
    name: load_search_prompt
    alias: null
  - module: graphrag.utils.api
    name: truncate
    alias: null
  - module: graphrag.utils.api
    name: update_context_data
    alias: null
  - module: graphrag.utils.cli
    name: redact
    alias: null
  functions:
  - name: global_search
    start_line: 64
    end_line: 124
    code: "async def global_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    community_level:\
      \ int | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform\
      \ a global search and return the context data and response.\n\n    Parameters\n\
      \    ----------\n    - config (GraphRagConfig): A graphrag configuration (from\
      \ settings.yaml)\n    - entities (pd.DataFrame): A DataFrame containing the\
      \ final entities (from entities.parquet)\n    - communities (pd.DataFrame):\
      \ A DataFrame containing the final communities (from communities.parquet)\n\
      \    - community_reports (pd.DataFrame): A DataFrame containing the final community\
      \ reports (from community_reports.parquet)\n    - community_level (int): The\
      \ community level to search at.\n    - dynamic_community_selection (bool): Enable\
      \ dynamic community selection instead of using all community reports at a fixed\
      \ level. Note that you can still provide community_level cap the maximum level\
      \ to search.\n    - response_type (str): The type of response to return.\n \
      \   - query (str): The user query to search for.\n\n    Returns\n    -------\n\
      \    TODO: Document the search response type and format.\n    \"\"\"\n    init_loggers(config=config,\
      \ verbose=verbose, filename=\"query.log\")\n\n    callbacks = callbacks or []\n\
      \    full_response = \"\"\n    context_data = {}\n\n    def on_context(context:\
      \ Any) -> None:\n        nonlocal context_data\n        context_data = context\n\
      \n    local_callbacks = NoopQueryCallbacks()\n    local_callbacks.on_context\
      \ = on_context\n    callbacks.append(local_callbacks)\n\n    logger.debug(\"\
      Executing global search query: %s\", query)\n    async for chunk in global_search_streaming(\n\
      \        config=config,\n        entities=entities,\n        communities=communities,\n\
      \        community_reports=community_reports,\n        community_level=community_level,\n\
      \        dynamic_community_selection=dynamic_community_selection,\n        response_type=response_type,\n\
      \        query=query,\n        callbacks=callbacks,\n    ):\n        full_response\
      \ += chunk\n    logger.debug(\"Query response: %s\", truncate(full_response,\
      \ 400))\n    return full_response, context_data"
    signature: "def global_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    community_level:\
      \ int | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: callbacks.append
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::global_search_streaming
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::global_search
    called_by:
    - source: graphrag/api/query.py::multi_index_global_search
      type: internal
  - name: on_context
    start_line: 102
    end_line: 104
    code: "def on_context(context: Any) -> None:\n        nonlocal context_data\n\
      \        context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/api/query.py::on_context
    called_by: []
  - name: global_search_streaming
    start_line: 128
    end_line: 192
    code: "def global_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    community_level: int | None,\n    dynamic_community_selection: bool,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator:\n    \"\"\
      \"Perform a global search and return the context data and response via a generator.\n\
      \n    Context data is returned as a dictionary of lists, with one list entry\
      \ for each record.\n\n    Parameters\n    ----------\n    - config (GraphRagConfig):\
      \ A graphrag configuration (from settings.yaml)\n    - entities (pd.DataFrame):\
      \ A DataFrame containing the final entities (from entities.parquet)\n    - communities\
      \ (pd.DataFrame): A DataFrame containing the final communities (from communities.parquet)\n\
      \    - community_reports (pd.DataFrame): A DataFrame containing the final community\
      \ reports (from community_reports.parquet)\n    - community_level (int): The\
      \ community level to search at.\n    - dynamic_community_selection (bool): Enable\
      \ dynamic community selection instead of using all community reports at a fixed\
      \ level. Note that you can still provide community_level cap the maximum level\
      \ to search.\n    - response_type (str): The type of response to return.\n \
      \   - query (str): The user query to search for.\n\n    Returns\n    -------\n\
      \    TODO: Document the search response type and format.\n    \"\"\"\n    init_loggers(config=config,\
      \ verbose=verbose, filename=\"query.log\")\n\n    communities_ = read_indexer_communities(communities,\
      \ community_reports)\n    reports = read_indexer_reports(\n        community_reports,\n\
      \        communities,\n        community_level=community_level,\n        dynamic_community_selection=dynamic_community_selection,\n\
      \    )\n    entities_ = read_indexer_entities(\n        entities, communities,\
      \ community_level=community_level\n    )\n    map_prompt = load_search_prompt(config.root_dir,\
      \ config.global_search.map_prompt)\n    reduce_prompt = load_search_prompt(\n\
      \        config.root_dir, config.global_search.reduce_prompt\n    )\n    knowledge_prompt\
      \ = load_search_prompt(\n        config.root_dir, config.global_search.knowledge_prompt\n\
      \    )\n\n    logger.debug(\"Executing streaming global search query: %s\",\
      \ query)\n    search_engine = get_global_search_engine(\n        config,\n \
      \       reports=reports,\n        entities=entities_,\n        communities=communities_,\n\
      \        response_type=response_type,\n        dynamic_community_selection=dynamic_community_selection,\n\
      \        map_system_prompt=map_prompt,\n        reduce_system_prompt=reduce_prompt,\n\
      \        general_knowledge_inclusion_prompt=knowledge_prompt,\n        callbacks=callbacks,\n\
      \    )\n    return search_engine.stream_search(query=query)"
    signature: "def global_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    community_level: int | None,\n    dynamic_community_selection: bool,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_communities
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_entities
      type: internal
    - target: graphrag/utils/api.py::load_search_prompt
      type: internal
    - target: logger.debug
      type: unresolved
    - target: graphrag/query/factory.py::get_global_search_engine
      type: internal
    - target: search_engine.stream_search
      type: unresolved
    visibility: public
    node_id: graphrag/api/query.py::global_search_streaming
    called_by:
    - source: graphrag/api/query.py::global_search
      type: internal
  - name: multi_index_global_search
    start_line: 196
    end_line: 338
    code: "async def multi_index_global_search(\n    config: GraphRagConfig,\n   \
      \ entities_list: list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n\
      \    community_reports_list: list[pd.DataFrame],\n    index_names: list[str],\n\
      \    community_level: int | None,\n    dynamic_community_selection: bool,\n\
      \    response_type: str,\n    streaming: bool,\n    query: str,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n) -> tuple[\n\
      \    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform a global search across multiple\
      \ indexes and return the context data and response.\n\n    Parameters\n    ----------\n\
      \    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
      \    - entities_list (list[pd.DataFrame]): A list of DataFrames containing the\
      \ final entities (from entities.parquet)\n    - communities_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final communities (from communities.parquet)\n\
      \    - community_reports_list (list[pd.DataFrame]): A list of DataFrames containing\
      \ the final community reports (from community_reports.parquet)\n    - index_names\
      \ (list[str]): A list of index names.\n    - community_level (int): The community\
      \ level to search at.\n    - dynamic_community_selection (bool): Enable dynamic\
      \ community selection instead of using all community reports at a fixed level.\
      \ Note that you can still provide community_level cap the maximum level to search.\n\
      \    - response_type (str): The type of response to return.\n    - streaming\
      \ (bool): Whether to stream the results or not.\n    - query (str): The user\
      \ query to search for.\n\n    Returns\n    -------\n    TODO: Document the search\
      \ response type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    logger.warning(\n        \"Multi-index search\
      \ is deprecated and will be removed in GraphRAG v3.\"\n    )\n\n    # Streaming\
      \ not supported yet\n    if streaming:\n        message = \"Streaming not yet\
      \ implemented for multi_global_search\"\n        raise NotImplementedError(message)\n\
      \n    links = {\n        \"communities\": {},\n        \"community_reports\"\
      : {},\n        \"entities\": {},\n    }\n    max_vals = {\n        \"communities\"\
      : -1,\n        \"community_reports\": -1,\n        \"entities\": -1,\n    }\n\
      \n    communities_dfs = []\n    community_reports_dfs = []\n    entities_dfs\
      \ = []\n\n    for idx, index_name in enumerate(index_names):\n        # Prepare\
      \ each index's community reports dataframe for merging\n        community_reports_df\
      \ = community_reports_list[idx]\n        community_reports_df[\"community\"\
      ] = community_reports_df[\"community\"].astype(\n            int\n        )\n\
      \        for i in community_reports_df[\"community\"]:\n            links[\"\
      community_reports\"][i + max_vals[\"community_reports\"] + 1] = {\n        \
      \        \"index_name\": index_name,\n                \"id\": str(i),\n    \
      \        }\n        community_reports_df[\"community\"] += max_vals[\"community_reports\"\
      ] + 1\n        community_reports_df[\"human_readable_id\"] += max_vals[\"community_reports\"\
      ] + 1\n        max_vals[\"community_reports\"] = int(community_reports_df[\"\
      community\"].max())\n        community_reports_dfs.append(community_reports_df)\n\
      \n        # Prepare each index's communities dataframe for merging\n       \
      \ communities_df = communities_list[idx]\n        communities_df[\"community\"\
      ] = communities_df[\"community\"].astype(int)\n        communities_df[\"parent\"\
      ] = communities_df[\"parent\"].astype(int)\n        for i in communities_df[\"\
      community\"]:\n            links[\"communities\"][i + max_vals[\"communities\"\
      ] + 1] = {\n                \"index_name\": index_name,\n                \"\
      id\": str(i),\n            }\n        communities_df[\"community\"] += max_vals[\"\
      communities\"] + 1\n        communities_df[\"parent\"] = communities_df[\"parent\"\
      ].apply(\n            lambda x: x if x == -1 else x + max_vals[\"communities\"\
      ] + 1\n        )\n        communities_df[\"human_readable_id\"] += max_vals[\"\
      communities\"] + 1\n        # concat the index name to the entity_ids, since\
      \ this is used for joining later\n        communities_df[\"entity_ids\"] = communities_df[\"\
      entity_ids\"].apply(\n            lambda x, index_name=index_name: [i + f\"\
      -{index_name}\" for i in x]\n        )\n        max_vals[\"communities\"] =\
      \ int(communities_df[\"community\"].max())\n        communities_dfs.append(communities_df)\n\
      \n        # Prepare each index's entities dataframe for merging\n        entities_df\
      \ = entities_list[idx]\n        for i in entities_df[\"human_readable_id\"]:\n\
      \            links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n     \
      \           \"index_name\": index_name,\n                \"id\": i,\n      \
      \      }\n        entities_df[\"human_readable_id\"] += max_vals[\"entities\"\
      ] + 1\n        entities_df[\"title\"] = entities_df[\"title\"].apply(\n    \
      \        lambda x, index_name=index_name: x + f\"-{index_name}\"\n        )\n\
      \        entities_df[\"text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n\
      \            lambda x, index_name=index_name: [i + f\"-{index_name}\" for i\
      \ in x]\n        )\n        max_vals[\"entities\"] = int(entities_df[\"human_readable_id\"\
      ].max())\n        entities_dfs.append(entities_df)\n\n    # Merge the dataframes\n\
      \    community_reports_combined = pd.concat(\n        community_reports_dfs,\
      \ axis=0, ignore_index=True, sort=False\n    )\n    entities_combined = pd.concat(entities_dfs,\
      \ axis=0, ignore_index=True, sort=False)\n    communities_combined = pd.concat(\n\
      \        communities_dfs, axis=0, ignore_index=True, sort=False\n    )\n\n \
      \   logger.debug(\"Executing multi-index global search query: %s\", query)\n\
      \    result = await global_search(\n        config,\n        entities=entities_combined,\n\
      \        communities=communities_combined,\n        community_reports=community_reports_combined,\n\
      \        community_level=community_level,\n        dynamic_community_selection=dynamic_community_selection,\n\
      \        response_type=response_type,\n        query=query,\n        callbacks=callbacks,\n\
      \    )\n\n    # Update the context data by linking index names and community\
      \ ids\n    context = update_context_data(result[1], links)\n\n    logger.debug(\"\
      Query response: %s\", truncate(result[0], 400))  # type: ignore\n    return\
      \ (result[0], context)"
    signature: "def multi_index_global_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int\
      \ | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
      \    streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises:
    - NotImplementedError
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.warning
      type: unresolved
    - target: NotImplementedError
      type: builtin
    - target: enumerate
      type: builtin
    - target: community_reports_df["community"].astype
      type: unresolved
    - target: str
      type: builtin
    - target: int
      type: builtin
    - target: community_reports_df["community"].max
      type: unresolved
    - target: community_reports_dfs.append
      type: unresolved
    - target: communities_df["community"].astype
      type: unresolved
    - target: communities_df["parent"].astype
      type: unresolved
    - target: communities_df["parent"].apply
      type: unresolved
    - target: communities_df["entity_ids"].apply
      type: unresolved
    - target: communities_df["community"].max
      type: unresolved
    - target: communities_dfs.append
      type: unresolved
    - target: entities_df["title"].apply
      type: unresolved
    - target: entities_df["text_unit_ids"].apply
      type: unresolved
    - target: entities_df["human_readable_id"].max
      type: unresolved
    - target: entities_dfs.append
      type: unresolved
    - target: pandas::concat
      type: external
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::global_search
      type: internal
    - target: graphrag/utils/api.py::update_context_data
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::multi_index_global_search
    called_by: []
  - name: local_search
    start_line: 342
    end_line: 406
    code: "async def local_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates: pd.DataFrame\
      \ | None,\n    community_level: int,\n    response_type: str,\n    query: str,\n\
      \    callbacks: list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n\
      ) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform a local search and return\
      \ the context data and response.\n\n    ----------\n    - config (GraphRagConfig):\
      \ A graphrag configuration (from settings.yaml)\n    - entities (pd.DataFrame):\
      \ A DataFrame containing the final entities (from entities.parquet)\n    - community_reports\
      \ (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n\
      \    - text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet)\n    - relationships (pd.DataFrame): A DataFrame\
      \ containing the final relationships (from relationships.parquet)\n    - covariates\
      \ (pd.DataFrame): A DataFrame containing the final covariates (from covariates.parquet)\n\
      \    - community_level (int): The community level to search at.\n    - response_type\
      \ (str): The response type to return.\n    - query (str): The user query to\
      \ search for.\n\n    Returns\n    -------\n    TODO: Document the search response\
      \ type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    callbacks = callbacks or []\n    full_response\
      \ = \"\"\n    context_data = {}\n\n    def on_context(context: Any) -> None:\n\
      \        nonlocal context_data\n        context_data = context\n\n    local_callbacks\
      \ = NoopQueryCallbacks()\n    local_callbacks.on_context = on_context\n    callbacks.append(local_callbacks)\n\
      \n    logger.debug(\"Executing local search query: %s\", query)\n    async for\
      \ chunk in local_search_streaming(\n        config=config,\n        entities=entities,\n\
      \        communities=communities,\n        community_reports=community_reports,\n\
      \        text_units=text_units,\n        relationships=relationships,\n    \
      \    covariates=covariates,\n        community_level=community_level,\n    \
      \    response_type=response_type,\n        query=query,\n        callbacks=callbacks,\n\
      \    ):\n        full_response += chunk\n    logger.debug(\"Query response:\
      \ %s\", truncate(full_response, 400))\n    return full_response, context_data"
    signature: "def local_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates: pd.DataFrame\
      \ | None,\n    community_level: int,\n    response_type: str,\n    query: str,\n\
      \    callbacks: list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n\
      ) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: callbacks.append
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::local_search_streaming
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::local_search
    called_by:
    - source: graphrag/api/query.py::multi_index_local_search
      type: internal
  - name: on_context
    start_line: 382
    end_line: 384
    code: "def on_context(context: Any) -> None:\n        nonlocal context_data\n\
      \        context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/api/query.py::on_context
    called_by: []
  - name: local_search_streaming
    start_line: 410
    end_line: 472
    code: "def local_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates:\
      \ pd.DataFrame | None,\n    community_level: int,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> AsyncGenerator:\n    \"\"\"Perform a local search and\
      \ return the context data and response via a generator.\n\n    Parameters\n\
      \    ----------\n    - config (GraphRagConfig): A graphrag configuration (from\
      \ settings.yaml)\n    - entities (pd.DataFrame): A DataFrame containing the\
      \ final entities (from entities.parquet)\n    - community_reports (pd.DataFrame):\
      \ A DataFrame containing the final community reports (from community_reports.parquet)\n\
      \    - text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet)\n    - relationships (pd.DataFrame): A DataFrame\
      \ containing the final relationships (from relationships.parquet)\n    - covariates\
      \ (pd.DataFrame): A DataFrame containing the final covariates (from covariates.parquet)\n\
      \    - community_level (int): The community level to search at.\n    - response_type\
      \ (str): The response type to return.\n    - query (str): The user query to\
      \ search for.\n\n    Returns\n    -------\n    TODO: Document the search response\
      \ type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    vector_store_args = {}\n    for index, store\
      \ in config.vector_store.items():\n        vector_store_args[index] = store.model_dump()\n\
      \    msg = f\"Vector Store Args: {redact(vector_store_args)}\"\n    logger.debug(msg)\n\
      \n    description_embedding_store = get_embedding_store(\n        config_args=vector_store_args,\n\
      \        embedding_name=entity_description_embedding,\n    )\n\n    entities_\
      \ = read_indexer_entities(entities, communities, community_level)\n    covariates_\
      \ = read_indexer_covariates(covariates) if covariates is not None else []\n\
      \    prompt = load_search_prompt(config.root_dir, config.local_search.prompt)\n\
      \n    logger.debug(\"Executing streaming local search query: %s\", query)\n\
      \    search_engine = get_local_search_engine(\n        config=config,\n    \
      \    reports=read_indexer_reports(community_reports, communities, community_level),\n\
      \        text_units=read_indexer_text_units(text_units),\n        entities=entities_,\n\
      \        relationships=read_indexer_relationships(relationships),\n        covariates={\"\
      claims\": covariates_},\n        description_embedding_store=description_embedding_store,\n\
      \        response_type=response_type,\n        system_prompt=prompt,\n     \
      \   callbacks=callbacks,\n    )\n    return search_engine.stream_search(query=query)"
    signature: "def local_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates:\
      \ pd.DataFrame | None,\n    community_level: int,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> AsyncGenerator"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: config.vector_store.items
      type: unresolved
    - target: store.model_dump
      type: unresolved
    - target: graphrag/utils/cli.py::redact
      type: internal
    - target: logger.debug
      type: unresolved
    - target: graphrag/utils/api.py::get_embedding_store
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_entities
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_covariates
      type: internal
    - target: graphrag/utils/api.py::load_search_prompt
      type: internal
    - target: graphrag/query/factory.py::get_local_search_engine
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_text_units
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_relationships
      type: internal
    - target: search_engine.stream_search
      type: unresolved
    visibility: public
    node_id: graphrag/api/query.py::local_search_streaming
    called_by:
    - source: graphrag/api/query.py::local_search
      type: internal
  - name: multi_index_local_search
    start_line: 476
    end_line: 704
    code: "async def multi_index_local_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    covariates_list: list[pd.DataFrame] | None,\n   \
      \ index_names: list[str],\n    community_level: int,\n    response_type: str,\n\
      \    streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]:\n    \"\"\"Perform a local search across multiple indexes and return the\
      \ context data and response.\n\n    Parameters\n    ----------\n    - config\
      \ (GraphRagConfig): A graphrag configuration (from settings.yaml)\n    - entities_list\
      \ (list[pd.DataFrame]): A list of DataFrames containing the final entities (from\
      \ entities.parquet)\n    - community_reports_list (list[pd.DataFrame]): A list\
      \ of DataFrames containing the final community reports (from community_reports.parquet)\n\
      \    - text_units_list (list[pd.DataFrame]): A list of DataFrames containing\
      \ the final text units (from text_units.parquet)\n    - relationships_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final relationships (from relationships.parquet)\n\
      \    - covariates_list (list[pd.DataFrame]): [Optional] A list of DataFrames\
      \ containing the final covariates (from covariates.parquet)\n    - index_names\
      \ (list[str]): A list of index names.\n    - community_level (int): The community\
      \ level to search at.\n    - response_type (str): The response type to return.\n\
      \    - streaming (bool): Whether to stream the results or not.\n    - query\
      \ (str): The user query to search for.\n\n    Returns\n    -------\n    TODO:\
      \ Document the search response type and format.\n    \"\"\"\n    init_loggers(config=config,\
      \ verbose=verbose, filename=\"query.log\")\n\n    logger.warning(\n        \"\
      Multi-index search is deprecated and will be removed in GraphRAG v3.\"\n   \
      \ )\n    # Streaming not supported yet\n    if streaming:\n        message =\
      \ \"Streaming not yet implemented for multi_index_local_search\"\n        raise\
      \ NotImplementedError(message)\n\n    links = {\n        \"community_reports\"\
      : {},\n        \"communities\": {},\n        \"entities\": {},\n        \"text_units\"\
      : {},\n        \"relationships\": {},\n        \"covariates\": {},\n    }\n\
      \    max_vals = {\n        \"community_reports\": -1,\n        \"communities\"\
      : -1,\n        \"entities\": -1,\n        \"text_units\": 0,\n        \"relationships\"\
      : -1,\n        \"covariates\": 0,\n    }\n    community_reports_dfs = []\n \
      \   communities_dfs = []\n    entities_dfs = []\n    relationships_dfs = []\n\
      \    text_units_dfs = []\n    covariates_dfs = []\n\n    for idx, index_name\
      \ in enumerate(index_names):\n        # Prepare each index's communities dataframe\
      \ for merging\n        communities_df = communities_list[idx]\n        communities_df[\"\
      community\"] = communities_df[\"community\"].astype(int)\n        for i in communities_df[\"\
      community\"]:\n            links[\"communities\"][i + max_vals[\"communities\"\
      ] + 1] = {\n                \"index_name\": index_name,\n                \"\
      id\": str(i),\n            }\n        communities_df[\"community\"] += max_vals[\"\
      communities\"] + 1\n        communities_df[\"human_readable_id\"] += max_vals[\"\
      communities\"] + 1\n        # concat the index name to the entity_ids, since\
      \ this is used for joining later\n        communities_df[\"entity_ids\"] = communities_df[\"\
      entity_ids\"].apply(\n            lambda x, index_name=index_name: [i + f\"\
      -{index_name}\" for i in x]\n        )\n        max_vals[\"communities\"] =\
      \ int(communities_df[\"community\"].max())\n        communities_dfs.append(communities_df)\n\
      \n        # Prepare each index's community reports dataframe for merging\n \
      \       community_reports_df = community_reports_list[idx]\n        community_reports_df[\"\
      community\"] = community_reports_df[\"community\"].astype(\n            int\n\
      \        )\n        for i in community_reports_df[\"community\"]:\n        \
      \    links[\"community_reports\"][i + max_vals[\"community_reports\"] + 1] =\
      \ {\n                \"index_name\": index_name,\n                \"id\": str(i),\n\
      \            }\n        community_reports_df[\"community\"] += max_vals[\"community_reports\"\
      ] + 1\n        community_reports_df[\"human_readable_id\"] += max_vals[\"community_reports\"\
      ] + 1\n        max_vals[\"community_reports\"] = int(community_reports_df[\"\
      community\"].max())\n        community_reports_dfs.append(community_reports_df)\n\
      \n        # Prepare each index's entities dataframe for merging\n        entities_df\
      \ = entities_list[idx]\n        for i in entities_df[\"human_readable_id\"]:\n\
      \            links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n     \
      \           \"index_name\": index_name,\n                \"id\": i,\n      \
      \      }\n        entities_df[\"human_readable_id\"] += max_vals[\"entities\"\
      ] + 1\n        entities_df[\"title\"] = entities_df[\"title\"].apply(\n    \
      \        lambda x, index_name=index_name: x + f\"-{index_name}\"\n        )\n\
      \        entities_df[\"id\"] = entities_df[\"id\"].apply(\n            lambda\
      \ x, index_name=index_name: x + f\"-{index_name}\"\n        )\n        entities_df[\"\
      text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n            lambda\
      \ x, index_name=index_name: [i + f\"-{index_name}\" for i in x]\n        )\n\
      \        max_vals[\"entities\"] = int(entities_df[\"human_readable_id\"].max())\n\
      \        entities_dfs.append(entities_df)\n\n        # Prepare each index's\
      \ relationships dataframe for merging\n        relationships_df = relationships_list[idx]\n\
      \        for i in relationships_df[\"human_readable_id\"].astype(int):\n   \
      \         links[\"relationships\"][i + max_vals[\"relationships\"] + 1] = {\n\
      \                \"index_name\": index_name,\n                \"id\": i,\n \
      \           }\n        if max_vals[\"relationships\"] != -1:\n            col\
      \ = (\n                relationships_df[\"human_readable_id\"].astype(int)\n\
      \                + max_vals[\"relationships\"]\n                + 1\n      \
      \      )\n            relationships_df[\"human_readable_id\"] = col.astype(str)\n\
      \        relationships_df[\"source\"] = relationships_df[\"source\"].apply(\n\
      \            lambda x, index_name=index_name: x + f\"-{index_name}\"\n     \
      \   )\n        relationships_df[\"target\"] = relationships_df[\"target\"].apply(\n\
      \            lambda x, index_name=index_name: x + f\"-{index_name}\"\n     \
      \   )\n        relationships_df[\"text_unit_ids\"] = relationships_df[\"text_unit_ids\"\
      ].apply(\n            lambda x, index_name=index_name: [i + f\"-{index_name}\"\
      \ for i in x]\n        )\n        max_vals[\"relationships\"] = int(relationships_df[\"\
      human_readable_id\"].max())\n        relationships_dfs.append(relationships_df)\n\
      \n        # Prepare each index's text units dataframe for merging\n        text_units_df\
      \ = text_units_list[idx]\n        for i in range(text_units_df.shape[0]):\n\
      \            links[\"text_units\"][i + max_vals[\"text_units\"]] = {\n     \
      \           \"index_name\": index_name,\n                \"id\": i,\n      \
      \      }\n        text_units_df[\"id\"] = text_units_df[\"id\"].apply(\n   \
      \         lambda x, index_name=index_name: f\"{x}-{index_name}\"\n        )\n\
      \        text_units_df[\"human_readable_id\"] = (\n            text_units_df[\"\
      human_readable_id\"] + max_vals[\"text_units\"]\n        )\n        max_vals[\"\
      text_units\"] += text_units_df.shape[0]\n        text_units_dfs.append(text_units_df)\n\
      \n        # If presents, prepare each index's covariates dataframe for merging\n\
      \        if covariates_list is not None:\n            covariates_df = covariates_list[idx]\n\
      \            for i in covariates_df[\"human_readable_id\"].astype(int):\n  \
      \              links[\"covariates\"][i + max_vals[\"covariates\"]] = {\n   \
      \                 \"index_name\": index_name,\n                    \"id\": i,\n\
      \                }\n            covariates_df[\"id\"] = covariates_df[\"id\"\
      ].apply(\n                lambda x, index_name=index_name: f\"{x}-{index_name}\"\
      \n            )\n            covariates_df[\"human_readable_id\"] = (\n    \
      \            covariates_df[\"human_readable_id\"] + max_vals[\"covariates\"\
      ]\n            )\n            covariates_df[\"text_unit_id\"] = covariates_df[\"\
      text_unit_id\"].apply(\n                lambda x, index_name=index_name: x +\
      \ f\"-{index_name}\"\n            )\n            covariates_df[\"subject_id\"\
      ] = covariates_df[\"subject_id\"].apply(\n                lambda x, index_name=index_name:\
      \ x + f\"-{index_name}\"\n            )\n            max_vals[\"covariates\"\
      ] += covariates_df.shape[0]\n            covariates_dfs.append(covariates_df)\n\
      \n    # Merge the dataframes\n    communities_combined = pd.concat(\n      \
      \  communities_dfs, axis=0, ignore_index=True, sort=False\n    )\n    community_reports_combined\
      \ = pd.concat(\n        community_reports_dfs, axis=0, ignore_index=True, sort=False\n\
      \    )\n    entities_combined = pd.concat(entities_dfs, axis=0, ignore_index=True,\
      \ sort=False)\n    relationships_combined = pd.concat(\n        relationships_dfs,\
      \ axis=0, ignore_index=True, sort=False\n    )\n    text_units_combined = pd.concat(\n\
      \        text_units_dfs, axis=0, ignore_index=True, sort=False\n    )\n    covariates_combined\
      \ = None\n    if len(covariates_dfs) > 0:\n        covariates_combined = pd.concat(\n\
      \            covariates_dfs, axis=0, ignore_index=True, sort=False\n       \
      \ )\n    logger.debug(\"Executing multi-index local search query: %s\", query)\n\
      \    result = await local_search(\n        config,\n        entities=entities_combined,\n\
      \        communities=communities_combined,\n        community_reports=community_reports_combined,\n\
      \        text_units=text_units_combined,\n        relationships=relationships_combined,\n\
      \        covariates=covariates_combined,\n        community_level=community_level,\n\
      \        response_type=response_type,\n        query=query,\n        callbacks=callbacks,\n\
      \    )\n\n    # Update the context data by linking index names and community\
      \ ids\n    context = update_context_data(result[1], links)\n\n    logger.debug(\"\
      Query response: %s\", truncate(result[0], 400))  # type: ignore\n    return\
      \ (result[0], context)"
    signature: "def multi_index_local_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    covariates_list: list[pd.DataFrame] | None,\n   \
      \ index_names: list[str],\n    community_level: int,\n    response_type: str,\n\
      \    streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises:
    - NotImplementedError
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.warning
      type: unresolved
    - target: NotImplementedError
      type: builtin
    - target: enumerate
      type: builtin
    - target: communities_df["community"].astype
      type: unresolved
    - target: str
      type: builtin
    - target: communities_df["entity_ids"].apply
      type: unresolved
    - target: int
      type: builtin
    - target: communities_df["community"].max
      type: unresolved
    - target: communities_dfs.append
      type: unresolved
    - target: community_reports_df["community"].astype
      type: unresolved
    - target: community_reports_df["community"].max
      type: unresolved
    - target: community_reports_dfs.append
      type: unresolved
    - target: entities_df["title"].apply
      type: unresolved
    - target: entities_df["id"].apply
      type: unresolved
    - target: entities_df["text_unit_ids"].apply
      type: unresolved
    - target: entities_df["human_readable_id"].max
      type: unresolved
    - target: entities_dfs.append
      type: unresolved
    - target: relationships_df["human_readable_id"].astype
      type: unresolved
    - target: col.astype
      type: unresolved
    - target: relationships_df["source"].apply
      type: unresolved
    - target: relationships_df["target"].apply
      type: unresolved
    - target: relationships_df["text_unit_ids"].apply
      type: unresolved
    - target: relationships_df["human_readable_id"].max
      type: unresolved
    - target: relationships_dfs.append
      type: unresolved
    - target: range
      type: builtin
    - target: text_units_df["id"].apply
      type: unresolved
    - target: text_units_dfs.append
      type: unresolved
    - target: covariates_df["human_readable_id"].astype
      type: unresolved
    - target: covariates_df["id"].apply
      type: unresolved
    - target: covariates_df["text_unit_id"].apply
      type: unresolved
    - target: covariates_df["subject_id"].apply
      type: unresolved
    - target: covariates_dfs.append
      type: unresolved
    - target: pandas::concat
      type: external
    - target: len
      type: builtin
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::local_search
      type: internal
    - target: graphrag/utils/api.py::update_context_data
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::multi_index_local_search
    called_by: []
  - name: drift_search
    start_line: 708
    end_line: 769
    code: "async def drift_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level: int,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]:\n    \"\"\"Perform a DRIFT search and return the context data and response.\n\
      \n    Parameters\n    ----------\n    - config (GraphRagConfig): A graphrag\
      \ configuration (from settings.yaml)\n    - entities (pd.DataFrame): A DataFrame\
      \ containing the final entities (from entities.parquet)\n    - community_reports\
      \ (pd.DataFrame): A DataFrame containing the final community reports (from community_reports.parquet)\n\
      \    - text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet)\n    - relationships (pd.DataFrame): A DataFrame\
      \ containing the final relationships (from relationships.parquet)\n    - community_level\
      \ (int): The community level to search at.\n    - query (str): The user query\
      \ to search for.\n\n    Returns\n    -------\n    TODO: Document the search\
      \ response type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    callbacks = callbacks or []\n    full_response\
      \ = \"\"\n    context_data = {}\n\n    def on_context(context: Any) -> None:\n\
      \        nonlocal context_data\n        context_data = context\n\n    local_callbacks\
      \ = NoopQueryCallbacks()\n    local_callbacks.on_context = on_context\n    callbacks.append(local_callbacks)\n\
      \n    logger.debug(\"Executing drift search query: %s\", query)\n    async for\
      \ chunk in drift_search_streaming(\n        config=config,\n        entities=entities,\n\
      \        communities=communities,\n        community_reports=community_reports,\n\
      \        text_units=text_units,\n        relationships=relationships,\n    \
      \    community_level=community_level,\n        response_type=response_type,\n\
      \        query=query,\n        callbacks=callbacks,\n    ):\n        full_response\
      \ += chunk\n    logger.debug(\"Query response: %s\", truncate(full_response,\
      \ 400))\n    return full_response, context_data"
    signature: "def drift_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level: int,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: callbacks.append
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::drift_search_streaming
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::drift_search
    called_by:
    - source: graphrag/api/query.py::multi_index_drift_search
      type: internal
  - name: on_context
    start_line: 746
    end_line: 748
    code: "def on_context(context: Any) -> None:\n        nonlocal context_data\n\
      \        context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/api/query.py::on_context
    called_by: []
  - name: drift_search_streaming
    start_line: 773
    end_line: 841
    code: "def drift_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level:\
      \ int,\n    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator:\n    \"\"\
      \"Perform a DRIFT search and return the context data and response.\n\n    Parameters\n\
      \    ----------\n    - config (GraphRagConfig): A graphrag configuration (from\
      \ settings.yaml)\n    - entities (pd.DataFrame): A DataFrame containing the\
      \ final entities (from entities.parquet)\n    - community_reports (pd.DataFrame):\
      \ A DataFrame containing the final community reports (from community_reports.parquet)\n\
      \    - text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet)\n    - relationships (pd.DataFrame): A DataFrame\
      \ containing the final relationships (from relationships.parquet)\n    - community_level\
      \ (int): The community level to search at.\n    - query (str): The user query\
      \ to search for.\n\n    Returns\n    -------\n    TODO: Document the search\
      \ response type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    vector_store_args = {}\n    for index, store\
      \ in config.vector_store.items():\n        vector_store_args[index] = store.model_dump()\n\
      \    msg = f\"Vector Store Args: {redact(vector_store_args)}\"\n    logger.debug(msg)\n\
      \n    description_embedding_store = get_embedding_store(\n        config_args=vector_store_args,\n\
      \        embedding_name=entity_description_embedding,\n    )\n\n    full_content_embedding_store\
      \ = get_embedding_store(\n        config_args=vector_store_args,\n        embedding_name=community_full_content_embedding,\n\
      \    )\n\n    entities_ = read_indexer_entities(entities, communities, community_level)\n\
      \    reports = read_indexer_reports(community_reports, communities, community_level)\n\
      \    read_indexer_report_embeddings(reports, full_content_embedding_store)\n\
      \    prompt = load_search_prompt(config.root_dir, config.drift_search.prompt)\n\
      \    reduce_prompt = load_search_prompt(\n        config.root_dir, config.drift_search.reduce_prompt\n\
      \    )\n\n    logger.debug(\"Executing streaming drift search query: %s\", query)\n\
      \    search_engine = get_drift_search_engine(\n        config=config,\n    \
      \    reports=reports,\n        text_units=read_indexer_text_units(text_units),\n\
      \        entities=entities_,\n        relationships=read_indexer_relationships(relationships),\n\
      \        description_embedding_store=description_embedding_store,\n        local_system_prompt=prompt,\n\
      \        reduce_system_prompt=reduce_prompt,\n        response_type=response_type,\n\
      \        callbacks=callbacks,\n    )\n    return search_engine.stream_search(query=query)"
    signature: "def drift_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level:\
      \ int,\n    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: config.vector_store.items
      type: unresolved
    - target: store.model_dump
      type: unresolved
    - target: graphrag/utils/cli.py::redact
      type: internal
    - target: logger.debug
      type: unresolved
    - target: graphrag/utils/api.py::get_embedding_store
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_entities
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_report_embeddings
      type: internal
    - target: graphrag/utils/api.py::load_search_prompt
      type: internal
    - target: graphrag/query/factory.py::get_drift_search_engine
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_text_units
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_relationships
      type: internal
    - target: search_engine.stream_search
      type: unresolved
    visibility: public
    node_id: graphrag/api/query.py::drift_search_streaming
    called_by:
    - source: graphrag/api/query.py::drift_search
      type: internal
  - name: multi_index_drift_search
    start_line: 845
    end_line: 1053
    code: "async def multi_index_drift_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int,\n\
      \    response_type: str,\n    streaming: bool,\n    query: str,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n) -> tuple[\n\
      \    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform a DRIFT search across multiple\
      \ indexes and return the context data and response.\n\n    Parameters\n    ----------\n\
      \    - config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
      \    - entities_list (list[pd.DataFrame]): A list of DataFrames containing the\
      \ final entities (from entities.parquet)\n    - community_reports_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final community reports (from community_reports.parquet)\n\
      \    - text_units_list (list[pd.DataFrame]): A list of DataFrames containing\
      \ the final text units (from text_units.parquet)\n    - relationships_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final relationships (from relationships.parquet)\n\
      \    - index_names (list[str]): A list of index names.\n    - community_level\
      \ (int): The community level to search at.\n    - response_type (str): The response\
      \ type to return.\n    - streaming (bool): Whether to stream the results or\
      \ not.\n    - query (str): The user query to search for.\n\n    Returns\n  \
      \  -------\n    TODO: Document the search response type and format.\n    \"\"\
      \"\n    init_loggers(config=config, verbose=verbose, filename=\"query.log\"\
      )\n\n    logger.warning(\n        \"Multi-index search is deprecated and will\
      \ be removed in GraphRAG v3.\"\n    )\n\n    # Streaming not supported yet\n\
      \    if streaming:\n        message = \"Streaming not yet implemented for multi_drift_search\"\
      \n        raise NotImplementedError(message)\n\n    links = {\n        \"community_reports\"\
      : {},\n        \"communities\": {},\n        \"entities\": {},\n        \"text_units\"\
      : {},\n        \"relationships\": {},\n    }\n    max_vals = {\n        \"community_reports\"\
      : -1,\n        \"communities\": -1,\n        \"entities\": -1,\n        \"text_units\"\
      : 0,\n        \"relationships\": -1,\n    }\n\n    communities_dfs = []\n  \
      \  community_reports_dfs = []\n    entities_dfs = []\n    relationships_dfs\
      \ = []\n    text_units_dfs = []\n\n    for idx, index_name in enumerate(index_names):\n\
      \        # Prepare each index's communities dataframe for merging\n        communities_df\
      \ = communities_list[idx]\n        communities_df[\"community\"] = communities_df[\"\
      community\"].astype(int)\n        for i in communities_df[\"community\"]:\n\
      \            links[\"communities\"][i + max_vals[\"communities\"] + 1] = {\n\
      \                \"index_name\": index_name,\n                \"id\": str(i),\n\
      \            }\n        communities_df[\"community\"] += max_vals[\"communities\"\
      ] + 1\n        communities_df[\"human_readable_id\"] += max_vals[\"communities\"\
      ] + 1\n        # concat the index name to the entity_ids, since this is used\
      \ for joining later\n        communities_df[\"entity_ids\"] = communities_df[\"\
      entity_ids\"].apply(\n            lambda x, index_name=index_name: [i + f\"\
      -{index_name}\" for i in x]\n        )\n        max_vals[\"communities\"] =\
      \ int(communities_df[\"community\"].max())\n        communities_dfs.append(communities_df)\n\
      \n        # Prepare each index's community reports dataframe for merging\n \
      \       community_reports_df = community_reports_list[idx]\n        community_reports_df[\"\
      community\"] = community_reports_df[\"community\"].astype(\n            int\n\
      \        )\n        for i in community_reports_df[\"community\"]:\n        \
      \    links[\"community_reports\"][i + max_vals[\"community_reports\"] + 1] =\
      \ {\n                \"index_name\": index_name,\n                \"id\": str(i),\n\
      \            }\n        community_reports_df[\"community\"] += max_vals[\"community_reports\"\
      ] + 1\n        community_reports_df[\"human_readable_id\"] += max_vals[\"community_reports\"\
      ] + 1\n        community_reports_df[\"id\"] = community_reports_df[\"id\"].apply(\n\
      \            lambda x, index_name=index_name: x + f\"-{index_name}\"\n     \
      \   )\n        max_vals[\"community_reports\"] = int(community_reports_df[\"\
      community\"].max())\n        community_reports_dfs.append(community_reports_df)\n\
      \n        # Prepare each index's entities dataframe for merging\n        entities_df\
      \ = entities_list[idx]\n        for i in entities_df[\"human_readable_id\"]:\n\
      \            links[\"entities\"][i + max_vals[\"entities\"] + 1] = {\n     \
      \           \"index_name\": index_name,\n                \"id\": i,\n      \
      \      }\n        entities_df[\"human_readable_id\"] += max_vals[\"entities\"\
      ] + 1\n        entities_df[\"title\"] = entities_df[\"title\"].apply(\n    \
      \        lambda x, index_name=index_name: x + f\"-{index_name}\"\n        )\n\
      \        entities_df[\"id\"] = entities_df[\"id\"].apply(\n            lambda\
      \ x, index_name=index_name: x + f\"-{index_name}\"\n        )\n        entities_df[\"\
      text_unit_ids\"] = entities_df[\"text_unit_ids\"].apply(\n            lambda\
      \ x, index_name=index_name: [i + f\"-{index_name}\" for i in x]\n        )\n\
      \        max_vals[\"entities\"] = int(entities_df[\"human_readable_id\"].max())\n\
      \        entities_dfs.append(entities_df)\n\n        # Prepare each index's\
      \ relationships dataframe for merging\n        relationships_df = relationships_list[idx]\n\
      \        for i in relationships_df[\"human_readable_id\"].astype(int):\n   \
      \         links[\"relationships\"][i + max_vals[\"relationships\"] + 1] = {\n\
      \                \"index_name\": index_name,\n                \"id\": i,\n \
      \           }\n        if max_vals[\"relationships\"] != -1:\n            col\
      \ = (\n                relationships_df[\"human_readable_id\"].astype(int)\n\
      \                + max_vals[\"relationships\"]\n                + 1\n      \
      \      )\n            relationships_df[\"human_readable_id\"] = col.astype(str)\n\
      \        relationships_df[\"source\"] = relationships_df[\"source\"].apply(\n\
      \            lambda x, index_name=index_name: x + f\"-{index_name}\"\n     \
      \   )\n        relationships_df[\"target\"] = relationships_df[\"target\"].apply(\n\
      \            lambda x, index_name=index_name: x + f\"-{index_name}\"\n     \
      \   )\n        relationships_df[\"text_unit_ids\"] = relationships_df[\"text_unit_ids\"\
      ].apply(\n            lambda x, index_name=index_name: [i + f\"-{index_name}\"\
      \ for i in x]\n        )\n        max_vals[\"relationships\"] = int(\n     \
      \       relationships_df[\"human_readable_id\"].astype(int).max()\n        )\n\
      \n        relationships_dfs.append(relationships_df)\n\n        # Prepare each\
      \ index's text units dataframe for merging\n        text_units_df = text_units_list[idx]\n\
      \        for i in range(text_units_df.shape[0]):\n            links[\"text_units\"\
      ][i + max_vals[\"text_units\"]] = {\n                \"index_name\": index_name,\n\
      \                \"id\": i,\n            }\n        text_units_df[\"id\"] =\
      \ text_units_df[\"id\"].apply(\n            lambda x, index_name=index_name:\
      \ f\"{x}-{index_name}\"\n        )\n        text_units_df[\"human_readable_id\"\
      ] = (\n            text_units_df[\"human_readable_id\"] + max_vals[\"text_units\"\
      ]\n        )\n        max_vals[\"text_units\"] += text_units_df.shape[0]\n \
      \       text_units_dfs.append(text_units_df)\n\n    # Merge the dataframes\n\
      \    communities_combined = pd.concat(\n        communities_dfs, axis=0, ignore_index=True,\
      \ sort=False\n    )\n    community_reports_combined = pd.concat(\n        community_reports_dfs,\
      \ axis=0, ignore_index=True, sort=False\n    )\n    entities_combined = pd.concat(entities_dfs,\
      \ axis=0, ignore_index=True, sort=False)\n    relationships_combined = pd.concat(\n\
      \        relationships_dfs, axis=0, ignore_index=True, sort=False\n    )\n \
      \   text_units_combined = pd.concat(\n        text_units_dfs, axis=0, ignore_index=True,\
      \ sort=False\n    )\n\n    logger.debug(\"Executing multi-index drift search\
      \ query: %s\", query)\n    result = await drift_search(\n        config,\n \
      \       entities=entities_combined,\n        communities=communities_combined,\n\
      \        community_reports=community_reports_combined,\n        text_units=text_units_combined,\n\
      \        relationships=relationships_combined,\n        community_level=community_level,\n\
      \        response_type=response_type,\n        query=query,\n        callbacks=callbacks,\n\
      \    )\n\n    # Update the context data by linking index names and community\
      \ ids\n    context = {}\n    if type(result[1]) is dict:\n        for key in\
      \ result[1]:\n            context[key] = update_context_data(result[1][key],\
      \ links)\n    else:\n        context = result[1]\n\n    logger.debug(\"Query\
      \ response: %s\", truncate(result[0], 400))  # type: ignore\n    return (result[0],\
      \ context)"
    signature: "def multi_index_drift_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int,\n\
      \    response_type: str,\n    streaming: bool,\n    query: str,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n) -> tuple[\n\
      \    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises:
    - NotImplementedError
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.warning
      type: unresolved
    - target: NotImplementedError
      type: builtin
    - target: enumerate
      type: builtin
    - target: communities_df["community"].astype
      type: unresolved
    - target: str
      type: builtin
    - target: communities_df["entity_ids"].apply
      type: unresolved
    - target: int
      type: builtin
    - target: communities_df["community"].max
      type: unresolved
    - target: communities_dfs.append
      type: unresolved
    - target: community_reports_df["community"].astype
      type: unresolved
    - target: community_reports_df["id"].apply
      type: unresolved
    - target: community_reports_df["community"].max
      type: unresolved
    - target: community_reports_dfs.append
      type: unresolved
    - target: entities_df["title"].apply
      type: unresolved
    - target: entities_df["id"].apply
      type: unresolved
    - target: entities_df["text_unit_ids"].apply
      type: unresolved
    - target: entities_df["human_readable_id"].max
      type: unresolved
    - target: entities_dfs.append
      type: unresolved
    - target: relationships_df["human_readable_id"].astype
      type: unresolved
    - target: col.astype
      type: unresolved
    - target: relationships_df["source"].apply
      type: unresolved
    - target: relationships_df["target"].apply
      type: unresolved
    - target: relationships_df["text_unit_ids"].apply
      type: unresolved
    - target: relationships_df["human_readable_id"].astype(int).max
      type: unresolved
    - target: relationships_dfs.append
      type: unresolved
    - target: range
      type: builtin
    - target: text_units_df["id"].apply
      type: unresolved
    - target: text_units_dfs.append
      type: unresolved
    - target: pandas::concat
      type: external
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::drift_search
      type: internal
    - target: type
      type: builtin
    - target: graphrag/utils/api.py::update_context_data
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::multi_index_drift_search
    called_by: []
  - name: basic_search
    start_line: 1057
    end_line: 1102
    code: "async def basic_search(\n    config: GraphRagConfig,\n    text_units: pd.DataFrame,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform\
      \ a basic search and return the context data and response.\n\n    Parameters\n\
      \    ----------\n    - config (GraphRagConfig): A graphrag configuration (from\
      \ settings.yaml)\n    - text_units (pd.DataFrame): A DataFrame containing the\
      \ final text units (from text_units.parquet)\n    - query (str): The user query\
      \ to search for.\n\n    Returns\n    -------\n    TODO: Document the search\
      \ response type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    callbacks = callbacks or []\n    full_response\
      \ = \"\"\n    context_data = {}\n\n    def on_context(context: Any) -> None:\n\
      \        nonlocal context_data\n        context_data = context\n\n    local_callbacks\
      \ = NoopQueryCallbacks()\n    local_callbacks.on_context = on_context\n    callbacks.append(local_callbacks)\n\
      \n    logger.debug(\"Executing basic search query: %s\", query)\n    async for\
      \ chunk in basic_search_streaming(\n        config=config,\n        text_units=text_units,\n\
      \        query=query,\n        callbacks=callbacks,\n    ):\n        full_response\
      \ += chunk\n    logger.debug(\"Query response: %s\", truncate(full_response,\
      \ 400))\n    return full_response, context_data"
    signature: "def basic_search(\n    config: GraphRagConfig,\n    text_units: pd.DataFrame,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: callbacks.append
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::basic_search_streaming
      type: internal
    - target: graphrag/utils/api.py::truncate
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::basic_search
    called_by:
    - source: graphrag/api/query.py::multi_index_basic_search
      type: internal
  - name: on_context
    start_line: 1085
    end_line: 1087
    code: "def on_context(context: Any) -> None:\n        nonlocal context_data\n\
      \        context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/api/query.py::on_context
    called_by: []
  - name: basic_search_streaming
    start_line: 1106
    end_line: 1148
    code: "def basic_search_streaming(\n    config: GraphRagConfig,\n    text_units:\
      \ pd.DataFrame,\n    query: str,\n    callbacks: list[QueryCallbacks] | None\
      \ = None,\n    verbose: bool = False,\n) -> AsyncGenerator:\n    \"\"\"Perform\
      \ a local search and return the context data and response via a generator.\n\
      \n    Parameters\n    ----------\n    - config (GraphRagConfig): A graphrag\
      \ configuration (from settings.yaml)\n    - text_units (pd.DataFrame): A DataFrame\
      \ containing the final text units (from text_units.parquet)\n    - query (str):\
      \ The user query to search for.\n\n    Returns\n    -------\n    TODO: Document\
      \ the search response type and format.\n    \"\"\"\n    init_loggers(config=config,\
      \ verbose=verbose, filename=\"query.log\")\n\n    vector_store_args = {}\n \
      \   for index, store in config.vector_store.items():\n        vector_store_args[index]\
      \ = store.model_dump()\n    msg = f\"Vector Store Args: {redact(vector_store_args)}\"\
      \n    logger.debug(msg)\n\n    embedding_store = get_embedding_store(\n    \
      \    config_args=vector_store_args,\n        embedding_name=text_unit_text_embedding,\n\
      \    )\n\n    prompt = load_search_prompt(config.root_dir, config.basic_search.prompt)\n\
      \n    logger.debug(\"Executing streaming basic search query: %s\", query)\n\
      \    search_engine = get_basic_search_engine(\n        config=config,\n    \
      \    text_units=read_indexer_text_units(text_units),\n        text_unit_embeddings=embedding_store,\n\
      \        system_prompt=prompt,\n        callbacks=callbacks,\n    )\n    return\
      \ search_engine.stream_search(query=query)"
    signature: "def basic_search_streaming(\n    config: GraphRagConfig,\n    text_units:\
      \ pd.DataFrame,\n    query: str,\n    callbacks: list[QueryCallbacks] | None\
      \ = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: config.vector_store.items
      type: unresolved
    - target: store.model_dump
      type: unresolved
    - target: graphrag/utils/cli.py::redact
      type: internal
    - target: logger.debug
      type: unresolved
    - target: graphrag/utils/api.py::get_embedding_store
      type: internal
    - target: graphrag/utils/api.py::load_search_prompt
      type: internal
    - target: graphrag/query/factory.py::get_basic_search_engine
      type: internal
    - target: graphrag/query/indexer_adapters.py::read_indexer_text_units
      type: internal
    - target: search_engine.stream_search
      type: unresolved
    visibility: public
    node_id: graphrag/api/query.py::basic_search_streaming
    called_by:
    - source: graphrag/api/query.py::basic_search
      type: internal
  - name: multi_index_basic_search
    start_line: 1152
    end_line: 1226
    code: "async def multi_index_basic_search(\n    config: GraphRagConfig,\n    text_units_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    streaming: bool,\n \
      \   query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]:\n    \"\"\"Perform\
      \ a basic search across multiple indexes and return the context data and response.\n\
      \n    Parameters\n    ----------\n    - config (GraphRagConfig): A graphrag\
      \ configuration (from settings.yaml)\n    - text_units_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final text units (from text_units.parquet)\n\
      \    - index_names (list[str]): A list of index names.\n    - streaming (bool):\
      \ Whether to stream the results or not.\n    - query (str): The user query to\
      \ search for.\n\n    Returns\n    -------\n    TODO: Document the search response\
      \ type and format.\n    \"\"\"\n    init_loggers(config=config, verbose=verbose,\
      \ filename=\"query.log\")\n\n    logger.warning(\n        \"Multi-index search\
      \ is deprecated and will be removed in GraphRAG v3.\"\n    )\n\n    # Streaming\
      \ not supported yet\n    if streaming:\n        message = \"Streaming not yet\
      \ implemented for multi_basic_search\"\n        raise NotImplementedError(message)\n\
      \n    links = {\n        \"text_units\": {},\n    }\n    max_vals = {\n    \
      \    \"text_units\": 0,\n    }\n\n    text_units_dfs = []\n\n    for idx, index_name\
      \ in enumerate(index_names):\n        # Prepare each index's text units dataframe\
      \ for merging\n        text_units_df = text_units_list[idx]\n        for i in\
      \ range(text_units_df.shape[0]):\n            links[\"text_units\"][i + max_vals[\"\
      text_units\"]] = {\n                \"index_name\": index_name,\n          \
      \      \"id\": i,\n            }\n        text_units_df[\"id\"] = text_units_df[\"\
      id\"].apply(\n            lambda x, index_name=index_name: f\"{x}-{index_name}\"\
      \n        )\n        text_units_df[\"human_readable_id\"] = (\n            text_units_df[\"\
      human_readable_id\"] + max_vals[\"text_units\"]\n        )\n        max_vals[\"\
      text_units\"] += text_units_df.shape[0]\n        text_units_dfs.append(text_units_df)\n\
      \n    # Merge the dataframes\n    text_units_combined = pd.concat(\n       \
      \ text_units_dfs, axis=0, ignore_index=True, sort=False\n    )\n\n    logger.debug(\"\
      Executing multi-index basic search query: %s\", query)\n    return await basic_search(\n\
      \        config,\n        text_units=text_units_combined,\n        query=query,\n\
      \        callbacks=callbacks,\n    )"
    signature: "def multi_index_basic_search(\n    config: GraphRagConfig,\n    text_units_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    streaming: bool,\n \
      \   query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    decorators:
    - '@validate_call(config={"arbitrary_types_allowed": True})'
    raises:
    - NotImplementedError
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.warning
      type: unresolved
    - target: NotImplementedError
      type: builtin
    - target: enumerate
      type: builtin
    - target: range
      type: builtin
    - target: text_units_df["id"].apply
      type: unresolved
    - target: text_units_dfs.append
      type: unresolved
    - target: pandas::concat
      type: external
    - target: logger.debug
      type: unresolved
    - target: graphrag/api/query.py::basic_search
      type: internal
    visibility: public
    node_id: graphrag/api/query.py::multi_index_basic_search
    called_by: []
- file_name: graphrag/callbacks/__init__.py
  imports: []
  functions: []
- file_name: graphrag/callbacks/console_workflow_callbacks.py
  imports:
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.logger.progress
    name: Progress
    alias: null
  functions:
  - name: __init__
    start_line: 18
    end_line: 19
    code: "def __init__(self, verbose=False):\n        self._verbose = verbose"
    signature: def __init__(self, verbose=False)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.__init__
    called_by: []
  - name: pipeline_start
    start_line: 21
    end_line: 23
    code: "def pipeline_start(self, names: list[str]) -> None:\n        \"\"\"Execute\
      \ this callback to signal when the entire pipeline starts.\"\"\"\n        print(\"\
      Starting pipeline with workflows:\", \", \".join(names))"
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: '", ".join'
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_start
    called_by: []
  - name: pipeline_end
    start_line: 25
    end_line: 27
    code: "def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n   \
      \     \"\"\"Execute this callback to signal when the entire pipeline ends.\"\
      \"\"\n        print(\"Pipeline complete\")"
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    visibility: public
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_end
    called_by: []
  - name: workflow_start
    start_line: 29
    end_line: 31
    code: "def workflow_start(self, name: str, instance: object) -> None:\n      \
      \  \"\"\"Execute this callback when a workflow starts.\"\"\"\n        print(f\"\
      Starting workflow: {name}\")"
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    visibility: public
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_start
    called_by: []
  - name: workflow_end
    start_line: 33
    end_line: 38
    code: "def workflow_end(self, name: str, instance: object) -> None:\n        \"\
      \"\"Execute this callback when a workflow ends.\"\"\"\n        print(\"\") \
      \ # account for potential return on prior progress\n        print(f\"Workflow\
      \ complete: {name}\")\n        if self._verbose:\n            print(instance)"
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    visibility: public
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_end
    called_by: []
  - name: progress
    start_line: 40
    end_line: 46
    code: "def progress(self, progress: Progress) -> None:\n        \"\"\"Handle when\
      \ progress occurs.\"\"\"\n        complete = progress.completed_items or 0\n\
      \        total = progress.total_items or 1\n        percent = round((complete\
      \ / total) * 100)\n        start = f\"  {complete} / {total} \"\n        print(f\"\
      {start:{'.'}<{percent}}\", flush=True, end=\"\\r\")"
    signature: 'def progress(self, progress: Progress) -> None'
    decorators: []
    raises: []
    calls:
    - target: round
      type: builtin
    - target: print
      type: builtin
    visibility: public
    node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.progress
    called_by: []
- file_name: graphrag/callbacks/llm_callbacks.py
  imports:
  - module: typing
    name: Protocol
    alias: null
  functions:
  - name: on_llm_new_token
    start_line: 12
    end_line: 14
    code: "def on_llm_new_token(self, token: str):\n        \"\"\"Handle when a new\
      \ token is generated.\"\"\"\n        ..."
    signature: 'def on_llm_new_token(self, token: str)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback.on_llm_new_token
    called_by: []
- file_name: graphrag/callbacks/noop_query_callbacks.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  functions:
  - name: on_context
    start_line: 15
    end_line: 16
    code: "def on_context(self, context: Any) -> None:\n        \"\"\"Handle when\
      \ context data is constructed.\"\"\""
    signature: 'def on_context(self, context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_context
    called_by: []
  - name: on_map_response_start
    start_line: 18
    end_line: 19
    code: "def on_map_response_start(self, map_response_contexts: list[str]) -> None:\n\
      \        \"\"\"Handle the start of map operation.\"\"\""
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_start
    called_by: []
  - name: on_map_response_end
    start_line: 21
    end_line: 22
    code: "def on_map_response_end(self, map_response_outputs: list[SearchResult])\
      \ -> None:\n        \"\"\"Handle the end of map operation.\"\"\""
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_end
    called_by: []
  - name: on_reduce_response_start
    start_line: 24
    end_line: 27
    code: "def on_reduce_response_start(\n        self, reduce_response_context: str\
      \ | dict[str, Any]\n    ) -> None:\n        \"\"\"Handle the start of reduce\
      \ operation.\"\"\""
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_start
    called_by: []
  - name: on_reduce_response_end
    start_line: 29
    end_line: 30
    code: "def on_reduce_response_end(self, reduce_response_output: str) -> None:\n\
      \        \"\"\"Handle the end of reduce operation.\"\"\""
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_end
    called_by: []
  - name: on_llm_new_token
    start_line: 32
    end_line: 33
    code: "def on_llm_new_token(self, token):\n        \"\"\"Handle when a new token\
      \ is generated.\"\"\""
    signature: def on_llm_new_token(self, token)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_llm_new_token
    called_by: []
- file_name: graphrag/callbacks/noop_workflow_callbacks.py
  imports:
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.logger.progress
    name: Progress
    alias: null
  functions:
  - name: pipeline_start
    start_line: 14
    end_line: 15
    code: "def pipeline_start(self, names: list[str]) -> None:\n        \"\"\"Execute\
      \ this callback to signal when the entire pipeline starts.\"\"\""
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_start
    called_by: []
  - name: pipeline_end
    start_line: 17
    end_line: 18
    code: "def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n   \
      \     \"\"\"Execute this callback to signal when the entire pipeline ends.\"\
      \"\""
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_end
    called_by: []
  - name: workflow_start
    start_line: 20
    end_line: 21
    code: "def workflow_start(self, name: str, instance: object) -> None:\n      \
      \  \"\"\"Execute this callback when a workflow starts.\"\"\""
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_start
    called_by: []
  - name: workflow_end
    start_line: 23
    end_line: 24
    code: "def workflow_end(self, name: str, instance: object) -> None:\n        \"\
      \"\"Execute this callback when a workflow ends.\"\"\""
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_end
    called_by: []
  - name: progress
    start_line: 26
    end_line: 27
    code: "def progress(self, progress: Progress) -> None:\n        \"\"\"Handle when\
      \ progress occurs.\"\"\""
    signature: 'def progress(self, progress: Progress) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.progress
    called_by: []
- file_name: graphrag/callbacks/query_callbacks.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: graphrag.callbacks.llm_callbacks
    name: BaseLLMCallback
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  functions:
  - name: on_context
    start_line: 15
    end_line: 16
    code: "def on_context(self, context: Any) -> None:\n        \"\"\"Handle when\
      \ context data is constructed.\"\"\""
    signature: 'def on_context(self, context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_context
    called_by: []
  - name: on_map_response_start
    start_line: 18
    end_line: 19
    code: "def on_map_response_start(self, map_response_contexts: list[str]) -> None:\n\
      \        \"\"\"Handle the start of map operation.\"\"\""
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_start
    called_by: []
  - name: on_map_response_end
    start_line: 21
    end_line: 22
    code: "def on_map_response_end(self, map_response_outputs: list[SearchResult])\
      \ -> None:\n        \"\"\"Handle the end of map operation.\"\"\""
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_end
    called_by: []
  - name: on_reduce_response_start
    start_line: 24
    end_line: 27
    code: "def on_reduce_response_start(\n        self, reduce_response_context: str\
      \ | dict[str, Any]\n    ) -> None:\n        \"\"\"Handle the start of reduce\
      \ operation.\"\"\""
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_start
    called_by: []
  - name: on_reduce_response_end
    start_line: 29
    end_line: 30
    code: "def on_reduce_response_end(self, reduce_response_output: str) -> None:\n\
      \        \"\"\"Handle the end of reduce operation.\"\"\""
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_end
    called_by: []
  - name: on_llm_new_token
    start_line: 32
    end_line: 33
    code: "def on_llm_new_token(self, token) -> None:\n        \"\"\"Handle when a\
      \ new token is generated.\"\"\""
    signature: def on_llm_new_token(self, token) -> None
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_llm_new_token
    called_by: []
- file_name: graphrag/callbacks/workflow_callbacks.py
  imports:
  - module: typing
    name: Protocol
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.logger.progress
    name: Progress
    alias: null
  functions:
  - name: pipeline_start
    start_line: 19
    end_line: 21
    code: "def pipeline_start(self, names: list[str]) -> None:\n        \"\"\"Execute\
      \ this callback to signal when the entire pipeline starts.\"\"\"\n        ..."
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_start
    called_by: []
  - name: pipeline_end
    start_line: 23
    end_line: 25
    code: "def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n   \
      \     \"\"\"Execute this callback to signal when the entire pipeline ends.\"\
      \"\"\n        ..."
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_end
    called_by: []
  - name: workflow_start
    start_line: 27
    end_line: 29
    code: "def workflow_start(self, name: str, instance: object) -> None:\n      \
      \  \"\"\"Execute this callback when a workflow starts.\"\"\"\n        ..."
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_start
    called_by: []
  - name: workflow_end
    start_line: 31
    end_line: 33
    code: "def workflow_end(self, name: str, instance: object) -> None:\n        \"\
      \"\"Execute this callback when a workflow ends.\"\"\"\n        ..."
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_end
    called_by: []
  - name: progress
    start_line: 35
    end_line: 37
    code: "def progress(self, progress: Progress) -> None:\n        \"\"\"Handle when\
      \ progress occurs.\"\"\"\n        ..."
    signature: 'def progress(self, progress: Progress) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.progress
    called_by: []
- file_name: graphrag/callbacks/workflow_callbacks_manager.py
  imports:
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.logger.progress
    name: Progress
    alias: null
  functions:
  - name: __init__
    start_line: 16
    end_line: 18
    code: "def __init__(self):\n        \"\"\"Create a new instance of WorkflowCallbacksRegistry.\"\
      \"\"\n        self._callbacks = []"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.__init__
    called_by: []
  - name: register
    start_line: 20
    end_line: 22
    code: "def register(self, callbacks: WorkflowCallbacks) -> None:\n        \"\"\
      \"Register a new WorkflowCallbacks type.\"\"\"\n        self._callbacks.append(callbacks)"
    signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
    decorators: []
    raises: []
    calls:
    - target: self._callbacks.append
      type: instance
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.register
    called_by: []
  - name: pipeline_start
    start_line: 24
    end_line: 28
    code: "def pipeline_start(self, names: list[str]) -> None:\n        \"\"\"Execute\
      \ this callback when a the entire pipeline starts.\"\"\"\n        for callback\
      \ in self._callbacks:\n            if hasattr(callback, \"pipeline_start\"):\n\
      \                callback.pipeline_start(names)"
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    - target: callback.pipeline_start
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_start
    called_by: []
  - name: pipeline_end
    start_line: 30
    end_line: 34
    code: "def pipeline_end(self, results: list[PipelineRunResult]) -> None:\n   \
      \     \"\"\"Execute this callback when the entire pipeline ends.\"\"\"\n   \
      \     for callback in self._callbacks:\n            if hasattr(callback, \"\
      pipeline_end\"):\n                callback.pipeline_end(results)"
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    - target: callback.pipeline_end
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_end
    called_by: []
  - name: workflow_start
    start_line: 36
    end_line: 40
    code: "def workflow_start(self, name: str, instance: object) -> None:\n      \
      \  \"\"\"Execute this callback when a workflow starts.\"\"\"\n        for callback\
      \ in self._callbacks:\n            if hasattr(callback, \"workflow_start\"):\n\
      \                callback.workflow_start(name, instance)"
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    - target: callback.workflow_start
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_start
    called_by: []
  - name: workflow_end
    start_line: 42
    end_line: 46
    code: "def workflow_end(self, name: str, instance: object) -> None:\n        \"\
      \"\"Execute this callback when a workflow ends.\"\"\"\n        for callback\
      \ in self._callbacks:\n            if hasattr(callback, \"workflow_end\"):\n\
      \                callback.workflow_end(name, instance)"
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    - target: callback.workflow_end
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_end
    called_by: []
  - name: progress
    start_line: 48
    end_line: 52
    code: "def progress(self, progress: Progress) -> None:\n        \"\"\"Handle when\
      \ progress occurs.\"\"\"\n        for callback in self._callbacks:\n       \
      \     if hasattr(callback, \"progress\"):\n                callback.progress(progress)"
    signature: 'def progress(self, progress: Progress) -> None'
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    - target: callback.progress
      type: unresolved
    visibility: public
    node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.progress
    called_by: []
- file_name: graphrag/cli/__init__.py
  imports: []
  functions: []
- file_name: graphrag/cli/index.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: sys
    name: null
    alias: null
  - module: warnings
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.api
    name: null
    alias: api
  - module: graphrag.callbacks.console_workflow_callbacks
    name: ConsoleWorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: CacheType
    alias: null
  - module: graphrag.config.enums
    name: IndexingMethod
    alias: null
  - module: graphrag.config.load_config
    name: load_config
    alias: null
  - module: graphrag.index.validate_config
    name: validate_config_names
    alias: null
  - module: graphrag.utils.cli
    name: redact
    alias: null
  - module: signal
    name: null
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  functions:
  - name: _register_signal_handlers
    start_line: 25
    end_line: 39
    code: "def _register_signal_handlers():\n    import signal\n\n    def handle_signal(signum,\
      \ _):\n        # Handle the signal here\n        logger.debug(f\"Received signal\
      \ {signum}, exiting...\")  # noqa: G004\n        for task in asyncio.all_tasks():\n\
      \            task.cancel()\n        logger.debug(\"All tasks cancelled. Exiting...\"\
      )\n\n    # Register signal handlers for SIGINT and SIGHUP\n    signal.signal(signal.SIGINT,\
      \ handle_signal)\n\n    if sys.platform != \"win32\":\n        signal.signal(signal.SIGHUP,\
      \ handle_signal)"
    signature: def _register_signal_handlers()
    decorators: []
    raises: []
    calls:
    - target: signal::signal
      type: stdlib
    visibility: protected
    node_id: graphrag/cli/index.py::_register_signal_handlers
    called_by:
    - source: graphrag/cli/index.py::_run_index
      type: internal
  - name: handle_signal
    start_line: 28
    end_line: 33
    code: "def handle_signal(signum, _):\n        # Handle the signal here\n     \
      \   logger.debug(f\"Received signal {signum}, exiting...\")  # noqa: G004\n\
      \        for task in asyncio.all_tasks():\n            task.cancel()\n     \
      \   logger.debug(\"All tasks cancelled. Exiting...\")"
    signature: def handle_signal(signum, _)
    decorators: []
    raises: []
    calls:
    - target: logger.debug
      type: unresolved
    - target: asyncio::all_tasks
      type: stdlib
    - target: task.cancel
      type: unresolved
    visibility: public
    node_id: graphrag/cli/index.py::handle_signal
    called_by: []
  - name: index_cli
    start_line: 42
    end_line: 69
    code: "def index_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n    verbose:\
      \ bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath: Path\
      \ | None,\n    dry_run: bool,\n    skip_validation: bool,\n    output_dir: Path\
      \ | None,\n):\n    \"\"\"Run the pipeline with the given config.\"\"\"\n   \
      \ cli_overrides = {}\n    if output_dir:\n        cli_overrides[\"output.base_dir\"\
      ] = str(output_dir)\n        cli_overrides[\"reporting.base_dir\"] = str(output_dir)\n\
      \        cli_overrides[\"update_index_output.base_dir\"] = str(output_dir)\n\
      \    config = load_config(root_dir, config_filepath, cli_overrides)\n    _run_index(\n\
      \        config=config,\n        method=method,\n        is_update_run=False,\n\
      \        verbose=verbose,\n        memprofile=memprofile,\n        cache=cache,\n\
      \        dry_run=dry_run,\n        skip_validation=skip_validation,\n    )"
    signature: "def index_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n\
      \    verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
      \ Path | None,\n    dry_run: bool,\n    skip_validation: bool,\n    output_dir:\
      \ Path | None,\n)"
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/index.py::_run_index
      type: internal
    visibility: public
    node_id: graphrag/cli/index.py::index_cli
    called_by:
    - source: graphrag/cli/main.py::_index_cli
      type: internal
  - name: update_cli
    start_line: 72
    end_line: 100
    code: "def update_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n   \
      \ verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
      \ Path | None,\n    skip_validation: bool,\n    output_dir: Path | None,\n):\n\
      \    \"\"\"Run the pipeline with the given config.\"\"\"\n    cli_overrides\
      \ = {}\n    if output_dir:\n        cli_overrides[\"output.base_dir\"] = str(output_dir)\n\
      \        cli_overrides[\"reporting.base_dir\"] = str(output_dir)\n        cli_overrides[\"\
      update_index_output.base_dir\"] = str(output_dir)\n\n    config = load_config(root_dir,\
      \ config_filepath, cli_overrides)\n\n    _run_index(\n        config=config,\n\
      \        method=method,\n        is_update_run=True,\n        verbose=verbose,\n\
      \        memprofile=memprofile,\n        cache=cache,\n        dry_run=False,\n\
      \        skip_validation=skip_validation,\n    )"
    signature: "def update_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n\
      \    verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
      \ Path | None,\n    skip_validation: bool,\n    output_dir: Path | None,\n)"
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/index.py::_run_index
      type: internal
    visibility: public
    node_id: graphrag/cli/index.py::update_cli
    called_by:
    - source: graphrag/cli/main.py::_update_cli
      type: internal
  - name: _run_index
    start_line: 103
    end_line: 161
    code: "def _run_index(\n    config,\n    method,\n    is_update_run,\n    verbose,\n\
      \    memprofile,\n    cache,\n    dry_run,\n    skip_validation,\n):\n    #\
      \ Configure the root logger with the specified log level\n    from graphrag.logger.standard_logging\
      \ import init_loggers\n\n    # Initialize loggers and reporting config\n   \
      \ init_loggers(\n        config=config,\n        verbose=verbose,\n    )\n\n\
      \    if not cache:\n        config.cache.type = CacheType.none\n\n    if not\
      \ skip_validation:\n        validate_config_names(config)\n\n    logger.info(\"\
      Starting pipeline run. %s\", dry_run)\n    logger.info(\n        \"Using default\
      \ configuration: %s\",\n        redact(config.model_dump()),\n    )\n\n    if\
      \ dry_run:\n        logger.info(\"Dry run complete, exiting...\", True)\n  \
      \      sys.exit(0)\n\n    _register_signal_handlers()\n\n    outputs = asyncio.run(\n\
      \        api.build_index(\n            config=config,\n            method=method,\n\
      \            is_update_run=is_update_run,\n            memory_profile=memprofile,\n\
      \            callbacks=[ConsoleWorkflowCallbacks(verbose=verbose)],\n      \
      \      verbose=verbose,\n        )\n    )\n    encountered_errors = any(\n \
      \       output.errors and len(output.errors) > 0 for output in outputs\n   \
      \ )\n\n    if encountered_errors:\n        logger.error(\n            \"Errors\
      \ occurred during the pipeline run, see logs for more details.\"\n        )\n\
      \    else:\n        logger.info(\"All workflows completed successfully.\")\n\
      \n    sys.exit(1 if encountered_errors else 0)"
    signature: "def _run_index(\n    config,\n    method,\n    is_update_run,\n  \
      \  verbose,\n    memprofile,\n    cache,\n    dry_run,\n    skip_validation,\n\
      )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: graphrag/index/validate_config.py::validate_config_names
      type: internal
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/cli.py::redact
      type: internal
    - target: config.model_dump
      type: unresolved
    - target: sys::exit
      type: stdlib
    - target: graphrag/cli/index.py::_register_signal_handlers
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: graphrag.api::build_index
      type: internal
    - target: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks
      type: internal
    - target: any
      type: builtin
    - target: len
      type: builtin
    - target: logger.error
      type: unresolved
    visibility: protected
    node_id: graphrag/cli/index.py::_run_index
    called_by:
    - source: graphrag/cli/index.py::index_cli
      type: internal
    - source: graphrag/cli/index.py::update_cli
      type: internal
- file_name: graphrag/cli/initialize.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.config.init_content
    name: INIT_DOTENV
    alias: null
  - module: graphrag.config.init_content
    name: INIT_YAML
    alias: null
  - module: graphrag.prompts.index.community_report
    name: COMMUNITY_REPORT_PROMPT
    alias: null
  - module: graphrag.prompts.index.community_report_text_units
    name: COMMUNITY_REPORT_TEXT_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_claims
    name: EXTRACT_CLAIMS_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_graph
    name: GRAPH_EXTRACTION_PROMPT
    alias: null
  - module: graphrag.prompts.index.summarize_descriptions
    name: SUMMARIZE_PROMPT
    alias: null
  - module: graphrag.prompts.query.basic_search_system_prompt
    name: BASIC_SEARCH_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.drift_search_system_prompt
    name: DRIFT_LOCAL_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.drift_search_system_prompt
    name: DRIFT_REDUCE_PROMPT
    alias: null
  - module: graphrag.prompts.query.global_search_knowledge_system_prompt
    name: GENERAL_KNOWLEDGE_INSTRUCTION
    alias: null
  - module: graphrag.prompts.query.global_search_map_system_prompt
    name: MAP_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.global_search_reduce_system_prompt
    name: REDUCE_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.local_search_system_prompt
    name: LOCAL_SEARCH_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.question_gen_system_prompt
    name: QUESTION_SYSTEM_PROMPT
    alias: null
  functions:
  - name: initialize_project_at
    start_line: 37
    end_line: 95
    code: "def initialize_project_at(path: Path, force: bool) -> None:\n    \"\"\"\
      \n    Initialize the project at the given path.\n\n    Parameters\n    ----------\n\
      \    path : Path\n        The path at which to initialize the project.\n   \
      \ force : bool\n        Whether to force initialization even if the project\
      \ already exists.\n\n    Raises\n    ------\n    ValueError\n        If the\
      \ project already exists and force is False.\n    \"\"\"\n    logger.info(\"\
      Initializing project at %s\", path)\n    root = Path(path)\n    if not root.exists():\n\
      \        root.mkdir(parents=True, exist_ok=True)\n\n    settings_yaml = root\
      \ / \"settings.yaml\"\n    if settings_yaml.exists() and not force:\n      \
      \  msg = f\"Project already initialized at {root}\"\n        raise ValueError(msg)\n\
      \n    with settings_yaml.open(\"wb\") as file:\n        file.write(INIT_YAML.encode(encoding=\"\
      utf-8\", errors=\"strict\"))\n\n    dotenv = root / \".env\"\n    if not dotenv.exists()\
      \ or force:\n        with dotenv.open(\"wb\") as file:\n            file.write(INIT_DOTENV.encode(encoding=\"\
      utf-8\", errors=\"strict\"))\n\n    prompts_dir = root / \"prompts\"\n    if\
      \ not prompts_dir.exists():\n        prompts_dir.mkdir(parents=True, exist_ok=True)\n\
      \n    prompts = {\n        \"extract_graph\": GRAPH_EXTRACTION_PROMPT,\n   \
      \     \"summarize_descriptions\": SUMMARIZE_PROMPT,\n        \"extract_claims\"\
      : EXTRACT_CLAIMS_PROMPT,\n        \"community_report_graph\": COMMUNITY_REPORT_PROMPT,\n\
      \        \"community_report_text\": COMMUNITY_REPORT_TEXT_PROMPT,\n        \"\
      drift_search_system_prompt\": DRIFT_LOCAL_SYSTEM_PROMPT,\n        \"drift_reduce_prompt\"\
      : DRIFT_REDUCE_PROMPT,\n        \"global_search_map_system_prompt\": MAP_SYSTEM_PROMPT,\n\
      \        \"global_search_reduce_system_prompt\": REDUCE_SYSTEM_PROMPT,\n   \
      \     \"global_search_knowledge_system_prompt\": GENERAL_KNOWLEDGE_INSTRUCTION,\n\
      \        \"local_search_system_prompt\": LOCAL_SEARCH_SYSTEM_PROMPT,\n     \
      \   \"basic_search_system_prompt\": BASIC_SEARCH_SYSTEM_PROMPT,\n        \"\
      question_gen_system_prompt\": QUESTION_SYSTEM_PROMPT,\n    }\n\n    for name,\
      \ content in prompts.items():\n        prompt_file = prompts_dir / f\"{name}.txt\"\
      \n        if not prompt_file.exists() or force:\n            with prompt_file.open(\"\
      wb\") as file:\n                file.write(content.encode(encoding=\"utf-8\"\
      , errors=\"strict\"))"
    signature: 'def initialize_project_at(path: Path, force: bool) -> None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: logger.info
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: root.exists
      type: unresolved
    - target: root.mkdir
      type: unresolved
    - target: settings_yaml.exists
      type: unresolved
    - target: ValueError
      type: builtin
    - target: settings_yaml.open
      type: unresolved
    - target: file.write
      type: unresolved
    - target: graphrag/config/init_content.py::INIT_YAML::encode
      type: external
    - target: dotenv.exists
      type: unresolved
    - target: dotenv.open
      type: unresolved
    - target: graphrag/config/init_content.py::INIT_DOTENV::encode
      type: external
    - target: prompts_dir.exists
      type: unresolved
    - target: prompts_dir.mkdir
      type: unresolved
    - target: prompts.items
      type: unresolved
    - target: prompt_file.exists
      type: unresolved
    - target: prompt_file.open
      type: unresolved
    - target: content.encode
      type: unresolved
    visibility: public
    node_id: graphrag/cli/initialize.py::initialize_project_at
    called_by:
    - source: graphrag/cli/main.py::_initialize_cli
      type: internal
- file_name: graphrag/cli/main.py
  imports:
  - module: os
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typer
    name: null
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: IndexingMethod
    alias: null
  - module: graphrag.config.enums
    name: SearchMethod
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: LIMIT
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: MAX_TOKEN_COUNT
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: N_SUBSET_MAX
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: K
    alias: null
  - module: graphrag.prompt_tune.types
    name: DocSelectionType
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.cli.initialize
    name: initialize_project_at
    alias: null
  - module: graphrag.cli.index
    name: index_cli
    alias: null
  - module: graphrag.cli.index
    name: update_cli
    alias: null
  - module: asyncio
    name: null
    alias: null
  - module: graphrag.cli.prompt_tune
    name: prompt_tune
    alias: null
  - module: graphrag.cli.query
    name: run_basic_search
    alias: null
  - module: graphrag.cli.query
    name: run_drift_search
    alias: null
  - module: graphrag.cli.query
    name: run_global_search
    alias: null
  - module: graphrag.cli.query
    name: run_local_search
    alias: null
  functions:
  - name: path_autocomplete
    start_line: 30
    end_line: 76
    code: "def path_autocomplete(\n    file_okay: bool = True,\n    dir_okay: bool\
      \ = True,\n    readable: bool = True,\n    writable: bool = False,\n    match_wildcard:\
      \ str | None = None,\n) -> Callable[[str], list[str]]:\n    \"\"\"Autocomplete\
      \ file and directory paths.\"\"\"\n\n    def wildcard_match(string: str, pattern:\
      \ str) -> bool:\n        regex = re.escape(pattern).replace(r\"\\?\", \".\"\
      ).replace(r\"\\*\", \".*\")\n        return re.fullmatch(regex, string) is not\
      \ None\n\n    from pathlib import Path\n\n    def completer(incomplete: str)\
      \ -> list[str]:\n        # List items in the current directory as Path objects\n\
      \        items = Path().iterdir()\n        completions = []\n\n        for item\
      \ in items:\n            # Filter based on file/directory properties\n     \
      \       if not file_okay and item.is_file():\n                continue\n   \
      \         if not dir_okay and item.is_dir():\n                continue\n   \
      \         if readable and not os.access(item, os.R_OK):\n                continue\n\
      \            if writable and not os.access(item, os.W_OK):\n               \
      \ continue\n\n            # Append the name of the matching item\n         \
      \   completions.append(item.name)\n\n        # Apply wildcard matching if required\n\
      \        if match_wildcard:\n            completions = filter(\n           \
      \     lambda i: wildcard_match(i, match_wildcard)\n                if match_wildcard\n\
      \                else False,\n                completions,\n            )\n\n\
      \        # Return completions that start with the given incomplete string\n\
      \        return [i for i in completions if i.startswith(incomplete)]\n\n   \
      \ return completer"
    signature: "def path_autocomplete(\n    file_okay: bool = True,\n    dir_okay:\
      \ bool = True,\n    readable: bool = True,\n    writable: bool = False,\n  \
      \  match_wildcard: str | None = None,\n) -> Callable[[str], list[str]]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/cli/main.py::path_autocomplete
    called_by: []
  - name: wildcard_match
    start_line: 39
    end_line: 41
    code: "def wildcard_match(string: str, pattern: str) -> bool:\n        regex =\
      \ re.escape(pattern).replace(r\"\\?\", \".\").replace(r\"\\*\", \".*\")\n  \
      \      return re.fullmatch(regex, string) is not None"
    signature: 'def wildcard_match(string: str, pattern: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: re::escape(pattern).replace(r"\?", ".").replace
      type: stdlib
    - target: re::escape(pattern).replace
      type: stdlib
    - target: re::escape
      type: stdlib
    - target: re::fullmatch
      type: stdlib
    visibility: public
    node_id: graphrag/cli/main.py::wildcard_match
    called_by:
    - source: graphrag/cli/main.py::completer
      type: internal
  - name: completer
    start_line: 45
    end_line: 74
    code: "def completer(incomplete: str) -> list[str]:\n        # List items in the\
      \ current directory as Path objects\n        items = Path().iterdir()\n    \
      \    completions = []\n\n        for item in items:\n            # Filter based\
      \ on file/directory properties\n            if not file_okay and item.is_file():\n\
      \                continue\n            if not dir_okay and item.is_dir():\n\
      \                continue\n            if readable and not os.access(item, os.R_OK):\n\
      \                continue\n            if writable and not os.access(item, os.W_OK):\n\
      \                continue\n\n            # Append the name of the matching item\n\
      \            completions.append(item.name)\n\n        # Apply wildcard matching\
      \ if required\n        if match_wildcard:\n            completions = filter(\n\
      \                lambda i: wildcard_match(i, match_wildcard)\n             \
      \   if match_wildcard\n                else False,\n                completions,\n\
      \            )\n\n        # Return completions that start with the given incomplete\
      \ string\n        return [i for i in completions if i.startswith(incomplete)]"
    signature: 'def completer(incomplete: str) -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: Path().iterdir
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: item.is_file
      type: unresolved
    - target: item.is_dir
      type: unresolved
    - target: os::access
      type: stdlib
    - target: completions.append
      type: unresolved
    - target: filter
      type: builtin
    - target: graphrag/cli/main.py::wildcard_match
      type: internal
    - target: i.startswith
      type: unresolved
    visibility: public
    node_id: graphrag/cli/main.py::completer
    called_by: []
  - name: _initialize_cli
    start_line: 95
    end_line: 116
    code: "def _initialize_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    force: bool = typer.Option(\n\
      \        False,\n        \"--force\",\n        \"-f\",\n        help=\"Force\
      \ initialization even if the project already exists.\",\n    ),\n) -> None:\n\
      \    \"\"\"Generate a default configuration file.\"\"\"\n    from graphrag.cli.initialize\
      \ import initialize_project_at\n\n    initialize_project_at(path=root, force=force)"
    signature: "def _initialize_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    force: bool = typer.Option(\n\
      \        False,\n        \"--force\",\n        \"-f\",\n        help=\"Force\
      \ initialization even if the project already exists.\",\n    ),\n) -> None"
    decorators:
    - '@app.command("init")'
    raises: []
    calls:
    - target: typer::Option
      type: external
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/cli/initialize.py::initialize_project_at
      type: internal
    visibility: protected
    node_id: graphrag/cli/main.py::_initialize_cli
    called_by: []
  - name: _index_cli
    start_line: 120
    end_line: 203
    code: "def _index_cli(\n    config: Path | None = typer.Option(\n        None,\n\
      \        \"--config\",\n        \"-c\",\n        help=\"The configuration to\
      \ use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging\",\n    ),\n    memprofile: bool = typer.Option(\n  \
      \      False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling\",\n    ),\n    dry_run: bool = typer.Option(\n    \
      \    False,\n        \"--dry-run\",\n        help=(\n            \"Run the indexing\
      \ pipeline without executing any steps \"\n            \"to inspect and validate\
      \ the configuration.\"\n        ),\n    ),\n    cache: bool = typer.Option(\n\
      \        True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None:\n  \
      \  \"\"\"Build a knowledge graph index.\"\"\"\n    from graphrag.cli.index import\
      \ index_cli\n\n    index_cli(\n        root_dir=root,\n        verbose=verbose,\n\
      \        memprofile=memprofile,\n        cache=cache,\n        config_filepath=config,\n\
      \        dry_run=dry_run,\n        skip_validation=skip_validation,\n      \
      \  output_dir=output,\n        method=method,\n    )"
    signature: "def _index_cli(\n    config: Path | None = typer.Option(\n       \
      \ None,\n        \"--config\",\n        \"-c\",\n        help=\"The configuration\
      \ to use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging\",\n    ),\n    memprofile: bool = typer.Option(\n  \
      \      False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling\",\n    ),\n    dry_run: bool = typer.Option(\n    \
      \    False,\n        \"--dry-run\",\n        help=(\n            \"Run the indexing\
      \ pipeline without executing any steps \"\n            \"to inspect and validate\
      \ the configuration.\"\n        ),\n    ),\n    cache: bool = typer.Option(\n\
      \        True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None"
    decorators:
    - '@app.command("index")'
    raises: []
    calls:
    - target: typer::Option
      type: external
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/cli/index.py::index_cli
      type: internal
    visibility: protected
    node_id: graphrag/cli/main.py::_index_cli
    called_by: []
  - name: _update_cli
    start_line: 207
    end_line: 285
    code: "def _update_cli(\n    config: Path | None = typer.Option(\n        None,\n\
      \        \"--config\",\n        \"-c\",\n        help=\"The configuration to\
      \ use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging.\",\n    ),\n    memprofile: bool = typer.Option(\n \
      \       False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling.\",\n    ),\n    cache: bool = typer.Option(\n     \
      \   True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None:\n  \
      \  \"\"\"\n    Update an existing knowledge graph index.\n\n    Applies a default\
      \ output configuration (if not provided by config), saving the new index to\
      \ the local file system in the `update_output` folder.\n    \"\"\"\n    from\
      \ graphrag.cli.index import update_cli\n\n    update_cli(\n        root_dir=root,\n\
      \        verbose=verbose,\n        memprofile=memprofile,\n        cache=cache,\n\
      \        config_filepath=config,\n        skip_validation=skip_validation,\n\
      \        output_dir=output,\n        method=method,\n    )"
    signature: "def _update_cli(\n    config: Path | None = typer.Option(\n      \
      \  None,\n        \"--config\",\n        \"-c\",\n        help=\"The configuration\
      \ to use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging.\",\n    ),\n    memprofile: bool = typer.Option(\n \
      \       False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling.\",\n    ),\n    cache: bool = typer.Option(\n     \
      \   True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None"
    decorators:
    - '@app.command("update")'
    raises: []
    calls:
    - target: typer::Option
      type: external
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/cli/index.py::update_cli
      type: internal
    visibility: protected
    node_id: graphrag/cli/main.py::_update_cli
    called_by: []
  - name: _prompt_tune_cli
    start_line: 289
    end_line: 410
    code: "def _prompt_tune_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        exists=True,\n        dir_okay=True,\n        writable=True,\n  \
      \      resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
      \    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the prompt tuning pipeline with verbose\
      \ logging.\",\n    ),\n    domain: str | None = typer.Option(\n        None,\n\
      \        \"--domain\",\n        help=(\n            \"The domain your input\
      \ data is related to. \"\n            \"For example 'space science', 'microbiology',\
      \ 'environmental news'. \"\n            \"If not defined, a domain will be inferred\
      \ from the input data.\"\n        ),\n    ),\n    selection_method: DocSelectionType\
      \ = typer.Option(\n        DocSelectionType.RANDOM.value,\n        \"--selection-method\"\
      ,\n        help=\"The text chunk selection method.\",\n    ),\n    n_subset_max:\
      \ int = typer.Option(\n        N_SUBSET_MAX,\n        \"--n-subset-max\",\n\
      \        help=\"The number of text chunks to embed when --selection-method=auto.\"\
      ,\n    ),\n    k: int = typer.Option(\n        K,\n        \"--k\",\n      \
      \  help=\"The maximum number of documents to select from each centroid when\
      \ --selection-method=auto.\",\n    ),\n    limit: int = typer.Option(\n    \
      \    LIMIT,\n        \"--limit\",\n        help=\"The number of documents to\
      \ load when --selection-method={random,top}.\",\n    ),\n    max_tokens: int\
      \ = typer.Option(\n        MAX_TOKEN_COUNT,\n        \"--max-tokens\",\n   \
      \     help=\"The max token count for prompt generation.\",\n    ),\n    min_examples_required:\
      \ int = typer.Option(\n        2,\n        \"--min-examples-required\",\n  \
      \      help=\"The minimum number of examples to generate/include in the entity\
      \ extraction prompt.\",\n    ),\n    chunk_size: int = typer.Option(\n     \
      \   graphrag_config_defaults.chunks.size,\n        \"--chunk-size\",\n     \
      \   help=\"The size of each example text chunk. Overrides chunks.size in the\
      \ configuration file.\",\n    ),\n    overlap: int = typer.Option(\n       \
      \ graphrag_config_defaults.chunks.overlap,\n        \"--overlap\",\n       \
      \ help=\"The overlap size for chunking documents. Overrides chunks.overlap in\
      \ the configuration file.\",\n    ),\n    language: str | None = typer.Option(\n\
      \        None,\n        \"--language\",\n        help=\"The primary language\
      \ used for inputs and outputs in graphrag prompts.\",\n    ),\n    discover_entity_types:\
      \ bool = typer.Option(\n        True,\n        \"--discover-entity-types/--no-discover-entity-types\"\
      ,\n        help=\"Discover and extract unspecified entity types.\",\n    ),\n\
      \    output: Path = typer.Option(\n        Path(\"prompts\"),\n        \"--output\"\
      ,\n        \"-o\",\n        help=\"The directory to save prompts to, relative\
      \ to the project root directory.\",\n        dir_okay=True,\n        writable=True,\n\
      \        resolve_path=True,\n    ),\n) -> None:\n    \"\"\"Generate custom graphrag\
      \ prompts with your own data (i.e. auto templating).\"\"\"\n    import asyncio\n\
      \n    from graphrag.cli.prompt_tune import prompt_tune\n\n    loop = asyncio.get_event_loop()\n\
      \    loop.run_until_complete(\n        prompt_tune(\n            root=root,\n\
      \            config=config,\n            domain=domain,\n            verbose=verbose,\n\
      \            selection_method=selection_method,\n            limit=limit,\n\
      \            max_tokens=max_tokens,\n            chunk_size=chunk_size,\n  \
      \          overlap=overlap,\n            language=language,\n            discover_entity_types=discover_entity_types,\n\
      \            output=output,\n            n_subset_max=n_subset_max,\n      \
      \      k=k,\n            min_examples_required=min_examples_required,\n    \
      \    )\n    )"
    signature: "def _prompt_tune_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        exists=True,\n        dir_okay=True,\n        writable=True,\n  \
      \      resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
      \    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the prompt tuning pipeline with verbose\
      \ logging.\",\n    ),\n    domain: str | None = typer.Option(\n        None,\n\
      \        \"--domain\",\n        help=(\n            \"The domain your input\
      \ data is related to. \"\n            \"For example 'space science', 'microbiology',\
      \ 'environmental news'. \"\n            \"If not defined, a domain will be inferred\
      \ from the input data.\"\n        ),\n    ),\n    selection_method: DocSelectionType\
      \ = typer.Option(\n        DocSelectionType.RANDOM.value,\n        \"--selection-method\"\
      ,\n        help=\"The text chunk selection method.\",\n    ),\n    n_subset_max:\
      \ int = typer.Option(\n        N_SUBSET_MAX,\n        \"--n-subset-max\",\n\
      \        help=\"The number of text chunks to embed when --selection-method=auto.\"\
      ,\n    ),\n    k: int = typer.Option(\n        K,\n        \"--k\",\n      \
      \  help=\"The maximum number of documents to select from each centroid when\
      \ --selection-method=auto.\",\n    ),\n    limit: int = typer.Option(\n    \
      \    LIMIT,\n        \"--limit\",\n        help=\"The number of documents to\
      \ load when --selection-method={random,top}.\",\n    ),\n    max_tokens: int\
      \ = typer.Option(\n        MAX_TOKEN_COUNT,\n        \"--max-tokens\",\n   \
      \     help=\"The max token count for prompt generation.\",\n    ),\n    min_examples_required:\
      \ int = typer.Option(\n        2,\n        \"--min-examples-required\",\n  \
      \      help=\"The minimum number of examples to generate/include in the entity\
      \ extraction prompt.\",\n    ),\n    chunk_size: int = typer.Option(\n     \
      \   graphrag_config_defaults.chunks.size,\n        \"--chunk-size\",\n     \
      \   help=\"The size of each example text chunk. Overrides chunks.size in the\
      \ configuration file.\",\n    ),\n    overlap: int = typer.Option(\n       \
      \ graphrag_config_defaults.chunks.overlap,\n        \"--overlap\",\n       \
      \ help=\"The overlap size for chunking documents. Overrides chunks.overlap in\
      \ the configuration file.\",\n    ),\n    language: str | None = typer.Option(\n\
      \        None,\n        \"--language\",\n        help=\"The primary language\
      \ used for inputs and outputs in graphrag prompts.\",\n    ),\n    discover_entity_types:\
      \ bool = typer.Option(\n        True,\n        \"--discover-entity-types/--no-discover-entity-types\"\
      ,\n        help=\"Discover and extract unspecified entity types.\",\n    ),\n\
      \    output: Path = typer.Option(\n        Path(\"prompts\"),\n        \"--output\"\
      ,\n        \"-o\",\n        help=\"The directory to save prompts to, relative\
      \ to the project root directory.\",\n        dir_okay=True,\n        writable=True,\n\
      \        resolve_path=True,\n    ),\n) -> None"
    decorators:
    - '@app.command("prompt-tune")'
    raises: []
    calls:
    - target: typer::Option
      type: external
    - target: pathlib::Path
      type: stdlib
    - target: asyncio::get_event_loop
      type: stdlib
    - target: loop.run_until_complete
      type: unresolved
    - target: graphrag/cli/prompt_tune.py::prompt_tune
      type: internal
    visibility: protected
    node_id: graphrag/cli/main.py::_prompt_tune_cli
    called_by: []
  - name: _query_cli
    start_line: 414
    end_line: 545
    code: "def _query_cli(\n    method: SearchMethod = typer.Option(\n        ...,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The query algorithm\
      \ to use.\",\n    ),\n    query: str = typer.Option(\n        ...,\n       \
      \ \"--query\",\n        \"-q\",\n        help=\"The query to execute.\",\n \
      \   ),\n    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the query with verbose logging.\",\n\
      \    ),\n    data: Path | None = typer.Option(\n        None,\n        \"--data\"\
      ,\n        \"-d\",\n        help=\"Index output directory (contains the parquet\
      \ files).\",\n        exists=True,\n        dir_okay=True,\n        readable=True,\n\
      \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n   \
      \ ),\n    root: Path = typer.Option(\n        Path(),\n        \"--root\",\n\
      \        \"-r\",\n        help=\"The project root directory.\",\n        exists=True,\n\
      \        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    community_level: int\
      \ = typer.Option(\n        2,\n        \"--community-level\",\n        help=(\n\
      \            \"Leiden hierarchy level from which to load community reports.\
      \ \"\n            \"Higher values represent smaller communities.\"\n       \
      \ ),\n    ),\n    dynamic_community_selection: bool = typer.Option(\n      \
      \  False,\n        \"--dynamic-community-selection/--no-dynamic-selection\"\
      ,\n        help=\"Use global search with dynamic community selection.\",\n \
      \   ),\n    response_type: str = typer.Option(\n        \"Multiple Paragraphs\"\
      ,\n        \"--response-type\",\n        help=(\n            \"Free-form description\
      \ of the desired response format \"\n            \"(e.g. 'Single Sentence',\
      \ 'List of 3-7 Points', etc.).\"\n        ),\n    ),\n    streaming: bool =\
      \ typer.Option(\n        False,\n        \"--streaming/--no-streaming\",\n \
      \       help=\"Print the response in a streaming manner.\",\n    ),\n) -> None:\n\
      \    \"\"\"Query a knowledge graph index.\"\"\"\n    from graphrag.cli.query\
      \ import (\n        run_basic_search,\n        run_drift_search,\n        run_global_search,\n\
      \        run_local_search,\n    )\n\n    match method:\n        case SearchMethod.LOCAL:\n\
      \            run_local_search(\n                config_filepath=config,\n  \
      \              data_dir=data,\n                root_dir=root,\n            \
      \    community_level=community_level,\n                response_type=response_type,\n\
      \                streaming=streaming,\n                query=query,\n      \
      \          verbose=verbose,\n            )\n        case SearchMethod.GLOBAL:\n\
      \            run_global_search(\n                config_filepath=config,\n \
      \               data_dir=data,\n                root_dir=root,\n           \
      \     community_level=community_level,\n                dynamic_community_selection=dynamic_community_selection,\n\
      \                response_type=response_type,\n                streaming=streaming,\n\
      \                query=query,\n                verbose=verbose,\n          \
      \  )\n        case SearchMethod.DRIFT:\n            run_drift_search(\n    \
      \            config_filepath=config,\n                data_dir=data,\n     \
      \           root_dir=root,\n                community_level=community_level,\n\
      \                streaming=streaming,\n                response_type=response_type,\n\
      \                query=query,\n                verbose=verbose,\n          \
      \  )\n        case SearchMethod.BASIC:\n            run_basic_search(\n    \
      \            config_filepath=config,\n                data_dir=data,\n     \
      \           root_dir=root,\n                streaming=streaming,\n         \
      \       query=query,\n                verbose=verbose,\n            )\n    \
      \    case _:\n            raise ValueError(INVALID_METHOD_ERROR)"
    signature: "def _query_cli(\n    method: SearchMethod = typer.Option(\n      \
      \  ...,\n        \"--method\",\n        \"-m\",\n        help=\"The query algorithm\
      \ to use.\",\n    ),\n    query: str = typer.Option(\n        ...,\n       \
      \ \"--query\",\n        \"-q\",\n        help=\"The query to execute.\",\n \
      \   ),\n    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the query with verbose logging.\",\n\
      \    ),\n    data: Path | None = typer.Option(\n        None,\n        \"--data\"\
      ,\n        \"-d\",\n        help=\"Index output directory (contains the parquet\
      \ files).\",\n        exists=True,\n        dir_okay=True,\n        readable=True,\n\
      \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n   \
      \ ),\n    root: Path = typer.Option(\n        Path(),\n        \"--root\",\n\
      \        \"-r\",\n        help=\"The project root directory.\",\n        exists=True,\n\
      \        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    community_level: int\
      \ = typer.Option(\n        2,\n        \"--community-level\",\n        help=(\n\
      \            \"Leiden hierarchy level from which to load community reports.\
      \ \"\n            \"Higher values represent smaller communities.\"\n       \
      \ ),\n    ),\n    dynamic_community_selection: bool = typer.Option(\n      \
      \  False,\n        \"--dynamic-community-selection/--no-dynamic-selection\"\
      ,\n        help=\"Use global search with dynamic community selection.\",\n \
      \   ),\n    response_type: str = typer.Option(\n        \"Multiple Paragraphs\"\
      ,\n        \"--response-type\",\n        help=(\n            \"Free-form description\
      \ of the desired response format \"\n            \"(e.g. 'Single Sentence',\
      \ 'List of 3-7 Points', etc.).\"\n        ),\n    ),\n    streaming: bool =\
      \ typer.Option(\n        False,\n        \"--streaming/--no-streaming\",\n \
      \       help=\"Print the response in a streaming manner.\",\n    ),\n) -> None"
    decorators:
    - '@app.command("query")'
    raises:
    - ValueError
    calls:
    - target: typer::Option
      type: external
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/cli/query.py::run_local_search
      type: internal
    - target: graphrag/cli/query.py::run_global_search
      type: internal
    - target: graphrag/cli/query.py::run_drift_search
      type: internal
    - target: graphrag/cli/query.py::run_basic_search
      type: internal
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/cli/main.py::_query_cli
    called_by: []
- file_name: graphrag/cli/prompt_tune.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.api
    name: null
    alias: api
  - module: graphrag.config.load_config
    name: load_config
    alias: null
  - module: graphrag.prompt_tune.generator.community_report_summarization
    name: COMMUNITY_SUMMARIZATION_FILENAME
    alias: null
  - module: graphrag.prompt_tune.generator.entity_summarization_prompt
    name: ENTITY_SUMMARIZATION_FILENAME
    alias: null
  - module: graphrag.prompt_tune.generator.extract_graph_prompt
    name: EXTRACT_GRAPH_FILENAME
    alias: null
  - module: graphrag.utils.cli
    name: redact
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  functions:
  - name: prompt_tune
    start_line: 25
    end_line: 117
    code: "async def prompt_tune(\n    root: Path,\n    config: Path | None,\n   \
      \ domain: str | None,\n    verbose: bool,\n    selection_method: api.DocSelectionType,\n\
      \    limit: int,\n    max_tokens: int,\n    chunk_size: int,\n    overlap: int,\n\
      \    language: str | None,\n    discover_entity_types: bool,\n    output: Path,\n\
      \    n_subset_max: int,\n    k: int,\n    min_examples_required: int,\n):\n\
      \    \"\"\"Prompt tune the model.\n\n    Parameters\n    ----------\n    - config:\
      \ The configuration file.\n    - root: The root directory.\n    - domain: The\
      \ domain to map the input documents to.\n    - verbose: Enable verbose logging.\n\
      \    - selection_method: The chunk selection method.\n    - limit: The limit\
      \ of chunks to load.\n    - max_tokens: The maximum number of tokens to use\
      \ on entity extraction prompts.\n    - chunk_size: The chunk token size to use.\n\
      \    - language: The language to use for the prompts.\n    - discover_entity_types:\
      \ Generate entity types.\n    - output: The output folder to store the prompts.\n\
      \    - n_subset_max: The number of text chunks to embed when using auto selection\
      \ method.\n    - k: The number of documents to select when using auto selection\
      \ method.\n    - min_examples_required: The minimum number of examples required\
      \ for entity extraction prompts.\n    \"\"\"\n    root_path = Path(root).resolve()\n\
      \    graph_config = load_config(root_path, config)\n\n    # override chunking\
      \ config in the configuration\n    if chunk_size != graph_config.chunks.size:\n\
      \        graph_config.chunks.size = chunk_size\n\n    if overlap != graph_config.chunks.overlap:\n\
      \        graph_config.chunks.overlap = overlap\n\n    # configure the root logger\
      \ with the specified log level\n    from graphrag.logger.standard_logging import\
      \ init_loggers\n\n    # initialize loggers with config\n    init_loggers(config=graph_config,\
      \ verbose=verbose, filename=\"prompt-tuning.log\")\n\n    logger.info(\"Starting\
      \ prompt tune.\")\n    logger.info(\n        \"Using default configuration:\
      \ %s\",\n        redact(graph_config.model_dump()),\n    )\n\n    prompts =\
      \ await api.generate_indexing_prompts(\n        config=graph_config,\n     \
      \   chunk_size=chunk_size,\n        overlap=overlap,\n        limit=limit,\n\
      \        selection_method=selection_method,\n        domain=domain,\n      \
      \  language=language,\n        max_tokens=max_tokens,\n        discover_entity_types=discover_entity_types,\n\
      \        min_examples_required=min_examples_required,\n        n_subset_max=n_subset_max,\n\
      \        k=k,\n        verbose=verbose,\n    )\n\n    output_path = output.resolve()\n\
      \    if output_path:\n        logger.info(\"Writing prompts to %s\", output_path)\n\
      \        output_path.mkdir(parents=True, exist_ok=True)\n        extract_graph_prompt_path\
      \ = output_path / EXTRACT_GRAPH_FILENAME\n        entity_summarization_prompt_path\
      \ = output_path / ENTITY_SUMMARIZATION_FILENAME\n        community_summarization_prompt_path\
      \ = (\n            output_path / COMMUNITY_SUMMARIZATION_FILENAME\n        )\n\
      \        # write files to output path\n        with extract_graph_prompt_path.open(\"\
      wb\") as file:\n            file.write(prompts[0].encode(encoding=\"utf-8\"\
      , errors=\"strict\"))\n        with entity_summarization_prompt_path.open(\"\
      wb\") as file:\n            file.write(prompts[1].encode(encoding=\"utf-8\"\
      , errors=\"strict\"))\n        with community_summarization_prompt_path.open(\"\
      wb\") as file:\n            file.write(prompts[2].encode(encoding=\"utf-8\"\
      , errors=\"strict\"))\n        logger.info(\"Prompts written to %s\", output_path)\n\
      \    else:\n        logger.error(\"No output path provided. Skipping writing\
      \ prompts.\")"
    signature: "def prompt_tune(\n    root: Path,\n    config: Path | None,\n    domain:\
      \ str | None,\n    verbose: bool,\n    selection_method: api.DocSelectionType,\n\
      \    limit: int,\n    max_tokens: int,\n    chunk_size: int,\n    overlap: int,\n\
      \    language: str | None,\n    discover_entity_types: bool,\n    output: Path,\n\
      \    n_subset_max: int,\n    k: int,\n    min_examples_required: int,\n)"
    decorators: []
    raises: []
    calls:
    - target: Path(root).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/cli.py::redact
      type: internal
    - target: graph_config.model_dump
      type: unresolved
    - target: graphrag.api::generate_indexing_prompts
      type: internal
    - target: output.resolve
      type: unresolved
    - target: output_path.mkdir
      type: unresolved
    - target: extract_graph_prompt_path.open
      type: unresolved
    - target: file.write
      type: unresolved
    - target: prompts[0].encode
      type: unresolved
    - target: entity_summarization_prompt_path.open
      type: unresolved
    - target: prompts[1].encode
      type: unresolved
    - target: community_summarization_prompt_path.open
      type: unresolved
    - target: prompts[2].encode
      type: unresolved
    - target: logger.error
      type: unresolved
    visibility: public
    node_id: graphrag/cli/prompt_tune.py::prompt_tune
    called_by:
    - source: graphrag/cli/main.py::_prompt_tune_cli
      type: internal
- file_name: graphrag/cli/query.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: sys
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.api
    name: null
    alias: api
  - module: graphrag.callbacks.noop_query_callbacks
    name: NoopQueryCallbacks
    alias: null
  - module: graphrag.config.load_config
    name: load_config
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: storage_has_table
    alias: null
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: run_global_search
    start_line: 24
    end_line: 133
    code: "def run_global_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int | None,\n    dynamic_community_selection:\
      \ bool,\n    response_type: str,\n    streaming: bool,\n    query: str,\n  \
      \  verbose: bool,\n):\n    \"\"\"Perform a global search with a given query.\n\
      \n    Loads index files required for global search and calls the Query API.\n\
      \    \"\"\"\n    root = root_dir.resolve()\n    cli_overrides = {}\n    if data_dir:\n\
      \        cli_overrides[\"output.base_dir\"] = str(data_dir)\n    config = load_config(root,\
      \ config_filepath, cli_overrides)\n\n    dataframe_dict = _resolve_output_files(\n\
      \        config=config,\n        output_list=[\n            \"entities\",\n\
      \            \"communities\",\n            \"community_reports\",\n        ],\n\
      \        optional_list=[],\n    )\n\n    # Call the Multi-Index Global Search\
      \ API\n    if dataframe_dict[\"multi-index\"]:\n        final_entities_list\
      \ = dataframe_dict[\"entities\"]\n        final_communities_list = dataframe_dict[\"\
      communities\"]\n        final_community_reports_list = dataframe_dict[\"community_reports\"\
      ]\n        index_names = dataframe_dict[\"index_names\"]\n\n        response,\
      \ context_data = asyncio.run(\n            api.multi_index_global_search(\n\
      \                config=config,\n                entities_list=final_entities_list,\n\
      \                communities_list=final_communities_list,\n                community_reports_list=final_community_reports_list,\n\
      \                index_names=index_names,\n                community_level=community_level,\n\
      \                dynamic_community_selection=dynamic_community_selection,\n\
      \                response_type=response_type,\n                streaming=streaming,\n\
      \                query=query,\n                verbose=verbose,\n          \
      \  )\n        )\n        print(response)\n        return response, context_data\n\
      \n    # Otherwise, call the Single-Index Global Search API\n    final_entities:\
      \ pd.DataFrame = dataframe_dict[\"entities\"]\n    final_communities: pd.DataFrame\
      \ = dataframe_dict[\"communities\"]\n    final_community_reports: pd.DataFrame\
      \ = dataframe_dict[\"community_reports\"]\n\n    if streaming:\n\n        async\
      \ def run_streaming_search():\n            full_response = \"\"\n          \
      \  context_data = {}\n\n            def on_context(context: Any) -> None:\n\
      \                nonlocal context_data\n                context_data = context\n\
      \n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.global_search_streaming(\n\
      \                config=config,\n                entities=final_entities,\n\
      \                communities=final_communities,\n                community_reports=final_community_reports,\n\
      \                community_level=community_level,\n                dynamic_community_selection=dynamic_community_selection,\n\
      \                response_type=response_type,\n                query=query,\n\
      \                callbacks=[callbacks],\n                verbose=verbose,\n\
      \            ):\n                full_response += stream_chunk\n           \
      \     print(stream_chunk, end=\"\")\n                sys.stdout.flush()\n  \
      \          print()\n            return full_response, context_data\n\n     \
      \   return asyncio.run(run_streaming_search())\n    # not streaming\n    response,\
      \ context_data = asyncio.run(\n        api.global_search(\n            config=config,\n\
      \            entities=final_entities,\n            communities=final_communities,\n\
      \            community_reports=final_community_reports,\n            community_level=community_level,\n\
      \            dynamic_community_selection=dynamic_community_selection,\n    \
      \        response_type=response_type,\n            query=query,\n          \
      \  verbose=verbose,\n        )\n    )\n    print(response)\n\n    return response,\
      \ context_data"
    signature: "def run_global_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int | None,\n    dynamic_community_selection:\
      \ bool,\n    response_type: str,\n    streaming: bool,\n    query: str,\n  \
      \  verbose: bool,\n)"
    decorators: []
    raises: []
    calls:
    - target: root_dir.resolve
      type: unresolved
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: graphrag.api::multi_index_global_search
      type: internal
    - target: print
      type: builtin
    - target: graphrag/cli/query.py::run_streaming_search
      type: internal
    - target: graphrag.api::global_search
      type: internal
    visibility: public
    node_id: graphrag/cli/query.py::run_global_search
    called_by:
    - source: graphrag/cli/main.py::_query_cli
      type: internal
  - name: run_streaming_search
    start_line: 87
    end_line: 114
    code: "async def run_streaming_search():\n            full_response = \"\"\n \
      \           context_data = {}\n\n            def on_context(context: Any) ->\
      \ None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.global_search_streaming(\n\
      \                config=config,\n                entities=final_entities,\n\
      \                communities=final_communities,\n                community_reports=final_community_reports,\n\
      \                community_level=community_level,\n                dynamic_community_selection=dynamic_community_selection,\n\
      \                response_type=response_type,\n                query=query,\n\
      \                callbacks=[callbacks],\n                verbose=verbose,\n\
      \            ):\n                full_response += stream_chunk\n           \
      \     print(stream_chunk, end=\"\")\n                sys.stdout.flush()\n  \
      \          print()\n            return full_response, context_data"
    signature: def run_streaming_search()
    decorators: []
    raises: []
    calls:
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: graphrag.api::global_search_streaming
      type: internal
    - target: print
      type: builtin
    - target: sys::stdout.flush
      type: stdlib
    visibility: public
    node_id: graphrag/cli/query.py::run_streaming_search
    called_by:
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
  - name: on_context
    start_line: 91
    end_line: 93
    code: "def on_context(context: Any) -> None:\n                nonlocal context_data\n\
      \                context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/cli/query.py::on_context
    called_by: []
  - name: run_local_search
    start_line: 136
    end_line: 265
    code: "def run_local_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n):\n    \"\
      \"\"Perform a local search with a given query.\n\n    Loads index files required\
      \ for local search and calls the Query API.\n    \"\"\"\n    root = root_dir.resolve()\n\
      \    cli_overrides = {}\n    if data_dir:\n        cli_overrides[\"output.base_dir\"\
      ] = str(data_dir)\n    config = load_config(root, config_filepath, cli_overrides)\n\
      \n    dataframe_dict = _resolve_output_files(\n        config=config,\n    \
      \    output_list=[\n            \"communities\",\n            \"community_reports\"\
      ,\n            \"text_units\",\n            \"relationships\",\n           \
      \ \"entities\",\n        ],\n        optional_list=[\n            \"covariates\"\
      ,\n        ],\n    )\n    # Call the Multi-Index Local Search API\n    if dataframe_dict[\"\
      multi-index\"]:\n        final_entities_list = dataframe_dict[\"entities\"]\n\
      \        final_communities_list = dataframe_dict[\"communities\"]\n        final_community_reports_list\
      \ = dataframe_dict[\"community_reports\"]\n        final_text_units_list = dataframe_dict[\"\
      text_units\"]\n        final_relationships_list = dataframe_dict[\"relationships\"\
      ]\n        index_names = dataframe_dict[\"index_names\"]\n\n        # If any\
      \ covariates tables are missing from any index, set the covariates list to None\n\
      \        if len(dataframe_dict[\"covariates\"]) != dataframe_dict[\"num_indexes\"\
      ]:\n            final_covariates_list = None\n        else:\n            final_covariates_list\
      \ = dataframe_dict[\"covariates\"]\n\n        response, context_data = asyncio.run(\n\
      \            api.multi_index_local_search(\n                config=config,\n\
      \                entities_list=final_entities_list,\n                communities_list=final_communities_list,\n\
      \                community_reports_list=final_community_reports_list,\n    \
      \            text_units_list=final_text_units_list,\n                relationships_list=final_relationships_list,\n\
      \                covariates_list=final_covariates_list,\n                index_names=index_names,\n\
      \                community_level=community_level,\n                response_type=response_type,\n\
      \                streaming=streaming,\n                query=query,\n      \
      \          verbose=verbose,\n            )\n        )\n        print(response)\n\
      \n        return response, context_data\n\n    # Otherwise, call the Single-Index\
      \ Local Search API\n    final_communities: pd.DataFrame = dataframe_dict[\"\
      communities\"]\n    final_community_reports: pd.DataFrame = dataframe_dict[\"\
      community_reports\"]\n    final_text_units: pd.DataFrame = dataframe_dict[\"\
      text_units\"]\n    final_relationships: pd.DataFrame = dataframe_dict[\"relationships\"\
      ]\n    final_entities: pd.DataFrame = dataframe_dict[\"entities\"]\n    final_covariates:\
      \ pd.DataFrame | None = dataframe_dict[\"covariates\"]\n\n    if streaming:\n\
      \n        async def run_streaming_search():\n            full_response = \"\"\
      \n            context_data = {}\n\n            def on_context(context: Any)\
      \ -> None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.local_search_streaming(\n\
      \                config=config,\n                entities=final_entities,\n\
      \                communities=final_communities,\n                community_reports=final_community_reports,\n\
      \                text_units=final_text_units,\n                relationships=final_relationships,\n\
      \                covariates=final_covariates,\n                community_level=community_level,\n\
      \                response_type=response_type,\n                query=query,\n\
      \                callbacks=[callbacks],\n                verbose=verbose,\n\
      \            ):\n                full_response += stream_chunk\n           \
      \     print(stream_chunk, end=\"\")\n                sys.stdout.flush()\n  \
      \          print()\n            return full_response, context_data\n\n     \
      \   return asyncio.run(run_streaming_search())\n    # not streaming\n    response,\
      \ context_data = asyncio.run(\n        api.local_search(\n            config=config,\n\
      \            entities=final_entities,\n            communities=final_communities,\n\
      \            community_reports=final_community_reports,\n            text_units=final_text_units,\n\
      \            relationships=final_relationships,\n            covariates=final_covariates,\n\
      \            community_level=community_level,\n            response_type=response_type,\n\
      \            query=query,\n            verbose=verbose,\n        )\n    )\n\
      \    print(response)\n\n    return response, context_data"
    signature: "def run_local_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
    decorators: []
    raises: []
    calls:
    - target: root_dir.resolve
      type: unresolved
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - target: len
      type: builtin
    - target: asyncio::run
      type: stdlib
    - target: graphrag.api::multi_index_local_search
      type: internal
    - target: print
      type: builtin
    - target: graphrag/cli/query.py::run_streaming_search
      type: internal
    - target: graphrag.api::local_search
      type: internal
    visibility: public
    node_id: graphrag/cli/query.py::run_local_search
    called_by:
    - source: graphrag/cli/main.py::_query_cli
      type: internal
  - name: run_streaming_search
    start_line: 215
    end_line: 244
    code: "async def run_streaming_search():\n            full_response = \"\"\n \
      \           context_data = {}\n\n            def on_context(context: Any) ->\
      \ None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.local_search_streaming(\n\
      \                config=config,\n                entities=final_entities,\n\
      \                communities=final_communities,\n                community_reports=final_community_reports,\n\
      \                text_units=final_text_units,\n                relationships=final_relationships,\n\
      \                covariates=final_covariates,\n                community_level=community_level,\n\
      \                response_type=response_type,\n                query=query,\n\
      \                callbacks=[callbacks],\n                verbose=verbose,\n\
      \            ):\n                full_response += stream_chunk\n           \
      \     print(stream_chunk, end=\"\")\n                sys.stdout.flush()\n  \
      \          print()\n            return full_response, context_data"
    signature: def run_streaming_search()
    decorators: []
    raises: []
    calls:
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: graphrag.api::local_search_streaming
      type: internal
    - target: print
      type: builtin
    - target: sys::stdout.flush
      type: stdlib
    visibility: public
    node_id: graphrag/cli/query.py::run_streaming_search
    called_by:
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
  - name: on_context
    start_line: 219
    end_line: 221
    code: "def on_context(context: Any) -> None:\n                nonlocal context_data\n\
      \                context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/cli/query.py::on_context
    called_by: []
  - name: run_drift_search
    start_line: 268
    end_line: 386
    code: "def run_drift_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n):\n    \"\
      \"\"Perform a local search with a given query.\n\n    Loads index files required\
      \ for local search and calls the Query API.\n    \"\"\"\n    root = root_dir.resolve()\n\
      \    cli_overrides = {}\n    if data_dir:\n        cli_overrides[\"output.base_dir\"\
      ] = str(data_dir)\n    config = load_config(root, config_filepath, cli_overrides)\n\
      \n    dataframe_dict = _resolve_output_files(\n        config=config,\n    \
      \    output_list=[\n            \"communities\",\n            \"community_reports\"\
      ,\n            \"text_units\",\n            \"relationships\",\n           \
      \ \"entities\",\n        ],\n    )\n\n    # Call the Multi-Index Drift Search\
      \ API\n    if dataframe_dict[\"multi-index\"]:\n        final_entities_list\
      \ = dataframe_dict[\"entities\"]\n        final_communities_list = dataframe_dict[\"\
      communities\"]\n        final_community_reports_list = dataframe_dict[\"community_reports\"\
      ]\n        final_text_units_list = dataframe_dict[\"text_units\"]\n        final_relationships_list\
      \ = dataframe_dict[\"relationships\"]\n        index_names = dataframe_dict[\"\
      index_names\"]\n\n        response, context_data = asyncio.run(\n          \
      \  api.multi_index_drift_search(\n                config=config,\n         \
      \       entities_list=final_entities_list,\n                communities_list=final_communities_list,\n\
      \                community_reports_list=final_community_reports_list,\n    \
      \            text_units_list=final_text_units_list,\n                relationships_list=final_relationships_list,\n\
      \                index_names=index_names,\n                community_level=community_level,\n\
      \                response_type=response_type,\n                streaming=streaming,\n\
      \                query=query,\n                verbose=verbose,\n          \
      \  )\n        )\n        print(response)\n\n        return response, context_data\n\
      \n    # Otherwise, call the Single-Index Drift Search API\n    final_communities:\
      \ pd.DataFrame = dataframe_dict[\"communities\"]\n    final_community_reports:\
      \ pd.DataFrame = dataframe_dict[\"community_reports\"]\n    final_text_units:\
      \ pd.DataFrame = dataframe_dict[\"text_units\"]\n    final_relationships: pd.DataFrame\
      \ = dataframe_dict[\"relationships\"]\n    final_entities: pd.DataFrame = dataframe_dict[\"\
      entities\"]\n\n    if streaming:\n\n        async def run_streaming_search():\n\
      \            full_response = \"\"\n            context_data = {}\n\n       \
      \     def on_context(context: Any) -> None:\n                nonlocal context_data\n\
      \                context_data = context\n\n            callbacks = NoopQueryCallbacks()\n\
      \            callbacks.on_context = on_context\n\n            async for stream_chunk\
      \ in api.drift_search_streaming(\n                config=config,\n         \
      \       entities=final_entities,\n                communities=final_communities,\n\
      \                community_reports=final_community_reports,\n              \
      \  text_units=final_text_units,\n                relationships=final_relationships,\n\
      \                community_level=community_level,\n                response_type=response_type,\n\
      \                query=query,\n                callbacks=[callbacks],\n    \
      \            verbose=verbose,\n            ):\n                full_response\
      \ += stream_chunk\n                print(stream_chunk, end=\"\")\n         \
      \       sys.stdout.flush()\n            print()\n            return full_response,\
      \ context_data\n\n        return asyncio.run(run_streaming_search())\n\n   \
      \ # not streaming\n    response, context_data = asyncio.run(\n        api.drift_search(\n\
      \            config=config,\n            entities=final_entities,\n        \
      \    communities=final_communities,\n            community_reports=final_community_reports,\n\
      \            text_units=final_text_units,\n            relationships=final_relationships,\n\
      \            community_level=community_level,\n            response_type=response_type,\n\
      \            query=query,\n            verbose=verbose,\n        )\n    )\n\
      \    print(response)\n\n    return response, context_data"
    signature: "def run_drift_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
    decorators: []
    raises: []
    calls:
    - target: root_dir.resolve
      type: unresolved
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: graphrag.api::multi_index_drift_search
      type: internal
    - target: print
      type: builtin
    - target: graphrag/cli/query.py::run_streaming_search
      type: internal
    - target: graphrag.api::drift_search
      type: internal
    visibility: public
    node_id: graphrag/cli/query.py::run_drift_search
    called_by:
    - source: graphrag/cli/main.py::_query_cli
      type: internal
  - name: run_streaming_search
    start_line: 337
    end_line: 365
    code: "async def run_streaming_search():\n            full_response = \"\"\n \
      \           context_data = {}\n\n            def on_context(context: Any) ->\
      \ None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.drift_search_streaming(\n\
      \                config=config,\n                entities=final_entities,\n\
      \                communities=final_communities,\n                community_reports=final_community_reports,\n\
      \                text_units=final_text_units,\n                relationships=final_relationships,\n\
      \                community_level=community_level,\n                response_type=response_type,\n\
      \                query=query,\n                callbacks=[callbacks],\n    \
      \            verbose=verbose,\n            ):\n                full_response\
      \ += stream_chunk\n                print(stream_chunk, end=\"\")\n         \
      \       sys.stdout.flush()\n            print()\n            return full_response,\
      \ context_data"
    signature: def run_streaming_search()
    decorators: []
    raises: []
    calls:
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: graphrag.api::drift_search_streaming
      type: internal
    - target: print
      type: builtin
    - target: sys::stdout.flush
      type: stdlib
    visibility: public
    node_id: graphrag/cli/query.py::run_streaming_search
    called_by:
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
  - name: on_context
    start_line: 341
    end_line: 343
    code: "def on_context(context: Any) -> None:\n                nonlocal context_data\n\
      \                context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/cli/query.py::on_context
    called_by: []
  - name: run_basic_search
    start_line: 389
    end_line: 474
    code: "def run_basic_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    streaming: bool,\n    query: str,\n\
      \    verbose: bool,\n):\n    \"\"\"Perform a basics search with a given query.\n\
      \n    Loads index files required for basic search and calls the Query API.\n\
      \    \"\"\"\n    root = root_dir.resolve()\n    cli_overrides = {}\n    if data_dir:\n\
      \        cli_overrides[\"output.base_dir\"] = str(data_dir)\n    config = load_config(root,\
      \ config_filepath, cli_overrides)\n\n    dataframe_dict = _resolve_output_files(\n\
      \        config=config,\n        output_list=[\n            \"text_units\",\n\
      \        ],\n    )\n\n    # Call the Multi-Index Basic Search API\n    if dataframe_dict[\"\
      multi-index\"]:\n        final_text_units_list = dataframe_dict[\"text_units\"\
      ]\n        index_names = dataframe_dict[\"index_names\"]\n\n        response,\
      \ context_data = asyncio.run(\n            api.multi_index_basic_search(\n \
      \               config=config,\n                text_units_list=final_text_units_list,\n\
      \                index_names=index_names,\n                streaming=streaming,\n\
      \                query=query,\n                verbose=verbose,\n          \
      \  )\n        )\n        print(response)\n\n        return response, context_data\n\
      \n    # Otherwise, call the Single-Index Basic Search API\n    final_text_units:\
      \ pd.DataFrame = dataframe_dict[\"text_units\"]\n\n    if streaming:\n\n   \
      \     async def run_streaming_search():\n            full_response = \"\"\n\
      \            context_data = {}\n\n            def on_context(context: Any) ->\
      \ None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.basic_search_streaming(\n\
      \                config=config,\n                text_units=final_text_units,\n\
      \                query=query,\n                callbacks=[callbacks],\n    \
      \            verbose=verbose,\n            ):\n                full_response\
      \ += stream_chunk\n                print(stream_chunk, end=\"\")\n         \
      \       sys.stdout.flush()\n            print()\n            return full_response,\
      \ context_data\n\n        return asyncio.run(run_streaming_search())\n    #\
      \ not streaming\n    response, context_data = asyncio.run(\n        api.basic_search(\n\
      \            config=config,\n            text_units=final_text_units,\n    \
      \        query=query,\n            verbose=verbose,\n        )\n    )\n    print(response)\n\
      \n    return response, context_data"
    signature: "def run_basic_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    streaming: bool,\n    query: str,\n\
      \    verbose: bool,\n)"
    decorators: []
    raises: []
    calls:
    - target: root_dir.resolve
      type: unresolved
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: graphrag.api::multi_index_basic_search
      type: internal
    - target: print
      type: builtin
    - target: graphrag/cli/query.py::run_streaming_search
      type: internal
    - target: graphrag.api::basic_search
      type: internal
    visibility: public
    node_id: graphrag/cli/query.py::run_basic_search
    called_by:
    - source: graphrag/cli/main.py::_query_cli
      type: internal
  - name: run_streaming_search
    start_line: 438
    end_line: 460
    code: "async def run_streaming_search():\n            full_response = \"\"\n \
      \           context_data = {}\n\n            def on_context(context: Any) ->\
      \ None:\n                nonlocal context_data\n                context_data\
      \ = context\n\n            callbacks = NoopQueryCallbacks()\n            callbacks.on_context\
      \ = on_context\n\n            async for stream_chunk in api.basic_search_streaming(\n\
      \                config=config,\n                text_units=final_text_units,\n\
      \                query=query,\n                callbacks=[callbacks],\n    \
      \            verbose=verbose,\n            ):\n                full_response\
      \ += stream_chunk\n                print(stream_chunk, end=\"\")\n         \
      \       sys.stdout.flush()\n            print()\n            return full_response,\
      \ context_data"
    signature: def run_streaming_search()
    decorators: []
    raises: []
    calls:
    - target: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
      type: internal
    - target: graphrag.api::basic_search_streaming
      type: internal
    - target: print
      type: builtin
    - target: sys::stdout.flush
      type: stdlib
    visibility: public
    node_id: graphrag/cli/query.py::run_streaming_search
    called_by:
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
  - name: on_context
    start_line: 442
    end_line: 444
    code: "def on_context(context: Any) -> None:\n                nonlocal context_data\n\
      \                context_data = context"
    signature: 'def on_context(context: Any) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/cli/query.py::on_context
    called_by: []
  - name: _resolve_output_files
    start_line: 477
    end_line: 534
    code: "def _resolve_output_files(\n    config: GraphRagConfig,\n    output_list:\
      \ list[str],\n    optional_list: list[str] | None = None,\n) -> dict[str, Any]:\n\
      \    \"\"\"Read indexing output files to a dataframe dict.\"\"\"\n    dataframe_dict\
      \ = {}\n\n    # Loading output files for multi-index search\n    if config.outputs:\n\
      \        dataframe_dict[\"multi-index\"] = True\n        dataframe_dict[\"num_indexes\"\
      ] = len(config.outputs)\n        dataframe_dict[\"index_names\"] = config.outputs.keys()\n\
      \        for output in config.outputs.values():\n            storage_obj = create_storage_from_config(output)\n\
      \            for name in output_list:\n                if name not in dataframe_dict:\n\
      \                    dataframe_dict[name] = []\n                df_value = asyncio.run(\n\
      \                    load_table_from_storage(name=name, storage=storage_obj)\n\
      \                )\n                dataframe_dict[name].append(df_value)\n\n\
      \            # for optional output files, do not append if the dataframe does\
      \ not exist\n            if optional_list:\n                for optional_file\
      \ in optional_list:\n                    if optional_file not in dataframe_dict:\n\
      \                        dataframe_dict[optional_file] = []\n              \
      \      file_exists = asyncio.run(\n                        storage_has_table(optional_file,\
      \ storage_obj)\n                    )\n                    if file_exists:\n\
      \                        df_value = asyncio.run(\n                         \
      \   load_table_from_storage(\n                                name=optional_file,\
      \ storage=storage_obj\n                            )\n                     \
      \   )\n                        dataframe_dict[optional_file].append(df_value)\n\
      \        return dataframe_dict\n    # Loading output files for single-index\
      \ search\n    dataframe_dict[\"multi-index\"] = False\n    storage_obj = create_storage_from_config(config.output)\n\
      \    for name in output_list:\n        df_value = asyncio.run(load_table_from_storage(name=name,\
      \ storage=storage_obj))\n        dataframe_dict[name] = df_value\n\n    # for\
      \ optional output files, set the dict entry to None instead of erroring out\
      \ if it does not exist\n    if optional_list:\n        for optional_file in\
      \ optional_list:\n            file_exists = asyncio.run(storage_has_table(optional_file,\
      \ storage_obj))\n            if file_exists:\n                df_value = asyncio.run(\n\
      \                    load_table_from_storage(name=optional_file, storage=storage_obj)\n\
      \                )\n                dataframe_dict[optional_file] = df_value\n\
      \            else:\n                dataframe_dict[optional_file] = None\n \
      \   return dataframe_dict"
    signature: "def _resolve_output_files(\n    config: GraphRagConfig,\n    output_list:\
      \ list[str],\n    optional_list: list[str] | None = None,\n) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: config.outputs.keys
      type: unresolved
    - target: config.outputs.values
      type: unresolved
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: dataframe_dict[name].append
      type: unresolved
    - target: graphrag/utils/storage.py::storage_has_table
      type: internal
    - target: dataframe_dict[optional_file].append
      type: unresolved
    visibility: protected
    node_id: graphrag/cli/query.py::_resolve_output_files
    called_by:
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
- file_name: graphrag/config/__init__.py
  imports: []
  functions: []
- file_name: graphrag/config/create_graphrag_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: create_graphrag_config
    start_line: 12
    end_line: 43
    code: "def create_graphrag_config(\n    values: dict[str, Any] | None = None,\n\
      \    root_dir: str | None = None,\n) -> GraphRagConfig:\n    \"\"\"Load Configuration\
      \ Parameters from a dictionary.\n\n    Parameters\n    ----------\n    values\
      \ : dict[str, Any] | None\n        Dictionary of configuration values to pass\
      \ into pydantic model.\n    root_dir : str | None\n        Root directory for\
      \ the project.\n    skip_validation : bool\n        Skip pydantic model validation\
      \ of the configuration.\n        This is useful for testing and mocking purposes\
      \ but\n        should not be used in the core code or API.\n\n    Returns\n\
      \    -------\n    GraphRagConfig\n        The configuration object.\n\n    Raises\n\
      \    ------\n    ValidationError\n        If the configuration values do not\
      \ satisfy pydantic validation.\n    \"\"\"\n    values = values or {}\n    if\
      \ root_dir:\n        root_path = Path(root_dir).resolve()\n        values[\"\
      root_dir\"] = str(root_path)\n    return GraphRagConfig(**values)"
    signature: "def create_graphrag_config(\n    values: dict[str, Any] | None = None,\n\
      \    root_dir: str | None = None,\n) -> GraphRagConfig"
    decorators: []
    raises: []
    calls:
    - target: Path(root_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: str
      type: builtin
    - target: graphrag/config/models/graph_rag_config.py::GraphRagConfig
      type: internal
    visibility: public
    node_id: graphrag/config/create_graphrag_config.py::create_graphrag_config
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
    - source: tests/unit/config/test_config.py::test_missing_openai_required_api_key
      type: internal
    - source: tests/unit/config/test_config.py::test_missing_azure_api_key
      type: internal
    - source: tests/unit/config/test_config.py::test_conflicting_auth_type
      type: internal
    - source: tests/unit/config/test_config.py::test_conflicting_azure_api_key
      type: internal
    - source: tests/unit/config/test_config.py::test_missing_azure_api_base
      type: internal
    - source: tests/unit/config/test_config.py::test_missing_azure_api_version
      type: internal
    - source: tests/unit/config/test_config.py::test_default_config
      type: internal
    - source: tests/unit/indexing/test_init_content.py::test_init_yaml
      type: internal
    - source: tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
    - source: tests/verbs/test_extract_covariates.py::test_extract_covariates
      type: internal
    - source: tests/verbs/test_extract_graph.py::test_extract_graph
      type: internal
    - source: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
      type: internal
    - source: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
      type: internal
    - source: tests/verbs/test_pipeline_state.py::test_pipeline_state
      type: internal
    - source: tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
      type: internal
    - source: tests/verbs/test_prune_graph.py::test_prune_graph
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
      type: internal
- file_name: graphrag/config/defaults.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: dataclasses
    name: field
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.embeddings
    name: default_embeddings
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.enums
    name: AuthType
    alias: null
  - module: graphrag.config.enums
    name: CacheType
    alias: null
  - module: graphrag.config.enums
    name: ChunkStrategyType
    alias: null
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.config.enums
    name: NounPhraseExtractorType
    alias: null
  - module: graphrag.config.enums
    name: ReportingType
    alias: null
  - module: graphrag.config.enums
    name: StorageType
    alias: null
  - module: graphrag.config.enums
    name: VectorStoreType
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.stop_words
    name: EN_STOP_WORDS
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter
    name: RateLimiter
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.static_rate_limiter
    name: StaticRateLimiter
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.exponential_retry
    name: ExponentialRetry
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.incremental_wait_retry
    name: IncrementalWaitRetry
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.native_wait_retry
    name: NativeRetry
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.random_wait_retry
    name: RandomWaitRetry
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions: []
- file_name: graphrag/config/embeddings.py
  imports: []
  functions:
  - name: create_index_name
    start_line: 32
    end_line: 48
    code: "def create_index_name(\n    container_name: str, embedding_name: str, validate:\
      \ bool = True\n) -> str:\n    \"\"\"\n    Create a index name for the embedding\
      \ store.\n\n    Within any given vector store, we can have multiple sets of\
      \ embeddings organized into projects.\n    The `container` param is used for\
      \ this partitioning, and is added as a prefix to the index name for differentiation.\n\
      \n    The embedding name is fixed, with the available list defined in graphrag.index.config.embeddings\n\
      \n    Note that we use dot notation in our names, but many vector stores do\
      \ not support this - so we convert to dashes.\n    \"\"\"\n    if validate and\
      \ embedding_name not in all_embeddings:\n        msg = f\"Invalid embedding\
      \ name: {embedding_name}\"\n        raise KeyError(msg)\n    return f\"{container_name}-{embedding_name}\"\
      .replace(\".\", \"-\")"
    signature: "def create_index_name(\n    container_name: str, embedding_name: str,\
      \ validate: bool = True\n) -> str"
    decorators: []
    raises:
    - KeyError
    calls:
    - target: KeyError
      type: builtin
    - target: f"{container_name}-{embedding_name}".replace
      type: unresolved
    visibility: public
    node_id: graphrag/config/embeddings.py::create_index_name
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::_get_index_name
      type: internal
    - source: graphrag/utils/api.py::get_embedding_store
      type: internal
    - source: tests/unit/utils/test_embeddings.py::test_create_index_name
      type: internal
    - source: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_throws
      type: internal
    - source: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_does_not_throw
      type: internal
- file_name: graphrag/config/enums.py
  imports:
  - module: enum
    name: Enum
    alias: null
  functions:
  - name: __repr__
    start_line: 25
    end_line: 27
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::CacheType.__repr__
    called_by: []
  - name: __repr__
    start_line: 40
    end_line: 42
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::InputFileType.__repr__
    called_by: []
  - name: __repr__
    start_line: 57
    end_line: 59
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::StorageType.__repr__
    called_by: []
  - name: __repr__
    start_line: 78
    end_line: 80
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::ReportingType.__repr__
    called_by: []
  - name: __repr__
    start_line: 100
    end_line: 102
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::ModelType.__repr__
    called_by: []
  - name: __repr__
    start_line: 125
    end_line: 127
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::ChunkStrategyType.__repr__
    called_by: []
  - name: __str__
    start_line: 138
    end_line: 140
    code: "def __str__(self):\n        \"\"\"Return the string representation of the\
      \ enum value.\"\"\"\n        return self.value"
    signature: def __str__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/enums.py::SearchMethod.__str__
    called_by: []
- file_name: graphrag/config/environment_reader.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  - module: contextlib
    name: contextmanager
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: environs
    name: Env
    alias: null
  functions:
  - name: read_key
    start_line: 19
    end_line: 23
    code: "def read_key(value: KeyValue) -> str:\n    \"\"\"Read a key value.\"\"\"\
      \n    if not isinstance(value, str):\n        return value.value.lower()\n \
      \   return value.lower()"
    signature: 'def read_key(value: KeyValue) -> str'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: value.value.lower
      type: unresolved
    - target: value.lower
      type: unresolved
    visibility: public
    node_id: graphrag/config/environment_reader.py::read_key
    called_by:
    - source: graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
      type: internal
    - source: graphrag/config/environment_reader.py::EnvironmentReader.str
      type: internal
    - source: graphrag/config/environment_reader.py::EnvironmentReader.int
      type: internal
    - source: graphrag/config/environment_reader.py::EnvironmentReader.bool
      type: internal
    - source: graphrag/config/environment_reader.py::EnvironmentReader.float
      type: internal
    - source: graphrag/config/environment_reader.py::EnvironmentReader.list
      type: internal
  - name: __init__
    start_line: 32
    end_line: 34
    code: "def __init__(self, env: Env):\n        self._env = env\n        self._config_stack\
      \ = []"
    signature: 'def __init__(self, env: Env)'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.__init__
    called_by: []
  - name: env
    start_line: 37
    end_line: 39
    code: "def env(self):\n        \"\"\"Get the environment object.\"\"\"\n     \
      \   return self._env"
    signature: def env(self)
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.env
    called_by: []
  - name: _read_env
    start_line: 41
    end_line: 52
    code: "def _read_env(\n        self, env_key: str | list[str], default_value:\
      \ T, read: Callable[[str, T], T]\n    ) -> T | None:\n        if isinstance(env_key,\
      \ str):\n            env_key = [env_key]\n\n        for k in env_key:\n    \
      \        result = read(k.upper(), default_value)\n            if result is not\
      \ default_value:\n                return result\n\n        return default_value"
    signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
      \ T, read: Callable[[str, T], T]\n    ) -> T | None"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: read
      type: ambiguous
      candidates:
      - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
      - unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read
      - unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read
    - target: k.upper
      type: unresolved
    visibility: protected
    node_id: graphrag/config/environment_reader.py::EnvironmentReader._read_env
    called_by: []
  - name: envvar_prefix
    start_line: 54
    end_line: 58
    code: "def envvar_prefix(self, prefix: KeyValue):\n        \"\"\"Set the environment\
      \ variable prefix.\"\"\"\n        prefix = read_key(prefix)\n        prefix\
      \ = f\"{prefix}_\".upper()\n        return self._env.prefixed(prefix)"
    signature: 'def envvar_prefix(self, prefix: KeyValue)'
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: f"{prefix}_".upper
      type: unresolved
    - target: self._env.prefixed
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
    called_by: []
  - name: use
    start_line: 60
    end_line: 71
    code: "def use(self, value: Any | None):\n        \"\"\"Create a context manager\
      \ to push the value into the config_stack.\"\"\"\n\n        @contextmanager\n\
      \        def config_context():\n            self._config_stack.append(value\
      \ or {})\n            try:\n                yield\n            finally:\n  \
      \              self._config_stack.pop()\n\n        return config_context()"
    signature: 'def use(self, value: Any | None)'
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::config_context
      type: internal
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.use
    called_by: []
  - name: config_context
    start_line: 64
    end_line: 69
    code: "def config_context():\n            self._config_stack.append(value or {})\n\
      \            try:\n                yield\n            finally:\n           \
      \     self._config_stack.pop()"
    signature: def config_context()
    decorators:
    - '@contextmanager'
    raises: []
    calls:
    - target: self._config_stack.append
      type: instance
    - target: self._config_stack.pop
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.config_context
    called_by: []
  - name: section
    start_line: 74
    end_line: 76
    code: "def section(self) -> dict:\n        \"\"\"Get the current section.\"\"\"\
      \n        return self._config_stack[-1] if self._config_stack else {}"
    signature: def section(self) -> dict
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.section
    called_by: []
  - name: str
    start_line: 78
    end_line: 91
    code: "def str(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
      \ | None = None,\n        default_value: str | None = None,\n    ) -> str |\
      \ None:\n        \"\"\"Read a configuration value.\"\"\"\n        key = read_key(key)\n\
      \        if self.section and key in self.section:\n            return self.section[key]\n\
      \n        return self._read_env(\n            env_key or key, default_value,\
      \ (lambda k, dv: self._env(k, dv))\n        )"
    signature: "def str(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: str | None = None,\n    )\
      \ -> str | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: graphrag/config/environment_reader.py::_read_env
      type: internal
    - target: self._env
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.str
    called_by: []
  - name: int
    start_line: 93
    end_line: 105
    code: "def int(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
      \ | None = None,\n        default_value: int | None = None,\n    ) -> int |\
      \ None:\n        \"\"\"Read an integer configuration value.\"\"\"\n        key\
      \ = read_key(key)\n        if self.section and key in self.section:\n      \
      \      return int(self.section[key])\n        return self._read_env(\n     \
      \       env_key or key, default_value, lambda k, dv: self._env.int(k, dv)\n\
      \        )"
    signature: "def int(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: int | None = None,\n    )\
      \ -> int | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: int
      type: builtin
    - target: graphrag/config/environment_reader.py::_read_env
      type: internal
    - target: self._env.int
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.int
    called_by: []
  - name: bool
    start_line: 107
    end_line: 120
    code: "def bool(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
      \ | None = None,\n        default_value: bool | None = None,\n    ) -> bool\
      \ | None:\n        \"\"\"Read an integer configuration value.\"\"\"\n      \
      \  key = read_key(key)\n        if self.section and key in self.section:\n \
      \           return bool(self.section[key])\n\n        return self._read_env(\n\
      \            env_key or key, default_value, lambda k, dv: self._env.bool(k,\
      \ dv)\n        )"
    signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: bool | None = None,\n   \
      \ ) -> bool | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: bool
      type: builtin
    - target: graphrag/config/environment_reader.py::_read_env
      type: internal
    - target: self._env.bool
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.bool
    called_by: []
  - name: float
    start_line: 122
    end_line: 134
    code: "def float(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
      \ | None = None,\n        default_value: float | None = None,\n    ) -> float\
      \ | None:\n        \"\"\"Read a float configuration value.\"\"\"\n        key\
      \ = read_key(key)\n        if self.section and key in self.section:\n      \
      \      return float(self.section[key])\n        return self._read_env(\n   \
      \         env_key or key, default_value, lambda k, dv: self._env.float(k, dv)\n\
      \        )"
    signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: float | None = None,\n  \
      \  ) -> float | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: float
      type: builtin
    - target: graphrag/config/environment_reader.py::_read_env
      type: internal
    - target: self._env.float
      type: instance
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.float
    called_by: []
  - name: list
    start_line: 136
    end_line: 155
    code: "def list(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
      \ | None = None,\n        default_value: list | None = None,\n    ) -> list\
      \ | None:\n        \"\"\"Parse an list configuration value.\"\"\"\n        key\
      \ = read_key(key)\n        result = None\n        if self.section and key in\
      \ self.section:\n            result = self.section[key]\n            if isinstance(result,\
      \ list):\n                return result\n\n        if result is None:\n    \
      \        result = self.str(key, env_key)\n        if result is not None:\n \
      \           result = [s.strip() for s in result.split(\",\")]\n            return\
      \ [s for s in result if s]\n        return default_value"
    signature: "def list(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: list | None = None,\n   \
      \ ) -> list | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/environment_reader.py::read_key
      type: internal
    - target: isinstance
      type: builtin
    - target: graphrag/config/environment_reader.py::str
      type: internal
    - target: s.strip
      type: unresolved
    - target: result.split
      type: unresolved
    visibility: public
    node_id: graphrag/config/environment_reader.py::EnvironmentReader.list
    called_by: []
- file_name: graphrag/config/errors.py
  imports: []
  functions:
  - name: __init__
    start_line: 9
    end_line: 15
    code: "def __init__(self, llm_type: str, azure_auth_type: str | None = None) ->\
      \ None:\n        \"\"\"Init method definition.\"\"\"\n        msg = f\"API Key\
      \ is required for {llm_type}\"\n        if azure_auth_type:\n            msg\
      \ += f\" when using {azure_auth_type} authentication\"\n        msg += \". Please\
      \ rerun `graphrag init` and set the API_KEY.\"\n        super().__init__(msg)"
    signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None = None)
      -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/config/errors.py::ApiKeyMissingError.__init__
    called_by: []
  - name: __init__
    start_line: 21
    end_line: 24
    code: "def __init__(self, llm_type: str) -> None:\n        \"\"\"Init method definition.\"\
      \"\"\n        msg = f\"API Base is required for {llm_type}. Please rerun `graphrag\
      \ init` and set the api_base.\"\n        super().__init__(msg)"
    signature: 'def __init__(self, llm_type: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/config/errors.py::AzureApiBaseMissingError.__init__
    called_by: []
  - name: __init__
    start_line: 30
    end_line: 33
    code: "def __init__(self, llm_type: str) -> None:\n        \"\"\"Init method definition.\"\
      \"\"\n        msg = f\"API Version is required for {llm_type}. Please rerun\
      \ `graphrag init` and set the api_version.\"\n        super().__init__(msg)"
    signature: 'def __init__(self, llm_type: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/config/errors.py::AzureApiVersionMissingError.__init__
    called_by: []
  - name: __init__
    start_line: 39
    end_line: 42
    code: "def __init__(self, key: str = \"\") -> None:\n        \"\"\"Init method\
      \ definition.\"\"\"\n        msg = f'A {key} model configuration is required.\
      \ Please rerun `graphrag init` and set models[\"{key}\"] in settings.yaml.'\n\
      \        super().__init__(msg)"
    signature: 'def __init__(self, key: str = "") -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/config/errors.py::LanguageModelConfigMissingError.__init__
    called_by: []
  - name: __init__
    start_line: 48
    end_line: 50
    code: "def __init__(self, msg: str) -> None:\n        \"\"\"Init method definition.\"\
      \"\"\n        super().__init__(msg)"
    signature: 'def __init__(self, msg: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/config/errors.py::ConflictingSettingsError.__init__
    called_by: []
- file_name: graphrag/config/get_embedding_settings.py
  imports:
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: get_embedding_settings
    start_line: 9
    end_line: 38
    code: "def get_embedding_settings(\n    settings: GraphRagConfig,\n    vector_store_params:\
      \ dict | None = None,\n) -> dict:\n    \"\"\"Transform GraphRAG config into\
      \ settings for workflows.\"\"\"\n    embeddings_llm_settings = settings.get_language_model_config(\n\
      \        settings.embed_text.model_id\n    )\n    vector_store_settings = settings.get_vector_store_config(\n\
      \        settings.embed_text.vector_store_id\n    ).model_dump()\n\n    #\n\
      \    # If we get to this point, settings.vector_store is defined, and there's\
      \ a specific setting for this embedding.\n    # settings.vector_store.base contains\
      \ connection information, or may be undefined\n    # settings.vector_store.<vector_name>\
      \ contains the specific settings for this embedding\n    #\n    strategy = settings.embed_text.resolved_strategy(\n\
      \        embeddings_llm_settings\n    )  # get the default strategy\n    strategy.update({\n\
      \        \"vector_store\": {\n            **(vector_store_params or {}),\n \
      \           **(vector_store_settings),\n        }\n    })  # update the default\
      \ strategy with the vector store settings\n    # This ensures the vector store\
      \ config is part of the strategy and not the global config\n    return {\n \
      \       \"strategy\": strategy,\n    }"
    signature: "def get_embedding_settings(\n    settings: GraphRagConfig,\n    vector_store_params:\
      \ dict | None = None,\n) -> dict"
    decorators: []
    raises: []
    calls:
    - target: settings.get_language_model_config
      type: unresolved
    - target: "settings.get_vector_store_config(\n        settings.embed_text.vector_store_id\n\
        \    ).model_dump"
      type: unresolved
    - target: settings.get_vector_store_config
      type: unresolved
    - target: settings.embed_text.resolved_strategy
      type: unresolved
    - target: strategy.update
      type: unresolved
    visibility: public
    node_id: graphrag/config/get_embedding_settings.py::get_embedding_settings
    called_by:
    - source: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_text_embeddings.py::run_workflow
      type: internal
- file_name: graphrag/config/init_content.py
  imports:
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.defaults
    name: language_model_defaults
    alias: null
  - module: graphrag.config.defaults
    name: vector_store_defaults
    alias: null
  functions: []
- file_name: graphrag/config/load_config.py
  imports:
  - module: json
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: string
    name: Template
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: yaml
    name: null
    alias: null
  - module: dotenv
    name: load_dotenv
    alias: null
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: _search_for_config_in_root_dir
    start_line: 21
    end_line: 46
    code: "def _search_for_config_in_root_dir(root: str | Path) -> Path | None:\n\
      \    \"\"\"Resolve the config path from the given root directory.\n\n    Parameters\n\
      \    ----------\n    root : str | Path\n        The path to the root directory\
      \ containing the config file.\n        Searches for a default config file (settings.{yaml,yml,json}).\n\
      \n    Returns\n    -------\n    Path | None\n        returns a Path if there\
      \ is a config in the root directory\n        Otherwise returns None.\n    \"\
      \"\"\n    root = Path(root)\n\n    if not root.is_dir():\n        msg = f\"\
      Invalid config path: {root} is not a directory\"\n        raise FileNotFoundError(msg)\n\
      \n    for file in _default_config_files:\n        if (root / file).is_file():\n\
      \            return root / file\n\n    return None"
    signature: 'def _search_for_config_in_root_dir(root: str | Path) -> Path | None'
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: root.is_dir
      type: unresolved
    - target: FileNotFoundError
      type: builtin
    - target: (root / file).is_file
      type: unresolved
    visibility: protected
    node_id: graphrag/config/load_config.py::_search_for_config_in_root_dir
    called_by:
    - source: graphrag/config/load_config.py::_get_config_path
      type: internal
  - name: _parse_env_variables
    start_line: 49
    end_line: 67
    code: "def _parse_env_variables(text: str) -> str:\n    \"\"\"Parse environment\
      \ variables in the configuration text.\n\n    Parameters\n    ----------\n \
      \   text : str\n        The configuration text.\n\n    Returns\n    -------\n\
      \    str\n        The configuration text with environment variables parsed.\n\
      \n    Raises\n    ------\n    KeyError\n        If an environment variable is\
      \ not found.\n    \"\"\"\n    return Template(text).substitute(os.environ)"
    signature: 'def _parse_env_variables(text: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: Template(text).substitute
      type: unresolved
    - target: string::Template
      type: stdlib
    visibility: protected
    node_id: graphrag/config/load_config.py::_parse_env_variables
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
  - name: _load_dotenv
    start_line: 70
    end_line: 81
    code: "def _load_dotenv(config_path: Path | str) -> None:\n    \"\"\"Load the\
      \ .env file if it exists in the same directory as the config file.\n\n    Parameters\n\
      \    ----------\n    config_path : Path | str\n        The path to the config\
      \ file.\n    \"\"\"\n    config_path = Path(config_path)\n    dotenv_path =\
      \ config_path.parent / \".env\"\n    if dotenv_path.exists():\n        load_dotenv(dotenv_path)"
    signature: 'def _load_dotenv(config_path: Path | str) -> None'
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: dotenv_path.exists
      type: unresolved
    - target: dotenv::load_dotenv
      type: external
    visibility: protected
    node_id: graphrag/config/load_config.py::_load_dotenv
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
  - name: _get_config_path
    start_line: 84
    end_line: 112
    code: "def _get_config_path(root_dir: Path, config_filepath: Path | None) -> Path:\n\
      \    \"\"\"Get the configuration file path.\n\n    Parameters\n    ----------\n\
      \    root_dir : str | Path\n        The root directory of the project. Will\
      \ search for the config file in this directory.\n    config_filepath : str |\
      \ None\n        The path to the config file.\n        If None, searches for\
      \ config file in root.\n\n    Returns\n    -------\n    Path\n        The configuration\
      \ file path.\n    \"\"\"\n    if config_filepath:\n        config_path = config_filepath.resolve()\n\
      \        if not config_path.exists():\n            msg = f\"Specified Config\
      \ file not found: {config_path}\"\n            raise FileNotFoundError(msg)\n\
      \    else:\n        config_path = _search_for_config_in_root_dir(root_dir)\n\
      \n    if not config_path:\n        msg = f\"Config file not found in root directory:\
      \ {root_dir}\"\n        raise FileNotFoundError(msg)\n\n    return config_path"
    signature: 'def _get_config_path(root_dir: Path, config_filepath: Path | None)
      -> Path'
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: config_filepath.resolve
      type: unresolved
    - target: config_path.exists
      type: unresolved
    - target: FileNotFoundError
      type: builtin
    - target: graphrag/config/load_config.py::_search_for_config_in_root_dir
      type: internal
    visibility: protected
    node_id: graphrag/config/load_config.py::_get_config_path
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
  - name: _apply_overrides
    start_line: 115
    end_line: 129
    code: "def _apply_overrides(data: dict[str, Any], overrides: dict[str, Any]) ->\
      \ None:\n    \"\"\"Apply the overrides to the raw configuration.\"\"\"\n   \
      \ for key, value in overrides.items():\n        keys = key.split(\".\")\n  \
      \      target = data\n        current_path = keys[0]\n        for k in keys[:-1]:\n\
      \            current_path += f\".{k}\"\n            target_obj = target.get(k,\
      \ {})\n            if not isinstance(target_obj, dict):\n                msg\
      \ = f\"Cannot override non-dict value: data[{current_path}] is not a dict.\"\
      \n                raise TypeError(msg)\n            target[k] = target_obj\n\
      \            target = target[k]\n        target[keys[-1]] = value"
    signature: 'def _apply_overrides(data: dict[str, Any], overrides: dict[str, Any])
      -> None'
    decorators: []
    raises:
    - TypeError
    calls:
    - target: overrides.items
      type: unresolved
    - target: key.split
      type: unresolved
    - target: target.get
      type: unresolved
    - target: isinstance
      type: builtin
    - target: TypeError
      type: builtin
    visibility: protected
    node_id: graphrag/config/load_config.py::_apply_overrides
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
  - name: _parse
    start_line: 132
    end_line: 143
    code: "def _parse(file_extension: str, contents: str) -> dict[str, Any]:\n   \
      \ \"\"\"Parse configuration.\"\"\"\n    match file_extension:\n        case\
      \ \".yaml\" | \".yml\":\n            return yaml.safe_load(contents)\n     \
      \   case \".json\":\n            return json.loads(contents)\n        case _:\n\
      \            msg = (\n                f\"Unable to parse config. Unsupported\
      \ file extension: {file_extension}\"\n            )\n            raise ValueError(msg)"
    signature: 'def _parse(file_extension: str, contents: str) -> dict[str, Any]'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: yaml::safe_load
      type: external
    - target: json::loads
      type: stdlib
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/load_config.py::_parse
    called_by:
    - source: graphrag/config/load_config.py::load_config
      type: internal
  - name: load_config
    start_line: 146
    end_line: 191
    code: "def load_config(\n    root_dir: Path,\n    config_filepath: Path | None\
      \ = None,\n    cli_overrides: dict[str, Any] | None = None,\n) -> GraphRagConfig:\n\
      \    \"\"\"Load configuration from a file.\n\n    Parameters\n    ----------\n\
      \    root_dir : str | Path\n        The root directory of the project. Will\
      \ search for the config file in this directory.\n    config_filepath : str |\
      \ None\n        The path to the config file.\n        If None, searches for\
      \ config file in root.\n    cli_overrides : dict[str, Any] | None\n        A\
      \ flat dictionary of cli overrides.\n        Example: {'output.base_dir': 'override_value'}\n\
      \n    Returns\n    -------\n    GraphRagConfig\n        The loaded configuration.\n\
      \n    Raises\n    ------\n    FileNotFoundError\n        If the config file\
      \ is not found.\n    ValueError\n        If the config file extension is not\
      \ supported.\n    TypeError\n        If applying cli overrides to the config\
      \ fails.\n    KeyError\n        If config file references a non-existent environment\
      \ variable.\n    ValidationError\n        If there are pydantic validation errors\
      \ when instantiating the config.\n    \"\"\"\n    root = root_dir.resolve()\n\
      \    config_path = _get_config_path(root, config_filepath)\n    _load_dotenv(config_path)\n\
      \    config_extension = config_path.suffix\n    config_text = config_path.read_text(encoding=\"\
      utf-8\")\n    config_text = _parse_env_variables(config_text)\n    config_data\
      \ = _parse(config_extension, config_text)\n    if cli_overrides:\n        _apply_overrides(config_data,\
      \ cli_overrides)\n    return create_graphrag_config(config_data, root_dir=str(root))"
    signature: "def load_config(\n    root_dir: Path,\n    config_filepath: Path |\
      \ None = None,\n    cli_overrides: dict[str, Any] | None = None,\n) -> GraphRagConfig"
    decorators: []
    raises: []
    calls:
    - target: root_dir.resolve
      type: unresolved
    - target: graphrag/config/load_config.py::_get_config_path
      type: internal
    - target: graphrag/config/load_config.py::_load_dotenv
      type: internal
    - target: config_path.read_text
      type: unresolved
    - target: graphrag/config/load_config.py::_parse_env_variables
      type: internal
    - target: graphrag/config/load_config.py::_parse
      type: internal
    - target: graphrag/config/load_config.py::_apply_overrides
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/config/load_config.py::load_config
    called_by:
    - source: graphrag/cli/index.py::index_cli
      type: internal
    - source: graphrag/cli/index.py::update_cli
      type: internal
    - source: graphrag/cli/prompt_tune.py::prompt_tune
      type: internal
    - source: graphrag/cli/query.py::run_global_search
      type: internal
    - source: graphrag/cli/query.py::run_local_search
      type: internal
    - source: graphrag/cli/query.py::run_drift_search
      type: internal
    - source: graphrag/cli/query.py::run_basic_search
      type: internal
    - source: tests/unit/config/test_config.py::test_load_minimal_config
      type: internal
    - source: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
      type: internal
    - source: tests/unit/config/test_config.py::test_load_config_missing_env_vars
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
      type: internal
- file_name: graphrag/config/models/__init__.py
  imports: []
  functions: []
- file_name: graphrag/config/models/basic_search_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/cache_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: CacheType
    alias: null
  functions: []
- file_name: graphrag/config/models/chunking_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: ChunkStrategyType
    alias: null
  functions: []
- file_name: graphrag/config/models/cluster_graph_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/community_reports_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: CreateCommunityReportsStrategyType
    alias: null
  functions:
  - name: resolved_strategy
    start_line: 42
    end_line: 65
    code: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
      \    ) -> dict:\n        \"\"\"Get the resolved community report extraction\
      \ strategy.\"\"\"\n        from graphrag.index.operations.summarize_communities.typing\
      \ import (\n            CreateCommunityReportsStrategyType,\n        )\n\n \
      \       return self.strategy or {\n            \"type\": CreateCommunityReportsStrategyType.graph_intelligence,\n\
      \            \"llm\": model_config.model_dump(),\n            \"graph_prompt\"\
      : (Path(root_dir) / self.graph_prompt).read_text(\n                encoding=\"\
      utf-8\"\n            )\n            if self.graph_prompt\n            else None,\n\
      \            \"text_prompt\": (Path(root_dir) / self.text_prompt).read_text(\n\
      \                encoding=\"utf-8\"\n            )\n            if self.text_prompt\n\
      \            else None,\n            \"max_report_length\": self.max_length,\n\
      \            \"max_input_length\": self.max_input_length,\n        }"
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: model_config.model_dump
      type: unresolved
    - target: (Path(root_dir) / self.graph_prompt).read_text
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: (Path(root_dir) / self.text_prompt).read_text
      type: unresolved
    visibility: public
    node_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig.resolved_strategy
    called_by: []
- file_name: graphrag/config/models/drift_search_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/embed_graph_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/extract_claims_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: resolved_strategy
    start_line: 42
    end_line: 55
    code: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
      \    ) -> dict:\n        \"\"\"Get the resolved claim extraction strategy.\"\
      \"\"\n        return self.strategy or {\n            \"llm\": model_config.model_dump(),\n\
      \            \"extraction_prompt\": (Path(root_dir) / self.prompt).read_text(\n\
      \                encoding=\"utf-8\"\n            )\n            if self.prompt\n\
      \            else None,\n            \"claim_description\": self.description,\n\
      \            \"max_gleanings\": self.max_gleanings,\n        }"
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: model_config.model_dump
      type: unresolved
    - target: (Path(root_dir) / self.prompt).read_text
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: public
    node_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig.resolved_strategy
    called_by: []
- file_name: graphrag/config/models/extract_graph_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: ExtractEntityStrategyType
    alias: null
  functions:
  - name: resolved_strategy
    start_line: 38
    end_line: 55
    code: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
      \    ) -> dict:\n        \"\"\"Get the resolved entity extraction strategy.\"\
      \"\"\n        from graphrag.index.operations.extract_graph.typing import (\n\
      \            ExtractEntityStrategyType,\n        )\n\n        return self.strategy\
      \ or {\n            \"type\": ExtractEntityStrategyType.graph_intelligence,\n\
      \            \"llm\": model_config.model_dump(),\n            \"extraction_prompt\"\
      : (Path(root_dir) / self.prompt).read_text(\n                encoding=\"utf-8\"\
      \n            )\n            if self.prompt\n            else None,\n      \
      \      \"max_gleanings\": self.max_gleanings,\n        }"
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: model_config.model_dump
      type: unresolved
    - target: (Path(root_dir) / self.prompt).read_text
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: public
    node_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig.resolved_strategy
    called_by: []
- file_name: graphrag/config/models/extract_graph_nlp_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.enums
    name: NounPhraseExtractorType
    alias: null
  functions: []
- file_name: graphrag/config/models/global_search_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/graph_rag_config.py
  imports:
  - module: dataclasses
    name: asdict
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: devtools
    name: pformat
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: pydantic
    name: model_validator
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: VectorStoreType
    alias: null
  - module: graphrag.config.errors
    name: LanguageModelConfigMissingError
    alias: null
  - module: graphrag.config.models.basic_search_config
    name: BasicSearchConfig
    alias: null
  - module: graphrag.config.models.cache_config
    name: CacheConfig
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.config.models.cluster_graph_config
    name: ClusterGraphConfig
    alias: null
  - module: graphrag.config.models.community_reports_config
    name: CommunityReportsConfig
    alias: null
  - module: graphrag.config.models.drift_search_config
    name: DRIFTSearchConfig
    alias: null
  - module: graphrag.config.models.embed_graph_config
    name: EmbedGraphConfig
    alias: null
  - module: graphrag.config.models.extract_claims_config
    name: ClaimExtractionConfig
    alias: null
  - module: graphrag.config.models.extract_graph_config
    name: ExtractGraphConfig
    alias: null
  - module: graphrag.config.models.extract_graph_nlp_config
    name: ExtractGraphNLPConfig
    alias: null
  - module: graphrag.config.models.global_search_config
    name: GlobalSearchConfig
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.config.models.local_search_config
    name: LocalSearchConfig
    alias: null
  - module: graphrag.config.models.prune_graph_config
    name: PruneGraphConfig
    alias: null
  - module: graphrag.config.models.reporting_config
    name: ReportingConfig
    alias: null
  - module: graphrag.config.models.snapshots_config
    name: SnapshotsConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.config.models.summarize_descriptions_config
    name: SummarizeDescriptionsConfig
    alias: null
  - module: graphrag.config.models.text_embedding_config
    name: TextEmbeddingConfig
    alias: null
  - module: graphrag.config.models.umap_config
    name: UmapConfig
    alias: null
  - module: graphrag.config.models.vector_store_config
    name: VectorStoreConfig
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter_factory
    name: RateLimiterFactory
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry_factory
    name: RetryFactory
    alias: null
  functions:
  - name: __repr__
    start_line: 51
    end_line: 53
    code: "def __repr__(self) -> str:\n        \"\"\"Get a string representation.\"\
      \"\"\n        return pformat(self, highlight=False)"
    signature: def __repr__(self) -> str
    decorators: []
    raises: []
    calls:
    - target: devtools::pformat
      type: external
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__repr__
    called_by: []
  - name: __str__
    start_line: 55
    end_line: 57
    code: "def __str__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return self.model_dump_json(indent=4)"
    signature: def __str__(self)
    decorators: []
    raises: []
    calls:
    - target: self.model_dump_json
      type: instance
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__str__
    called_by: []
  - name: _validate_root_dir
    start_line: 64
    end_line: 73
    code: "def _validate_root_dir(self) -> None:\n        \"\"\"Validate the root\
      \ directory.\"\"\"\n        if self.root_dir.strip() == \"\":\n            self.root_dir\
      \ = str(Path.cwd())\n\n        root_dir = Path(self.root_dir).resolve()\n  \
      \      if not root_dir.is_dir():\n            msg = f\"Invalid root directory:\
      \ {self.root_dir} is not a directory.\"\n            raise FileNotFoundError(msg)\n\
      \        self.root_dir = str(root_dir)"
    signature: def _validate_root_dir(self) -> None
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: self.root_dir.strip
      type: instance
    - target: str
      type: builtin
    - target: pathlib::Path::cwd
      type: external
    - target: Path(self.root_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: root_dir.is_dir
      type: unresolved
    - target: FileNotFoundError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_root_dir
    called_by: []
  - name: _validate_models
    start_line: 80
    end_line: 96
    code: "def _validate_models(self) -> None:\n        \"\"\"Validate the models\
      \ configuration.\n\n        Ensure both a default chat model and default embedding\
      \ model\n        have been defined. Other models may also be defined but\n \
      \       defaults are required for the time being as places of the\n        code\
      \ fallback to default model configs instead\n        of specifying a specific\
      \ model.\n\n        TODO: Don't fallback to default models elsewhere in the\
      \ code.\n        Forcing code to specify a model to use and allowing for any\n\
      \        names for model configurations.\n        \"\"\"\n        if defs.DEFAULT_CHAT_MODEL_ID\
      \ not in self.models:\n            raise LanguageModelConfigMissingError(defs.DEFAULT_CHAT_MODEL_ID)\n\
      \        if defs.DEFAULT_EMBEDDING_MODEL_ID not in self.models:\n          \
      \  raise LanguageModelConfigMissingError(defs.DEFAULT_EMBEDDING_MODEL_ID)"
    signature: def _validate_models(self) -> None
    decorators: []
    raises:
    - LanguageModelConfigMissingError
    calls:
    - target: graphrag/config/errors.py::LanguageModelConfigMissingError
      type: internal
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_models
    called_by: []
  - name: _validate_retry_services
    start_line: 98
    end_line: 112
    code: "def _validate_retry_services(self) -> None:\n        \"\"\"Validate the\
      \ retry services configuration.\"\"\"\n        retry_factory = RetryFactory()\n\
      \n        for model_id, model in self.models.items():\n            if model.retry_strategy\
      \ != \"none\":\n                if model.retry_strategy not in retry_factory:\n\
      \                    msg = f\"Retry strategy '{model.retry_strategy}' for model\
      \ '{model_id}' is not registered. Available strategies: {', '.join(retry_factory.keys())}\"\
      \n                    raise ValueError(msg)\n\n                _ = retry_factory.create(\n\
      \                    strategy=model.retry_strategy,\n                    max_retries=model.max_retries,\n\
      \                    max_retry_wait=model.max_retry_wait,\n                )"
    signature: def _validate_retry_services(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/language_model/providers/litellm/services/retry/retry_factory.py::RetryFactory
      type: internal
    - target: self.models.items
      type: instance
    - target: ''', ''.join'
      type: unresolved
    - target: retry_factory.keys
      type: unresolved
    - target: ValueError
      type: builtin
    - target: retry_factory.create
      type: unresolved
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_retry_services
    called_by: []
  - name: _validate_rate_limiter_services
    start_line: 114
    end_line: 137
    code: "def _validate_rate_limiter_services(self) -> None:\n        \"\"\"Validate\
      \ the rate limiter services configuration.\"\"\"\n        rate_limiter_factory\
      \ = RateLimiterFactory()\n\n        for model_id, model in self.models.items():\n\
      \            if model.rate_limit_strategy is not None:\n                if model.rate_limit_strategy\
      \ not in rate_limiter_factory:\n                    msg = f\"Rate Limiter strategy\
      \ '{model.rate_limit_strategy}' for model '{model_id}' is not registered. Available\
      \ strategies: {', '.join(rate_limiter_factory.keys())}\"\n                 \
      \   raise ValueError(msg)\n\n                rpm = (\n                    model.requests_per_minute\n\
      \                    if type(model.requests_per_minute) is int\n           \
      \         else None\n                )\n                tpm = (\n          \
      \          model.tokens_per_minute\n                    if type(model.tokens_per_minute)\
      \ is int\n                    else None\n                )\n               \
      \ if rpm is not None or tpm is not None:\n                    _ = rate_limiter_factory.create(\n\
      \                        strategy=model.rate_limit_strategy, rpm=rpm, tpm=tpm\n\
      \                    )"
    signature: def _validate_rate_limiter_services(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py::RateLimiterFactory
      type: internal
    - target: self.models.items
      type: instance
    - target: ''', ''.join'
      type: unresolved
    - target: rate_limiter_factory.keys
      type: unresolved
    - target: ValueError
      type: builtin
    - target: type
      type: builtin
    - target: rate_limiter_factory.create
      type: unresolved
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_rate_limiter_services
    called_by: []
  - name: _validate_input_pattern
    start_line: 144
    end_line: 150
    code: "def _validate_input_pattern(self) -> None:\n        \"\"\"Validate the\
      \ input file pattern based on the specified type.\"\"\"\n        if len(self.input.file_pattern)\
      \ == 0:\n            if self.input.file_type == defs.InputFileType.text:\n \
      \               self.input.file_pattern = \".*\\\\.txt$\"\n            else:\n\
      \                self.input.file_pattern = f\".*\\\\.{self.input.file_type.value}$\""
    signature: def _validate_input_pattern(self) -> None
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_pattern
    called_by: []
  - name: _validate_input_base_dir
    start_line: 152
    end_line: 160
    code: "def _validate_input_base_dir(self) -> None:\n        \"\"\"Validate the\
      \ input base directory.\"\"\"\n        if self.input.storage.type == defs.StorageType.file:\n\
      \            if self.input.storage.base_dir.strip() == \"\":\n             \
      \   msg = \"input storage base directory is required for file input storage.\
      \ Please rerun `graphrag init` and set the input storage configuration.\"\n\
      \                raise ValueError(msg)\n            self.input.storage.base_dir\
      \ = str(\n                (Path(self.root_dir) / self.input.storage.base_dir).resolve()\n\
      \            )"
    signature: def _validate_input_base_dir(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.input.storage.base_dir.strip
      type: instance
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / self.input.storage.base_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_base_dir
    called_by: []
  - name: _validate_output_base_dir
    start_line: 174
    end_line: 182
    code: "def _validate_output_base_dir(self) -> None:\n        \"\"\"Validate the\
      \ output base directory.\"\"\"\n        if self.output.type == defs.StorageType.file:\n\
      \            if self.output.base_dir.strip() == \"\":\n                msg =\
      \ \"output base directory is required for file output. Please rerun `graphrag\
      \ init` and set the output configuration.\"\n                raise ValueError(msg)\n\
      \            self.output.base_dir = str(\n                (Path(self.root_dir)\
      \ / self.output.base_dir).resolve()\n            )"
    signature: def _validate_output_base_dir(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.output.base_dir.strip
      type: instance
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / self.output.base_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_output_base_dir
    called_by: []
  - name: _validate_multi_output_base_dirs
    start_line: 189
    end_line: 199
    code: "def _validate_multi_output_base_dirs(self) -> None:\n        \"\"\"Validate\
      \ the outputs dict base directories.\"\"\"\n        if self.outputs:\n     \
      \       for output in self.outputs.values():\n                if output.type\
      \ == defs.StorageType.file:\n                    if output.base_dir.strip()\
      \ == \"\":\n                        msg = \"Output base directory is required\
      \ for file output. Please rerun `graphrag init` and set the output configuration.\"\
      \n                        raise ValueError(msg)\n                    output.base_dir\
      \ = str(\n                        (Path(self.root_dir) / output.base_dir).resolve()\n\
      \                    )"
    signature: def _validate_multi_output_base_dirs(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.outputs.values
      type: instance
    - target: output.base_dir.strip
      type: unresolved
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / output.base_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_multi_output_base_dirs
    called_by: []
  - name: _validate_update_index_output_base_dir
    start_line: 209
    end_line: 217
    code: "def _validate_update_index_output_base_dir(self) -> None:\n        \"\"\
      \"Validate the update index output base directory.\"\"\"\n        if self.update_index_output.type\
      \ == defs.StorageType.file:\n            if self.update_index_output.base_dir.strip()\
      \ == \"\":\n                msg = \"update_index_output base directory is required\
      \ for file output. Please rerun `graphrag init` and set the update_index_output\
      \ configuration.\"\n                raise ValueError(msg)\n            self.update_index_output.base_dir\
      \ = str(\n                (Path(self.root_dir) / self.update_index_output.base_dir).resolve()\n\
      \            )"
    signature: def _validate_update_index_output_base_dir(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.update_index_output.base_dir.strip
      type: instance
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / self.update_index_output.base_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_update_index_output_base_dir
    called_by: []
  - name: _validate_reporting_base_dir
    start_line: 229
    end_line: 237
    code: "def _validate_reporting_base_dir(self) -> None:\n        \"\"\"Validate\
      \ the reporting base directory.\"\"\"\n        if self.reporting.type == defs.ReportingType.file:\n\
      \            if self.reporting.base_dir.strip() == \"\":\n                msg\
      \ = \"Reporting base directory is required for file reporting. Please rerun\
      \ `graphrag init` and set the reporting configuration.\"\n                raise\
      \ ValueError(msg)\n            self.reporting.base_dir = str(\n            \
      \    (Path(self.root_dir) / self.reporting.base_dir).resolve()\n           \
      \ )"
    signature: def _validate_reporting_base_dir(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.reporting.base_dir.strip
      type: instance
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / self.reporting.base_dir).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_reporting_base_dir
    called_by: []
  - name: _validate_vector_store_db_uri
    start_line: 341
    end_line: 348
    code: "def _validate_vector_store_db_uri(self) -> None:\n        \"\"\"Validate\
      \ the vector store configuration.\"\"\"\n        for store in self.vector_store.values():\n\
      \            if store.type == VectorStoreType.LanceDB:\n                if not\
      \ store.db_uri or store.db_uri.strip == \"\":\n                    msg = \"\
      Vector store URI is required for LanceDB. Please rerun `graphrag init` and set\
      \ the vector store configuration.\"\n                    raise ValueError(msg)\n\
      \                store.db_uri = str((Path(self.root_dir) / store.db_uri).resolve())"
    signature: def _validate_vector_store_db_uri(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.vector_store.values
      type: instance
    - target: ValueError
      type: builtin
    - target: str
      type: builtin
    - target: (Path(self.root_dir) / store.db_uri).resolve
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_vector_store_db_uri
    called_by: []
  - name: _validate_factories
    start_line: 350
    end_line: 353
    code: "def _validate_factories(self) -> None:\n        \"\"\"Validate the factories\
      \ used in the configuration.\"\"\"\n        self._validate_retry_services()\n\
      \        self._validate_rate_limiter_services()"
    signature: def _validate_factories(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/graph_rag_config.py::_validate_retry_services
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_rate_limiter_services
      type: internal
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_factories
    called_by: []
  - name: get_language_model_config
    start_line: 355
    end_line: 377
    code: "def get_language_model_config(self, model_id: str) -> LanguageModelConfig:\n\
      \        \"\"\"Get a model configuration by ID.\n\n        Parameters\n    \
      \    ----------\n        model_id : str\n            The ID of the model to\
      \ get. Should match an ID in the models list.\n\n        Returns\n        -------\n\
      \        LanguageModelConfig\n            The model configuration if found.\n\
      \n        Raises\n        ------\n        ValueError\n            If the model\
      \ ID is not found in the configuration.\n        \"\"\"\n        if model_id\
      \ not in self.models:\n            err_msg = f\"Model ID {model_id} not found\
      \ in configuration. Please rerun `graphrag init` and set the model configuration.\"\
      \n            raise ValueError(err_msg)\n\n        return self.models[model_id]"
    signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_language_model_config
    called_by: []
  - name: get_vector_store_config
    start_line: 379
    end_line: 401
    code: "def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig:\n\
      \        \"\"\"Get a vector store configuration by ID.\n\n        Parameters\n\
      \        ----------\n        vector_store_id : str\n            The ID of the\
      \ vector store to get. Should match an ID in the vector_store list.\n\n    \
      \    Returns\n        -------\n        VectorStoreConfig\n            The vector\
      \ store configuration if found.\n\n        Raises\n        ------\n        ValueError\n\
      \            If the vector store ID is not found in the configuration.\n   \
      \     \"\"\"\n        if vector_store_id not in self.vector_store:\n       \
      \     err_msg = f\"Vector Store ID {vector_store_id} not found in configuration.\
      \ Please rerun `graphrag init` and set the vector store configuration.\"\n \
      \           raise ValueError(err_msg)\n\n        return self.vector_store[vector_store_id]"
    signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_vector_store_config
    called_by: []
  - name: _validate_model
    start_line: 404
    end_line: 416
    code: "def _validate_model(self):\n        \"\"\"Validate the model configuration.\"\
      \"\"\n        self._validate_root_dir()\n        self._validate_models()\n \
      \       self._validate_input_pattern()\n        self._validate_input_base_dir()\n\
      \        self._validate_reporting_base_dir()\n        self._validate_output_base_dir()\n\
      \        self._validate_multi_output_base_dirs()\n        self._validate_update_index_output_base_dir()\n\
      \        self._validate_vector_store_db_uri()\n        self._validate_factories()\n\
      \        return self"
    signature: def _validate_model(self)
    decorators:
    - '@model_validator(mode="after")'
    raises: []
    calls:
    - target: graphrag/config/models/graph_rag_config.py::_validate_root_dir
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_models
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_input_pattern
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_input_base_dir
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_reporting_base_dir
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_output_base_dir
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_multi_output_base_dirs
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_update_index_output_base_dir
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_vector_store_db_uri
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::_validate_factories
      type: internal
    visibility: protected
    node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_model
    called_by: []
- file_name: graphrag/config/models/input_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  functions: []
- file_name: graphrag/config/models/language_model_config.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Literal
    alias: null
  - module: tiktoken
    name: null
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: pydantic
    name: model_validator
    alias: null
  - module: graphrag.config.defaults
    name: language_model_defaults
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.enums
    name: AuthType
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.config.errors
    name: ApiKeyMissingError
    alias: null
  - module: graphrag.config.errors
    name: AzureApiBaseMissingError
    alias: null
  - module: graphrag.config.errors
    name: AzureApiVersionMissingError
    alias: null
  - module: graphrag.config.errors
    name: ConflictingSettingsError
    alias: null
  - module: graphrag.language_model.factory
    name: ModelFactory
    alias: null
  functions:
  - name: _validate_api_key
    start_line: 33
    end_line: 60
    code: "def _validate_api_key(self) -> None:\n        \"\"\"Validate the API key.\n\
      \n        API Key is required when using OpenAI API\n        or when using Azure\
      \ API with API Key authentication.\n        For the time being, this check is\
      \ extra verbose for clarity.\n        It will also raise an exception if an\
      \ API Key is provided\n        when one is not expected such as the case of\
      \ using Azure\n        Managed Identity.\n\n        Raises\n        ------\n\
      \        ApiKeyMissingError\n            If the API key is missing and is required.\n\
      \        \"\"\"\n        if self.auth_type == AuthType.APIKey and (\n      \
      \      self.api_key is None or self.api_key.strip() == \"\"\n        ):\n  \
      \          raise ApiKeyMissingError(\n                self.type,\n         \
      \       self.auth_type.value,\n            )\n\n        if (self.auth_type ==\
      \ AuthType.AzureManagedIdentity) and (\n            self.api_key is not None\
      \ and self.api_key.strip() != \"\"\n        ):\n            msg = \"API Key\
      \ should not be provided when using Azure Managed Identity. Please rerun `graphrag\
      \ init` and remove the api_key when using Azure Managed Identity.\"\n      \
      \      raise ConflictingSettingsError(msg)"
    signature: def _validate_api_key(self) -> None
    decorators: []
    raises:
    - ApiKeyMissingError
    - ConflictingSettingsError
    calls:
    - target: self.api_key.strip
      type: instance
    - target: graphrag/config/errors.py::ApiKeyMissingError
      type: internal
    - target: graphrag/config/errors.py::ConflictingSettingsError
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_key
    called_by: []
  - name: _validate_auth_type
    start_line: 67
    end_line: 85
    code: "def _validate_auth_type(self) -> None:\n        \"\"\"Validate the authentication\
      \ type.\n\n        auth_type must be api_key when using OpenAI and\n       \
      \ can be either api_key or azure_managed_identity when using AOI.\n\n      \
      \  Raises\n        ------\n        ConflictingSettingsError\n            If\
      \ the Azure authentication type conflicts with the model being used.\n     \
      \   \"\"\"\n        if (\n            self.auth_type == AuthType.AzureManagedIdentity\n\
      \            and self.type != ModelType.AzureOpenAIChat\n            and self.type\
      \ != ModelType.AzureOpenAIEmbedding\n            and self.model_provider !=\
      \ \"azure\"  # indicates Litellm + AOI\n        ):\n            msg = f\"auth_type\
      \ of azure_managed_identity is not supported for model type {self.type}. Please\
      \ rerun `graphrag init` and set the auth_type to api_key.\"\n            raise\
      \ ConflictingSettingsError(msg)"
    signature: def _validate_auth_type(self) -> None
    decorators: []
    raises:
    - ConflictingSettingsError
    calls:
    - target: graphrag/config/errors.py::ConflictingSettingsError
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_auth_type
    called_by: []
  - name: _validate_type
    start_line: 89
    end_line: 108
    code: "def _validate_type(self) -> None:\n        \"\"\"Validate the model type.\n\
      \n        Raises\n        ------\n        KeyError\n            If the model\
      \ name is not recognized.\n        \"\"\"\n        # Type should be contained\
      \ by the registered models\n        if not ModelFactory.is_supported_model(self.type):\n\
      \            msg = f\"Model type {self.type} is not recognized, must be one\
      \ of {ModelFactory.get_chat_models() + ModelFactory.get_embedding_models()}.\"\
      \n            raise KeyError(msg)\n        if self.type in [\n            \"\
      openai_chat\",\n            \"openai_embedding\",\n            \"azure_openai_chat\"\
      ,\n            \"azure_openai_embedding\",\n        ]:\n            msg = f\"\
      Model config based on fnllm is deprecated and will be removed in GraphRAG v3,\
      \ please use {ModelType.Chat} or {ModelType.Embedding} instead to switch to\
      \ LiteLLM config.\"\n            logger.warning(msg)"
    signature: def _validate_type(self) -> None
    decorators: []
    raises:
    - KeyError
    calls:
    - target: graphrag/language_model/factory.py::ModelFactory::is_supported_model
      type: external
    - target: graphrag/language_model/factory.py::ModelFactory::get_chat_models
      type: external
    - target: graphrag/language_model/factory.py::ModelFactory::get_embedding_models
      type: external
    - target: KeyError
      type: builtin
    - target: logger.warning
      type: unresolved
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_type
    called_by: []
  - name: _validate_model_provider
    start_line: 115
    end_line: 129
    code: "def _validate_model_provider(self) -> None:\n        \"\"\"Validate the\
      \ model provider.\n\n        Required when using Litellm.\n\n        Raises\n\
      \        ------\n        KeyError\n            If the model provider is not\
      \ recognized.\n        \"\"\"\n        if (self.type == ModelType.Chat or self.type\
      \ == ModelType.Embedding) and (\n            self.model_provider is None or\
      \ self.model_provider.strip() == \"\"\n        ):\n            msg = f\"Model\
      \ provider must be specified when using type == {self.type}.\"\n           \
      \ raise KeyError(msg)"
    signature: def _validate_model_provider(self) -> None
    decorators: []
    raises:
    - KeyError
    calls:
    - target: self.model_provider.strip
      type: instance
    - target: KeyError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model_provider
    called_by: []
  - name: _validate_encoding_model
    start_line: 137
    end_line: 161
    code: "def _validate_encoding_model(self) -> None:\n        \"\"\"Validate the\
      \ encoding model.\n\n        The default behavior is to use an encoding model\
      \ that matches the LLM model.\n        LiteLLM supports 100+ models and their\
      \ tokenization. There is no need to\n        set the encoding model when using\
      \ the new LiteLLM provider as was done with fnllm provider.\n\n        Users\
      \ can still manually specify a tiktoken based encoding model to use even with\
      \ the LiteLLM provider\n        in which case the specified encoding model will\
      \ be used regardless of the LLM model being used, even if\n        it is not\
      \ an openai based model.\n\n        If not using LiteLLM provider, set the encoding\
      \ model based on the LLM model name.\n        This is for backward compatibility\
      \ with existing fnllm provider until fnllm is removed.\n\n        Raises\n \
      \       ------\n        KeyError\n            If the model name is not recognized.\n\
      \        \"\"\"\n        if (\n            self.type != ModelType.Chat\n   \
      \         and self.type != ModelType.Embedding\n            and self.encoding_model.strip()\
      \ == \"\"\n        ):\n            self.encoding_model = tiktoken.encoding_name_for_model(self.model)"
    signature: def _validate_encoding_model(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self.encoding_model.strip
      type: instance
    - target: tiktoken::encoding_name_for_model
      type: external
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_encoding_model
    called_by: []
  - name: _validate_api_base
    start_line: 168
    end_line: 183
    code: "def _validate_api_base(self) -> None:\n        \"\"\"Validate the API base.\n\
      \n        Required when using AOI.\n\n        Raises\n        ------\n     \
      \   AzureApiBaseMissingError\n            If the API base is missing and is\
      \ required.\n        \"\"\"\n        if (\n            self.type == ModelType.AzureOpenAIChat\n\
      \            or self.type == ModelType.AzureOpenAIEmbedding\n            or\
      \ self.model_provider == \"azure\"  # indicates Litellm + AOI\n        ) and\
      \ (self.api_base is None or self.api_base.strip() == \"\"):\n            raise\
      \ AzureApiBaseMissingError(self.type)"
    signature: def _validate_api_base(self) -> None
    decorators: []
    raises:
    - AzureApiBaseMissingError
    calls:
    - target: self.api_base.strip
      type: instance
    - target: graphrag/config/errors.py::AzureApiBaseMissingError
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_base
    called_by: []
  - name: _validate_api_version
    start_line: 190
    end_line: 205
    code: "def _validate_api_version(self) -> None:\n        \"\"\"Validate the API\
      \ version.\n\n        Required when using AOI.\n\n        Raises\n        ------\n\
      \        AzureApiBaseMissingError\n            If the API base is missing and\
      \ is required.\n        \"\"\"\n        if (\n            self.type == ModelType.AzureOpenAIChat\n\
      \            or self.type == ModelType.AzureOpenAIEmbedding\n            or\
      \ self.model_provider == \"azure\"  # indicates Litellm + AOI\n        ) and\
      \ (self.api_version is None or self.api_version.strip() == \"\"):\n        \
      \    raise AzureApiVersionMissingError(self.type)"
    signature: def _validate_api_version(self) -> None
    decorators: []
    raises:
    - AzureApiVersionMissingError
    calls:
    - target: self.api_version.strip
      type: instance
    - target: graphrag/config/errors.py::AzureApiVersionMissingError
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_version
    called_by: []
  - name: _validate_deployment_name
    start_line: 212
    end_line: 228
    code: "def _validate_deployment_name(self) -> None:\n        \"\"\"Validate the\
      \ deployment name.\n\n        Required when using AOI.\n\n        Raises\n \
      \       ------\n        AzureDeploymentNameMissingError\n            If the\
      \ deployment name is missing and is required.\n        \"\"\"\n        if (\n\
      \            self.type == ModelType.AzureOpenAIChat\n            or self.type\
      \ == ModelType.AzureOpenAIEmbedding\n            or self.model_provider == \"\
      azure\"  # indicates Litellm + AOI\n        ) and (self.deployment_name is None\
      \ or self.deployment_name.strip() == \"\"):\n            msg = f\"deployment_name\
      \ is not set for Azure-hosted model. This will default to your model name ({self.model}).\
      \ If different, this should be set.\"\n            logger.debug(msg)"
    signature: def _validate_deployment_name(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self.deployment_name.strip
      type: instance
    - target: logger.debug
      type: unresolved
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_deployment_name
    called_by: []
  - name: _validate_tokens_per_minute
    start_line: 255
    end_line: 274
    code: "def _validate_tokens_per_minute(self) -> None:\n        \"\"\"Validate\
      \ the tokens per minute.\n\n        Raises\n        ------\n        ValueError\n\
      \            If the tokens per minute is less than 0.\n        \"\"\"\n    \
      \    # If the value is a number, check if it is less than 1\n        if isinstance(self.tokens_per_minute,\
      \ int) and self.tokens_per_minute < 1:\n            msg = f\"Tokens per minute\
      \ must be a non zero positive number, 'auto' or null. Suggested value: {language_model_defaults.tokens_per_minute}.\"\
      \n            raise ValueError(msg)\n\n        if (\n            (self.type\
      \ == ModelType.Chat or self.type == ModelType.Embedding)\n            and self.rate_limit_strategy\
      \ is not None\n            and self.tokens_per_minute == \"auto\"\n        ):\n\
      \            msg = f\"tokens_per_minute cannot be set to 'auto' when using type\
      \ '{self.type}'. Please set it to a positive integer or null to disable.\"\n\
      \            raise ValueError(msg)"
    signature: def _validate_tokens_per_minute(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: isinstance
      type: builtin
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_tokens_per_minute
    called_by: []
  - name: _validate_requests_per_minute
    start_line: 281
    end_line: 300
    code: "def _validate_requests_per_minute(self) -> None:\n        \"\"\"Validate\
      \ the requests per minute.\n\n        Raises\n        ------\n        ValueError\n\
      \            If the requests per minute is less than 0.\n        \"\"\"\n  \
      \      # If the value is a number, check if it is less than 1\n        if isinstance(self.requests_per_minute,\
      \ int) and self.requests_per_minute < 1:\n            msg = f\"Requests per\
      \ minute must be a non zero positive number, 'auto' or null. Suggested value:\
      \ {language_model_defaults.requests_per_minute}.\"\n            raise ValueError(msg)\n\
      \n        if (\n            (self.type == ModelType.Chat or self.type == ModelType.Embedding)\n\
      \            and self.rate_limit_strategy is not None\n            and self.requests_per_minute\
      \ == \"auto\"\n        ):\n            msg = f\"requests_per_minute cannot be\
      \ set to 'auto' when using type '{self.type}'. Please set it to a positive integer\
      \ or null to disable.\"\n            raise ValueError(msg)"
    signature: def _validate_requests_per_minute(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: isinstance
      type: builtin
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_requests_per_minute
    called_by: []
  - name: _validate_max_retries
    start_line: 316
    end_line: 326
    code: "def _validate_max_retries(self) -> None:\n        \"\"\"Validate the maximum\
      \ retries.\n\n        Raises\n        ------\n        ValueError\n         \
      \   If the maximum retries is less than 0.\n        \"\"\"\n        if self.max_retries\
      \ < 1:\n            msg = f\"Maximum retries must be greater than or equal to\
      \ 1. Suggested value: {language_model_defaults.max_retries}.\"\n           \
      \ raise ValueError(msg)"
    signature: def _validate_max_retries(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_max_retries
    called_by: []
  - name: _validate_azure_settings
    start_line: 376
    end_line: 390
    code: "def _validate_azure_settings(self) -> None:\n        \"\"\"Validate the\
      \ Azure settings.\n\n        Raises\n        ------\n        AzureApiBaseMissingError\n\
      \            If the API base is missing and is required.\n        AzureApiVersionMissingError\n\
      \            If the API version is missing and is required.\n        AzureDeploymentNameMissingError\n\
      \            If the deployment name is missing and is required.\n        \"\"\
      \"\n        self._validate_api_base()\n        self._validate_api_version()\n\
      \        self._validate_deployment_name()"
    signature: def _validate_azure_settings(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::_validate_api_base
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_api_version
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_deployment_name
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_azure_settings
    called_by: []
  - name: _validate_model
    start_line: 393
    end_line: 403
    code: "def _validate_model(self):\n        self._validate_type()\n        self._validate_model_provider()\n\
      \        self._validate_auth_type()\n        self._validate_api_key()\n    \
      \    self._validate_tokens_per_minute()\n        self._validate_requests_per_minute()\n\
      \        self._validate_max_retries()\n        self._validate_azure_settings()\n\
      \        self._validate_encoding_model()\n        return self"
    signature: def _validate_model(self)
    decorators:
    - '@model_validator(mode="after")'
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::_validate_type
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_model_provider
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_auth_type
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_api_key
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_tokens_per_minute
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_requests_per_minute
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_max_retries
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_azure_settings
      type: internal
    - target: graphrag/config/models/language_model_config.py::_validate_encoding_model
      type: internal
    visibility: protected
    node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model
    called_by: []
- file_name: graphrag/config/models/local_search_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/prune_graph_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/reporting_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: ReportingType
    alias: null
  functions: []
- file_name: graphrag/config/models/snapshots_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/storage_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: pydantic
    name: field_validator
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: StorageType
    alias: null
  functions:
  - name: validate_base_dir
    start_line: 30
    end_line: 35
    code: "def validate_base_dir(cls, value, info):\n        \"\"\"Ensure that base_dir\
      \ is a valid filesystem path when using local storage.\"\"\"\n        # info.data\
      \ contains other field values, including 'type'\n        if info.data.get(\"\
      type\") != StorageType.file:\n            return value\n        return str(Path(value))"
    signature: def validate_base_dir(cls, value, info)
    decorators:
    - '@field_validator("base_dir", mode="before")'
    - '@classmethod'
    raises: []
    calls:
    - target: info.data.get
      type: unresolved
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    visibility: public
    node_id: graphrag/config/models/storage_config.py::StorageConfig.validate_base_dir
    called_by: []
- file_name: graphrag/config/models/summarize_descriptions_config.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.summarize_descriptions
    name: SummarizeStrategyType
    alias: null
  functions:
  - name: resolved_strategy
    start_line: 38
    end_line: 56
    code: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
      \    ) -> dict:\n        \"\"\"Get the resolved description summarization strategy.\"\
      \"\"\n        from graphrag.index.operations.summarize_descriptions.summarize_descriptions\
      \ import (\n            SummarizeStrategyType,\n        )\n\n        return\
      \ self.strategy or {\n            \"type\": SummarizeStrategyType.graph_intelligence,\n\
      \            \"llm\": model_config.model_dump(),\n            \"summarize_prompt\"\
      : (Path(root_dir) / self.prompt).read_text(\n                encoding=\"utf-8\"\
      \n            )\n            if self.prompt\n            else None,\n      \
      \      \"max_summary_length\": self.max_length,\n            \"max_input_tokens\"\
      : self.max_input_tokens,\n        }"
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: model_config.model_dump
      type: unresolved
    - target: (Path(root_dir) / self.prompt).read_text
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: public
    node_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig.resolved_strategy
    called_by: []
- file_name: graphrag/config/models/text_embedding_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.embed_text.embed_text
    name: TextEmbedStrategyType
    alias: null
  functions:
  - name: resolved_strategy
    start_line: 40
    end_line: 52
    code: "def resolved_strategy(self, model_config: LanguageModelConfig) -> dict:\n\
      \        \"\"\"Get the resolved text embedding strategy.\"\"\"\n        from\
      \ graphrag.index.operations.embed_text.embed_text import (\n            TextEmbedStrategyType,\n\
      \        )\n\n        return self.strategy or {\n            \"type\": TextEmbedStrategyType.openai,\n\
      \            \"llm\": model_config.model_dump(),\n            \"num_threads\"\
      : model_config.concurrent_requests,\n            \"batch_size\": self.batch_size,\n\
      \            \"batch_max_tokens\": self.batch_max_tokens,\n        }"
    signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) ->
      dict'
    decorators: []
    raises: []
    calls:
    - target: model_config.model_dump
      type: unresolved
    visibility: public
    node_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig.resolved_strategy
    called_by: []
- file_name: graphrag/config/models/umap_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  functions: []
- file_name: graphrag/config/models/vector_store_config.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: pydantic
    name: model_validator
    alias: null
  - module: graphrag.config.defaults
    name: vector_store_defaults
    alias: null
  - module: graphrag.config.embeddings
    name: all_embeddings
    alias: null
  - module: graphrag.config.enums
    name: VectorStoreType
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  functions:
  - name: _validate_db_uri
    start_line: 27
    end_line: 38
    code: "def _validate_db_uri(self) -> None:\n        \"\"\"Validate the database\
      \ URI.\"\"\"\n        if self.type == VectorStoreType.LanceDB.value and (\n\
      \            self.db_uri is None or self.db_uri.strip() == \"\"\n        ):\n\
      \            self.db_uri = vector_store_defaults.db_uri\n\n        if self.type\
      \ != VectorStoreType.LanceDB.value and (\n            self.db_uri is not None\
      \ and self.db_uri.strip() != \"\"\n        ):\n            msg = \"vector_store.db_uri\
      \ is only used when vector_store.type == lancedb. Please rerun `graphrag init`\
      \ and select the correct vector store type.\"\n            raise ValueError(msg)"
    signature: def _validate_db_uri(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.db_uri.strip
      type: instance
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_db_uri
    called_by: []
  - name: _validate_url
    start_line: 45
    end_line: 63
    code: "def _validate_url(self) -> None:\n        \"\"\"Validate the database URL.\"\
      \"\"\n        if self.type == VectorStoreType.AzureAISearch and (\n        \
      \    self.url is None or self.url.strip() == \"\"\n        ):\n            msg\
      \ = \"vector_store.url is required when vector_store.type == azure_ai_search.\
      \ Please rerun `graphrag init` and select the correct vector store type.\"\n\
      \            raise ValueError(msg)\n\n        if self.type == VectorStoreType.CosmosDB\
      \ and (\n            self.url is None or self.url.strip() == \"\"\n        ):\n\
      \            msg = \"vector_store.url is required when vector_store.type ==\
      \ cosmos_db. Please rerun `graphrag init` and select the correct vector store\
      \ type.\"\n            raise ValueError(msg)\n\n        if self.type == VectorStoreType.LanceDB\
      \ and (\n            self.url is not None and self.url.strip() != \"\"\n   \
      \     ):\n            msg = \"vector_store.url is only used when vector_store.type\
      \ == azure_ai_search or vector_store.type == cosmos_db. Please rerun `graphrag\
      \ init` and select the correct vector store type.\"\n            raise ValueError(msg)"
    signature: def _validate_url(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.url.strip
      type: instance
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_url
    called_by: []
  - name: _validate_embeddings_schema
    start_line: 92
    end_line: 103
    code: "def _validate_embeddings_schema(self) -> None:\n        \"\"\"Validate\
      \ the embeddings schema.\"\"\"\n        for name in self.embeddings_schema:\n\
      \            if name not in all_embeddings:\n                msg = f\"vector_store.embeddings_schema\
      \ contains an invalid embedding schema name: {name}. Please update your settings.yaml\
      \ and select the correct embedding schema names.\"\n                raise ValueError(msg)\n\
      \n        if self.type == VectorStoreType.CosmosDB:\n            for id_field\
      \ in self.embeddings_schema:\n                if id_field != \"id\":\n     \
      \               msg = \"When using CosmosDB, the id_field in embeddings_schema\
      \ must be 'id'. Please update your settings.yaml and set the id_field to 'id'.\"\
      \n                    raise ValueError(msg)"
    signature: def _validate_embeddings_schema(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_embeddings_schema
    called_by: []
  - name: _validate_model
    start_line: 106
    end_line: 111
    code: "def _validate_model(self):\n        \"\"\"Validate the model.\"\"\"\n \
      \       self._validate_db_uri()\n        self._validate_url()\n        self._validate_embeddings_schema()\n\
      \        return self"
    signature: def _validate_model(self)
    decorators:
    - '@model_validator(mode="after")'
    raises: []
    calls:
    - target: graphrag/config/models/vector_store_config.py::_validate_db_uri
      type: internal
    - target: graphrag/config/models/vector_store_config.py::_validate_url
      type: internal
    - target: graphrag/config/models/vector_store_config.py::_validate_embeddings_schema
      type: internal
    visibility: protected
    node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_model
    called_by: []
- file_name: graphrag/config/models/vector_store_schema_config.py
  imports:
  - module: re
    name: null
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: pydantic
    name: model_validator
    alias: null
  functions:
  - name: is_valid_field_name
    start_line: 15
    end_line: 17
    code: "def is_valid_field_name(field: str) -> bool:\n    \"\"\"Check if a field\
      \ name is valid for CosmosDB.\"\"\"\n    return bool(VALID_IDENTIFIER_REGEX.match(field))"
    signature: 'def is_valid_field_name(field: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: bool
      type: builtin
    - target: VALID_IDENTIFIER_REGEX.match
      type: unresolved
    visibility: public
    node_id: graphrag/config/models/vector_store_schema_config.py::is_valid_field_name
    called_by:
    - source: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
      type: internal
  - name: _validate_schema
    start_line: 50
    end_line: 60
    code: "def _validate_schema(self) -> None:\n        \"\"\"Validate the schema.\"\
      \"\"\n        for field in [\n            self.id_field,\n            self.vector_field,\n\
      \            self.text_field,\n            self.attributes_field,\n        ]:\n\
      \            if not is_valid_field_name(field):\n                msg = f\"Unsafe\
      \ or invalid field name: {field}\"\n                raise ValueError(msg)"
    signature: def _validate_schema(self) -> None
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/config/models/vector_store_schema_config.py::is_valid_field_name
      type: internal
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
    called_by: []
  - name: _validate_model
    start_line: 63
    end_line: 66
    code: "def _validate_model(self):\n        \"\"\"Validate the model.\"\"\"\n \
      \       self._validate_schema()\n        return self"
    signature: def _validate_model(self)
    decorators:
    - '@model_validator(mode="after")'
    raises: []
    calls:
    - target: graphrag/config/models/vector_store_schema_config.py::_validate_schema
      type: internal
    visibility: protected
    node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_model
    called_by: []
- file_name: graphrag/config/read_dotenv.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: dotenv
    name: dotenv_values
    alias: null
  functions:
  - name: read_dotenv
    start_line: 15
    end_line: 25
    code: "def read_dotenv(root: str) -> None:\n    \"\"\"Read a .env file in the\
      \ given root path.\"\"\"\n    env_path = Path(root) / \".env\"\n    if env_path.exists():\n\
      \        logger.info(\"Loading pipeline .env file\")\n        env_config = dotenv_values(f\"\
      {env_path}\")\n        for key, value in env_config.items():\n            if\
      \ key not in os.environ:\n                os.environ[key] = value or \"\"\n\
      \    else:\n        logger.info(\"No .env file found at %s\", root)"
    signature: 'def read_dotenv(root: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: env_path.exists
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: dotenv::dotenv_values
      type: external
    - target: env_config.items
      type: unresolved
    visibility: public
    node_id: graphrag/config/read_dotenv.py::read_dotenv
    called_by: []
- file_name: graphrag/data_model/__init__.py
  imports: []
  functions: []
- file_name: graphrag/data_model/community.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.named
    name: Named
    alias: null
  functions:
  - name: from_dict
    start_line: 47
    end_line: 79
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
      \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n      \
      \  entities_key: str = \"entity_ids\",\n        relationships_key: str = \"\
      relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n    \
      \    covariates_key: str = \"covariate_ids\",\n        parent_key: str = \"\
      parent\",\n        children_key: str = \"children\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"Community\":\n        \"\"\"Create a new community\
      \ from the dict data.\"\"\"\n        return Community(\n            id=d[id_key],\n\
      \            title=d[title_key],\n            level=d[level_key],\n        \
      \    parent=d[parent_key],\n            children=d[children_key],\n        \
      \    short_id=d.get(short_id_key),\n            entity_ids=d.get(entities_key),\n\
      \            relationship_ids=d.get(relationships_key),\n            text_unit_ids=d.get(text_units_key),\n\
      \            covariate_ids=d.get(covariates_key),\n            attributes=d.get(attributes_key),\n\
      \            size=d.get(size_key),\n            period=d.get(period_key),\n\
      \        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
      \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n      \
      \  entities_key: str = \"entity_ids\",\n        relationships_key: str = \"\
      relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n    \
      \    covariates_key: str = \"covariate_ids\",\n        parent_key: str = \"\
      parent\",\n        children_key: str = \"children\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"Community\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: Community
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/community.py::Community.from_dict
    called_by: []
- file_name: graphrag/data_model/community_report.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.named
    name: Named
    alias: null
  functions:
  - name: from_dict
    start_line: 41
    end_line: 67
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
      \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        summary_key: str = \"summary\",\n        full_content_key: str = \"\
      full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"CommunityReport\":\n        \"\"\"Create a new\
      \ community report from the dict data.\"\"\"\n        return CommunityReport(\n\
      \            id=d[id_key],\n            title=d[title_key],\n            community_id=d[community_id_key],\n\
      \            short_id=d.get(short_id_key),\n            summary=d[summary_key],\n\
      \            full_content=d[full_content_key],\n            rank=d[rank_key],\n\
      \            attributes=d.get(attributes_key),\n            size=d.get(size_key),\n\
      \            period=d.get(period_key),\n        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
      \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        summary_key: str = \"summary\",\n        full_content_key: str = \"\
      full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"CommunityReport\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: CommunityReport
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/community_report.py::CommunityReport.from_dict
    called_by: []
- file_name: graphrag/data_model/covariate.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.identified
    name: Identified
    alias: null
  functions:
  - name: from_dict
    start_line: 36
    end_line: 54
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        subject_id_key: str = \"subject_id\",\n        covariate_type_key:\
      \ str = \"covariate_type\",\n        short_id_key: str = \"human_readable_id\"\
      ,\n        text_unit_ids_key: str = \"text_unit_ids\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Covariate\":\n        \"\"\"Create a new\
      \ covariate from the dict data.\"\"\"\n        return Covariate(\n         \
      \   id=d[id_key],\n            short_id=d.get(short_id_key),\n            subject_id=d[subject_id_key],\n\
      \            covariate_type=d.get(covariate_type_key, \"claim\"),\n        \
      \    text_unit_ids=d.get(text_unit_ids_key),\n            attributes=d.get(attributes_key),\n\
      \        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        subject_id_key: str = \"subject_id\",\n   \
      \     covariate_type_key: str = \"covariate_type\",\n        short_id_key: str\
      \ = \"human_readable_id\",\n        text_unit_ids_key: str = \"text_unit_ids\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Covariate\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: Covariate
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/covariate.py::Covariate.from_dict
    called_by: []
- file_name: graphrag/data_model/document.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: dataclasses
    name: field
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.named
    name: Named
    alias: null
  functions:
  - name: from_dict
    start_line: 29
    end_line: 49
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n      \
      \  title_key: str = \"title\",\n        type_key: str = \"type\",\n        text_key:\
      \ str = \"text\",\n        text_units_key: str = \"text_units\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Document\":\n        \"\"\"Create a new\
      \ document from the dict data.\"\"\"\n        return Document(\n           \
      \ id=d[id_key],\n            short_id=d.get(short_id_key),\n            title=d[title_key],\n\
      \            type=d.get(type_key, \"text\"),\n            text=d[text_key],\n\
      \            text_unit_ids=d.get(text_units_key, []),\n            attributes=d.get(attributes_key),\n\
      \        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      text_key: str = \"text\",\n        text_units_key: str = \"text_units\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Document\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: Document
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/document.py::Document.from_dict
    called_by: []
- file_name: graphrag/data_model/entity.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.named
    name: Named
    alias: null
  functions:
  - name: from_dict
    start_line: 41
    end_line: 69
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n      \
      \  title_key: str = \"title\",\n        type_key: str = \"type\",\n        description_key:\
      \ str = \"description\",\n        description_embedding_key: str = \"description_embedding\"\
      ,\n        name_embedding_key: str = \"name_embedding\",\n        community_key:\
      \ str = \"community\",\n        text_unit_ids_key: str = \"text_unit_ids\",\n\
      \        rank_key: str = \"degree\",\n        attributes_key: str = \"attributes\"\
      ,\n    ) -> \"Entity\":\n        \"\"\"Create a new entity from the dict data.\"\
      \"\"\n        return Entity(\n            id=d[id_key],\n            title=d[title_key],\n\
      \            short_id=d.get(short_id_key),\n            type=d.get(type_key),\n\
      \            description=d.get(description_key),\n            name_embedding=d.get(name_embedding_key),\n\
      \            description_embedding=d.get(description_embedding_key),\n     \
      \       community_ids=d.get(community_key),\n            rank=d.get(rank_key,\
      \ 1),\n            text_unit_ids=d.get(text_unit_ids_key),\n            attributes=d.get(attributes_key),\n\
      \        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      description_key: str = \"description\",\n        description_embedding_key:\
      \ str = \"description_embedding\",\n        name_embedding_key: str = \"name_embedding\"\
      ,\n        community_key: str = \"community\",\n        text_unit_ids_key: str\
      \ = \"text_unit_ids\",\n        rank_key: str = \"degree\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Entity\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: Entity
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/entity.py::Entity.from_dict
    called_by: []
- file_name: graphrag/data_model/identified.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  functions: []
- file_name: graphrag/data_model/named.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: graphrag.data_model.identified
    name: Identified
    alias: null
  functions: []
- file_name: graphrag/data_model/relationship.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.identified
    name: Identified
    alias: null
  functions:
  - name: from_dict
    start_line: 41
    end_line: 65
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n      \
      \  source_key: str = \"source\",\n        target_key: str = \"target\",\n  \
      \      description_key: str = \"description\",\n        rank_key: str = \"rank\"\
      ,\n        weight_key: str = \"weight\",\n        text_unit_ids_key: str = \"\
      text_unit_ids\",\n        attributes_key: str = \"attributes\",\n    ) -> \"\
      Relationship\":\n        \"\"\"Create a new relationship from the dict data.\"\
      \"\"\n        return Relationship(\n            id=d[id_key],\n            short_id=d.get(short_id_key),\n\
      \            source=d[source_key],\n            target=d[target_key],\n    \
      \        rank=d.get(rank_key, 1),\n            description=d.get(description_key),\n\
      \            weight=d.get(weight_key, 1.0),\n            text_unit_ids=d.get(text_unit_ids_key),\n\
      \            attributes=d.get(attributes_key),\n        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        source_key: str = \"source\",\n        target_key: str = \"target\"\
      ,\n        description_key: str = \"description\",\n        rank_key: str =\
      \ \"rank\",\n        weight_key: str = \"weight\",\n        text_unit_ids_key:\
      \ str = \"text_unit_ids\",\n        attributes_key: str = \"attributes\",\n\
      \    ) -> \"Relationship\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: Relationship
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/relationship.py::Relationship.from_dict
    called_by: []
- file_name: graphrag/data_model/schemas.py
  imports: []
  functions: []
- file_name: graphrag/data_model/text_unit.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.identified
    name: Identified
    alias: null
  functions:
  - name: from_dict
    start_line: 38
    end_line: 62
    code: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
      \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n      \
      \  text_key: str = \"text\",\n        entities_key: str = \"entity_ids\",\n\
      \        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
      \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n    \
      \    document_ids_key: str = \"document_ids\",\n        attributes_key: str\
      \ = \"attributes\",\n    ) -> \"TextUnit\":\n        \"\"\"Create a new text\
      \ unit from the dict data.\"\"\"\n        return TextUnit(\n            id=d[id_key],\n\
      \            short_id=d.get(short_id_key),\n            text=d[text_key],\n\
      \            entity_ids=d.get(entities_key),\n            relationship_ids=d.get(relationships_key),\n\
      \            covariate_ids=d.get(covariates_key),\n            n_tokens=d.get(n_tokens_key),\n\
      \            document_ids=d.get(document_ids_key),\n            attributes=d.get(attributes_key),\n\
      \        )"
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        text_key: str = \"text\",\n        entities_key: str = \"entity_ids\"\
      ,\n        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
      \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n    \
      \    document_ids_key: str = \"document_ids\",\n        attributes_key: str\
      \ = \"attributes\",\n    ) -> \"TextUnit\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: TextUnit
      type: unresolved
    - target: d.get
      type: unresolved
    visibility: public
    node_id: graphrag/data_model/text_unit.py::TextUnit.from_dict
    called_by: []
- file_name: graphrag/data_model/types.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  functions: []
- file_name: graphrag/factory/__init__.py
  imports: []
  functions: []
- file_name: graphrag/factory/factory.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: typing
    name: Generic
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  functions:
  - name: __new__
    start_line: 18
    end_line: 22
    code: "def __new__(cls, *args: Any, **kwargs: Any) -> \"Factory\":\n        \"\
      \"\"Create a new instance of Factory if it does not exist.\"\"\"\n        if\
      \ cls._instance is None:\n            cls._instance = super().__new__(cls, *args,\
      \ **kwargs)\n        return cls._instance"
    signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
    decorators: []
    raises: []
    calls:
    - target: super().__new__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/factory/factory.py::Factory.__new__
    called_by: []
  - name: __init__
    start_line: 24
    end_line: 27
    code: "def __init__(self):\n        if not hasattr(self, \"_initialized\"):\n\
      \            self._services: dict[str, Callable[..., T]] = {}\n            self._initialized\
      \ = True"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    visibility: protected
    node_id: graphrag/factory/factory.py::Factory.__init__
    called_by: []
  - name: __contains__
    start_line: 29
    end_line: 31
    code: "def __contains__(self, strategy: str) -> bool:\n        \"\"\"Check if\
      \ a strategy is registered.\"\"\"\n        return strategy in self._services"
    signature: 'def __contains__(self, strategy: str) -> bool'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/factory/factory.py::Factory.__contains__
    called_by: []
  - name: keys
    start_line: 33
    end_line: 35
    code: "def keys(self) -> list[str]:\n        \"\"\"Get a list of registered strategy\
      \ names.\"\"\"\n        return list(self._services.keys())"
    signature: def keys(self) -> list[str]
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: self._services.keys
      type: instance
    visibility: public
    node_id: graphrag/factory/factory.py::Factory.keys
    called_by: []
  - name: register
    start_line: 37
    end_line: 46
    code: "def register(self, *, strategy: str, service_initializer: Callable[...,\
      \ T]) -> None:\n        \"\"\"\n        Register a new service.\n\n        Args\n\
      \        ----\n            strategy: The name of the strategy.\n           \
      \ service_initializer: A callable that creates an instance of T.\n        \"\
      \"\"\n        self._services[strategy] = service_initializer"
    signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
      T]) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/factory/factory.py::Factory.register
    called_by: []
  - name: create
    start_line: 48
    end_line: 68
    code: "def create(self, *, strategy: str, **kwargs: Any) -> T:\n        \"\"\"\
      \n        Create a service instance based on the strategy.\n\n        Args\n\
      \        ----\n            strategy: The name of the strategy.\n           \
      \ **kwargs: Additional arguments to pass to the service initializer.\n\n   \
      \     Returns\n        -------\n            An instance of T.\n\n        Raises\n\
      \        ------\n            ValueError: If the strategy is not registered.\n\
      \        \"\"\"\n        if strategy not in self._services:\n            msg\
      \ = f\"Strategy '{strategy}' is not registered.\"\n            raise ValueError(msg)\n\
      \        return self._services[strategy](**kwargs)"
    signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/factory/factory.py::Factory.create
    called_by: []
- file_name: graphrag/index/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/input/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/input/csv.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: io
    name: BytesIO
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.util
    name: load_files
    alias: null
  - module: graphrag.index.input.util
    name: process_data_columns
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: load_csv
    start_line: 18
    end_line: 43
    code: "async def load_csv(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame:\n    \"\"\"Load csv inputs from a directory.\"\"\"\n    logger.info(\"\
      Loading csv files from %s\", config.storage.base_dir)\n\n    async def load_file(path:\
      \ str, group: dict | None) -> pd.DataFrame:\n        if group is None:\n   \
      \         group = {}\n        buffer = BytesIO(await storage.get(path, as_bytes=True))\n\
      \        data = pd.read_csv(buffer, encoding=config.encoding)\n        additional_keys\
      \ = group.keys()\n        if len(additional_keys) > 0:\n            data[[*additional_keys]]\
      \ = data.apply(\n                lambda _row: pd.Series([group[key] for key\
      \ in additional_keys]), axis=1\n            )\n\n        data = process_data_columns(data,\
      \ config, path)\n\n        creation_date = await storage.get_creation_date(path)\n\
      \        data[\"creation_date\"] = data.apply(lambda _: creation_date, axis=1)\n\
      \n        return data\n\n    return await load_files(load_file, config, storage)"
    signature: "def load_csv(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/input/util.py::load_files
      type: internal
    visibility: public
    node_id: graphrag/index/input/csv.py::load_csv
    called_by: []
  - name: load_file
    start_line: 25
    end_line: 41
    code: "async def load_file(path: str, group: dict | None) -> pd.DataFrame:\n \
      \       if group is None:\n            group = {}\n        buffer = BytesIO(await\
      \ storage.get(path, as_bytes=True))\n        data = pd.read_csv(buffer, encoding=config.encoding)\n\
      \        additional_keys = group.keys()\n        if len(additional_keys) > 0:\n\
      \            data[[*additional_keys]] = data.apply(\n                lambda\
      \ _row: pd.Series([group[key] for key in additional_keys]), axis=1\n       \
      \     )\n\n        data = process_data_columns(data, config, path)\n\n     \
      \   creation_date = await storage.get_creation_date(path)\n        data[\"creation_date\"\
      ] = data.apply(lambda _: creation_date, axis=1)\n\n        return data"
    signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: io::BytesIO
      type: stdlib
    - target: storage.get
      type: unresolved
    - target: pandas::read_csv
      type: external
    - target: group.keys
      type: unresolved
    - target: len
      type: builtin
    - target: data.apply
      type: unresolved
    - target: pandas::Series
      type: external
    - target: graphrag/index/input/util.py::process_data_columns
      type: internal
    - target: storage.get_creation_date
      type: unresolved
    visibility: public
    node_id: graphrag/index/input/csv.py::load_file
    called_by: []
- file_name: graphrag/index/input/factory.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.csv
    name: load_csv
    alias: null
  - module: graphrag.index.input.json
    name: load_json
    alias: null
  - module: graphrag.index.input.text
    name: load_text
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: create_input
    start_line: 27
    end_line: 56
    code: "async def create_input(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame:\n    \"\"\"Instantiate input data for a pipeline.\"\"\"\n\
      \    logger.info(\"loading input from root_dir=%s\", config.storage.base_dir)\n\
      \n    if config.file_type in loaders:\n        logger.info(\"Loading Input %s\"\
      , config.file_type)\n        loader = loaders[config.file_type]\n        result\
      \ = await loader(config, storage)\n        # Convert metadata columns to strings\
      \ and collapse them into a JSON object\n        if config.metadata:\n      \
      \      if all(col in result.columns for col in config.metadata):\n         \
      \       # Collapse the metadata columns into a single JSON object column\n \
      \               result[\"metadata\"] = result[config.metadata].apply(\n    \
      \                lambda row: row.to_dict(), axis=1\n                )\n    \
      \        else:\n                value_error_msg = (\n                    \"\
      One or more metadata columns not found in the DataFrame.\"\n               \
      \ )\n                raise ValueError(value_error_msg)\n\n            result[config.metadata]\
      \ = result[config.metadata].astype(str)\n\n        return cast(\"pd.DataFrame\"\
      , result)\n\n    msg = f\"Unknown input type {config.file_type}\"\n    raise\
      \ ValueError(msg)"
    signature: "def create_input(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: logger.info
      type: unresolved
    - target: loader
      type: unresolved
    - target: all
      type: builtin
    - target: result[config.metadata].apply
      type: unresolved
    - target: row.to_dict
      type: unresolved
    - target: ValueError
      type: builtin
    - target: result[config.metadata].astype
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/index/input/factory.py::create_input
    called_by:
    - source: graphrag/index/workflows/load_input_documents.py::load_input_documents
      type: internal
    - source: graphrag/index/workflows/load_update_documents.py::load_update_documents
      type: internal
    - source: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
      type: internal
- file_name: graphrag/index/input/json.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.util
    name: load_files
    alias: null
  - module: graphrag.index.input.util
    name: process_data_columns
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: load_json
    start_line: 18
    end_line: 47
    code: "async def load_json(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame:\n    \"\"\"Load json inputs from a directory.\"\"\"\n   \
      \ logger.info(\"Loading json files from %s\", config.storage.base_dir)\n\n \
      \   async def load_file(path: str, group: dict | None) -> pd.DataFrame:\n  \
      \      if group is None:\n            group = {}\n        text = await storage.get(path,\
      \ encoding=config.encoding)\n        as_json = json.loads(text)\n        # json\
      \ file could just be a single object, or an array of objects\n        rows =\
      \ as_json if isinstance(as_json, list) else [as_json]\n        data = pd.DataFrame(rows)\n\
      \n        additional_keys = group.keys()\n        if len(additional_keys) >\
      \ 0:\n            data[[*additional_keys]] = data.apply(\n                lambda\
      \ _row: pd.Series([group[key] for key in additional_keys]), axis=1\n       \
      \     )\n\n        data = process_data_columns(data, config, path)\n\n     \
      \   creation_date = await storage.get_creation_date(path)\n        data[\"creation_date\"\
      ] = data.apply(lambda _: creation_date, axis=1)\n\n        return data\n\n \
      \   return await load_files(load_file, config, storage)"
    signature: "def load_json(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/input/util.py::load_files
      type: internal
    visibility: public
    node_id: graphrag/index/input/json.py::load_json
    called_by: []
  - name: load_file
    start_line: 25
    end_line: 45
    code: "async def load_file(path: str, group: dict | None) -> pd.DataFrame:\n \
      \       if group is None:\n            group = {}\n        text = await storage.get(path,\
      \ encoding=config.encoding)\n        as_json = json.loads(text)\n        # json\
      \ file could just be a single object, or an array of objects\n        rows =\
      \ as_json if isinstance(as_json, list) else [as_json]\n        data = pd.DataFrame(rows)\n\
      \n        additional_keys = group.keys()\n        if len(additional_keys) >\
      \ 0:\n            data[[*additional_keys]] = data.apply(\n                lambda\
      \ _row: pd.Series([group[key] for key in additional_keys]), axis=1\n       \
      \     )\n\n        data = process_data_columns(data, config, path)\n\n     \
      \   creation_date = await storage.get_creation_date(path)\n        data[\"creation_date\"\
      ] = data.apply(lambda _: creation_date, axis=1)\n\n        return data"
    signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: storage.get
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: isinstance
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: group.keys
      type: unresolved
    - target: len
      type: builtin
    - target: data.apply
      type: unresolved
    - target: pandas::Series
      type: external
    - target: graphrag/index/input/util.py::process_data_columns
      type: internal
    - target: storage.get_creation_date
      type: unresolved
    visibility: public
    node_id: graphrag/index/input/json.py::load_file
    called_by: []
- file_name: graphrag/index/input/text.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.util
    name: load_files
    alias: null
  - module: graphrag.index.utils.hashing
    name: gen_sha512_hash
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: load_text
    start_line: 19
    end_line: 35
    code: "async def load_text(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame:\n    \"\"\"Load text inputs from a directory.\"\"\"\n\n \
      \   async def load_file(path: str, group: dict | None = None) -> pd.DataFrame:\n\
      \        if group is None:\n            group = {}\n        text = await storage.get(path,\
      \ encoding=config.encoding)\n        new_item = {**group, \"text\": text}\n\
      \        new_item[\"id\"] = gen_sha512_hash(new_item, new_item.keys())\n   \
      \     new_item[\"title\"] = str(Path(path).name)\n        new_item[\"creation_date\"\
      ] = await storage.get_creation_date(path)\n        return pd.DataFrame([new_item])\n\
      \n    return await load_files(load_file, config, storage)"
    signature: "def load_text(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/input/util.py::load_files
      type: internal
    visibility: public
    node_id: graphrag/index/input/text.py::load_text
    called_by: []
  - name: load_file
    start_line: 25
    end_line: 33
    code: "async def load_file(path: str, group: dict | None = None) -> pd.DataFrame:\n\
      \        if group is None:\n            group = {}\n        text = await storage.get(path,\
      \ encoding=config.encoding)\n        new_item = {**group, \"text\": text}\n\
      \        new_item[\"id\"] = gen_sha512_hash(new_item, new_item.keys())\n   \
      \     new_item[\"title\"] = str(Path(path).name)\n        new_item[\"creation_date\"\
      ] = await storage.get_creation_date(path)\n        return pd.DataFrame([new_item])"
    signature: 'def load_file(path: str, group: dict | None = None) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: storage.get
      type: unresolved
    - target: graphrag/index/utils/hashing.py::gen_sha512_hash
      type: internal
    - target: new_item.keys
      type: unresolved
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: storage.get_creation_date
      type: unresolved
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: graphrag/index/input/text.py::load_file
    called_by: []
- file_name: graphrag/index/input/util.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.utils.hashing
    name: gen_sha512_hash
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: load_files
    start_line: 19
    end_line: 53
    code: "async def load_files(\n    loader: Any,\n    config: InputConfig,\n   \
      \ storage: PipelineStorage,\n) -> pd.DataFrame:\n    \"\"\"Load files from storage\
      \ and apply a loader function.\"\"\"\n    files = list(\n        storage.find(\n\
      \            re.compile(config.file_pattern),\n            file_filter=config.file_filter,\n\
      \        )\n    )\n\n    if len(files) == 0:\n        msg = f\"No {config.file_type}\
      \ files found in {config.storage.base_dir}\"\n        raise ValueError(msg)\n\
      \n    files_loaded = []\n\n    for file, group in files:\n        try:\n   \
      \         files_loaded.append(await loader(file, group))\n        except Exception\
      \ as e:  # noqa: BLE001 (catching Exception is fine here)\n            logger.warning(\"\
      Warning! Error loading file %s. Skipping...\", file)\n            logger.warning(\"\
      Error: %s\", e)\n\n    logger.info(\n        \"Found %d %s files, loading %d\"\
      , len(files), config.file_type, len(files_loaded)\n    )\n    result = pd.concat(files_loaded)\n\
      \    total_files_log = (\n        f\"Total number of unfiltered {config.file_type}\
      \ rows: {len(result)}\"\n    )\n    logger.info(total_files_log)\n    return\
      \ result"
    signature: "def load_files(\n    loader: Any,\n    config: InputConfig,\n    storage:\
      \ PipelineStorage,\n) -> pd.DataFrame"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: len
      type: builtin
    - target: ValueError
      type: builtin
    - target: files_loaded.append
      type: unresolved
    - target: loader
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: pandas::concat
      type: external
    visibility: public
    node_id: graphrag/index/input/util.py::load_files
    called_by:
    - source: graphrag/index/input/csv.py::load_csv
      type: internal
    - source: graphrag/index/input/json.py::load_json
      type: internal
    - source: graphrag/index/input/text.py::load_text
      type: internal
  - name: process_data_columns
    start_line: 56
    end_line: 86
    code: "def process_data_columns(\n    documents: pd.DataFrame, config: InputConfig,\
      \ path: str\n) -> pd.DataFrame:\n    \"\"\"Process configured data columns of\
      \ a DataFrame.\"\"\"\n    if \"id\" not in documents.columns:\n        documents[\"\
      id\"] = documents.apply(\n            lambda x: gen_sha512_hash(x, x.keys()),\
      \ axis=1\n        )\n    if config.text_column is not None and \"text\" not\
      \ in documents.columns:\n        if config.text_column not in documents.columns:\n\
      \            logger.warning(\n                \"text_column %s not found in\
      \ csv file %s\",\n                config.text_column,\n                path,\n\
      \            )\n        else:\n            documents[\"text\"] = documents.apply(lambda\
      \ x: x[config.text_column], axis=1)\n    if config.title_column is not None:\n\
      \        if config.title_column not in documents.columns:\n            logger.warning(\n\
      \                \"title_column %s not found in csv file %s\",\n           \
      \     config.title_column,\n                path,\n            )\n        else:\n\
      \            documents[\"title\"] = documents.apply(\n                lambda\
      \ x: x[config.title_column], axis=1\n            )\n    else:\n        documents[\"\
      title\"] = documents.apply(lambda _: path, axis=1)\n    return documents"
    signature: "def process_data_columns(\n    documents: pd.DataFrame, config: InputConfig,\
      \ path: str\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: documents.apply
      type: unresolved
    - target: graphrag/index/utils/hashing.py::gen_sha512_hash
      type: internal
    - target: x.keys
      type: unresolved
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/index/input/util.py::process_data_columns
    called_by:
    - source: graphrag/index/input/csv.py::load_file
      type: internal
    - source: graphrag/index/input/json.py::load_file
      type: internal
- file_name: graphrag/index/operations/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/build_noun_graph/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  imports:
  - module: itertools
    name: combinations
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.noop_pipeline_cache
    name: NoopPipelineCache
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.base
    name: BaseNounPhraseExtractor
    alias: null
  - module: graphrag.index.utils.derive_from_rows
    name: derive_from_rows
    alias: null
  - module: graphrag.index.utils.graphs
    name: calculate_pmi_edge_weights
    alias: null
  - module: graphrag.index.utils.hashing
    name: gen_sha512_hash
    alias: null
  functions:
  - name: build_noun_graph
    start_line: 22
    end_line: 40
    code: "async def build_noun_graph(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    normalize_edge_weights: bool,\n    num_threads:\
      \ int = 4,\n    async_mode: AsyncType = AsyncType.Threaded,\n    cache: PipelineCache\
      \ | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Build a\
      \ noun graph from text units.\"\"\"\n    text_units = text_unit_df.loc[:, [\"\
      id\", \"text\"]]\n    nodes_df = await _extract_nodes(\n        text_units,\n\
      \        text_analyzer,\n        num_threads=num_threads,\n        async_mode=async_mode,\n\
      \        cache=cache,\n    )\n    edges_df = _extract_edges(nodes_df, normalize_edge_weights=normalize_edge_weights)\n\
      \    return (nodes_df, edges_df)"
    signature: "def build_noun_graph(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    normalize_edge_weights: bool,\n    num_threads:\
      \ int = 4,\n    async_mode: AsyncType = AsyncType.Threaded,\n    cache: PipelineCache\
      \ | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
      type: internal
    - target: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
      type: internal
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
    called_by:
    - source: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
      type: internal
  - name: _extract_nodes
    start_line: 43
    end_line: 89
    code: "async def _extract_nodes(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    num_threads: int = 4,\n    async_mode: AsyncType\
      \ = AsyncType.Threaded,\n    cache: PipelineCache | None = None,\n) -> pd.DataFrame:\n\
      \    \"\"\"\n    Extract initial nodes and edges from text units.\n\n    Input:\
      \ text unit df with schema [id, text, document_id]\n    Returns a dataframe\
      \ with schema [id, title, frequency, text_unit_ids].\n    \"\"\"\n    cache\
      \ = cache or NoopPipelineCache()\n    cache = cache.child(\"extract_noun_phrases\"\
      )\n\n    async def extract(row):\n        text = row[\"text\"]\n        attrs\
      \ = {\"text\": text, \"analyzer\": str(text_analyzer)}\n        key = gen_sha512_hash(attrs,\
      \ attrs.keys())\n        result = await cache.get(key)\n        if not result:\n\
      \            result = text_analyzer.extract(text)\n            await cache.set(key,\
      \ result)\n        return result\n\n    text_unit_df[\"noun_phrases\"] = await\
      \ derive_from_rows(\n        text_unit_df,\n        extract,\n        num_threads=num_threads,\n\
      \        async_type=async_mode,\n        progress_msg=\"extract noun phrases\
      \ progress: \",\n    )\n\n    noun_node_df = text_unit_df.explode(\"noun_phrases\"\
      )\n    noun_node_df = noun_node_df.rename(\n        columns={\"noun_phrases\"\
      : \"title\", \"id\": \"text_unit_id\"}\n    )\n\n    # group by title and count\
      \ the number of text units\n    grouped_node_df = (\n        noun_node_df.groupby(\"\
      title\").agg({\"text_unit_id\": list}).reset_index()\n    )\n    grouped_node_df\
      \ = grouped_node_df.rename(columns={\"text_unit_id\": \"text_unit_ids\"})\n\
      \    grouped_node_df[\"frequency\"] = grouped_node_df[\"text_unit_ids\"].apply(len)\n\
      \    grouped_node_df = grouped_node_df[[\"title\", \"frequency\", \"text_unit_ids\"\
      ]]\n    return grouped_node_df.loc[:, [\"title\", \"frequency\", \"text_unit_ids\"\
      ]]"
    signature: "def _extract_nodes(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    num_threads: int = 4,\n    async_mode: AsyncType\
      \ = AsyncType.Threaded,\n    cache: PipelineCache | None = None,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag.cache.noop_pipeline_cache::NoopPipelineCache
      type: internal
    - target: cache.child
      type: unresolved
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
    - target: text_unit_df.explode
      type: unresolved
    - target: noun_node_df.rename
      type: unresolved
    - target: 'noun_node_df.groupby("title").agg({"text_unit_id": list}).reset_index'
      type: unresolved
    - target: noun_node_df.groupby("title").agg
      type: unresolved
    - target: noun_node_df.groupby
      type: unresolved
    - target: grouped_node_df.rename
      type: unresolved
    - target: grouped_node_df["text_unit_ids"].apply
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
    called_by:
    - source: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
      type: internal
  - name: extract
    start_line: 59
    end_line: 67
    code: "async def extract(row):\n        text = row[\"text\"]\n        attrs =\
      \ {\"text\": text, \"analyzer\": str(text_analyzer)}\n        key = gen_sha512_hash(attrs,\
      \ attrs.keys())\n        result = await cache.get(key)\n        if not result:\n\
      \            result = text_analyzer.extract(text)\n            await cache.set(key,\
      \ result)\n        return result"
    signature: def extract(row)
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: graphrag/index/utils/hashing.py::gen_sha512_hash
      type: internal
    - target: attrs.keys
      type: unresolved
    - target: cache.get
      type: unresolved
    - target: text_analyzer.extract
      type: unresolved
    - target: cache.set
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::extract
    called_by: []
  - name: _extract_edges
    start_line: 92
    end_line: 140
    code: "def _extract_edges(\n    nodes_df: pd.DataFrame,\n    normalize_edge_weights:\
      \ bool = True,\n) -> pd.DataFrame:\n    \"\"\"\n    Extract edges from nodes.\n\
      \n    Nodes appear in the same text unit are connected.\n    Input: nodes_df\
      \ with schema [id, title, frequency, text_unit_ids]\n    Returns: edges_df with\
      \ schema [source, target, weight, text_unit_ids]\n    \"\"\"\n    text_units_df\
      \ = nodes_df.explode(\"text_unit_ids\")\n    text_units_df = text_units_df.rename(columns={\"\
      text_unit_ids\": \"text_unit_id\"})\n\n    text_units_df = (\n        text_units_df.groupby(\"\
      text_unit_id\")\n        .agg({\"title\": lambda x: list(x) if len(x) > 1 else\
      \ np.nan})\n        .reset_index()\n    )\n    text_units_df = text_units_df.dropna()\n\
      \    titles = text_units_df[\"title\"].tolist()\n    all_edges: list[list[tuple[str,\
      \ str]]] = [list(combinations(t, 2)) for t in titles]\n\n    text_units_df =\
      \ text_units_df.assign(edges=all_edges)  # type: ignore\n    edge_df = text_units_df.explode(\"\
      edges\")[[\"edges\", \"text_unit_id\"]]\n\n    edge_df[[\"source\", \"target\"\
      ]] = edge_df.loc[:, \"edges\"].to_list()\n    edge_df[\"min_source\"] = edge_df[[\"\
      source\", \"target\"]].min(axis=1)\n    edge_df[\"max_target\"] = edge_df[[\"\
      source\", \"target\"]].max(axis=1)\n    edge_df = edge_df.drop(columns=[\"source\"\
      , \"target\"]).rename(\n        columns={\"min_source\": \"source\", \"max_target\"\
      : \"target\"}  # type: ignore\n    )\n\n    edge_df = edge_df[(edge_df.source.notna())\
      \ & (edge_df.target.notna())]\n    edge_df = edge_df.drop(columns=[\"edges\"\
      ])\n    # group by source and target, count the number of text units\n    grouped_edge_df\
      \ = (\n        edge_df.groupby([\"source\", \"target\"]).agg({\"text_unit_id\"\
      : list}).reset_index()\n    )\n    grouped_edge_df = grouped_edge_df.rename(columns={\"\
      text_unit_id\": \"text_unit_ids\"})\n    grouped_edge_df[\"weight\"] = grouped_edge_df[\"\
      text_unit_ids\"].apply(len)\n    grouped_edge_df = grouped_edge_df.loc[\n  \
      \      :, [\"source\", \"target\", \"weight\", \"text_unit_ids\"]\n    ]\n \
      \   if normalize_edge_weights:\n        # use PMI weight instead of raw weight\n\
      \        grouped_edge_df = calculate_pmi_edge_weights(nodes_df, grouped_edge_df)\n\
      \n    return grouped_edge_df"
    signature: "def _extract_edges(\n    nodes_df: pd.DataFrame,\n    normalize_edge_weights:\
      \ bool = True,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: nodes_df.explode
      type: unresolved
    - target: text_units_df.rename
      type: unresolved
    - target: "text_units_df.groupby(\"text_unit_id\")\n        .agg({\"title\": lambda\
        \ x: list(x) if len(x) > 1 else np.nan})\n        .reset_index"
      type: unresolved
    - target: "text_units_df.groupby(\"text_unit_id\")\n        .agg"
      type: unresolved
    - target: text_units_df.groupby
      type: unresolved
    - target: list
      type: builtin
    - target: len
      type: builtin
    - target: text_units_df.dropna
      type: unresolved
    - target: text_units_df["title"].tolist
      type: unresolved
    - target: itertools::combinations
      type: stdlib
    - target: text_units_df.assign
      type: unresolved
    - target: text_units_df.explode
      type: unresolved
    - target: edge_df.loc[:, "edges"].to_list
      type: unresolved
    - target: edge_df[["source", "target"]].min
      type: unresolved
    - target: edge_df[["source", "target"]].max
      type: unresolved
    - target: edge_df.drop(columns=["source", "target"]).rename
      type: unresolved
    - target: edge_df.drop
      type: unresolved
    - target: edge_df.source.notna
      type: unresolved
    - target: edge_df.target.notna
      type: unresolved
    - target: 'edge_df.groupby(["source", "target"]).agg({"text_unit_id": list}).reset_index'
      type: unresolved
    - target: edge_df.groupby(["source", "target"]).agg
      type: unresolved
    - target: edge_df.groupby
      type: unresolved
    - target: grouped_edge_df.rename
      type: unresolved
    - target: grouped_edge_df["text_unit_ids"].apply
      type: unresolved
    - target: graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
    called_by:
    - source: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
      type: internal
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: abc
    name: ABCMeta
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: spacy
    name: null
    alias: null
  - module: spacy.cli.download
    name: download
    alias: null
  functions:
  - name: __init__
    start_line: 17
    end_line: 29
    code: "def __init__(\n        self,\n        model_name: str | None,\n       \
      \ exclude_nouns: list[str] | None = None,\n        max_word_length: int = 15,\n\
      \        word_delimiter: str = \" \",\n    ) -> None:\n        self.model_name\
      \ = model_name\n        self.max_word_length = max_word_length\n        if exclude_nouns\
      \ is None:\n            exclude_nouns = []\n        self.exclude_nouns = [noun.upper()\
      \ for noun in exclude_nouns]\n        self.word_delimiter = word_delimiter"
    signature: "def __init__(\n        self,\n        model_name: str | None,\n  \
      \      exclude_nouns: list[str] | None = None,\n        max_word_length: int\
      \ = 15,\n        word_delimiter: str = \" \",\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: noun.upper
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__init__
    called_by: []
  - name: extract
    start_line: 32
    end_line: 40
    code: "def extract(self, text: str) -> list[str]:\n        \"\"\"\n        Extract\
      \ noun phrases from text.\n\n        Args:\n            text: Text.\n\n    \
      \    Returns: List of noun phrases.\n        \"\"\""
    signature: 'def extract(self, text: str) -> list[str]'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.extract
    called_by: []
  - name: __str__
    start_line: 43
    end_line: 44
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the extractor, used for cache key generation.\"\"\""
    signature: def __str__(self) -> str
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__str__
    called_by: []
  - name: load_spacy_model
    start_line: 47
    end_line: 61
    code: "def load_spacy_model(\n        model_name: str, exclude: list[str] | None\
      \ = None\n    ) -> spacy.language.Language:\n        \"\"\"Load a SpaCy model.\"\
      \"\"\n        if exclude is None:\n            exclude = []\n        try:\n\
      \            return spacy.load(model_name, exclude=exclude)\n        except\
      \ OSError:\n            msg = f\"Model `{model_name}` not found. Attempting\
      \ to download...\"\n            logger.info(msg)\n            from spacy.cli.download\
      \ import download\n\n            download(model_name)\n            return spacy.load(model_name,\
      \ exclude=exclude)"
    signature: "def load_spacy_model(\n        model_name: str, exclude: list[str]\
      \ | None = None\n    ) -> spacy.language.Language"
    decorators:
    - '@staticmethod'
    raises: []
    calls:
    - target: spacy::load
      type: external
    - target: logger.info
      type: unresolved
    - target: spacy.cli.download::download
      type: external
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.load_spacy_model
    called_by: []
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: spacy.tokens.doc
    name: Doc
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.base
    name: BaseNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: has_valid_token_length
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: is_compound
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: is_valid_entity
    alias: null
  functions:
  - name: __init__
    start_line: 23
    end_line: 69
    code: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
      \        noun_phrase_tags: list[str],\n    ):\n        \"\"\"\n        Noun\
      \ phrase extractor combining CFG-based noun-chunk extraction and NER.\n\n  \
      \      CFG-based extraction was based on TextBlob's fast NP extractor implementation:\n\
      \        This extractor tends to be faster than the dependency-parser-based\
      \ extractors but grammars may need to be changed for different languages.\n\n\
      \        Args:\n            model_name: SpaCy model name.\n            max_word_length:\
      \ Maximum length (in character) of each extracted word.\n            include_named_entities:\
      \ Whether to include named entities in noun phrases\n            exclude_entity_tags:\
      \ list of named entity tags to exclude in noun phrases.\n            exclude_pos_tags:\
      \ List of POS tags to remove in noun phrases.\n            word_delimiter: Delimiter\
      \ for joining words.\n            noun_phrase_grammars: CFG for matching noun\
      \ phrases.\n        \"\"\"\n        super().__init__(\n            model_name=model_name,\n\
      \            max_word_length=max_word_length,\n            exclude_nouns=exclude_nouns,\n\
      \            word_delimiter=word_delimiter,\n        )\n        self.include_named_entities\
      \ = include_named_entities\n        self.exclude_entity_tags = exclude_entity_tags\n\
      \        if not include_named_entities:\n            self.nlp = self.load_spacy_model(\n\
      \                model_name, exclude=[\"lemmatizer\", \"parser\", \"ner\"]\n\
      \            )\n        else:\n            self.nlp = self.load_spacy_model(\n\
      \                model_name, exclude=[\"lemmatizer\", \"parser\"]\n        \
      \    )\n\n        self.exclude_pos_tags = exclude_pos_tags\n        self.noun_phrase_grammars\
      \ = noun_phrase_grammars\n        self.noun_phrase_tags = noun_phrase_tags"
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
      \        noun_phrase_tags: list[str],\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: self.load_spacy_model
      type: instance
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__init__
    called_by: []
  - name: extract
    start_line: 71
    end_line: 124
    code: "def extract(\n        self,\n        text: str,\n    ) -> list[str]:\n\
      \        \"\"\"\n        Extract noun phrases from text. Noun phrases may include\
      \ named entities and noun chunks, which are filtered based on some heuristics.\n\
      \n        Args:\n            text: Text.\n\n        Returns: List of noun phrases.\n\
      \        \"\"\"\n        doc = self.nlp(text)\n\n        filtered_noun_phrases\
      \ = set()\n        if self.include_named_entities:\n            # extract noun\
      \ chunks + entities then filter overlapping spans\n            entities = [\n\
      \                (ent.text, ent.label_)\n                for ent in doc.ents\n\
      \                if ent.label_ not in self.exclude_entity_tags\n           \
      \ ]\n            entity_texts = set({ent[0] for ent in entities})\n        \
      \    cfg_matches = self.extract_cfg_matches(doc)\n            noun_phrases =\
      \ entities + [\n                np for np in cfg_matches if np[0] not in entity_texts\n\
      \            ]\n\n            # filter noun phrases based on heuristics\n  \
      \          tagged_noun_phrases = [\n                self._tag_noun_phrases(np,\
      \ entity_texts) for np in noun_phrases\n            ]\n            for tagged_np\
      \ in tagged_noun_phrases:\n                if (tagged_np[\"is_valid_entity\"\
      ]) or (\n                    (\n                        len(tagged_np[\"cleaned_tokens\"\
      ]) > 1\n                        or tagged_np[\"has_compound_words\"]\n     \
      \               )\n                    and tagged_np[\"has_valid_tokens\"]\n\
      \                ):\n                    filtered_noun_phrases.add(tagged_np[\"\
      cleaned_text\"])\n        else:\n            noun_phrases = self.extract_cfg_matches(doc)\n\
      \            tagged_noun_phrases = [self._tag_noun_phrases(np) for np in noun_phrases]\n\
      \            for tagged_np in tagged_noun_phrases:\n                if (tagged_np[\"\
      has_proper_nouns\"]) or (\n                    (\n                        len(tagged_np[\"\
      cleaned_tokens\"]) > 1\n                        or tagged_np[\"has_compound_words\"\
      ]\n                    )\n                    and tagged_np[\"has_valid_tokens\"\
      ]\n                ):\n                    filtered_noun_phrases.add(tagged_np[\"\
      cleaned_text\"])\n        return list(filtered_noun_phrases)"
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    decorators: []
    raises: []
    calls:
    - target: self.nlp
      type: instance
    - target: set
      type: builtin
    - target: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::extract_cfg_matches
      type: internal
    - target: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::_tag_noun_phrases
      type: internal
    - target: len
      type: builtin
    - target: filtered_noun_phrases.add
      type: unresolved
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract
    called_by: []
  - name: extract_cfg_matches
    start_line: 126
    end_line: 151
    code: "def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]:\n   \
      \     \"\"\"Return noun phrases that match a given context-free grammar.\"\"\
      \"\n        tagged_tokens = [\n            (token.text, token.pos_)\n      \
      \      for token in doc\n            if token.pos_ not in self.exclude_pos_tags\n\
      \            and token.is_space is False\n            and token.text != \"-\"\
      \n        ]\n        merge = True\n        while merge:\n            merge =\
      \ False\n            for index in range(len(tagged_tokens) - 1):\n         \
      \       first, second = tagged_tokens[index], tagged_tokens[index + 1]\n   \
      \             key = first[1], second[1]\n                value = self.noun_phrase_grammars.get(key,\
      \ None)\n                if value:\n                    # find a matching pattern,\
      \ pop the two tokens and insert the merged one\n                    merge =\
      \ True\n                    tagged_tokens.pop(index)\n                    tagged_tokens.pop(index)\n\
      \                    match = f\"{first[0]}{self.word_delimiter}{second[0]}\"\
      \n                    pos = value\n                    tagged_tokens.insert(index,\
      \ (match, pos))\n                    break\n        return [t for t in tagged_tokens\
      \ if t[1] in self.noun_phrase_tags]"
    signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
    decorators: []
    raises: []
    calls:
    - target: range
      type: builtin
    - target: len
      type: builtin
    - target: self.noun_phrase_grammars.get
      type: instance
    - target: tagged_tokens.pop
      type: unresolved
    - target: tagged_tokens.insert
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract_cfg_matches
    called_by: []
  - name: _tag_noun_phrases
    start_line: 153
    end_line: 177
    code: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str], entities:\
      \ set[str] | None = None\n    ) -> dict[str, Any]:\n        \"\"\"Extract attributes\
      \ of a noun chunk, to be used for filtering.\"\"\"\n        tokens = noun_chunk[0].split(self.word_delimiter)\n\
      \        cleaned_tokens = [\n            token for token in tokens if token.upper()\
      \ not in self.exclude_nouns\n        ]\n\n        has_valid_entity = False\n\
      \        if entities and noun_chunk[0] in entities:\n            has_valid_entity\
      \ = is_valid_entity(noun_chunk, cleaned_tokens)\n\n        return {\n      \
      \      \"cleaned_tokens\": cleaned_tokens,\n            \"cleaned_text\": self.word_delimiter.join(cleaned_tokens)\n\
      \            .replace(\"\\n\", \"\")\n            .upper(),\n            \"\
      is_valid_entity\": has_valid_entity,\n            \"has_proper_nouns\": (noun_chunk[1]\
      \ == \"PROPN\"),\n            \"has_compound_words\": is_compound(cleaned_tokens),\n\
      \            \"has_valid_tokens\": has_valid_token_length(\n               \
      \ cleaned_tokens, self.max_word_length\n            ),\n        }"
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str],\
      \ entities: set[str] | None = None\n    ) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: noun_chunk[0].split
      type: unresolved
    - target: token.upper
      type: unresolved
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
      type: internal
    - target: "self.word_delimiter.join(cleaned_tokens)\n            .replace(\"\\\
        n\", \"\")\n            .upper"
      type: instance
    - target: "self.word_delimiter.join(cleaned_tokens)\n            .replace"
      type: instance
    - target: self.word_delimiter.join
      type: instance
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
      type: internal
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
    called_by: []
  - name: __str__
    start_line: 179
    end_line: 181
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the extractor, used for cache key generation.\"\"\"\n        return f\"cfg_{self.model_name}_{self.max_word_length}_{self.include_named_entities}_{self.exclude_entity_tags}_{self.exclude_pos_tags}_{self.exclude_nouns}_{self.word_delimiter}_{self.noun_phrase_grammars}_{self.noun_phrase_tags}\""
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__str__
    called_by: []
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  imports:
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: NounPhraseExtractorType
    alias: null
  - module: graphrag.config.models.extract_graph_nlp_config
    name: TextAnalyzerConfig
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.base
    name: BaseNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.cfg_extractor
    name: CFGNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.regex_extractor
    name: RegexENNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.stop_words
    name: EN_STOP_WORDS
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.syntactic_parsing_extractor
    name: SyntacticNounPhraseExtractor
    alias: null
  functions:
  - name: register
    start_line: 33
    end_line: 35
    code: "def register(cls, np_extractor_type: str, np_extractor: type):\n      \
      \  \"\"\"Register a vector store type.\"\"\"\n        cls.np_extractor_types[np_extractor_type]\
      \ = np_extractor"
    signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.register
    called_by: []
  - name: get_np_extractor
    start_line: 38
    end_line: 75
    code: "def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor:\n\
      \        \"\"\"Get the noun phrase extractor type from a string.\"\"\"\n   \
      \     np_extractor_type = config.extractor_type\n        exclude_nouns = config.exclude_nouns\n\
      \        if exclude_nouns is None:\n            exclude_nouns = EN_STOP_WORDS\n\
      \        match np_extractor_type:\n            case NounPhraseExtractorType.Syntactic:\n\
      \                return SyntacticNounPhraseExtractor(\n                    model_name=config.model_name,\n\
      \                    max_word_length=config.max_word_length,\n             \
      \       include_named_entities=config.include_named_entities,\n            \
      \        exclude_entity_tags=config.exclude_entity_tags,\n                 \
      \   exclude_pos_tags=config.exclude_pos_tags,\n                    exclude_nouns=exclude_nouns,\n\
      \                    word_delimiter=config.word_delimiter,\n               \
      \ )\n            case NounPhraseExtractorType.CFG:\n                grammars\
      \ = {}\n                for key, value in config.noun_phrase_grammars.items():\n\
      \                    grammars[tuple(key.split(\",\"))] = value\n           \
      \     return CFGNounPhraseExtractor(\n                    model_name=config.model_name,\n\
      \                    max_word_length=config.max_word_length,\n             \
      \       include_named_entities=config.include_named_entities,\n            \
      \        exclude_entity_tags=config.exclude_entity_tags,\n                 \
      \   exclude_pos_tags=config.exclude_pos_tags,\n                    exclude_nouns=exclude_nouns,\n\
      \                    word_delimiter=config.word_delimiter,\n               \
      \     noun_phrase_grammars=grammars,\n                    noun_phrase_tags=config.noun_phrase_tags,\n\
      \                )\n            case NounPhraseExtractorType.RegexEnglish:\n\
      \                return RegexENNounPhraseExtractor(\n                    exclude_nouns=exclude_nouns,\n\
      \                    max_word_length=config.max_word_length,\n             \
      \       word_delimiter=config.word_delimiter,\n                )"
    signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor
      type: internal
    - target: config.noun_phrase_grammars.items
      type: unresolved
    - target: tuple
      type: builtin
    - target: key.split
      type: unresolved
    - target: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor
      type: internal
    - target: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor
      type: internal
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.get_np_extractor
    called_by: []
  - name: create_noun_phrase_extractor
    start_line: 78
    end_line: 82
    code: "def create_noun_phrase_extractor(\n    analyzer_config: TextAnalyzerConfig,\n\
      ) -> BaseNounPhraseExtractor:\n    \"\"\"Create a noun phrase extractor from\
      \ a configuration.\"\"\"\n    return NounPhraseExtractorFactory.get_np_extractor(analyzer_config)"
    signature: "def create_noun_phrase_extractor(\n    analyzer_config: TextAnalyzerConfig,\n\
      ) -> BaseNounPhraseExtractor"
    decorators: []
    raises: []
    calls:
    - target: NounPhraseExtractorFactory.get_np_extractor
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::create_noun_phrase_extractor
    called_by:
    - source: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
      type: internal
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py
  imports: []
  functions:
  - name: is_compound
    start_line: 7
    end_line: 12
    code: "def is_compound(tokens: list[str]) -> bool:\n    \"\"\"List of tokens forms\
      \ a compound noun phrase.\"\"\"\n    return any(\n        \"-\" in token and\
      \ len(token.strip()) > 1 and len(token.strip().split(\"-\")) > 1\n        for\
      \ token in tokens\n    )"
    signature: 'def is_compound(tokens: list[str]) -> bool'
    decorators: []
    raises: []
    calls:
    - target: any
      type: builtin
    - target: len
      type: builtin
    - target: token.strip
      type: unresolved
    - target: token.strip().split
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
    called_by:
    - source: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
      type: internal
    - source: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
      type: internal
    - source: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
      type: internal
  - name: has_valid_token_length
    start_line: 15
    end_line: 17
    code: "def has_valid_token_length(tokens: list[str], max_length: int) -> bool:\n\
      \    \"\"\"Check if all tokens have valid length.\"\"\"\n    return all(len(token)\
      \ <= max_length for token in tokens)"
    signature: 'def has_valid_token_length(tokens: list[str], max_length: int) ->
      bool'
    decorators: []
    raises: []
    calls:
    - target: all
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
    called_by:
    - source: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
      type: internal
    - source: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
      type: internal
  - name: is_valid_entity
    start_line: 20
    end_line: 25
    code: "def is_valid_entity(entity: tuple[str, str], tokens: list[str]) -> bool:\n\
      \    \"\"\"Check if the entity is valid.\"\"\"\n    return (entity[1] not in\
      \ [\"CARDINAL\", \"ORDINAL\"] and len(tokens) > 0) or (\n        entity[1] in\
      \ [\"CARDINAL\", \"ORDINAL\"]\n        and (len(tokens) > 1 or is_compound(tokens))\n\
      \    )"
    signature: 'def is_valid_entity(entity: tuple[str, str], tokens: list[str]) ->
      bool'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
      type: internal
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
    called_by:
    - source: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
      type: internal
    - source: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
      type: internal
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  imports:
  - module: re
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: nltk
    name: null
    alias: null
  - module: textblob
    name: TextBlob
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.base
    name: BaseNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.resource_loader
    name: download_if_not_exists
    alias: null
  functions:
  - name: __init__
    start_line: 23
    end_line: 58
    code: "def __init__(\n        self,\n        exclude_nouns: list[str],\n     \
      \   max_word_length: int,\n        word_delimiter: str,\n    ):\n        \"\"\
      \"\n        Noun phrase extractor for English based on TextBlob's fast NP extractor,\
      \ which uses a regex POS tagger and context-free grammars to detect noun phrases.\n\
      \n        NOTE: This is the extractor used in the first bencharmking of LazyGraphRAG\
      \ but it only works for English.\n        It is much faster but likely less\
      \ accurate than the syntactic parser-based extractor.\n        TODO: Reimplement\
      \ this using SpaCy to remove TextBlob dependency.\n\n        Args:\n       \
      \     max_word_length: Maximum length (in character) of each extracted word.\n\
      \            word_delimiter: Delimiter for joining words.\n        \"\"\"\n\
      \        super().__init__(\n            model_name=None,\n            max_word_length=max_word_length,\n\
      \            exclude_nouns=exclude_nouns,\n            word_delimiter=word_delimiter,\n\
      \        )\n        # download corpora\n        download_if_not_exists(\"brown\"\
      )\n        download_if_not_exists(\"treebank\")\n        download_if_not_exists(\"\
      averaged_perceptron_tagger_eng\")\n\n        # download tokenizers\n       \
      \ download_if_not_exists(\"punkt\")\n        download_if_not_exists(\"punkt_tab\"\
      )\n\n        # Preload the corpora to avoid lazy loading issues due to\n   \
      \     # race conditions when running multi-threaded jobs.\n        nltk.corpus.brown.ensure_loaded()\n\
      \        nltk.corpus.treebank.ensure_loaded()"
    signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n\
      \        max_word_length: int,\n        word_delimiter: str,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py::download_if_not_exists
      type: internal
    - target: nltk::corpus.brown.ensure_loaded
      type: external
    - target: nltk::corpus.treebank.ensure_loaded
      type: external
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
    called_by: []
  - name: extract
    start_line: 60
    end_line: 87
    code: "def extract(\n        self,\n        text: str,\n    ) -> list[str]:\n\
      \        \"\"\"\n        Extract noun phrases from text using regex patterns.\n\
      \n        Args:\n            text: Text.\n\n        Returns: List of noun phrases.\n\
      \        \"\"\"\n        blob = TextBlob(text)\n        proper_nouns = [token[0].upper()\
      \ for token in blob.tags if token[1] == \"NNP\"]  # type: ignore\n        tagged_noun_phrases\
      \ = [\n            self._tag_noun_phrases(chunk, proper_nouns)\n           \
      \ for chunk in blob.noun_phrases  # type: ignore\n        ]\n\n        filtered_noun_phrases\
      \ = set()\n        for tagged_np in tagged_noun_phrases:\n            if (\n\
      \                tagged_np[\"has_proper_nouns\"]\n                or len(tagged_np[\"\
      cleaned_tokens\"]) > 1\n                or tagged_np[\"has_compound_words\"\
      ]\n            ) and tagged_np[\"has_valid_tokens\"]:\n                filtered_noun_phrases.add(tagged_np[\"\
      cleaned_text\"])\n        return list(filtered_noun_phrases)"
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    decorators: []
    raises: []
    calls:
    - target: textblob::TextBlob
      type: external
    - target: token[0].upper
      type: unresolved
    - target: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::_tag_noun_phrases
      type: internal
    - target: set
      type: builtin
    - target: len
      type: builtin
    - target: filtered_noun_phrases.add
      type: unresolved
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.extract
    called_by: []
  - name: _tag_noun_phrases
    start_line: 89
    end_line: 119
    code: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
      \ list[str] | None = None\n    ) -> dict[str, Any]:\n        \"\"\"Extract attributes\
      \ of a noun chunk, to be used for filtering.\"\"\"\n        if all_proper_nouns\
      \ is None:\n            all_proper_nouns = []\n        tokens = [token for token\
      \ in re.split(r\"[\\s]+\", noun_phrase) if len(token) > 0]\n        cleaned_tokens\
      \ = [\n            token for token in tokens if token.upper() not in self.exclude_nouns\n\
      \        ]\n        has_proper_nouns = any(\n            token.upper() in all_proper_nouns\
      \ for token in cleaned_tokens\n        )\n        has_compound_words = any(\n\
      \            \"-\" in token\n            and len(token.strip()) > 1\n      \
      \      and len(token.strip().split(\"-\")) > 1\n            for token in cleaned_tokens\n\
      \        )\n        has_valid_tokens = all(\n            re.match(r\"^[a-zA-Z0-9\\\
      -]+\\n?$\", token) for token in cleaned_tokens\n        ) and all(len(token)\
      \ <= self.max_word_length for token in cleaned_tokens)\n        return {\n \
      \           \"cleaned_tokens\": cleaned_tokens,\n            \"cleaned_text\"\
      : self.word_delimiter.join(token for token in cleaned_tokens)\n            .replace(\"\
      \\n\", \"\")\n            .upper(),\n            \"has_proper_nouns\": has_proper_nouns,\n\
      \            \"has_compound_words\": has_compound_words,\n            \"has_valid_tokens\"\
      : has_valid_tokens,\n        }"
    signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
      \ list[str] | None = None\n    ) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: re::split
      type: stdlib
    - target: len
      type: builtin
    - target: token.upper
      type: unresolved
    - target: any
      type: builtin
    - target: token.strip
      type: unresolved
    - target: token.strip().split
      type: unresolved
    - target: all
      type: builtin
    - target: re::match
      type: stdlib
    - target: "self.word_delimiter.join(token for token in cleaned_tokens)\n     \
        \       .replace(\"\\n\", \"\")\n            .upper"
      type: instance
    - target: "self.word_delimiter.join(token for token in cleaned_tokens)\n     \
        \       .replace"
      type: instance
    - target: self.word_delimiter.join
      type: instance
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor._tag_noun_phrases
    called_by: []
  - name: __str__
    start_line: 121
    end_line: 123
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the extractor, used for cache key generation.\"\"\"\n        return f\"regex_en_{self.exclude_nouns}_{self.max_word_length}_{self.word_delimiter}\""
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__str__
    called_by: []
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py
  imports:
  - module: nltk
    name: null
    alias: null
  functions:
  - name: download_if_not_exists
    start_line: 9
    end_line: 38
    code: "def download_if_not_exists(resource_name) -> bool:\n    \"\"\"Download\
      \ nltk resources if they haven't been already.\"\"\"\n    # look under all possible\
      \ categories\n    root_categories = [\n        \"corpora\",\n        \"tokenizers\"\
      ,\n        \"taggers\",\n        \"chunkers\",\n        \"classifiers\",\n \
      \       \"stemmers\",\n        \"stopwords\",\n        \"languages\",\n    \
      \    \"frequent\",\n        \"gate\",\n        \"models\",\n        \"mt\",\n\
      \        \"sentiment\",\n        \"similarity\",\n    ]\n    for category in\
      \ root_categories:\n        try:\n            # if found, stop looking and avoid\
      \ downloading\n            nltk.find(f\"{category}/{resource_name}\")\n    \
      \        return True  # noqa: TRY300\n        except LookupError:\n        \
      \    continue\n\n    # is not found, download\n    nltk.download(resource_name)\n\
      \    return False"
    signature: def download_if_not_exists(resource_name) -> bool
    decorators: []
    raises: []
    calls:
    - target: nltk::find
      type: external
    - target: nltk::download
      type: external
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py::download_if_not_exists
    called_by:
    - source: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
      type: internal
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/stop_words.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: spacy.tokens.span
    name: Span
    alias: null
  - module: spacy.util
    name: filter_spans
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.base
    name: BaseNounPhraseExtractor
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: has_valid_token_length
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: is_compound
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.np_validator
    name: is_valid_entity
    alias: null
  functions:
  - name: __init__
    start_line: 24
    end_line: 61
    code: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n    ):\n        \"\"\"\n        Noun phrase extractor\
      \ based on dependency parsing and NER using SpaCy.\n\n        This extractor\
      \ tends to produce more accurate results than regex-based extractors but is\
      \ slower.\n        Also, it can be used for different languages by using the\
      \ corresponding models from SpaCy.\n\n        Args:\n            model_name:\
      \ SpaCy model name.\n            max_word_length: Maximum length (in character)\
      \ of each extracted word.\n            include_named_entities: Whether to include\
      \ named entities in noun phrases\n            exclude_entity_tags: list of named\
      \ entity tags to exclude in noun phrases.\n            exclude_pos_tags: List\
      \ of POS tags to remove in noun phrases.\n            word_delimiter: Delimiter\
      \ for joining words.\n        \"\"\"\n        super().__init__(\n          \
      \  model_name=model_name,\n            max_word_length=max_word_length,\n  \
      \          exclude_nouns=exclude_nouns,\n            word_delimiter=word_delimiter,\n\
      \        )\n        self.include_named_entities = include_named_entities\n \
      \       self.exclude_entity_tags = exclude_entity_tags\n        if not include_named_entities:\n\
      \            self.nlp = self.load_spacy_model(model_name, exclude=[\"lemmatizer\"\
      , \"ner\"])\n        else:\n            self.nlp = self.load_spacy_model(model_name,\
      \ exclude=[\"lemmatizer\"])\n\n        self.exclude_pos_tags = exclude_pos_tags"
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: self.load_spacy_model
      type: instance
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__init__
    called_by: []
  - name: extract
    start_line: 63
    end_line: 121
    code: "def extract(\n        self,\n        text: str,\n    ) -> list[str]:\n\
      \        \"\"\"\n        Extract noun phrases from text. Noun phrases may include\
      \ named entities and noun chunks, which are filtered based on some heuristics.\n\
      \n        Args:\n            text: Text.\n\n        Returns: List of noun phrases.\n\
      \        \"\"\"\n        doc = self.nlp(text)\n\n        filtered_noun_phrases\
      \ = set()\n        if self.include_named_entities:\n            # extract noun\
      \ chunks + entities then filter overlapping spans\n            entities = [\n\
      \                ent for ent in doc.ents if ent.label_ not in self.exclude_entity_tags\n\
      \            ]\n            spans = entities + list(doc.noun_chunks)\n     \
      \       spans = filter_spans(spans)\n\n            # reading missing entities\n\
      \            missing_entities = [\n                ent\n                for\
      \ ent in entities\n                if not any(ent.text in span.text for span\
      \ in spans)\n            ]\n            spans.extend(missing_entities)\n\n \
      \           # filtering noun phrases based on some heuristics\n            tagged_noun_phrases\
      \ = [\n                self._tag_noun_phrases(span, entities) for span in spans\n\
      \            ]\n            for tagged_np in tagged_noun_phrases:\n        \
      \        if (tagged_np[\"is_valid_entity\"]) or (\n                    (\n \
      \                       len(tagged_np[\"cleaned_tokens\"]) > 1\n           \
      \             or tagged_np[\"has_compound_words\"]\n                    )\n\
      \                    and tagged_np[\"has_valid_tokens\"]\n                ):\n\
      \                    filtered_noun_phrases.add(tagged_np[\"cleaned_text\"])\n\
      \        else:\n            tagged_noun_phrases = [\n                self._tag_noun_phrases(chunk,\
      \ []) for chunk in doc.noun_chunks\n            ]\n            for tagged_np\
      \ in tagged_noun_phrases:\n                if (tagged_np[\"has_proper_noun\"\
      ]) or (\n                    (\n                        len(tagged_np[\"cleaned_tokens\"\
      ]) > 1\n                        or tagged_np[\"has_compound_words\"]\n     \
      \               )\n                    and tagged_np[\"has_valid_tokens\"]\n\
      \                ):\n                    filtered_noun_phrases.add(tagged_np[\"\
      cleaned_text\"])\n\n        return list(filtered_noun_phrases)"
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    decorators: []
    raises: []
    calls:
    - target: self.nlp
      type: instance
    - target: set
      type: builtin
    - target: list
      type: builtin
    - target: spacy.util::filter_spans
      type: external
    - target: any
      type: builtin
    - target: spans.extend
      type: unresolved
    - target: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::_tag_noun_phrases
      type: internal
    - target: len
      type: builtin
    - target: filtered_noun_phrases.add
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.extract
    called_by: []
  - name: _tag_noun_phrases
    start_line: 123
    end_line: 158
    code: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities: list[Span]\n\
      \    ) -> dict[str, Any]:\n        \"\"\"Extract attributes of a noun chunk,\
      \ to be used for filtering.\"\"\"\n        cleaned_tokens = [\n            token\n\
      \            for token in noun_chunk\n            if token.pos_ not in self.exclude_pos_tags\n\
      \            and token.text.upper() not in self.exclude_nouns\n            and\
      \ token.is_space is False\n            and not token.is_punct\n        ]\n \
      \       cleaned_token_texts = [token.text for token in cleaned_tokens]\n   \
      \     cleaned_text_string = (\n            self.word_delimiter.join(cleaned_token_texts).replace(\"\
      \\n\", \"\").upper()\n        )\n\n        noun_chunk_entity_labels = [\n  \
      \          (ent.text, ent.label_) for ent in entities if noun_chunk.text ==\
      \ ent.text\n        ]\n        if noun_chunk_entity_labels:\n            noun_chunk_entity_label\
      \ = noun_chunk_entity_labels[0]\n            valid_entity = is_valid_entity(noun_chunk_entity_label,\
      \ cleaned_token_texts)\n        else:\n            valid_entity = False\n\n\
      \        return {\n            \"cleaned_tokens\": cleaned_tokens,\n       \
      \     \"cleaned_text\": cleaned_text_string,\n            \"is_valid_entity\"\
      : valid_entity,\n            \"has_proper_nouns\": any(token.pos_ == \"PROPN\"\
      \ for token in cleaned_tokens),\n            \"has_compound_words\": is_compound(cleaned_token_texts),\n\
      \            \"has_valid_tokens\": has_valid_token_length(\n               \
      \ cleaned_token_texts, self.max_word_length\n            ),\n        }"
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities:\
      \ list[Span]\n    ) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: token.text.upper
      type: unresolved
    - target: self.word_delimiter.join(cleaned_token_texts).replace("\n", "").upper
      type: instance
    - target: self.word_delimiter.join(cleaned_token_texts).replace
      type: instance
    - target: self.word_delimiter.join
      type: instance
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
      type: internal
    - target: any
      type: builtin
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
      type: internal
    - target: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
    called_by: []
  - name: __str__
    start_line: 160
    end_line: 162
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the extractor, used for cache key generation.\"\"\"\n        return f\"syntactic_{self.model_name}_{self.max_word_length}_{self.include_named_entities}_{self.exclude_entity_tags}_{self.exclude_pos_tags}_{self.exclude_nouns}_{self.word_delimiter}\""
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__str__
    called_by: []
- file_name: graphrag/index/operations/chunk_text/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/chunk_text/bootstrap.py
  imports:
  - module: warnings
    name: null
    alias: null
  - module: nltk
    name: null
    alias: null
  - module: nltk.corpus
    name: wordnet
    alias: null
  functions:
  - name: bootstrap
    start_line: 15
    end_line: 31
    code: "def bootstrap():\n    \"\"\"Bootstrap definition.\"\"\"\n    global initialized_nltk\n\
      \    if not initialized_nltk:\n        import nltk\n        from nltk.corpus\
      \ import wordnet as wn\n\n        nltk.download(\"punkt\")\n        nltk.download(\"\
      punkt_tab\")\n        nltk.download(\"averaged_perceptron_tagger\")\n      \
      \  nltk.download(\"averaged_perceptron_tagger_eng\")\n        nltk.download(\"\
      maxent_ne_chunker\")\n        nltk.download(\"maxent_ne_chunker_tab\")\n   \
      \     nltk.download(\"words\")\n        nltk.download(\"wordnet\")\n       \
      \ wn.ensure_loaded()\n        initialized_nltk = True"
    signature: def bootstrap()
    decorators: []
    raises: []
    calls:
    - target: nltk::download
      type: external
    - target: wn.ensure_loaded
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
    called_by:
    - source: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
      type: internal
- file_name: graphrag/index/operations/chunk_text/chunk_text.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkStrategyType
    alias: null
  - module: graphrag.index.operations.chunk_text.typing
    name: ChunkInput
    alias: null
  - module: graphrag.index.operations.chunk_text.typing
    name: ChunkStrategy
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: run_tokens
    alias: null
  - module: graphrag.index.operations.chunk_text.bootstrap
    name: bootstrap
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: run_sentences
    alias: null
  functions:
  - name: chunk_text
    start_line: 19
    end_line: 79
    code: "def chunk_text(\n    input: pd.DataFrame,\n    column: str,\n    size:\
      \ int,\n    overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    callbacks: WorkflowCallbacks,\n) -> pd.Series:\n    \"\"\"\n    Chunk a\
      \ piece of text into smaller pieces.\n\n    ## Usage\n    ```yaml\n    args:\n\
      \        column: <column name> # The name of the column containing the text\
      \ to chunk, this can either be a column with text, or a column with a list[tuple[doc_id,\
      \ str]]\n        strategy: <strategy config> # The strategy to use to chunk\
      \ the text, see below for more details\n    ```\n\n    ## Strategies\n    The\
      \ text chunk verb uses a strategy to chunk the text. The strategy is an object\
      \ which defines the strategy to use. The following strategies are available:\n\
      \n    ### tokens\n    This strategy uses the [tokens] library to chunk a piece\
      \ of text. The strategy config is as follows:\n\n    ```yaml\n    strategy:\
      \ tokens\n    size: 1200 # Optional, The chunk size to use, default: 1200\n\
      \    overlap: 100 # Optional, The chunk overlap to use, default: 100\n    ```\n\
      \n    ### sentence\n    This strategy uses the nltk library to chunk a piece\
      \ of text into sentences. The strategy config is as follows:\n\n    ```yaml\n\
      \    strategy: sentence\n    ```\n    \"\"\"\n    strategy_exec = load_strategy(strategy)\n\
      \n    num_total = _get_num_total(input, column)\n    tick = progress_ticker(callbacks.progress,\
      \ num_total)\n\n    # collapse the config back to a single object to support\
      \ \"polymorphic\" function call\n    config = ChunkingConfig(size=size, overlap=overlap,\
      \ encoding_model=encoding_model)\n\n    return cast(\n        \"pd.Series\"\
      ,\n        input.apply(\n            cast(\n                \"Any\",\n     \
      \           lambda x: run_strategy(\n                    strategy_exec,\n  \
      \                  x[column],\n                    config,\n               \
      \     tick,\n                ),\n            ),\n            axis=1,\n     \
      \   ),\n    )"
    signature: "def chunk_text(\n    input: pd.DataFrame,\n    column: str,\n    size:\
      \ int,\n    overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    callbacks: WorkflowCallbacks,\n) -> pd.Series"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
      type: internal
    - target: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
      type: internal
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: typing::cast
      type: stdlib
    - target: input.apply
      type: unresolved
    - target: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
      type: internal
    visibility: public
    node_id: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
    called_by:
    - source: graphrag/index/workflows/create_base_text_units.py::chunker
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_chunk_text
      type: internal
  - name: run_strategy
    start_line: 82
    end_line: 111
    code: "def run_strategy(\n    strategy_exec: ChunkStrategy,\n    input: ChunkInput,\n\
      \    config: ChunkingConfig,\n    tick: ProgressTicker,\n) -> list[str | tuple[list[str]\
      \ | None, str, int]]:\n    \"\"\"Run strategy method definition.\"\"\"\n   \
      \ if isinstance(input, str):\n        return [item.text_chunk for item in strategy_exec([input],\
      \ config, tick)]\n\n    # We can work with both just a list of text content\n\
      \    # or a list of tuples of (document_id, text content)\n    # text_to_chunk\
      \ = '''\n    texts = [item if isinstance(item, str) else item[1] for item in\
      \ input]\n\n    strategy_results = strategy_exec(texts, config, tick)\n\n  \
      \  results = []\n    for strategy_result in strategy_results:\n        doc_indices\
      \ = strategy_result.source_doc_indices\n        if isinstance(input[doc_indices[0]],\
      \ str):\n            results.append(strategy_result.text_chunk)\n        else:\n\
      \            doc_ids = [input[doc_idx][0] for doc_idx in doc_indices]\n    \
      \        results.append((\n                doc_ids,\n                strategy_result.text_chunk,\n\
      \                strategy_result.n_tokens,\n            ))\n    return results"
    signature: "def run_strategy(\n    strategy_exec: ChunkStrategy,\n    input: ChunkInput,\n\
      \    config: ChunkingConfig,\n    tick: ProgressTicker,\n) -> list[str | tuple[list[str]\
      \ | None, str, int]]"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: strategy_exec
      type: unresolved
    - target: results.append
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
    called_by:
    - source: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_str
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_str
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple_same_doc
      type: internal
  - name: load_strategy
    start_line: 114
    end_line: 130
    code: "def load_strategy(strategy: ChunkStrategyType) -> ChunkStrategy:\n    \"\
      \"\"Load strategy method definition.\"\"\"\n    match strategy:\n        case\
      \ ChunkStrategyType.tokens:\n            from graphrag.index.operations.chunk_text.strategies\
      \ import run_tokens\n\n            return run_tokens\n        case ChunkStrategyType.sentence:\n\
      \            # NLTK\n            from graphrag.index.operations.chunk_text.bootstrap\
      \ import bootstrap\n            from graphrag.index.operations.chunk_text.strategies\
      \ import run_sentences\n\n            bootstrap()\n            return run_sentences\n\
      \        case _:\n            msg = f\"Unknown strategy: {strategy}\"\n    \
      \        raise ValueError(msg)"
    signature: 'def load_strategy(strategy: ChunkStrategyType) -> ChunkStrategy'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
      type: internal
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
    called_by:
    - source: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_tokens
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_sentence
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_none
      type: internal
  - name: _get_num_total
    start_line: 133
    end_line: 140
    code: "def _get_num_total(output: pd.DataFrame, column: str) -> int:\n    num_total\
      \ = 0\n    for row in output[column]:\n        if isinstance(row, str):\n  \
      \          num_total += 1\n        else:\n            num_total += len(row)\n\
      \    return num_total"
    signature: 'def _get_num_total(output: pd.DataFrame, column: str) -> int'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: len
      type: builtin
    visibility: protected
    node_id: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
    called_by:
    - source: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_default
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_array
      type: internal
- file_name: graphrag/index/operations/chunk_text/strategies.py
  imports:
  - module: collections.abc
    name: Iterable
    alias: null
  - module: nltk
    name: null
    alias: null
  - module: tiktoken
    name: null
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.index.operations.chunk_text.typing
    name: TextChunk
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: TokenChunkerOptions
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: split_multiple_texts_on_tokens
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  functions:
  - name: get_encoding_fn
    start_line: 20
    end_line: 32
    code: "def get_encoding_fn(encoding_name):\n    \"\"\"Get the encoding model.\"\
      \"\"\n    enc = tiktoken.get_encoding(encoding_name)\n\n    def encode(text:\
      \ str) -> list[int]:\n        if not isinstance(text, str):\n            text\
      \ = f\"{text}\"\n        return enc.encode(text)\n\n    def decode(tokens: list[int])\
      \ -> str:\n        return enc.decode(tokens)\n\n    return encode, decode"
    signature: def get_encoding_fn(encoding_name)
    decorators: []
    raises: []
    calls:
    - target: tiktoken::get_encoding
      type: external
    visibility: public
    node_id: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
    called_by:
    - source: graphrag/index/operations/chunk_text/strategies.py::run_tokens
      type: internal
    - source: graphrag/index/workflows/create_base_text_units.py::chunker
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_encode
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_decode
      type: internal
  - name: encode
    start_line: 24
    end_line: 27
    code: "def encode(text: str) -> list[int]:\n        if not isinstance(text, str):\n\
      \            text = f\"{text}\"\n        return enc.encode(text)"
    signature: 'def encode(text: str) -> list[int]'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: enc.encode
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/chunk_text/strategies.py::encode
    called_by: []
  - name: decode
    start_line: 29
    end_line: 30
    code: "def decode(tokens: list[int]) -> str:\n        return enc.decode(tokens)"
    signature: 'def decode(tokens: list[int]) -> str'
    decorators: []
    raises: []
    calls:
    - target: enc.decode
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/chunk_text/strategies.py::decode
    called_by: []
  - name: run_tokens
    start_line: 35
    end_line: 55
    code: "def run_tokens(\n    input: list[str],\n    config: ChunkingConfig,\n \
      \   tick: ProgressTicker,\n) -> Iterable[TextChunk]:\n    \"\"\"Chunks text\
      \ into chunks based on encoding tokens.\"\"\"\n    tokens_per_chunk = config.size\n\
      \    chunk_overlap = config.overlap\n    encoding_name = config.encoding_model\n\
      \n    encode, decode = get_encoding_fn(encoding_name)\n    return split_multiple_texts_on_tokens(\n\
      \        input,\n        TokenChunkerOptions(\n            chunk_overlap=chunk_overlap,\n\
      \            tokens_per_chunk=tokens_per_chunk,\n            encode=encode,\n\
      \            decode=decode,\n        ),\n        tick,\n    )"
    signature: "def run_tokens(\n    input: list[str],\n    config: ChunkingConfig,\n\
      \    tick: ProgressTicker,\n) -> Iterable[TextChunk]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
      type: internal
    - target: graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
      type: internal
    - target: graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
      type: internal
    visibility: public
    node_id: graphrag/index/operations/chunk_text/strategies.py::run_tokens
    called_by:
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
      type: internal
  - name: run_sentences
    start_line: 58
    end_line: 69
    code: "def run_sentences(\n    input: list[str], _config: ChunkingConfig, tick:\
      \ ProgressTicker\n) -> Iterable[TextChunk]:\n    \"\"\"Chunks text into multiple\
      \ parts by sentence.\"\"\"\n    for doc_idx, text in enumerate(input):\n   \
      \     sentences = nltk.sent_tokenize(text)\n        for sentence in sentences:\n\
      \            yield TextChunk(\n                text_chunk=sentence,\n      \
      \          source_doc_indices=[doc_idx],\n            )\n        tick(1)"
    signature: "def run_sentences(\n    input: list[str], _config: ChunkingConfig,\
      \ tick: ProgressTicker\n) -> Iterable[TextChunk]"
    decorators: []
    raises: []
    calls:
    - target: enumerate
      type: builtin
    - target: nltk::sent_tokenize
      type: external
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    - target: tick
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/chunk_text/strategies.py::run_sentences
    called_by:
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
      type: internal
    - source: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
      type: internal
- file_name: graphrag/index/operations/chunk_text/typing.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  functions: []
- file_name: graphrag/index/operations/cluster_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graspologic.partition
    name: hierarchical_leiden
    alias: null
  - module: graphrag.index.utils.stable_lcc
    name: stable_largest_connected_component
    alias: null
  functions:
  - name: cluster_graph
    start_line: 19
    end_line: 53
    code: "def cluster_graph(\n    graph: nx.Graph,\n    max_cluster_size: int,\n\
      \    use_lcc: bool,\n    seed: int | None = None,\n) -> Communities:\n    \"\
      \"\"Apply a hierarchical clustering algorithm to a graph.\"\"\"\n    if len(graph.nodes)\
      \ == 0:\n        logger.warning(\"Graph has no nodes\")\n        return []\n\
      \n    node_id_to_community_map, parent_mapping = _compute_leiden_communities(\n\
      \        graph=graph,\n        max_cluster_size=max_cluster_size,\n        use_lcc=use_lcc,\n\
      \        seed=seed,\n    )\n\n    levels = sorted(node_id_to_community_map.keys())\n\
      \n    clusters: dict[int, dict[int, list[str]]] = {}\n    for level in levels:\n\
      \        result = {}\n        clusters[level] = result\n        for node_id,\
      \ raw_community_id in node_id_to_community_map[level].items():\n           \
      \ community_id = raw_community_id\n            if community_id not in result:\n\
      \                result[community_id] = []\n            result[community_id].append(node_id)\n\
      \n    results: Communities = []\n    for level in clusters:\n        for cluster_id,\
      \ nodes in clusters[level].items():\n            results.append((level, cluster_id,\
      \ parent_mapping[cluster_id], nodes))\n    return results"
    signature: "def cluster_graph(\n    graph: nx.Graph,\n    max_cluster_size: int,\n\
      \    use_lcc: bool,\n    seed: int | None = None,\n) -> Communities"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
      type: internal
    - target: sorted
      type: builtin
    - target: node_id_to_community_map.keys
      type: unresolved
    - target: node_id_to_community_map[level].items
      type: unresolved
    - target: result[community_id].append
      type: unresolved
    - target: clusters[level].items
      type: unresolved
    - target: results.append
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/cluster_graph.py::cluster_graph
    called_by:
    - source: graphrag/index/workflows/create_communities.py::create_communities
      type: internal
  - name: _compute_leiden_communities
    start_line: 57
    end_line: 80
    code: "def _compute_leiden_communities(\n    graph: nx.Graph | nx.DiGraph,\n \
      \   max_cluster_size: int,\n    use_lcc: bool,\n    seed: int | None = None,\n\
      ) -> tuple[dict[int, dict[str, int]], dict[int, int]]:\n    \"\"\"Return Leiden\
      \ root communities and their hierarchy mapping.\"\"\"\n    if use_lcc:\n   \
      \     graph = stable_largest_connected_component(graph)\n\n    community_mapping\
      \ = hierarchical_leiden(\n        graph, max_cluster_size=max_cluster_size,\
      \ random_seed=seed\n    )\n    results: dict[int, dict[str, int]] = {}\n   \
      \ hierarchy: dict[int, int] = {}\n    for partition in community_mapping:\n\
      \        results[partition.level] = results.get(partition.level, {})\n     \
      \   results[partition.level][partition.node] = partition.cluster\n\n       \
      \ hierarchy[partition.cluster] = (\n            partition.parent_cluster if\
      \ partition.parent_cluster is not None else -1\n        )\n\n    return results,\
      \ hierarchy"
    signature: "def _compute_leiden_communities(\n    graph: nx.Graph | nx.DiGraph,\n\
      \    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int | None = None,\n\
      ) -> tuple[dict[int, dict[str, int]], dict[int, int]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
    - target: graspologic.partition::hierarchical_leiden
      type: external
    - target: results.get
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
    called_by:
    - source: graphrag/index/operations/cluster_graph.py::cluster_graph
      type: internal
- file_name: graphrag/index/operations/compute_degree.py
  imports:
  - module: networkx
    name: null
    alias: nx
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: compute_degree
    start_line: 10
    end_line: 15
    code: "def compute_degree(graph: nx.Graph) -> pd.DataFrame:\n    \"\"\"Create\
      \ a new DataFrame with the degree of each node in the graph.\"\"\"\n    return\
      \ pd.DataFrame([\n        {\"title\": node, \"degree\": int(degree)}\n     \
      \   for node, degree in graph.degree  # type: ignore\n    ])"
    signature: 'def compute_degree(graph: nx.Graph) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: int
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/compute_degree.py::compute_degree
    called_by:
    - source: graphrag/index/operations/finalize_entities.py::finalize_entities
      type: internal
    - source: graphrag/index/operations/finalize_relationships.py::finalize_relationships
      type: internal
- file_name: graphrag/index/operations/compute_edge_combined_degree.py
  imports:
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: compute_edge_combined_degree
    start_line: 11
    end_line: 39
    code: "def compute_edge_combined_degree(\n    edge_df: pd.DataFrame,\n    node_degree_df:\
      \ pd.DataFrame,\n    node_name_column: str,\n    node_degree_column: str,\n\
      \    edge_source_column: str,\n    edge_target_column: str,\n) -> pd.Series:\n\
      \    \"\"\"Compute the combined degree for each edge in a graph.\"\"\"\n\n \
      \   def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame:\n   \
      \     degree_column = _degree_colname(column)\n        result = df.merge(\n\
      \            node_degree_df.rename(\n                columns={node_name_column:\
      \ column, node_degree_column: degree_column}\n            ),\n            on=column,\n\
      \            how=\"left\",\n        )\n        result[degree_column] = result[degree_column].fillna(0)\n\
      \        return result\n\n    output_df = join_to_degree(edge_df, edge_source_column)\n\
      \    output_df = join_to_degree(output_df, edge_target_column)\n    output_df[\"\
      combined_degree\"] = (\n        output_df[_degree_colname(edge_source_column)]\n\
      \        + output_df[_degree_colname(edge_target_column)]\n    )\n    return\
      \ cast(\"pd.Series\", output_df[\"combined_degree\"])"
    signature: "def compute_edge_combined_degree(\n    edge_df: pd.DataFrame,\n  \
      \  node_degree_df: pd.DataFrame,\n    node_name_column: str,\n    node_degree_column:\
      \ str,\n    edge_source_column: str,\n    edge_target_column: str,\n) -> pd.Series"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
      type: internal
    - target: graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
      type: internal
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
    called_by:
    - source: graphrag/index/operations/finalize_relationships.py::finalize_relationships
      type: internal
  - name: join_to_degree
    start_line: 21
    end_line: 31
    code: "def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame:\n  \
      \      degree_column = _degree_colname(column)\n        result = df.merge(\n\
      \            node_degree_df.rename(\n                columns={node_name_column:\
      \ column, node_degree_column: degree_column}\n            ),\n            on=column,\n\
      \            how=\"left\",\n        )\n        result[degree_column] = result[degree_column].fillna(0)\n\
      \        return result"
    signature: 'def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
      type: internal
    - target: df.merge
      type: unresolved
    - target: node_degree_df.rename
      type: unresolved
    - target: result[degree_column].fillna
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
    called_by:
    - source: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
      type: internal
  - name: _degree_colname
    start_line: 42
    end_line: 43
    code: "def _degree_colname(column: str) -> str:\n    return f\"{column}_degree\""
    signature: 'def _degree_colname(column: str) -> str'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
    called_by:
    - source: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
      type: internal
    - source: graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
      type: internal
- file_name: graphrag/index/operations/create_graph.py
  imports:
  - module: networkx
    name: null
    alias: nx
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: create_graph
    start_line: 10
    end_line: 23
    code: "def create_graph(\n    edges: pd.DataFrame,\n    edge_attr: list[str |\
      \ int] | None = None,\n    nodes: pd.DataFrame | None = None,\n    node_id:\
      \ str = \"title\",\n) -> nx.Graph:\n    \"\"\"Create a networkx graph from nodes\
      \ and edges dataframes.\"\"\"\n    graph = nx.from_pandas_edgelist(edges, edge_attr=edge_attr)\n\
      \n    if nodes is not None:\n        nodes.set_index(node_id, inplace=True)\n\
      \        graph.add_nodes_from((n, dict(d)) for n, d in nodes.iterrows())\n\n\
      \    return graph"
    signature: "def create_graph(\n    edges: pd.DataFrame,\n    edge_attr: list[str\
      \ | int] | None = None,\n    nodes: pd.DataFrame | None = None,\n    node_id:\
      \ str = \"title\",\n) -> nx.Graph"
    decorators: []
    raises: []
    calls:
    - target: networkx::from_pandas_edgelist
      type: external
    - target: nodes.set_index
      type: unresolved
    - target: graph.add_nodes_from
      type: unresolved
    - target: dict
      type: builtin
    - target: nodes.iterrows
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/create_graph.py::create_graph
    called_by:
    - source: graphrag/index/operations/finalize_entities.py::finalize_entities
      type: internal
    - source: graphrag/index/operations/finalize_relationships.py::finalize_relationships
      type: internal
    - source: graphrag/index/workflows/create_communities.py::create_communities
      type: internal
    - source: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/prune_graph.py::prune_graph
      type: internal
- file_name: graphrag/index/operations/embed_graph/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/embed_graph/embed_graph.py
  imports:
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.config.models.embed_graph_config
    name: EmbedGraphConfig
    alias: null
  - module: graphrag.index.operations.embed_graph.embed_node2vec
    name: embed_node2vec
    alias: null
  - module: graphrag.index.operations.embed_graph.typing
    name: NodeEmbeddings
    alias: null
  - module: graphrag.index.utils.stable_lcc
    name: stable_largest_connected_component
    alias: null
  functions:
  - name: embed_graph
    start_line: 16
    end_line: 50
    code: "def embed_graph(\n    graph: nx.Graph,\n    config: EmbedGraphConfig,\n\
      ) -> NodeEmbeddings:\n    \"\"\"\n    Embed a graph into a vector space using\
      \ node2vec. The graph is expected to be in nx.Graph format. The operation outputs\
      \ a mapping between node name and vector.\n\n    ## Usage\n    ```yaml\n   \
      \ dimensions: 1536 # Optional, The number of dimensions to use for the embedding,\
      \ default: 1536\n    num_walks: 10 # Optional, The number of walks to use for\
      \ the embedding, default: 10\n    walk_length: 40 # Optional, The walk length\
      \ to use for the embedding, default: 40\n    window_size: 2 # Optional, The\
      \ window size to use for the embedding, default: 2\n    iterations: 3 # Optional,\
      \ The number of iterations to use for the embedding, default: 3\n    random_seed:\
      \ 86 # Optional, The random seed to use for the embedding, default: 86\n   \
      \ ```\n    \"\"\"\n    if config.use_lcc:\n        graph = stable_largest_connected_component(graph)\n\
      \n    # create graph embedding using node2vec\n    embeddings = embed_node2vec(\n\
      \        graph=graph,\n        dimensions=config.dimensions,\n        num_walks=config.num_walks,\n\
      \        walk_length=config.walk_length,\n        window_size=config.window_size,\n\
      \        iterations=config.iterations,\n        random_seed=config.random_seed,\n\
      \    )\n\n    pairs = zip(embeddings.nodes, embeddings.embeddings.tolist(),\
      \ strict=True)\n    sorted_pairs = sorted(pairs, key=lambda x: x[0])\n\n   \
      \ return dict(sorted_pairs)"
    signature: "def embed_graph(\n    graph: nx.Graph,\n    config: EmbedGraphConfig,\n\
      ) -> NodeEmbeddings"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
    - target: graphrag/index/operations/embed_graph/embed_node2vec.py::embed_node2vec
      type: internal
    - target: zip
      type: builtin
    - target: embeddings.embeddings.tolist
      type: unresolved
    - target: sorted
      type: builtin
    - target: dict
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
    called_by:
    - source: graphrag/index/operations/finalize_entities.py::finalize_entities
      type: internal
- file_name: graphrag/index/operations/embed_graph/embed_node2vec.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: numpy
    name: null
    alias: np
  - module: graspologic
    name: null
    alias: gc
  functions:
  - name: embed_node2vec
    start_line: 20
    end_line: 43
    code: "def embed_node2vec(\n    graph: nx.Graph | nx.DiGraph,\n    dimensions:\
      \ int = 1536,\n    num_walks: int = 10,\n    walk_length: int = 40,\n    window_size:\
      \ int = 2,\n    iterations: int = 3,\n    random_seed: int = 86,\n) -> NodeEmbeddings:\n\
      \    \"\"\"Generate node embeddings using Node2Vec.\"\"\"\n    # NOTE: This\
      \ import is done here to reduce the initial import time of the graphrag package\n\
      \    import graspologic as gc\n\n    # generate embedding\n    lcc_tensors =\
      \ gc.embed.node2vec_embed(  # type: ignore\n        graph=graph,\n        dimensions=dimensions,\n\
      \        window_size=window_size,\n        iterations=iterations,\n        num_walks=num_walks,\n\
      \        walk_length=walk_length,\n        random_seed=random_seed,\n    )\n\
      \    return NodeEmbeddings(embeddings=lcc_tensors[0], nodes=lcc_tensors[1])"
    signature: "def embed_node2vec(\n    graph: nx.Graph | nx.DiGraph,\n    dimensions:\
      \ int = 1536,\n    num_walks: int = 10,\n    walk_length: int = 40,\n    window_size:\
      \ int = 2,\n    iterations: int = 3,\n    random_seed: int = 86,\n) -> NodeEmbeddings"
    decorators: []
    raises: []
    calls:
    - target: graspologic::embed.node2vec_embed
      type: external
    - target: NodeEmbeddings
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/embed_graph/embed_node2vec.py::embed_node2vec
    called_by:
    - source: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
      type: internal
- file_name: graphrag/index/operations/embed_graph/typing.py
  imports:
  - module: typing
    name: Any
    alias: null
  functions: []
- file_name: graphrag/index/operations/embed_text/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/embed_text/embed_text.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.embeddings
    name: create_index_name
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.typing
    name: TextEmbeddingStrategy
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.factory
    name: VectorStoreFactory
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.openai
    name: run
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.mock
    name: run
    alias: null
  functions:
  - name: __repr__
    start_line: 34
    end_line: 36
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType.__repr__
    called_by: []
  - name: embed_text
    start_line: 39
    end_line: 78
    code: "async def embed_text(\n    input: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    embed_column: str,\n    strategy: dict,\n  \
      \  embedding_name: str,\n    id_column: str = \"id\",\n    title_column: str\
      \ | None = None,\n):\n    \"\"\"Embed a piece of text into a vector space. The\
      \ operation outputs a new column containing a mapping between doc_id and vector.\"\
      \"\"\n    vector_store_config = strategy.get(\"vector_store\")\n\n    if vector_store_config:\n\
      \        index_name = _get_index_name(vector_store_config, embedding_name)\n\
      \        vector_store: BaseVectorStore = _create_vector_store(\n           \
      \ vector_store_config, index_name, embedding_name\n        )\n        vector_store_workflow_config\
      \ = vector_store_config.get(\n            embedding_name, vector_store_config\n\
      \        )\n        return await _text_embed_with_vector_store(\n          \
      \  input=input,\n            callbacks=callbacks,\n            cache=cache,\n\
      \            embed_column=embed_column,\n            strategy=strategy,\n  \
      \          vector_store=vector_store,\n            vector_store_config=vector_store_workflow_config,\n\
      \            id_column=id_column,\n            title_column=title_column,\n\
      \        )\n\n    return await _text_embed_in_memory(\n        input=input,\n\
      \        callbacks=callbacks,\n        cache=cache,\n        embed_column=embed_column,\n\
      \        strategy=strategy,\n    )"
    signature: "def embed_text(\n    input: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    embed_column: str,\n    strategy: dict,\n  \
      \  embedding_name: str,\n    id_column: str = \"id\",\n    title_column: str\
      \ | None = None,\n)"
    decorators: []
    raises: []
    calls:
    - target: strategy.get
      type: unresolved
    - target: graphrag/index/operations/embed_text/embed_text.py::_get_index_name
      type: internal
    - target: graphrag/index/operations/embed_text/embed_text.py::_create_vector_store
      type: internal
    - target: vector_store_config.get
      type: unresolved
    - target: graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
      type: internal
    - target: graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
      type: internal
    visibility: public
    node_id: graphrag/index/operations/embed_text/embed_text.py::embed_text
    called_by:
    - source: graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
      type: internal
  - name: _text_embed_in_memory
    start_line: 81
    end_line: 95
    code: "async def _text_embed_in_memory(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column: str,\n  \
      \  strategy: dict,\n):\n    strategy_type = strategy[\"type\"]\n    strategy_exec\
      \ = load_strategy(strategy_type)\n    strategy_config = {**strategy}\n\n   \
      \ texts: list[str] = input[embed_column].to_numpy().tolist()\n    result = await\
      \ strategy_exec(texts, callbacks, cache, strategy_config)\n\n    return result.embeddings"
    signature: "def _text_embed_in_memory(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column: str,\n  \
      \  strategy: dict,\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/embed_text/embed_text.py::load_strategy
      type: internal
    - target: input[embed_column].to_numpy().tolist
      type: unresolved
    - target: input[embed_column].to_numpy
      type: unresolved
    - target: strategy_exec
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::embed_text
      type: internal
  - name: _text_embed_with_vector_store
    start_line: 98
    end_line: 183
    code: "async def _text_embed_with_vector_store(\n    input: pd.DataFrame,\n  \
      \  callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column:\
      \ str,\n    strategy: dict[str, Any],\n    vector_store: BaseVectorStore,\n\
      \    vector_store_config: dict,\n    id_column: str = \"id\",\n    title_column:\
      \ str | None = None,\n):\n    strategy_type = strategy[\"type\"]\n    strategy_exec\
      \ = load_strategy(strategy_type)\n    strategy_config = {**strategy}\n\n   \
      \ # Get vector-storage configuration\n    insert_batch_size: int = (\n     \
      \   vector_store_config.get(\"batch_size\") or DEFAULT_EMBEDDING_BATCH_SIZE\n\
      \    )\n\n    overwrite: bool = vector_store_config.get(\"overwrite\", True)\n\
      \n    if embed_column not in input.columns:\n        msg = f\"Column {embed_column}\
      \ not found in input dataframe with columns {input.columns}\"\n        raise\
      \ ValueError(msg)\n    title = title_column or embed_column\n    if title not\
      \ in input.columns:\n        msg = (\n            f\"Column {title} not found\
      \ in input dataframe with columns {input.columns}\"\n        )\n        raise\
      \ ValueError(msg)\n    if id_column not in input.columns:\n        msg = f\"\
      Column {id_column} not found in input dataframe with columns {input.columns}\"\
      \n        raise ValueError(msg)\n\n    total_rows = 0\n    for row in input[embed_column]:\n\
      \        if isinstance(row, list):\n            total_rows += len(row)\n   \
      \     else:\n            total_rows += 1\n\n    i = 0\n    starting_index =\
      \ 0\n\n    all_results = []\n\n    num_total_batches = (input.shape[0] + insert_batch_size\
      \ - 1) // insert_batch_size\n    while insert_batch_size * i < input.shape[0]:\n\
      \        logger.info(\n            \"uploading text embeddings batch %d/%d of\
      \ size %d to vector store\",\n            i + 1,\n            num_total_batches,\n\
      \            insert_batch_size,\n        )\n        batch = input.iloc[insert_batch_size\
      \ * i : insert_batch_size * (i + 1)]\n        texts: list[str] = batch[embed_column].to_numpy().tolist()\n\
      \        titles: list[str] = batch[title].to_numpy().tolist()\n        ids:\
      \ list[str] = batch[id_column].to_numpy().tolist()\n        result = await strategy_exec(texts,\
      \ callbacks, cache, strategy_config)\n        if result.embeddings:\n      \
      \      embeddings = [\n                embedding for embedding in result.embeddings\
      \ if embedding is not None\n            ]\n            all_results.extend(embeddings)\n\
      \n        vectors = result.embeddings or []\n        documents: list[VectorStoreDocument]\
      \ = []\n        for doc_id, doc_text, doc_title, doc_vector in zip(\n      \
      \      ids, texts, titles, vectors, strict=True\n        ):\n            if\
      \ type(doc_vector) is np.ndarray:\n                doc_vector = doc_vector.tolist()\n\
      \            document = VectorStoreDocument(\n                id=doc_id,\n \
      \               text=doc_text,\n                vector=doc_vector,\n       \
      \         attributes={\"title\": doc_title},\n            )\n            documents.append(document)\n\
      \n        vector_store.load_documents(documents, overwrite and i == 0)\n   \
      \     starting_index += len(documents)\n        i += 1\n\n    return all_results"
    signature: "def _text_embed_with_vector_store(\n    input: pd.DataFrame,\n   \
      \ callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column:\
      \ str,\n    strategy: dict[str, Any],\n    vector_store: BaseVectorStore,\n\
      \    vector_store_config: dict,\n    id_column: str = \"id\",\n    title_column:\
      \ str | None = None,\n)"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/index/operations/embed_text/embed_text.py::load_strategy
      type: internal
    - target: vector_store_config.get
      type: unresolved
    - target: ValueError
      type: builtin
    - target: isinstance
      type: builtin
    - target: len
      type: builtin
    - target: logger.info
      type: unresolved
    - target: batch[embed_column].to_numpy().tolist
      type: unresolved
    - target: batch[embed_column].to_numpy
      type: unresolved
    - target: batch[title].to_numpy().tolist
      type: unresolved
    - target: batch[title].to_numpy
      type: unresolved
    - target: batch[id_column].to_numpy().tolist
      type: unresolved
    - target: batch[id_column].to_numpy
      type: unresolved
    - target: strategy_exec
      type: unresolved
    - target: all_results.extend
      type: unresolved
    - target: zip
      type: builtin
    - target: type
      type: builtin
    - target: doc_vector.tolist
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: documents.append
      type: unresolved
    - target: vector_store.load_documents
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::embed_text
      type: internal
  - name: _create_vector_store
    start_line: 186
    end_line: 217
    code: "def _create_vector_store(\n    vector_store_config: dict, index_name: str,\
      \ embedding_name: str | None = None\n) -> BaseVectorStore:\n    vector_store_type:\
      \ str = str(vector_store_config.get(\"type\"))\n\n    embeddings_schema: dict[str,\
      \ VectorStoreSchemaConfig] = vector_store_config.get(\n        \"embeddings_schema\"\
      , {}\n    )\n    single_embedding_config: VectorStoreSchemaConfig = VectorStoreSchemaConfig()\n\
      \n    if (\n        embeddings_schema is not None\n        and embedding_name\
      \ is not None\n        and embedding_name in embeddings_schema\n    ):\n   \
      \     raw_config = embeddings_schema[embedding_name]\n        if isinstance(raw_config,\
      \ dict):\n            single_embedding_config = VectorStoreSchemaConfig(**raw_config)\n\
      \        else:\n            single_embedding_config = raw_config\n\n    if single_embedding_config.index_name\
      \ is None:\n        single_embedding_config.index_name = index_name\n\n    vector_store\
      \ = VectorStoreFactory().create_vector_store(\n        vector_store_schema_config=single_embedding_config,\n\
      \        vector_store_type=vector_store_type,\n        **vector_store_config,\n\
      \    )\n\n    vector_store.connect(**vector_store_config)\n    return vector_store"
    signature: "def _create_vector_store(\n    vector_store_config: dict, index_name:\
      \ str, embedding_name: str | None = None\n) -> BaseVectorStore"
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: vector_store_config.get
      type: unresolved
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    - target: VectorStoreFactory().create_vector_store
      type: unresolved
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory
      type: internal
    - target: vector_store.connect
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/embed_text.py::_create_vector_store
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::embed_text
      type: internal
  - name: _get_index_name
    start_line: 220
    end_line: 226
    code: "def _get_index_name(vector_store_config: dict, embedding_name: str) ->\
      \ str:\n    container_name = vector_store_config.get(\"container_name\", \"\
      default\")\n    index_name = create_index_name(container_name, embedding_name)\n\
      \n    msg = f\"using vector store {vector_store_config.get('type')} with container_name\
      \ {container_name} for embedding {embedding_name}: {index_name}\"\n    logger.info(msg)\n\
      \    return index_name"
    signature: 'def _get_index_name(vector_store_config: dict, embedding_name: str)
      -> str'
    decorators: []
    raises: []
    calls:
    - target: vector_store_config.get
      type: unresolved
    - target: graphrag/config/embeddings.py::create_index_name
      type: internal
    - target: logger.info
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/embed_text.py::_get_index_name
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::embed_text
      type: internal
  - name: load_strategy
    start_line: 229
    end_line: 246
    code: "def load_strategy(strategy: TextEmbedStrategyType) -> TextEmbeddingStrategy:\n\
      \    \"\"\"Load strategy method definition.\"\"\"\n    match strategy:\n   \
      \     case TextEmbedStrategyType.openai:\n            from graphrag.index.operations.embed_text.strategies.openai\
      \ import (\n                run as run_openai,\n            )\n\n          \
      \  return run_openai\n        case TextEmbedStrategyType.mock:\n           \
      \ from graphrag.index.operations.embed_text.strategies.mock import (\n     \
      \           run as run_mock,\n            )\n\n            return run_mock\n\
      \        case _:\n            msg = f\"Unknown strategy: {strategy}\"\n    \
      \        raise ValueError(msg)"
    signature: 'def load_strategy(strategy: TextEmbedStrategyType) -> TextEmbeddingStrategy'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/embed_text/embed_text.py::load_strategy
    called_by:
    - source: graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
      type: internal
    - source: graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
      type: internal
- file_name: graphrag/index/operations/embed_text/strategies/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/embed_text/strategies/mock.py
  imports:
  - module: random
    name: null
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.typing
    name: TextEmbeddingResult
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  functions:
  - name: run
    start_line: 16
    end_line: 29
    code: "async def run(  # noqa RUF029 async is required for interface\n    input:\
      \ list[str],\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    _args: dict[str, Any],\n) -> TextEmbeddingResult:\n    \"\"\"Run the Claim\
      \ extraction chain.\"\"\"\n    input = input if isinstance(input, Iterable)\
      \ else [input]\n    ticker = progress_ticker(\n        callbacks.progress, len(input),\
      \ description=\"generate embeddings progress: \"\n    )\n    return TextEmbeddingResult(\n\
      \        embeddings=[_embed_text(cache, text, ticker) for text in input]\n \
      \   )"
    signature: "def run(  # noqa RUF029 async is required for interface\n    input:\
      \ list[str],\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    _args: dict[str, Any],\n) -> TextEmbeddingResult"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: len
      type: builtin
    - target: graphrag/index/operations/embed_text/strategies/typing.py::TextEmbeddingResult
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/mock.py::_embed_text
      type: internal
    visibility: public
    node_id: graphrag/index/operations/embed_text/strategies/mock.py::run
    called_by: []
  - name: _embed_text
    start_line: 32
    end_line: 35
    code: "def _embed_text(_cache: PipelineCache, _text: str, tick: ProgressTicker)\
      \ -> list[float]:\n    \"\"\"Embed a single piece of text.\"\"\"\n    tick(1)\n\
      \    return [random.random(), random.random(), random.random()]  # noqa S311"
    signature: 'def _embed_text(_cache: PipelineCache, _text: str, tick: ProgressTicker)
      -> list[float]'
    decorators: []
    raises: []
    calls:
    - target: tick
      type: unresolved
    - target: random::random
      type: stdlib
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/mock.py::_embed_text
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/mock.py::run
      type: internal
- file_name: graphrag/index/operations/embed_text/strategies/openai.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.typing
    name: TextEmbeddingResult
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: TokenTextSplitter
    alias: null
  - module: graphrag.index.utils.is_null
    name: is_null
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  functions:
  - name: run
    start_line: 26
    end_line: 76
    code: "async def run(\n    input: list[str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    args: dict[str, Any],\n) -> TextEmbeddingResult:\n\
      \    \"\"\"Run the Claim extraction chain.\"\"\"\n    if is_null(input):\n \
      \       return TextEmbeddingResult(embeddings=None)\n\n    batch_size = args.get(\"\
      batch_size\", 16)\n    batch_max_tokens = args.get(\"batch_max_tokens\", 8191)\n\
      \    llm_config = args[\"llm\"]\n    llm_config = LanguageModelConfig(**args[\"\
      llm\"])\n    splitter = _get_splitter(llm_config, batch_max_tokens)\n    model\
      \ = ModelManager().get_or_create_embedding_model(\n        name=\"text_embedding\"\
      ,\n        model_type=llm_config.type,\n        config=llm_config,\n       \
      \ callbacks=callbacks,\n        cache=cache,\n    )\n    semaphore: asyncio.Semaphore\
      \ = asyncio.Semaphore(args.get(\"num_threads\", 4))\n\n    # Break up the input\
      \ texts. The sizes here indicate how many snippets are in each input text\n\
      \    texts, input_sizes = _prepare_embed_texts(input, splitter)\n    text_batches\
      \ = _create_text_batches(\n        texts,\n        batch_size,\n        batch_max_tokens,\n\
      \        splitter,\n    )\n    logger.info(\n        \"embedding %d inputs via\
      \ %d snippets using %d batches. max_batch_size=%d, batch_max_tokens=%d\",\n\
      \        len(input),\n        len(texts),\n        len(text_batches),\n    \
      \    batch_size,\n        batch_max_tokens,\n    )\n    ticker = progress_ticker(\n\
      \        callbacks.progress,\n        len(text_batches),\n        description=\"\
      generate embeddings progress: \",\n    )\n\n    # Embed each chunk of snippets\n\
      \    embeddings = await _execute(model, text_batches, ticker, semaphore)\n \
      \   embeddings = _reconstitute_embeddings(embeddings, input_sizes)\n\n    return\
      \ TextEmbeddingResult(embeddings=embeddings)"
    signature: "def run(\n    input: list[str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    args: dict[str, Any],\n) -> TextEmbeddingResult"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/is_null.py::is_null
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/typing.py::TextEmbeddingResult
      type: internal
    - target: args.get
      type: unresolved
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
      type: internal
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: asyncio::Semaphore
      type: stdlib
    - target: graphrag/index/operations/embed_text/strategies/openai.py::_prepare_embed_texts
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/openai.py::_create_text_batches
      type: internal
    - target: logger.info
      type: unresolved
    - target: len
      type: builtin
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/openai.py::_execute
      type: internal
    - target: graphrag/index/operations/embed_text/strategies/openai.py::_reconstitute_embeddings
      type: internal
    visibility: public
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::run
    called_by: []
  - name: _get_splitter
    start_line: 79
    end_line: 85
    code: "def _get_splitter(\n    config: LanguageModelConfig, batch_max_tokens:\
      \ int\n) -> TokenTextSplitter:\n    return TokenTextSplitter(\n        tokenizer=get_tokenizer(model_config=config),\n\
      \        chunk_size=batch_max_tokens,\n    )"
    signature: "def _get_splitter(\n    config: LanguageModelConfig, batch_max_tokens:\
      \ int\n) -> TokenTextSplitter"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
  - name: _execute
    start_line: 88
    end_line: 104
    code: "async def _execute(\n    model: EmbeddingModel,\n    chunks: list[list[str]],\n\
      \    tick: ProgressTicker,\n    semaphore: asyncio.Semaphore,\n) -> list[list[float]]:\n\
      \    async def embed(chunk: list[str]):\n        async with semaphore:\n   \
      \         chunk_embeddings = await model.aembed_batch(chunk)\n            result\
      \ = np.array(chunk_embeddings)\n            tick(1)\n        return result\n\
      \n    futures = [embed(chunk) for chunk in chunks]\n    results = await asyncio.gather(*futures)\n\
      \    # merge results in a single list of lists (reduce the collect dimension)\n\
      \    return [item for sublist in results for item in sublist]"
    signature: "def _execute(\n    model: EmbeddingModel,\n    chunks: list[list[str]],\n\
      \    tick: ProgressTicker,\n    semaphore: asyncio.Semaphore,\n) -> list[list[float]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/embed_text/strategies/openai.py::embed
      type: internal
    - target: asyncio::gather
      type: stdlib
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::_execute
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
  - name: embed
    start_line: 94
    end_line: 99
    code: "async def embed(chunk: list[str]):\n        async with semaphore:\n   \
      \         chunk_embeddings = await model.aembed_batch(chunk)\n            result\
      \ = np.array(chunk_embeddings)\n            tick(1)\n        return result"
    signature: 'def embed(chunk: list[str])'
    decorators: []
    raises: []
    calls:
    - target: model.aembed_batch
      type: unresolved
    - target: numpy::array
      type: external
    - target: tick
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::embed
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::_execute
      type: internal
  - name: _create_text_batches
    start_line: 107
    end_line: 136
    code: "def _create_text_batches(\n    texts: list[str],\n    max_batch_size: int,\n\
      \    max_batch_tokens: int,\n    splitter: TokenTextSplitter,\n) -> list[list[str]]:\n\
      \    \"\"\"Create batches of texts to embed.\"\"\"\n    # https://learn.microsoft.com/en-us/azure/ai-services/openai/reference\n\
      \    # According to this embeddings reference, Azure limits us to 16 concurrent\
      \ embeddings and 8191 tokens per request\n    result = []\n    current_batch\
      \ = []\n    current_batch_tokens = 0\n\n    for text in texts:\n        token_count\
      \ = splitter.num_tokens(text)\n        if (\n            len(current_batch)\
      \ >= max_batch_size\n            or current_batch_tokens + token_count > max_batch_tokens\n\
      \        ):\n            result.append(current_batch)\n            current_batch\
      \ = []\n            current_batch_tokens = 0\n\n        current_batch.append(text)\n\
      \        current_batch_tokens += token_count\n\n    if len(current_batch) >\
      \ 0:\n        result.append(current_batch)\n\n    return result"
    signature: "def _create_text_batches(\n    texts: list[str],\n    max_batch_size:\
      \ int,\n    max_batch_tokens: int,\n    splitter: TokenTextSplitter,\n) -> list[list[str]]"
    decorators: []
    raises: []
    calls:
    - target: splitter.num_tokens
      type: unresolved
    - target: len
      type: builtin
    - target: result.append
      type: unresolved
    - target: current_batch.append
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::_create_text_batches
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
  - name: _prepare_embed_texts
    start_line: 139
    end_line: 155
    code: "def _prepare_embed_texts(\n    input: list[str], splitter: TokenTextSplitter\n\
      ) -> tuple[list[str], list[int]]:\n    sizes: list[int] = []\n    snippets:\
      \ list[str] = []\n\n    for text in input:\n        # Split the input text and\
      \ filter out any empty content\n        split_texts = splitter.split_text(text)\n\
      \        if split_texts is None:\n            continue\n        split_texts\
      \ = [text for text in split_texts if len(text) > 0]\n\n        sizes.append(len(split_texts))\n\
      \        snippets.extend(split_texts)\n\n    return snippets, sizes"
    signature: "def _prepare_embed_texts(\n    input: list[str], splitter: TokenTextSplitter\n\
      ) -> tuple[list[str], list[int]]"
    decorators: []
    raises: []
    calls:
    - target: splitter.split_text
      type: unresolved
    - target: len
      type: builtin
    - target: sizes.append
      type: unresolved
    - target: snippets.extend
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::_prepare_embed_texts
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
  - name: _reconstitute_embeddings
    start_line: 158
    end_line: 177
    code: "def _reconstitute_embeddings(\n    raw_embeddings: list[list[float]], sizes:\
      \ list[int]\n) -> list[list[float] | None]:\n    \"\"\"Reconstitute the embeddings\
      \ into the original input texts.\"\"\"\n    embeddings: list[list[float] | None]\
      \ = []\n    cursor = 0\n    for size in sizes:\n        if size == 0:\n    \
      \        embeddings.append(None)\n        elif size == 1:\n            embedding\
      \ = raw_embeddings[cursor]\n            embeddings.append(embedding)\n     \
      \       cursor += 1\n        else:\n            chunk = raw_embeddings[cursor\
      \ : cursor + size]\n            average = np.average(chunk, axis=0)\n      \
      \      normalized = average / np.linalg.norm(average)\n            embeddings.append(normalized.tolist())\n\
      \            cursor += size\n    return embeddings"
    signature: "def _reconstitute_embeddings(\n    raw_embeddings: list[list[float]],\
      \ sizes: list[int]\n) -> list[list[float] | None]"
    decorators: []
    raises: []
    calls:
    - target: embeddings.append
      type: unresolved
    - target: numpy::average
      type: external
    - target: numpy::linalg.norm
      type: external
    - target: normalized.tolist
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/embed_text/strategies/openai.py::_reconstitute_embeddings
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
- file_name: graphrag/index/operations/embed_text/strategies/typing.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  functions: []
- file_name: graphrag/index/operations/extract_covariates/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/extract_covariates/claim_extractor.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.index.extract_claims
    name: CONTINUE_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_claims
    name: EXTRACT_CLAIMS_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_claims
    name: LOOP_PROMPT
    alias: null
  functions:
  - name: __init__
    start_line: 50
    end_line: 85
    code: "def __init__(\n        self,\n        model_invoker: ChatModel,\n     \
      \   extraction_prompt: str | None = None,\n        input_text_key: str | None\
      \ = None,\n        input_entity_spec_key: str | None = None,\n        input_claim_description_key:\
      \ str | None = None,\n        input_resolved_entities_key: str | None = None,\n\
      \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n    ):\n        \"\"\"Init method definition.\"\"\"\n    \
      \    self._model = model_invoker\n        self._extraction_prompt = extraction_prompt\
      \ or EXTRACT_CLAIMS_PROMPT\n        self._input_text_key = input_text_key or\
      \ \"input_text\"\n        self._input_entity_spec_key = input_entity_spec_key\
      \ or \"entity_specs\"\n        self._tuple_delimiter_key = tuple_delimiter_key\
      \ or \"tuple_delimiter\"\n        self._record_delimiter_key = record_delimiter_key\
      \ or \"record_delimiter\"\n        self._completion_delimiter_key = (\n    \
      \        completion_delimiter_key or \"completion_delimiter\"\n        )\n \
      \       self._input_claim_description_key = (\n            input_claim_description_key\
      \ or \"claim_description\"\n        )\n        self._input_resolved_entities_key\
      \ = (\n            input_resolved_entities_key or \"resolved_entities\"\n  \
      \      )\n        self._max_gleanings = (\n            max_gleanings\n     \
      \       if max_gleanings is not None\n            else graphrag_config_defaults.extract_claims.max_gleanings\n\
      \        )\n        self._on_error = on_error or (lambda _e, _s, _d: None)"
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        input_text_key: str\
      \ | None = None,\n        input_entity_spec_key: str | None = None,\n      \
      \  input_claim_description_key: str | None = None,\n        input_resolved_entities_key:\
      \ str | None = None,\n        tuple_delimiter_key: str | None = None,\n    \
      \    record_delimiter_key: str | None = None,\n        completion_delimiter_key:\
      \ str | None = None,\n        max_gleanings: int | None = None,\n        on_error:\
      \ ErrorHandlerFn | None = None,\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__init__
    called_by: []
  - name: __call__
    start_line: 87
    end_line: 133
    code: "async def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
      \ dict | None = None\n    ) -> ClaimExtractorResult:\n        \"\"\"Call method\
      \ definition.\"\"\"\n        if prompt_variables is None:\n            prompt_variables\
      \ = {}\n        texts = inputs[self._input_text_key]\n        entity_spec =\
      \ str(inputs[self._input_entity_spec_key])\n        claim_description = inputs[self._input_claim_description_key]\n\
      \        resolved_entities = inputs.get(self._input_resolved_entities_key, {})\n\
      \        source_doc_map = {}\n\n        prompt_args = {\n            self._input_entity_spec_key:\
      \ entity_spec,\n            self._input_claim_description_key: claim_description,\n\
      \            self._tuple_delimiter_key: prompt_variables.get(self._tuple_delimiter_key)\n\
      \            or DEFAULT_TUPLE_DELIMITER,\n            self._record_delimiter_key:\
      \ prompt_variables.get(self._record_delimiter_key)\n            or DEFAULT_RECORD_DELIMITER,\n\
      \            self._completion_delimiter_key: prompt_variables.get(\n       \
      \         self._completion_delimiter_key\n            )\n            or DEFAULT_COMPLETION_DELIMITER,\n\
      \        }\n\n        all_claims: list[dict] = []\n        for doc_index, text\
      \ in enumerate(texts):\n            document_id = f\"d{doc_index}\"\n      \
      \      try:\n                claims = await self._process_document(prompt_args,\
      \ text, doc_index)\n                all_claims += [\n                    self._clean_claim(c,\
      \ document_id, resolved_entities) for c in claims\n                ]\n     \
      \           source_doc_map[document_id] = text\n            except Exception\
      \ as e:\n                logger.exception(\"error extracting claim\")\n    \
      \            self._on_error(\n                    e,\n                    traceback.format_exc(),\n\
      \                    {\"doc_index\": doc_index, \"text\": text},\n         \
      \       )\n                continue\n\n        return ClaimExtractorResult(\n\
      \            output=all_claims,\n            source_docs=source_doc_map,\n \
      \       )"
    signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
      \ dict | None = None\n    ) -> ClaimExtractorResult"
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: inputs.get
      type: unresolved
    - target: prompt_variables.get
      type: unresolved
    - target: enumerate
      type: builtin
    - target: graphrag/index/operations/extract_covariates/claim_extractor.py::_process_document
      type: internal
    - target: graphrag/index/operations/extract_covariates/claim_extractor.py::_clean_claim
      type: internal
    - target: logger.exception
      type: unresolved
    - target: self._on_error
      type: instance
    - target: traceback::format_exc
      type: stdlib
    - target: ClaimExtractorResult
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__call__
    called_by: []
  - name: _clean_claim
    start_line: 135
    end_line: 147
    code: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
      \ dict\n    ) -> dict:\n        # clean the parsed claims to remove any claims\
      \ with status = False\n        obj = claim.get(\"object_id\", claim.get(\"object\"\
      ))\n        subject = claim.get(\"subject_id\", claim.get(\"subject\"))\n\n\
      \        # If subject or object in resolved entities, then replace with resolved\
      \ entity\n        obj = resolved_entities.get(obj, obj)\n        subject = resolved_entities.get(subject,\
      \ subject)\n        claim[\"object_id\"] = obj\n        claim[\"subject_id\"\
      ] = subject\n        return claim"
    signature: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
      \ dict\n    ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: claim.get
      type: unresolved
    - target: resolved_entities.get
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._clean_claim
    called_by: []
  - name: _process_document
    start_line: 149
    end_line: 195
    code: "async def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
      \ int\n    ) -> list[dict]:\n        record_delimiter = prompt_args.get(\n \
      \           self._record_delimiter_key, DEFAULT_RECORD_DELIMITER\n        )\n\
      \        completion_delimiter = prompt_args.get(\n            self._completion_delimiter_key,\
      \ DEFAULT_COMPLETION_DELIMITER\n        )\n\n        response = await self._model.achat(\n\
      \            self._extraction_prompt.format(**{\n                self._input_text_key:\
      \ doc,\n                **prompt_args,\n            })\n        )\n        results\
      \ = response.output.content or \"\"\n        claims = results.strip().removesuffix(completion_delimiter)\n\
      \n        # if gleanings are specified, enter a loop to extract more claims\n\
      \        # there are two exit criteria: (a) we hit the configured max, (b) the\
      \ model says there are no more claims\n        if self._max_gleanings > 0:\n\
      \            for i in range(self._max_gleanings):\n                response\
      \ = await self._model.achat(\n                    CONTINUE_PROMPT,\n       \
      \             name=f\"extract-continuation-{i}\",\n                    history=response.history,\n\
      \                )\n                extension = response.output.content or \"\
      \"\n                claims += record_delimiter + extension.strip().removesuffix(\n\
      \                    completion_delimiter\n                )\n\n           \
      \     # If this isn't the last loop, check to see if we should continue\n  \
      \              if i >= self._max_gleanings - 1:\n                    break\n\
      \n                response = await self._model.achat(\n                    LOOP_PROMPT,\n\
      \                    name=f\"extract-loopcheck-{i}\",\n                    history=response.history,\n\
      \                )\n\n                if response.output.content != \"Y\":\n\
      \                    break\n\n        return self._parse_claim_tuples(results,\
      \ prompt_args)"
    signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
      \ int\n    ) -> list[dict]"
    decorators: []
    raises: []
    calls:
    - target: prompt_args.get
      type: unresolved
    - target: self._model.achat
      type: instance
    - target: self._extraction_prompt.format
      type: instance
    - target: results.strip().removesuffix
      type: unresolved
    - target: results.strip
      type: unresolved
    - target: range
      type: builtin
    - target: extension.strip().removesuffix
      type: unresolved
    - target: extension.strip
      type: unresolved
    - target: graphrag/index/operations/extract_covariates/claim_extractor.py::_parse_claim_tuples
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._process_document
    called_by: []
  - name: _parse_claim_tuples
    start_line: 197
    end_line: 236
    code: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
      \ dict\n    ) -> list[dict[str, Any]]:\n        \"\"\"Parse claim tuples.\"\"\
      \"\n        record_delimiter = prompt_variables.get(\n            self._record_delimiter_key,\
      \ DEFAULT_RECORD_DELIMITER\n        )\n        completion_delimiter = prompt_variables.get(\n\
      \            self._completion_delimiter_key, DEFAULT_COMPLETION_DELIMITER\n\
      \        )\n        tuple_delimiter = prompt_variables.get(\n            self._tuple_delimiter_key,\
      \ DEFAULT_TUPLE_DELIMITER\n        )\n\n        def pull_field(index: int, fields:\
      \ list[str]) -> str | None:\n            return fields[index].strip() if len(fields)\
      \ > index else None\n\n        result: list[dict[str, Any]] = []\n        claims_values\
      \ = (\n            claims.strip().removesuffix(completion_delimiter).split(record_delimiter)\n\
      \        )\n        for claim in claims_values:\n            claim = claim.strip().removeprefix(\"\
      (\").removesuffix(\")\")\n\n            # Ignore the completion delimiter\n\
      \            if claim == completion_delimiter:\n                continue\n\n\
      \            claim_fields = claim.split(tuple_delimiter)\n            result.append({\n\
      \                \"subject_id\": pull_field(0, claim_fields),\n            \
      \    \"object_id\": pull_field(1, claim_fields),\n                \"type\":\
      \ pull_field(2, claim_fields),\n                \"status\": pull_field(3, claim_fields),\n\
      \                \"start_date\": pull_field(4, claim_fields),\n            \
      \    \"end_date\": pull_field(5, claim_fields),\n                \"description\"\
      : pull_field(6, claim_fields),\n                \"source_text\": pull_field(7,\
      \ claim_fields),\n            })\n        return result"
    signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
      \ dict\n    ) -> list[dict[str, Any]]"
    decorators: []
    raises: []
    calls:
    - target: prompt_variables.get
      type: unresolved
    - target: claims.strip().removesuffix(completion_delimiter).split
      type: unresolved
    - target: claims.strip().removesuffix
      type: unresolved
    - target: claims.strip
      type: unresolved
    - target: claim.strip().removeprefix("(").removesuffix
      type: unresolved
    - target: claim.strip().removeprefix
      type: unresolved
    - target: claim.strip
      type: unresolved
    - target: claim.split
      type: unresolved
    - target: result.append
      type: unresolved
    - target: graphrag/index/operations/extract_covariates/claim_extractor.py::pull_field
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._parse_claim_tuples
    called_by: []
  - name: pull_field
    start_line: 211
    end_line: 212
    code: "def pull_field(index: int, fields: list[str]) -> str | None:\n        \
      \    return fields[index].strip() if len(fields) > index else None"
    signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
    decorators: []
    raises: []
    calls:
    - target: fields[index].strip
      type: unresolved
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.pull_field
    called_by: []
- file_name: graphrag/index/operations/extract_covariates/extract_covariates.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: dataclasses
    name: asdict
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.extract_covariates.claim_extractor
    name: ClaimExtractor
    alias: null
  - module: graphrag.index.operations.extract_covariates.typing
    name: Covariate
    alias: null
  - module: graphrag.index.operations.extract_covariates.typing
    name: CovariateExtractionResult
    alias: null
  - module: graphrag.index.utils.derive_from_rows
    name: derive_from_rows
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  functions:
  - name: extract_covariates
    start_line: 32
    end_line: 76
    code: "async def extract_covariates(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    column: str,\n    covariate_type:\
      \ str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    entity_types: list[str] | None = None,\n    num_threads: int = 4,\n):\n\
      \    \"\"\"Extract claims from a piece of text.\"\"\"\n    logger.debug(\"extract_covariates\
      \ strategy=%s\", strategy)\n    if entity_types is None:\n        entity_types\
      \ = DEFAULT_ENTITY_TYPES\n\n    resolved_entities_map = {}\n\n    strategy =\
      \ strategy or {}\n    strategy_config = {**strategy}\n\n    async def run_strategy(row):\n\
      \        text = row[column]\n        result = await run_extract_claims(\n  \
      \          input=text,\n            entity_types=entity_types,\n           \
      \ resolved_entities_map=resolved_entities_map,\n            callbacks=callbacks,\n\
      \            cache=cache,\n            strategy_config=strategy_config,\n  \
      \      )\n        return [\n            create_row_from_claim_data(row, item,\
      \ covariate_type)\n            for item in result.covariate_data\n        ]\n\
      \n    results = await derive_from_rows(\n        input,\n        run_strategy,\n\
      \        callbacks,\n        async_type=async_mode,\n        num_threads=num_threads,\n\
      \        progress_msg=\"extract covariates progress: \",\n    )\n    return\
      \ pd.DataFrame([item for row in results for item in row or []])"
    signature: "def extract_covariates(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    column: str,\n    covariate_type:\
      \ str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    entity_types: list[str] | None = None,\n    num_threads: int = 4,\n)"
    decorators: []
    raises: []
    calls:
    - target: logger.debug
      type: unresolved
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
    called_by:
    - source: graphrag/index/workflows/extract_covariates.py::run_workflow
      type: internal
  - name: run_strategy
    start_line: 53
    end_line: 66
    code: "async def run_strategy(row):\n        text = row[column]\n        result\
      \ = await run_extract_claims(\n            input=text,\n            entity_types=entity_types,\n\
      \            resolved_entities_map=resolved_entities_map,\n            callbacks=callbacks,\n\
      \            cache=cache,\n            strategy_config=strategy_config,\n  \
      \      )\n        return [\n            create_row_from_claim_data(row, item,\
      \ covariate_type)\n            for item in result.covariate_data\n        ]"
    signature: def run_strategy(row)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
      type: internal
    - target: graphrag/index/operations/extract_covariates/extract_covariates.py::create_row_from_claim_data
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
    called_by: []
  - name: create_row_from_claim_data
    start_line: 79
    end_line: 81
    code: "def create_row_from_claim_data(row, covariate_data: Covariate, covariate_type:\
      \ str):\n    \"\"\"Create a row from the claim data and the input row.\"\"\"\
      \n    return {**row, **asdict(covariate_data), \"covariate_type\": covariate_type}"
    signature: 'def create_row_from_claim_data(row, covariate_data: Covariate, covariate_type:
      str)'
    decorators: []
    raises: []
    calls:
    - target: dataclasses::asdict
      type: stdlib
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_row_from_claim_data
    called_by:
    - source: graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
      type: internal
  - name: run_extract_claims
    start_line: 84
    end_line: 137
    code: "async def run_extract_claims(\n    input: str | Iterable[str],\n    entity_types:\
      \ list[str],\n    resolved_entities_map: dict[str, str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    strategy_config: dict[str, Any],\n) -> CovariateExtractionResult:\n\
      \    \"\"\"Run the Claim extraction chain.\"\"\"\n    llm_config = LanguageModelConfig(**strategy_config[\"\
      llm\"])\n    llm = ModelManager().get_or_create_chat_model(\n        name=\"\
      extract_claims\",\n        model_type=llm_config.type,\n        config=llm_config,\n\
      \        callbacks=callbacks,\n        cache=cache,\n    )\n\n    extraction_prompt\
      \ = strategy_config.get(\"extraction_prompt\")\n    max_gleanings = strategy_config.get(\n\
      \        \"max_gleanings\", graphrag_config_defaults.extract_claims.max_gleanings\n\
      \    )\n    tuple_delimiter = strategy_config.get(\"tuple_delimiter\")\n   \
      \ record_delimiter = strategy_config.get(\"record_delimiter\")\n    completion_delimiter\
      \ = strategy_config.get(\"completion_delimiter\")\n\n    extractor = ClaimExtractor(\n\
      \        model_invoker=llm,\n        extraction_prompt=extraction_prompt,\n\
      \        max_gleanings=max_gleanings,\n        on_error=lambda e, s, d: logger.error(\n\
      \            \"Claim Extraction Error\", exc_info=e, extra={\"stack\": s, \"\
      details\": d}\n        ),\n    )\n\n    claim_description = strategy_config.get(\"\
      claim_description\")\n    if claim_description is None:\n        msg = \"claim_description\
      \ is required for claim extraction\"\n        raise ValueError(msg)\n\n    input\
      \ = [input] if isinstance(input, str) else input\n\n    results = await extractor({\n\
      \        \"input_text\": input,\n        \"entity_specs\": entity_types,\n \
      \       \"resolved_entities\": resolved_entities_map,\n        \"claim_description\"\
      : claim_description,\n        \"tuple_delimiter\": tuple_delimiter,\n      \
      \  \"record_delimiter\": record_delimiter,\n        \"completion_delimiter\"\
      : completion_delimiter,\n    })\n\n    claim_data = results.output\n    return\
      \ CovariateExtractionResult([create_covariate(item) for item in claim_data])"
    signature: "def run_extract_claims(\n    input: str | Iterable[str],\n    entity_types:\
      \ list[str],\n    resolved_entities_map: dict[str, str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    strategy_config: dict[str, Any],\n) -> CovariateExtractionResult"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: strategy_config.get
      type: unresolved
    - target: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor
      type: internal
    - target: logger.error
      type: unresolved
    - target: ValueError
      type: builtin
    - target: isinstance
      type: builtin
    - target: extractor
      type: unresolved
    - target: graphrag/index/operations/extract_covariates/typing.py::CovariateExtractionResult
      type: internal
    - target: graphrag/index/operations/extract_covariates/extract_covariates.py::create_covariate
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
    called_by:
    - source: graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
      type: internal
  - name: create_covariate
    start_line: 140
    end_line: 153
    code: "def create_covariate(item: dict[str, Any]) -> Covariate:\n    \"\"\"Create\
      \ a covariate from the item.\"\"\"\n    return Covariate(\n        subject_id=item.get(\"\
      subject_id\"),\n        object_id=item.get(\"object_id\"),\n        type=item.get(\"\
      type\"),\n        status=item.get(\"status\"),\n        start_date=item.get(\"\
      start_date\"),\n        end_date=item.get(\"end_date\"),\n        description=item.get(\"\
      description\"),\n        source_text=item.get(\"source_text\"),\n        record_id=item.get(\"\
      record_id\"),\n        id=item.get(\"id\"),\n    )"
    signature: 'def create_covariate(item: dict[str, Any]) -> Covariate'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_covariates/typing.py::Covariate
      type: internal
    - target: item.get
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_covariate
    called_by:
    - source: graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
      type: internal
- file_name: graphrag/index/operations/extract_covariates/typing.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  functions: []
- file_name: graphrag/index/operations/extract_graph/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/extract_graph/extract_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: Document
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: EntityExtractStrategy
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: ExtractEntityStrategyType
    alias: null
  - module: graphrag.index.utils.derive_from_rows
    name: derive_from_rows
    alias: null
  - module: graphrag.index.operations.extract_graph.graph_intelligence_strategy
    name: run_graph_intelligence
    alias: null
  functions:
  - name: extract_graph
    start_line: 27
    end_line: 82
    code: "async def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    text_column: str,\n   \
      \ id_column: str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types=DEFAULT_ENTITY_TYPES,\n    num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Extract a graph\
      \ from a piece of text using a language model.\"\"\"\n    logger.debug(\"entity_extract\
      \ strategy=%s\", strategy)\n    if entity_types is None:\n        entity_types\
      \ = DEFAULT_ENTITY_TYPES\n    strategy = strategy or {}\n    strategy_exec =\
      \ _load_strategy(\n        strategy.get(\"type\", ExtractEntityStrategyType.graph_intelligence)\n\
      \    )\n    strategy_config = {**strategy}\n\n    num_started = 0\n\n    async\
      \ def run_strategy(row):\n        nonlocal num_started\n        text = row[text_column]\n\
      \        id = row[id_column]\n        result = await strategy_exec(\n      \
      \      [Document(text=text, id=id)],\n            entity_types,\n          \
      \  cache,\n            strategy_config,\n        )\n        num_started += 1\n\
      \        return [result.entities, result.relationships, result.graph]\n\n  \
      \  results = await derive_from_rows(\n        text_units,\n        run_strategy,\n\
      \        callbacks,\n        async_type=async_mode,\n        num_threads=num_threads,\n\
      \        progress_msg=\"extract graph progress: \",\n    )\n\n    entity_dfs\
      \ = []\n    relationship_dfs = []\n    for result in results:\n        if result:\n\
      \            entity_dfs.append(pd.DataFrame(result[0]))\n            relationship_dfs.append(pd.DataFrame(result[1]))\n\
      \n    entities = _merge_entities(entity_dfs)\n    relationships = _merge_relationships(relationship_dfs)\n\
      \n    return (entities, relationships)"
    signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    text_column: str,\n   \
      \ id_column: str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types=DEFAULT_ENTITY_TYPES,\n    num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: logger.debug
      type: unresolved
    - target: graphrag/index/operations/extract_graph/extract_graph.py::_load_strategy
      type: internal
    - target: strategy.get
      type: unresolved
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
    - target: entity_dfs.append
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: relationship_dfs.append
      type: unresolved
    - target: graphrag/index/operations/extract_graph/extract_graph.py::_merge_entities
      type: internal
    - target: graphrag/index/operations/extract_graph/extract_graph.py::_merge_relationships
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
    called_by:
    - source: graphrag/index/workflows/extract_graph.py::run_workflow
      type: internal
  - name: run_strategy
    start_line: 50
    end_line: 61
    code: "async def run_strategy(row):\n        nonlocal num_started\n        text\
      \ = row[text_column]\n        id = row[id_column]\n        result = await strategy_exec(\n\
      \            [Document(text=text, id=id)],\n            entity_types,\n    \
      \        cache,\n            strategy_config,\n        )\n        num_started\
      \ += 1\n        return [result.entities, result.relationships, result.graph]"
    signature: def run_strategy(row)
    decorators: []
    raises: []
    calls:
    - target: strategy_exec
      type: unresolved
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_graph/extract_graph.py::run_strategy
    called_by: []
  - name: _load_strategy
    start_line: 85
    end_line: 97
    code: "def _load_strategy(strategy_type: ExtractEntityStrategyType) -> EntityExtractStrategy:\n\
      \    \"\"\"Load strategy method definition.\"\"\"\n    match strategy_type:\n\
      \        case ExtractEntityStrategyType.graph_intelligence:\n            from\
      \ graphrag.index.operations.extract_graph.graph_intelligence_strategy import\
      \ (\n                run_graph_intelligence,\n            )\n\n            return\
      \ run_graph_intelligence\n\n        case _:\n            msg = f\"Unknown strategy:\
      \ {strategy_type}\"\n            raise ValueError(msg)"
    signature: 'def _load_strategy(strategy_type: ExtractEntityStrategyType) -> EntityExtractStrategy'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/extract_graph.py::_load_strategy
    called_by:
    - source: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
      type: internal
  - name: _merge_entities
    start_line: 100
    end_line: 110
    code: "def _merge_entities(entity_dfs) -> pd.DataFrame:\n    all_entities = pd.concat(entity_dfs,\
      \ ignore_index=True)\n    return (\n        all_entities.groupby([\"title\"\
      , \"type\"], sort=False)\n        .agg(\n            description=(\"description\"\
      , list),\n            text_unit_ids=(\"source_id\", list),\n            frequency=(\"\
      source_id\", \"count\"),\n        )\n        .reset_index()\n    )"
    signature: def _merge_entities(entity_dfs) -> pd.DataFrame
    decorators: []
    raises: []
    calls:
    - target: pandas::concat
      type: external
    - target: "all_entities.groupby([\"title\", \"type\"], sort=False)\n        .agg(\n\
        \            description=(\"description\", list),\n            text_unit_ids=(\"\
        source_id\", list),\n            frequency=(\"source_id\", \"count\"),\n \
        \       )\n        .reset_index"
      type: unresolved
    - target: "all_entities.groupby([\"title\", \"type\"], sort=False)\n        .agg"
      type: unresolved
    - target: all_entities.groupby
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_entities
    called_by:
    - source: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
      type: internal
  - name: _merge_relationships
    start_line: 113
    end_line: 123
    code: "def _merge_relationships(relationship_dfs) -> pd.DataFrame:\n    all_relationships\
      \ = pd.concat(relationship_dfs, ignore_index=False)\n    return (\n        all_relationships.groupby([\"\
      source\", \"target\"], sort=False)\n        .agg(\n            description=(\"\
      description\", list),\n            text_unit_ids=(\"source_id\", list),\n  \
      \          weight=(\"weight\", \"sum\"),\n        )\n        .reset_index()\n\
      \    )"
    signature: def _merge_relationships(relationship_dfs) -> pd.DataFrame
    decorators: []
    raises: []
    calls:
    - target: pandas::concat
      type: external
    - target: "all_relationships.groupby([\"source\", \"target\"], sort=False)\n \
        \       .agg(\n            description=(\"description\", list),\n        \
        \    text_unit_ids=(\"source_id\", list),\n            weight=(\"weight\"\
        , \"sum\"),\n        )\n        .reset_index"
      type: unresolved
    - target: "all_relationships.groupby([\"source\", \"target\"], sort=False)\n \
        \       .agg"
      type: unresolved
    - target: all_relationships.groupby
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_relationships
    called_by:
    - source: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
      type: internal
- file_name: graphrag/index/operations/extract_graph/graph_extractor.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: collections.abc
    name: Mapping
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  - module: graphrag.index.utils.string
    name: clean_str
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.index.extract_graph
    name: CONTINUE_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_graph
    name: GRAPH_EXTRACTION_PROMPT
    alias: null
  - module: graphrag.prompts.index.extract_graph
    name: LOOP_PROMPT
    alias: null
  functions:
  - name: __init__
    start_line: 58
    end_line: 88
    code: "def __init__(\n        self,\n        model_invoker: ChatModel,\n     \
      \   tuple_delimiter_key: str | None = None,\n        record_delimiter_key: str\
      \ | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        prompt: str | None = None,\n        join_descriptions=True,\n     \
      \   max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn | None\
      \ = None,\n    ):\n        \"\"\"Init method definition.\"\"\"\n        # TODO:\
      \ streamline construction\n        self._model = model_invoker\n        self._join_descriptions\
      \ = join_descriptions\n        self._input_text_key = input_text_key or \"input_text\"\
      \n        self._tuple_delimiter_key = tuple_delimiter_key or \"tuple_delimiter\"\
      \n        self._record_delimiter_key = record_delimiter_key or \"record_delimiter\"\
      \n        self._completion_delimiter_key = (\n            completion_delimiter_key\
      \ or \"completion_delimiter\"\n        )\n        self._entity_types_key = entity_types_key\
      \ or \"entity_types\"\n        self._extraction_prompt = prompt or GRAPH_EXTRACTION_PROMPT\n\
      \        self._max_gleanings = (\n            max_gleanings\n            if\
      \ max_gleanings is not None\n            else graphrag_config_defaults.extract_graph.max_gleanings\n\
      \        )\n        self._on_error = on_error or (lambda _e, _s, _d: None)"
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
      \ str | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        prompt: str | None = None,\n        join_descriptions=True,\n     \
      \   max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn | None\
      \ = None,\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__init__
    called_by: []
  - name: __call__
    start_line: 90
    end_line: 141
    code: "async def __call__(\n        self, texts: list[str], prompt_variables:\
      \ dict[str, Any] | None = None\n    ) -> GraphExtractionResult:\n        \"\"\
      \"Call method definition.\"\"\"\n        if prompt_variables is None:\n    \
      \        prompt_variables = {}\n        all_records: dict[int, str] = {}\n \
      \       source_doc_map: dict[int, str] = {}\n\n        # Wire defaults into\
      \ the prompt variables\n        prompt_variables = {\n            **prompt_variables,\n\
      \            self._tuple_delimiter_key: prompt_variables.get(self._tuple_delimiter_key)\n\
      \            or DEFAULT_TUPLE_DELIMITER,\n            self._record_delimiter_key:\
      \ prompt_variables.get(self._record_delimiter_key)\n            or DEFAULT_RECORD_DELIMITER,\n\
      \            self._completion_delimiter_key: prompt_variables.get(\n       \
      \         self._completion_delimiter_key\n            )\n            or DEFAULT_COMPLETION_DELIMITER,\n\
      \            self._entity_types_key: \",\".join(\n                prompt_variables[self._entity_types_key]\
      \ or DEFAULT_ENTITY_TYPES\n            ),\n        }\n\n        for doc_index,\
      \ text in enumerate(texts):\n            try:\n                # Invoke the\
      \ entity extraction\n                result = await self._process_document(text,\
      \ prompt_variables)\n                source_doc_map[doc_index] = text\n    \
      \            all_records[doc_index] = result\n            except Exception as\
      \ e:\n                logger.exception(\"error extracting graph\")\n       \
      \         self._on_error(\n                    e,\n                    traceback.format_exc(),\n\
      \                    {\n                        \"doc_index\": doc_index,\n\
      \                        \"text\": text,\n                    },\n         \
      \       )\n\n        output = await self._process_results(\n            all_records,\n\
      \            prompt_variables.get(self._tuple_delimiter_key, DEFAULT_TUPLE_DELIMITER),\n\
      \            prompt_variables.get(self._record_delimiter_key, DEFAULT_RECORD_DELIMITER),\n\
      \        )\n\n        return GraphExtractionResult(\n            output=output,\n\
      \            source_docs=source_doc_map,\n        )"
    signature: "def __call__(\n        self, texts: list[str], prompt_variables: dict[str,\
      \ Any] | None = None\n    ) -> GraphExtractionResult"
    decorators: []
    raises: []
    calls:
    - target: prompt_variables.get
      type: unresolved
    - target: '",".join'
      type: unresolved
    - target: enumerate
      type: builtin
    - target: graphrag/index/operations/extract_graph/graph_extractor.py::_process_document
      type: internal
    - target: logger.exception
      type: unresolved
    - target: self._on_error
      type: instance
    - target: traceback::format_exc
      type: stdlib
    - target: graphrag/index/operations/extract_graph/graph_extractor.py::_process_results
      type: internal
    - target: GraphExtractionResult
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__call__
    called_by: []
  - name: _process_document
    start_line: 143
    end_line: 177
    code: "async def _process_document(\n        self, text: str, prompt_variables:\
      \ dict[str, str]\n    ) -> str:\n        response = await self._model.achat(\n\
      \            self._extraction_prompt.format(**{\n                **prompt_variables,\n\
      \                self._input_text_key: text,\n            }),\n        )\n \
      \       results = response.output.content or \"\"\n\n        # if gleanings\
      \ are specified, enter a loop to extract more entities\n        # there are\
      \ two exit criteria: (a) we hit the configured max, (b) the model says there\
      \ are no more entities\n        if self._max_gleanings > 0:\n            for\
      \ i in range(self._max_gleanings):\n                response = await self._model.achat(\n\
      \                    CONTINUE_PROMPT,\n                    name=f\"extract-continuation-{i}\"\
      ,\n                    history=response.history,\n                )\n      \
      \          results += response.output.content or \"\"\n\n                # if\
      \ this is the final glean, don't bother updating the continuation flag\n   \
      \             if i >= self._max_gleanings - 1:\n                    break\n\n\
      \                response = await self._model.achat(\n                    LOOP_PROMPT,\n\
      \                    name=f\"extract-loopcheck-{i}\",\n                    history=response.history,\n\
      \                )\n                if response.output.content != \"Y\":\n \
      \                   break\n\n        return results"
    signature: "def _process_document(\n        self, text: str, prompt_variables:\
      \ dict[str, str]\n    ) -> str"
    decorators: []
    raises: []
    calls:
    - target: self._model.achat
      type: instance
    - target: self._extraction_prompt.format
      type: instance
    - target: range
      type: builtin
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_document
    called_by: []
  - name: _process_results
    start_line: 179
    end_line: 290
    code: "async def _process_results(\n        self,\n        results: dict[int,\
      \ str],\n        tuple_delimiter: str,\n        record_delimiter: str,\n   \
      \ ) -> nx.Graph:\n        \"\"\"Parse the result string to create an undirected\
      \ unipartite graph.\n\n        Args:\n            - results - dict of results\
      \ from the extraction chain\n            - tuple_delimiter - delimiter between\
      \ tuples in an output record, default is '<|>'\n            - record_delimiter\
      \ - delimiter between records, default is '##'\n        Returns:\n         \
      \   - output - unipartite graph in graphML format\n        \"\"\"\n        graph\
      \ = nx.Graph()\n        for source_doc_id, extracted_data in results.items():\n\
      \            records = [r.strip() for r in extracted_data.split(record_delimiter)]\n\
      \n            for record in records:\n                record = re.sub(r\"^\\\
      (|\\)$\", \"\", record.strip())\n                record_attributes = record.split(tuple_delimiter)\n\
      \n                if record_attributes[0] == '\"entity\"' and len(record_attributes)\
      \ >= 4:\n                    # add this record as a node in the G\n        \
      \            entity_name = clean_str(record_attributes[1].upper())\n       \
      \             entity_type = clean_str(record_attributes[2].upper())\n      \
      \              entity_description = clean_str(record_attributes[3])\n\n    \
      \                if entity_name in graph.nodes():\n                        node\
      \ = graph.nodes[entity_name]\n                        if self._join_descriptions:\n\
      \                            node[\"description\"] = \"\\n\".join(\n       \
      \                         list({\n                                    *_unpack_descriptions(node),\n\
      \                                    entity_description,\n                 \
      \               })\n                            )\n                        else:\n\
      \                            if len(entity_description) > len(node[\"description\"\
      ]):\n                                node[\"description\"] = entity_description\n\
      \                        node[\"source_id\"] = \", \".join(\n              \
      \              list({\n                                *_unpack_source_ids(node),\n\
      \                                str(source_doc_id),\n                     \
      \       })\n                        )\n                        node[\"type\"\
      ] = (\n                            entity_type if entity_type != \"\" else node[\"\
      type\"]\n                        )\n                    else:\n            \
      \            graph.add_node(\n                            entity_name,\n   \
      \                         type=entity_type,\n                            description=entity_description,\n\
      \                            source_id=str(source_doc_id),\n               \
      \         )\n\n                if (\n                    record_attributes[0]\
      \ == '\"relationship\"'\n                    and len(record_attributes) >= 5\n\
      \                ):\n                    # add this record as edge\n       \
      \             source = clean_str(record_attributes[1].upper())\n           \
      \         target = clean_str(record_attributes[2].upper())\n               \
      \     edge_description = clean_str(record_attributes[3])\n                 \
      \   edge_source_id = clean_str(str(source_doc_id))\n                    try:\n\
      \                        weight = float(record_attributes[-1])\n           \
      \         except ValueError:\n                        weight = 1.0\n\n     \
      \               if source not in graph.nodes():\n                        graph.add_node(\n\
      \                            source,\n                            type=\"\"\
      ,\n                            description=\"\",\n                         \
      \   source_id=edge_source_id,\n                        )\n                 \
      \   if target not in graph.nodes():\n                        graph.add_node(\n\
      \                            target,\n                            type=\"\"\
      ,\n                            description=\"\",\n                         \
      \   source_id=edge_source_id,\n                        )\n                 \
      \   if graph.has_edge(source, target):\n                        edge_data =\
      \ graph.get_edge_data(source, target)\n                        if edge_data\
      \ is not None:\n                            weight += edge_data[\"weight\"]\n\
      \                            if self._join_descriptions:\n                 \
      \               edge_description = \"\\n\".join(\n                         \
      \           list({\n                                        *_unpack_descriptions(edge_data),\n\
      \                                        edge_description,\n               \
      \                     })\n                                )\n              \
      \              edge_source_id = \", \".join(\n                             \
      \   list({\n                                    *_unpack_source_ids(edge_data),\n\
      \                                    str(source_doc_id),\n                 \
      \               })\n                            )\n                    graph.add_edge(\n\
      \                        source,\n                        target,\n        \
      \                weight=weight,\n                        description=edge_description,\n\
      \                        source_id=edge_source_id,\n                    )\n\n\
      \        return graph"
    signature: "def _process_results(\n        self,\n        results: dict[int, str],\n\
      \        tuple_delimiter: str,\n        record_delimiter: str,\n    ) -> nx.Graph"
    decorators: []
    raises: []
    calls:
    - target: networkx::Graph
      type: external
    - target: results.items
      type: unresolved
    - target: r.strip
      type: unresolved
    - target: extracted_data.split
      type: unresolved
    - target: re::sub
      type: stdlib
    - target: record.strip
      type: unresolved
    - target: record.split
      type: unresolved
    - target: len
      type: builtin
    - target: graphrag/index/utils/string.py::clean_str
      type: internal
    - target: record_attributes[1].upper
      type: unresolved
    - target: record_attributes[2].upper
      type: unresolved
    - target: graph.nodes
      type: unresolved
    - target: '"\n".join'
      type: unresolved
    - target: list
      type: builtin
    - target: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_descriptions
      type: internal
    - target: '", ".join'
      type: unresolved
    - target: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_source_ids
      type: internal
    - target: str
      type: builtin
    - target: graph.add_node
      type: unresolved
    - target: float
      type: builtin
    - target: graph.has_edge
      type: unresolved
    - target: graph.get_edge_data
      type: unresolved
    - target: graph.add_edge
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
    called_by: []
  - name: _unpack_descriptions
    start_line: 293
    end_line: 295
    code: "def _unpack_descriptions(data: Mapping) -> list[str]:\n    value = data.get(\"\
      description\", None)\n    return [] if value is None else value.split(\"\\n\"\
      )"
    signature: 'def _unpack_descriptions(data: Mapping) -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: data.get
      type: unresolved
    - target: value.split
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_descriptions
    called_by:
    - source: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
      type: internal
  - name: _unpack_source_ids
    start_line: 298
    end_line: 300
    code: "def _unpack_source_ids(data: Mapping) -> list[str]:\n    value = data.get(\"\
      source_id\", None)\n    return [] if value is None else value.split(\", \")"
    signature: 'def _unpack_source_ids(data: Mapping) -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: data.get
      type: unresolved
    - target: value.split
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_source_ids
    called_by:
    - source: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
      type: internal
- file_name: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.extract_graph.graph_extractor
    name: GraphExtractor
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: Document
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: EntityExtractionResult
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: EntityTypes
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: StrategyConfig
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  functions:
  - name: run_graph_intelligence
    start_line: 26
    end_line: 42
    code: "async def run_graph_intelligence(\n    docs: list[Document],\n    entity_types:\
      \ EntityTypes,\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> EntityExtractionResult:\n\
      \    \"\"\"Run the graph intelligence entity extraction strategy.\"\"\"\n  \
      \  llm_config = LanguageModelConfig(**args[\"llm\"])\n\n    llm = ModelManager().get_or_create_chat_model(\n\
      \        name=\"extract_graph\",\n        model_type=llm_config.type,\n    \
      \    config=llm_config,\n        cache=cache,\n    )\n\n    return await run_extract_graph(llm,\
      \ docs, entity_types, args)"
    signature: "def run_graph_intelligence(\n    docs: list[Document],\n    entity_types:\
      \ EntityTypes,\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_graph_intelligence
    called_by: []
  - name: run_extract_graph
    start_line: 45
    end_line: 102
    code: "async def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
      \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult:\n\
      \    \"\"\"Run the entity extraction chain.\"\"\"\n    tuple_delimiter = args.get(\"\
      tuple_delimiter\", None)\n    record_delimiter = args.get(\"record_delimiter\"\
      , None)\n    completion_delimiter = args.get(\"completion_delimiter\", None)\n\
      \    extraction_prompt = args.get(\"extraction_prompt\", None)\n    max_gleanings\
      \ = args.get(\n        \"max_gleanings\", graphrag_config_defaults.extract_graph.max_gleanings\n\
      \    )\n\n    extractor = GraphExtractor(\n        model_invoker=model,\n  \
      \      prompt=extraction_prompt,\n        max_gleanings=max_gleanings,\n   \
      \     on_error=lambda e, s, d: logger.error(\n            \"Entity Extraction\
      \ Error\", exc_info=e, extra={\"stack\": s, \"details\": d}\n        ),\n  \
      \  )\n    text_list = [doc.text.strip() for doc in docs]\n\n    results = await\
      \ extractor(\n        list(text_list),\n        {\n            \"entity_types\"\
      : entity_types,\n            \"tuple_delimiter\": tuple_delimiter,\n       \
      \     \"record_delimiter\": record_delimiter,\n            \"completion_delimiter\"\
      : completion_delimiter,\n        },\n    )\n\n    graph = results.output\n \
      \   # Map the \"source_id\" back to the \"id\" field\n    for _, node in graph.nodes(data=True):\
      \  # type: ignore\n        if node is not None:\n            node[\"source_id\"\
      ] = \",\".join(\n                docs[int(id)].id for id in node[\"source_id\"\
      ].split(\",\")\n            )\n\n    for _, _, edge in graph.edges(data=True):\
      \  # type: ignore\n        if edge is not None:\n            edge[\"source_id\"\
      ] = \",\".join(\n                docs[int(id)].id for id in edge[\"source_id\"\
      ].split(\",\")\n            )\n\n    entities = [\n        ({\"title\": item[0],\
      \ **(item[1] or {})})\n        for item in graph.nodes(data=True)\n        if\
      \ item is not None\n    ]\n\n    relationships = nx.to_pandas_edgelist(graph)\n\
      \n    return EntityExtractionResult(entities, relationships, graph)"
    signature: "def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
      \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
    decorators: []
    raises: []
    calls:
    - target: args.get
      type: unresolved
    - target: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor
      type: internal
    - target: logger.error
      type: unresolved
    - target: doc.text.strip
      type: unresolved
    - target: extractor
      type: unresolved
    - target: list
      type: builtin
    - target: graph.nodes
      type: unresolved
    - target: '",".join'
      type: unresolved
    - target: int
      type: builtin
    - target: node["source_id"].split
      type: unresolved
    - target: graph.edges
      type: unresolved
    - target: edge["source_id"].split
      type: unresolved
    - target: networkx::to_pandas_edgelist
      type: external
    - target: graphrag/index/operations/extract_graph/typing.py::EntityExtractionResult
      type: internal
    visibility: public
    node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
    called_by:
    - source: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_graph_intelligence
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
      type: internal
- file_name: graphrag/index/operations/extract_graph/typing.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  functions:
  - name: __repr__
    start_line: 55
    end_line: 57
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType.__repr__
    called_by: []
- file_name: graphrag/index/operations/finalize_community_reports.py
  imports:
  - module: uuid
    name: uuid4
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: COMMUNITY_REPORTS_FINAL_COLUMNS
    alias: null
  functions:
  - name: finalize_community_reports
    start_line: 13
    end_line: 33
    code: "def finalize_community_reports(\n    reports: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform final\
      \ community reports.\"\"\"\n    # Merge with communities to add shared fields\n\
      \    community_reports = reports.merge(\n        communities.loc[:, [\"community\"\
      , \"parent\", \"children\", \"size\", \"period\"]],\n        on=\"community\"\
      ,\n        how=\"left\",\n        copy=False,\n    )\n\n    community_reports[\"\
      community\"] = community_reports[\"community\"].astype(int)\n    community_reports[\"\
      human_readable_id\"] = community_reports[\"community\"]\n    community_reports[\"\
      id\"] = [uuid4().hex for _ in range(len(community_reports))]\n\n    return community_reports.loc[\n\
      \        :,\n        COMMUNITY_REPORTS_FINAL_COLUMNS,\n    ]"
    signature: "def finalize_community_reports(\n    reports: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: reports.merge
      type: unresolved
    - target: community_reports["community"].astype
      type: unresolved
    - target: uuid::uuid4
      type: stdlib
    - target: range
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
- file_name: graphrag/index/operations/finalize_entities.py
  imports:
  - module: uuid
    name: uuid4
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.embed_graph_config
    name: EmbedGraphConfig
    alias: null
  - module: graphrag.data_model.schemas
    name: ENTITIES_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.operations.compute_degree
    name: compute_degree
    alias: null
  - module: graphrag.index.operations.create_graph
    name: create_graph
    alias: null
  - module: graphrag.index.operations.embed_graph.embed_graph
    name: embed_graph
    alias: null
  - module: graphrag.index.operations.layout_graph.layout_graph
    name: layout_graph
    alias: null
  functions:
  - name: finalize_entities
    start_line: 18
    end_line: 54
    code: "def finalize_entities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
      \ bool = False,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform final\
      \ entities.\"\"\"\n    graph = create_graph(relationships, edge_attr=[\"weight\"\
      ])\n    graph_embeddings = None\n    if embed_config is not None and embed_config.enabled:\n\
      \        graph_embeddings = embed_graph(\n            graph,\n            embed_config,\n\
      \        )\n    layout = layout_graph(\n        graph,\n        layout_enabled,\n\
      \        embeddings=graph_embeddings,\n    )\n    degrees = compute_degree(graph)\n\
      \    final_entities = (\n        entities.merge(layout, left_on=\"title\", right_on=\"\
      label\", how=\"left\")\n        .merge(degrees, on=\"title\", how=\"left\")\n\
      \        .drop_duplicates(subset=\"title\")\n    )\n    final_entities = final_entities.loc[entities[\"\
      title\"].notna()].reset_index()\n    # disconnected nodes and those with no\
      \ community even at level 0 can be missing degree\n    final_entities[\"degree\"\
      ] = final_entities[\"degree\"].fillna(0).astype(int)\n    final_entities.reset_index(inplace=True)\n\
      \    final_entities[\"human_readable_id\"] = final_entities.index\n    final_entities[\"\
      id\"] = final_entities[\"human_readable_id\"].apply(\n        lambda _x: str(uuid4())\n\
      \    )\n    return final_entities.loc[\n        :,\n        ENTITIES_FINAL_COLUMNS,\n\
      \    ]"
    signature: "def finalize_entities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
      \ bool = False,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/create_graph.py::create_graph
      type: internal
    - target: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
      type: internal
    - target: graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
      type: internal
    - target: graphrag/index/operations/compute_degree.py::compute_degree
      type: internal
    - target: "entities.merge(layout, left_on=\"title\", right_on=\"label\", how=\"\
        left\")\n        .merge(degrees, on=\"title\", how=\"left\")\n        .drop_duplicates"
      type: unresolved
    - target: "entities.merge(layout, left_on=\"title\", right_on=\"label\", how=\"\
        left\")\n        .merge"
      type: unresolved
    - target: entities.merge
      type: unresolved
    - target: final_entities.loc[entities["title"].notna()].reset_index
      type: unresolved
    - target: entities["title"].notna
      type: unresolved
    - target: final_entities["degree"].fillna(0).astype
      type: unresolved
    - target: final_entities["degree"].fillna
      type: unresolved
    - target: final_entities.reset_index
      type: unresolved
    - target: final_entities["human_readable_id"].apply
      type: unresolved
    - target: str
      type: builtin
    - target: uuid::uuid4
      type: stdlib
    visibility: public
    node_id: graphrag/index/operations/finalize_entities.py::finalize_entities
    called_by:
    - source: graphrag/index/workflows/finalize_graph.py::finalize_graph
      type: internal
- file_name: graphrag/index/operations/finalize_relationships.py
  imports:
  - module: uuid
    name: uuid4
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: RELATIONSHIPS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.operations.compute_degree
    name: compute_degree
    alias: null
  - module: graphrag.index.operations.compute_edge_combined_degree
    name: compute_edge_combined_degree
    alias: null
  - module: graphrag.index.operations.create_graph
    name: create_graph
    alias: null
  functions:
  - name: finalize_relationships
    start_line: 18
    end_line: 44
    code: "def finalize_relationships(\n    relationships: pd.DataFrame,\n) -> pd.DataFrame:\n\
      \    \"\"\"All the steps to transform final relationships.\"\"\"\n    graph\
      \ = create_graph(relationships, edge_attr=[\"weight\"])\n    degrees = compute_degree(graph)\n\
      \n    final_relationships = relationships.drop_duplicates(subset=[\"source\"\
      , \"target\"])\n    final_relationships[\"combined_degree\"] = compute_edge_combined_degree(\n\
      \        final_relationships,\n        degrees,\n        node_name_column=\"\
      title\",\n        node_degree_column=\"degree\",\n        edge_source_column=\"\
      source\",\n        edge_target_column=\"target\",\n    )\n\n    final_relationships.reset_index(inplace=True)\n\
      \    final_relationships[\"human_readable_id\"] = final_relationships.index\n\
      \    final_relationships[\"id\"] = final_relationships[\"human_readable_id\"\
      ].apply(\n        lambda _x: str(uuid4())\n    )\n\n    return final_relationships.loc[\n\
      \        :,\n        RELATIONSHIPS_FINAL_COLUMNS,\n    ]"
    signature: "def finalize_relationships(\n    relationships: pd.DataFrame,\n) ->\
      \ pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/create_graph.py::create_graph
      type: internal
    - target: graphrag/index/operations/compute_degree.py::compute_degree
      type: internal
    - target: relationships.drop_duplicates
      type: unresolved
    - target: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
      type: internal
    - target: final_relationships.reset_index
      type: unresolved
    - target: final_relationships["human_readable_id"].apply
      type: unresolved
    - target: str
      type: builtin
    - target: uuid::uuid4
      type: stdlib
    visibility: public
    node_id: graphrag/index/operations/finalize_relationships.py::finalize_relationships
    called_by:
    - source: graphrag/index/workflows/finalize_graph.py::finalize_graph
      type: internal
- file_name: graphrag/index/operations/graph_to_dataframes.py
  imports:
  - module: networkx
    name: null
    alias: nx
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: graph_to_dataframes
    start_line: 10
    end_line: 38
    code: "def graph_to_dataframes(\n    graph: nx.Graph,\n    node_columns: list[str]\
      \ | None = None,\n    edge_columns: list[str] | None = None,\n    node_id: str\
      \ = \"title\",\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Deconstructs\
      \ an nx.Graph into nodes and edges dataframes.\"\"\"\n    # nx graph nodes are\
      \ a tuple, and creating a df from them results in the id being the index\n \
      \   nodes = pd.DataFrame.from_dict(dict(graph.nodes(data=True)), orient=\"index\"\
      )\n    nodes[node_id] = nodes.index\n    nodes.reset_index(inplace=True, drop=True)\n\
      \n    edges = nx.to_pandas_edgelist(graph)\n\n    # we don't deal in directed\
      \ graphs, but we do need to ensure consistent ordering for df joins\n    # nx\
      \ loses the initial ordering\n    edges[\"min_source\"] = edges[[\"source\"\
      , \"target\"]].min(axis=1)\n    edges[\"max_target\"] = edges[[\"source\", \"\
      target\"]].max(axis=1)\n    edges = edges.drop(columns=[\"source\", \"target\"\
      ]).rename(\n        columns={\"min_source\": \"source\", \"max_target\": \"\
      target\"}  # type: ignore\n    )\n\n    if node_columns:\n        nodes = nodes.loc[:,\
      \ node_columns]\n\n    if edge_columns:\n        edges = edges.loc[:, edge_columns]\n\
      \n    return (nodes, edges)"
    signature: "def graph_to_dataframes(\n    graph: nx.Graph,\n    node_columns:\
      \ list[str] | None = None,\n    edge_columns: list[str] | None = None,\n   \
      \ node_id: str = \"title\",\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame.from_dict
      type: external
    - target: dict
      type: builtin
    - target: graph.nodes
      type: unresolved
    - target: nodes.reset_index
      type: unresolved
    - target: networkx::to_pandas_edgelist
      type: external
    - target: edges[["source", "target"]].min
      type: unresolved
    - target: edges[["source", "target"]].max
      type: unresolved
    - target: edges.drop(columns=["source", "target"]).rename
      type: unresolved
    - target: edges.drop
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/graph_to_dataframes.py::graph_to_dataframes
    called_by:
    - source: graphrag/index/workflows/prune_graph.py::prune_graph
      type: internal
- file_name: graphrag/index/operations/layout_graph/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/layout_graph/layout_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.index.operations.embed_graph.typing
    name: NodeEmbeddings
    alias: null
  - module: graphrag.index.operations.layout_graph.typing
    name: GraphLayout
    alias: null
  - module: graphrag.index.operations.layout_graph.umap
    name: run
    alias: null
  - module: graphrag.index.operations.layout_graph.zero
    name: run
    alias: null
  functions:
  - name: layout_graph
    start_line: 17
    end_line: 55
    code: "def layout_graph(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings | None,\n):\n    \"\"\"\n    Apply a layout algorithm to a\
      \ nx.Graph. The method returns a dataframe containing the node positions.\n\n\
      \    ## Usage\n    ```yaml\n    args:\n        graph: The nx.Graph to layout\n\
      \        embeddings: Embeddings for each node in the graph\n        strategy:\
      \ <strategy config> # See strategies section below\n    ```\n\n    ## Strategies\n\
      \    The layout graph verb uses a strategy to layout the graph. The strategy\
      \ is a json object which defines the strategy to use. The following strategies\
      \ are available:\n\n    ### umap\n    This strategy uses the umap algorithm\
      \ to layout a graph. The strategy config is as follows:\n    ```yaml\n    strategy:\n\
      \        type: umap\n        n_neighbors: 5 # Optional, The number of neighbors\
      \ to use for the umap algorithm, default: 5\n        min_dist: 0.75 # Optional,\
      \ The min distance to use for the umap algorithm, default: 0.75\n    ```\n \
      \   \"\"\"\n    layout = _run_layout(\n        graph,\n        enabled,\n  \
      \      embeddings if embeddings is not None else {},\n    )\n\n    layout_df\
      \ = pd.DataFrame(layout)\n    return layout_df.loc[\n        :,\n        [\"\
      label\", \"x\", \"y\", \"size\"],\n    ]"
    signature: "def layout_graph(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings | None,\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/layout_graph/layout_graph.py::_run_layout
      type: internal
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
    called_by:
    - source: graphrag/index/operations/finalize_entities.py::finalize_entities
      type: internal
  - name: _run_layout
    start_line: 58
    end_line: 84
    code: "def _run_layout(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings,\n) -> GraphLayout:\n    if enabled:\n        from graphrag.index.operations.layout_graph.umap\
      \ import (\n            run as run_umap,\n        )\n\n        return run_umap(\n\
      \            graph,\n            embeddings,\n            lambda e, stack, d:\
      \ logger.error(\n                \"Error in Umap\", exc_info=e, extra={\"stack\"\
      : stack, \"details\": d}\n            ),\n        )\n    from graphrag.index.operations.layout_graph.zero\
      \ import (\n        run as run_zero,\n    )\n\n    return run_zero(\n      \
      \  graph,\n        lambda e, stack, d: logger.error(\n            \"Error in\
      \ Zero\", exc_info=e, extra={\"stack\": stack, \"details\": d}\n        ),\n\
      \    )"
    signature: "def _run_layout(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings,\n) -> GraphLayout"
    decorators: []
    raises: []
    calls:
    - target: run_umap
      type: unresolved
    - target: logger.error
      type: unresolved
    - target: run_zero
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/layout_graph/layout_graph.py::_run_layout
    called_by:
    - source: graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
      type: internal
- file_name: graphrag/index/operations/layout_graph/typing.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  functions:
  - name: to_pandas
    start_line: 22
    end_line: 24
    code: "def to_pandas(self) -> tuple[str, float, float, str, float]:\n        \"\
      \"\"To pandas method definition.\"\"\"\n        return self.label, self.x, self.y,\
      \ self.cluster, self.size"
    signature: def to_pandas(self) -> tuple[str, float, float, str, float]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/operations/layout_graph/typing.py::NodePosition.to_pandas
    called_by: []
- file_name: graphrag/index/operations/layout_graph/umap.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: numpy
    name: null
    alias: np
  - module: graphrag.index.operations.embed_graph.typing
    name: NodeEmbeddings
    alias: null
  - module: graphrag.index.operations.layout_graph.typing
    name: GraphLayout
    alias: null
  - module: graphrag.index.operations.layout_graph.typing
    name: NodePosition
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  - module: umap
    name: null
    alias: null
  functions:
  - name: run
    start_line: 26
    end_line: 69
    code: "def run(\n    graph: nx.Graph,\n    embeddings: NodeEmbeddings,\n    on_error:\
      \ ErrorHandlerFn,\n) -> GraphLayout:\n    \"\"\"Run method definition.\"\"\"\
      \n    node_clusters = []\n    node_sizes = []\n\n    embeddings = _filter_raw_embeddings(embeddings)\n\
      \    nodes = list(embeddings.keys())\n    embedding_vectors = [embeddings[node_id]\
      \ for node_id in nodes]\n\n    for node_id in nodes:\n        node = graph.nodes[node_id]\n\
      \        cluster = node.get(\"cluster\", node.get(\"community\", -1))\n    \
      \    node_clusters.append(cluster)\n        size = node.get(\"degree\", node.get(\"\
      size\", 0))\n        node_sizes.append(size)\n\n    additional_args = {}\n \
      \   if len(node_clusters) > 0:\n        additional_args[\"node_categories\"\
      ] = node_clusters\n    if len(node_sizes) > 0:\n        additional_args[\"node_sizes\"\
      ] = node_sizes\n\n    try:\n        return compute_umap_positions(\n       \
      \     embedding_vectors=np.array(embedding_vectors),\n            node_labels=nodes,\n\
      \            **additional_args,\n        )\n    except Exception as e:\n   \
      \     logger.exception(\"Error running UMAP\")\n        on_error(e, traceback.format_exc(),\
      \ None)\n        # Umap may fail due to input sparseness or memory pressure.\n\
      \        # For now, in these cases, we'll just return a layout with all nodes\
      \ at (0, 0)\n        result = []\n        for i in range(len(nodes)):\n    \
      \        cluster = node_clusters[i] if len(node_clusters) > 0 else 1\n     \
      \       result.append(\n                NodePosition(x=0, y=0, label=nodes[i],\
      \ size=0, cluster=str(cluster))\n            )\n        return result"
    signature: "def run(\n    graph: nx.Graph,\n    embeddings: NodeEmbeddings,\n\
      \    on_error: ErrorHandlerFn,\n) -> GraphLayout"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/layout_graph/umap.py::_filter_raw_embeddings
      type: internal
    - target: list
      type: builtin
    - target: embeddings.keys
      type: unresolved
    - target: node.get
      type: unresolved
    - target: node_clusters.append
      type: unresolved
    - target: node_sizes.append
      type: unresolved
    - target: len
      type: builtin
    - target: graphrag/index/operations/layout_graph/umap.py::compute_umap_positions
      type: internal
    - target: numpy::array
      type: external
    - target: logger.exception
      type: unresolved
    - target: on_error
      type: ambiguous
      candidates: &id001
      - graphrag/language_model/events/base.py::ModelEventHandler.on_error
      - graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.on_error
      - graphrag/language_model/providers/fnllm/utils.py::on_error
    - target: traceback::format_exc
      type: stdlib
    - target: range
      type: builtin
    - target: result.append
      type: unresolved
    - target: graphrag/index/operations/layout_graph/typing.py::NodePosition
      type: internal
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/layout_graph/umap.py::run
    called_by: []
  - name: _filter_raw_embeddings
    start_line: 72
    end_line: 77
    code: "def _filter_raw_embeddings(embeddings: NodeEmbeddings) -> NodeEmbeddings:\n\
      \    return {\n        node_id: embedding\n        for node_id, embedding in\
      \ embeddings.items()\n        if embedding is not None\n    }"
    signature: 'def _filter_raw_embeddings(embeddings: NodeEmbeddings) -> NodeEmbeddings'
    decorators: []
    raises: []
    calls:
    - target: embeddings.items
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/layout_graph/umap.py::_filter_raw_embeddings
    called_by:
    - source: graphrag/index/operations/layout_graph/umap.py::run
      type: internal
  - name: compute_umap_positions
    start_line: 80
    end_line: 132
    code: "def compute_umap_positions(\n    embedding_vectors: np.ndarray,\n    node_labels:\
      \ list[str],\n    node_categories: list[int] | None = None,\n    node_sizes:\
      \ list[int] | None = None,\n    min_dist: float = 0.75,\n    n_neighbors: int\
      \ = 5,\n    spread: int = 1,\n    metric: str = \"euclidean\",\n    n_components:\
      \ int = 2,\n    random_state: int = 86,\n) -> list[NodePosition]:\n    \"\"\"\
      Project embedding vectors down to 2D/3D using UMAP.\"\"\"\n    # NOTE: This\
      \ import is done here to reduce the initial import time of the graphrag package\n\
      \    import umap\n\n    embedding_positions = umap.UMAP(\n        min_dist=min_dist,\n\
      \        n_neighbors=n_neighbors,\n        spread=spread,\n        n_components=n_components,\n\
      \        metric=metric,\n        random_state=random_state,\n    ).fit_transform(embedding_vectors)\n\
      \n    embedding_position_data: list[NodePosition] = []\n    for index, node_name\
      \ in enumerate(node_labels):\n        node_points = embedding_positions[index]\
      \  # type: ignore\n        node_category = 1 if node_categories is None else\
      \ node_categories[index]\n        node_size = 1 if node_sizes is None else node_sizes[index]\n\
      \n        if len(node_points) == 2:\n            embedding_position_data.append(\n\
      \                NodePosition(\n                    label=str(node_name),\n\
      \                    x=float(node_points[0]),\n                    y=float(node_points[1]),\n\
      \                    cluster=str(int(node_category)),\n                    size=int(node_size),\n\
      \                )\n            )\n        else:\n            embedding_position_data.append(\n\
      \                NodePosition(\n                    label=str(node_name),\n\
      \                    x=float(node_points[0]),\n                    y=float(node_points[1]),\n\
      \                    z=float(node_points[2]),\n                    cluster=str(int(node_category)),\n\
      \                    size=int(node_size),\n                )\n            )\n\
      \    return embedding_position_data"
    signature: "def compute_umap_positions(\n    embedding_vectors: np.ndarray,\n\
      \    node_labels: list[str],\n    node_categories: list[int] | None = None,\n\
      \    node_sizes: list[int] | None = None,\n    min_dist: float = 0.75,\n   \
      \ n_neighbors: int = 5,\n    spread: int = 1,\n    metric: str = \"euclidean\"\
      ,\n    n_components: int = 2,\n    random_state: int = 86,\n) -> list[NodePosition]"
    decorators: []
    raises: []
    calls:
    - target: "umap::UMAP(\n        min_dist=min_dist,\n        n_neighbors=n_neighbors,\n\
        \        spread=spread,\n        n_components=n_components,\n        metric=metric,\n\
        \        random_state=random_state,\n    ).fit_transform"
      type: external
    - target: umap::UMAP
      type: external
    - target: enumerate
      type: builtin
    - target: len
      type: builtin
    - target: embedding_position_data.append
      type: unresolved
    - target: graphrag/index/operations/layout_graph/typing.py::NodePosition
      type: internal
    - target: str
      type: builtin
    - target: float
      type: builtin
    - target: int
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/layout_graph/umap.py::compute_umap_positions
    called_by:
    - source: graphrag/index/operations/layout_graph/umap.py::run
      type: internal
- file_name: graphrag/index/operations/layout_graph/zero.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.index.operations.layout_graph.typing
    name: GraphLayout
    alias: null
  - module: graphrag.index.operations.layout_graph.typing
    name: NodePosition
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  functions:
  - name: run
    start_line: 24
    end_line: 60
    code: "def run(\n    graph: nx.Graph,\n    on_error: ErrorHandlerFn,\n) -> GraphLayout:\n\
      \    \"\"\"Run method definition.\"\"\"\n    node_clusters = []\n    node_sizes\
      \ = []\n\n    nodes = list(graph.nodes)\n\n    for node_id in nodes:\n     \
      \   node = graph.nodes[node_id]\n        cluster = node.get(\"cluster\", node.get(\"\
      community\", -1))\n        node_clusters.append(cluster)\n        size = node.get(\"\
      degree\", node.get(\"size\", 0))\n        node_sizes.append(size)\n\n    additional_args\
      \ = {}\n    if len(node_clusters) > 0:\n        additional_args[\"node_categories\"\
      ] = node_clusters\n    if len(node_sizes) > 0:\n        additional_args[\"node_sizes\"\
      ] = node_sizes\n\n    try:\n        return get_zero_positions(node_labels=nodes,\
      \ **additional_args)\n    except Exception as e:\n        logger.exception(\"\
      Error running zero-position\")\n        on_error(e, traceback.format_exc(),\
      \ None)\n        # Umap may fail due to input sparseness or memory pressure.\n\
      \        # For now, in these cases, we'll just return a layout with all nodes\
      \ at (0, 0)\n        result = []\n        for i in range(len(nodes)):\n    \
      \        cluster = node_clusters[i] if len(node_clusters) > 0 else 1\n     \
      \       result.append(\n                NodePosition(x=0, y=0, label=nodes[i],\
      \ size=0, cluster=str(cluster))\n            )\n        return result"
    signature: "def run(\n    graph: nx.Graph,\n    on_error: ErrorHandlerFn,\n) ->\
      \ GraphLayout"
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: node.get
      type: unresolved
    - target: node_clusters.append
      type: unresolved
    - target: node_sizes.append
      type: unresolved
    - target: len
      type: builtin
    - target: graphrag/index/operations/layout_graph/zero.py::get_zero_positions
      type: internal
    - target: logger.exception
      type: unresolved
    - target: on_error
      type: ambiguous
      candidates: *id001
    - target: traceback::format_exc
      type: stdlib
    - target: range
      type: builtin
    - target: result.append
      type: unresolved
    - target: graphrag/index/operations/layout_graph/typing.py::NodePosition
      type: internal
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/layout_graph/zero.py::run
    called_by: []
  - name: get_zero_positions
    start_line: 63
    end_line: 96
    code: "def get_zero_positions(\n    node_labels: list[str],\n    node_categories:\
      \ list[int] | None = None,\n    node_sizes: list[int] | None = None,\n    three_d:\
      \ bool | None = False,\n) -> list[NodePosition]:\n    \"\"\"Project embedding\
      \ vectors down to 2D/3D using UMAP.\"\"\"\n    embedding_position_data: list[NodePosition]\
      \ = []\n    for index, node_name in enumerate(node_labels):\n        node_category\
      \ = 1 if node_categories is None else node_categories[index]\n        node_size\
      \ = 1 if node_sizes is None else node_sizes[index]\n\n        if not three_d:\n\
      \            embedding_position_data.append(\n                NodePosition(\n\
      \                    label=str(node_name),\n                    x=0,\n     \
      \               y=0,\n                    cluster=str(int(node_category)),\n\
      \                    size=int(node_size),\n                )\n            )\n\
      \        else:\n            embedding_position_data.append(\n              \
      \  NodePosition(\n                    label=str(node_name),\n              \
      \      x=0,\n                    y=0,\n                    z=0,\n          \
      \          cluster=str(int(node_category)),\n                    size=int(node_size),\n\
      \                )\n            )\n    return embedding_position_data"
    signature: "def get_zero_positions(\n    node_labels: list[str],\n    node_categories:\
      \ list[int] | None = None,\n    node_sizes: list[int] | None = None,\n    three_d:\
      \ bool | None = False,\n) -> list[NodePosition]"
    decorators: []
    raises: []
    calls:
    - target: enumerate
      type: builtin
    - target: embedding_position_data.append
      type: unresolved
    - target: graphrag/index/operations/layout_graph/typing.py::NodePosition
      type: internal
    - target: str
      type: builtin
    - target: int
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/layout_graph/zero.py::get_zero_positions
    called_by:
    - source: graphrag/index/operations/layout_graph/zero.py::run
      type: internal
- file_name: graphrag/index/operations/prune_graph.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: graspologic
    name: null
    alias: glc
  - module: networkx
    name: null
    alias: nx
  - module: numpy
    name: null
    alias: np
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: networkx.classes.reportviews
    name: DegreeView
    alias: null
  functions:
  - name: prune_graph
    start_line: 18
    end_line: 83
    code: "def prune_graph(\n    graph: nx.Graph,\n    min_node_freq: int = 1,\n \
      \   max_node_freq_std: float | None = None,\n    min_node_degree: int = 1,\n\
      \    max_node_degree_std: float | None = None,\n    min_edge_weight_pct: float\
      \ = 40,\n    remove_ego_nodes: bool = False,\n    lcc_only: bool = False,\n\
      ) -> nx.Graph:\n    \"\"\"Prune graph by removing nodes that are out of frequency/degree\
      \ ranges and edges with low weights.\"\"\"\n    # remove ego nodes if needed\n\
      \    degree = cast(\"DegreeView\", graph.degree)\n    degrees = list(degree())\
      \  # type: ignore\n    if remove_ego_nodes:\n        # ego node is one with\
      \ highest degree\n        ego_node = max(degrees, key=lambda x: x[1])\n    \
      \    graph.remove_nodes_from([ego_node[0]])\n\n    # remove nodes that are not\
      \ within the predefined degree range\n    graph.remove_nodes_from([\n      \
      \  node for node, degree in degrees if degree < min_node_degree\n    ])\n  \
      \  if max_node_degree_std is not None:\n        upper_threshold = _get_upper_threshold_by_std(\n\
      \            [degree for _, degree in degrees], max_node_degree_std\n      \
      \  )\n        graph.remove_nodes_from([\n            node for node, degree in\
      \ degrees if degree > upper_threshold\n        ])\n\n    # remove nodes that\
      \ are not within the predefined frequency range\n    graph.remove_nodes_from([\n\
      \        node\n        for node, data in graph.nodes(data=True)\n        if\
      \ data[schemas.NODE_FREQUENCY] < min_node_freq\n    ])\n    if max_node_freq_std\
      \ is not None:\n        upper_threshold = _get_upper_threshold_by_std(\n   \
      \         [data[schemas.NODE_FREQUENCY] for _, data in graph.nodes(data=True)],\n\
      \            max_node_freq_std,\n        )\n        graph.remove_nodes_from([\n\
      \            node\n            for node, data in graph.nodes(data=True)\n  \
      \          if data[schemas.NODE_FREQUENCY] > upper_threshold\n        ])\n\n\
      \    # remove edges by min weight\n    if min_edge_weight_pct > 0:\n       \
      \ min_edge_weight = np.percentile(\n            [data[schemas.EDGE_WEIGHT] for\
      \ _, _, data in graph.edges(data=True)],\n            min_edge_weight_pct,\n\
      \        )\n        graph.remove_edges_from([\n            (source, target)\n\
      \            for source, target, data in graph.edges(data=True)\n          \
      \  if source in graph.nodes()\n            and target in graph.nodes()\n   \
      \         and data[schemas.EDGE_WEIGHT] < min_edge_weight\n        ])\n\n  \
      \  if lcc_only:\n        return glc.utils.largest_connected_component(graph)\
      \  # type: ignore\n\n    return graph"
    signature: "def prune_graph(\n    graph: nx.Graph,\n    min_node_freq: int = 1,\n\
      \    max_node_freq_std: float | None = None,\n    min_node_degree: int = 1,\n\
      \    max_node_degree_std: float | None = None,\n    min_edge_weight_pct: float\
      \ = 40,\n    remove_ego_nodes: bool = False,\n    lcc_only: bool = False,\n\
      ) -> nx.Graph"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: list
      type: builtin
    - target: degree
      type: unresolved
    - target: max
      type: builtin
    - target: graph.remove_nodes_from
      type: unresolved
    - target: graphrag/index/operations/prune_graph.py::_get_upper_threshold_by_std
      type: internal
    - target: graph.nodes
      type: unresolved
    - target: numpy::percentile
      type: external
    - target: graph.edges
      type: unresolved
    - target: graph.remove_edges_from
      type: unresolved
    - target: graspologic::utils.largest_connected_component
      type: external
    visibility: public
    node_id: graphrag/index/operations/prune_graph.py::prune_graph
    called_by:
    - source: graphrag/index/workflows/prune_graph.py::run_workflow
      type: internal
  - name: _get_upper_threshold_by_std
    start_line: 86
    end_line: 92
    code: "def _get_upper_threshold_by_std(\n    data: list[float] | list[int], std_trim:\
      \ float\n) -> float:\n    \"\"\"Get upper threshold by standard deviation.\"\
      \"\"\n    mean = np.mean(data)\n    std = np.std(data)\n    return mean + std_trim\
      \ * std  # type: ignore"
    signature: "def _get_upper_threshold_by_std(\n    data: list[float] | list[int],\
      \ std_trim: float\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: numpy::mean
      type: external
    - target: numpy::std
      type: external
    visibility: protected
    node_id: graphrag/index/operations/prune_graph.py::_get_upper_threshold_by_std
    called_by:
    - source: graphrag/index/operations/prune_graph.py::prune_graph
      type: internal
- file_name: graphrag/index/operations/snapshot_graphml.py
  imports:
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: snapshot_graphml
    start_line: 11
    end_line: 18
    code: "async def snapshot_graphml(\n    input: str | nx.Graph,\n    name: str,\n\
      \    storage: PipelineStorage,\n) -> None:\n    \"\"\"Take a entire snapshot\
      \ of a graph to standard graphml format.\"\"\"\n    graphml = input if isinstance(input,\
      \ str) else \"\\n\".join(nx.generate_graphml(input))\n    await storage.set(name\
      \ + \".graphml\", graphml)"
    signature: "def snapshot_graphml(\n    input: str | nx.Graph,\n    name: str,\n\
      \    storage: PipelineStorage,\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: '"\n".join'
      type: unresolved
    - target: networkx::generate_graphml
      type: external
    - target: storage.set
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/snapshot_graphml.py::snapshot_graphml
    called_by:
    - source: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
- file_name: graphrag/index/operations/summarize_communities/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/summarize_communities/build_mixed_context.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.index.operations.summarize_communities.graph_context.sort_context
    name: sort_context
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_mixed_context
    start_line: 14
    end_line: 73
    code: "def build_mixed_context(\n    context: list[dict], tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> str:\n    \"\"\"\n    Build parent context by\
      \ concatenating all sub-communities' contexts.\n\n    If the context exceeds\
      \ the limit, we use sub-community reports instead.\n    \"\"\"\n    sorted_context\
      \ = sorted(\n        context, key=lambda x: x[schemas.CONTEXT_SIZE], reverse=True\n\
      \    )\n\n    # replace local context with sub-community reports, starting from\
      \ the biggest sub-community\n    substitute_reports = []\n    final_local_contexts\
      \ = []\n    exceeded_limit = True\n    context_string = \"\"\n\n    for idx,\
      \ sub_community_context in enumerate(sorted_context):\n        if exceeded_limit:\n\
      \            if sub_community_context[schemas.FULL_CONTENT]:\n             \
      \   substitute_reports.append({\n                    schemas.COMMUNITY_ID: sub_community_context[schemas.SUB_COMMUNITY],\n\
      \                    schemas.FULL_CONTENT: sub_community_context[schemas.FULL_CONTENT],\n\
      \                })\n            else:\n                # this sub-community\
      \ has no report, so we will use its local context\n                final_local_contexts.extend(sub_community_context[schemas.ALL_CONTEXT])\n\
      \                continue\n\n            # add local context for the remaining\
      \ sub-communities\n            remaining_local_context = []\n            for\
      \ rid in range(idx + 1, len(sorted_context)):\n                remaining_local_context.extend(sorted_context[rid][schemas.ALL_CONTEXT])\n\
      \            new_context_string = sort_context(\n                local_context=remaining_local_context\
      \ + final_local_contexts,\n                tokenizer=tokenizer,\n          \
      \      sub_community_reports=substitute_reports,\n            )\n          \
      \  if tokenizer.num_tokens(new_context_string) <= max_context_tokens:\n    \
      \            exceeded_limit = False\n                context_string = new_context_string\n\
      \                break\n\n    if exceeded_limit:\n        # if all sub-community\
      \ reports exceed the limit, we add reports until context is full\n        substitute_reports\
      \ = []\n        for sub_community_context in sorted_context:\n            substitute_reports.append({\n\
      \                schemas.COMMUNITY_ID: sub_community_context[schemas.SUB_COMMUNITY],\n\
      \                schemas.FULL_CONTENT: sub_community_context[schemas.FULL_CONTENT],\n\
      \            })\n            new_context_string = pd.DataFrame(substitute_reports).to_csv(\n\
      \                index=False, sep=\",\"\n            )\n            if tokenizer.num_tokens(new_context_string)\
      \ > max_context_tokens:\n                break\n\n            context_string\
      \ = new_context_string\n    return context_string"
    signature: "def build_mixed_context(\n    context: list[dict], tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: sorted
      type: builtin
    - target: enumerate
      type: builtin
    - target: substitute_reports.append
      type: unresolved
    - target: final_local_contexts.extend
      type: unresolved
    - target: range
      type: builtin
    - target: len
      type: builtin
    - target: remaining_local_context.extend
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    - target: pandas::DataFrame(substitute_reports).to_csv
      type: external
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
      type: internal
- file_name: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.index.community_report
    name: COMMUNITY_REPORT_PROMPT
    alias: null
  functions:
  - name: __init__
    start_line: 59
    end_line: 70
    code: "def __init__(\n        self,\n        model_invoker: ChatModel,\n     \
      \   extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n        max_report_length: int | None = None,\n    ):\n  \
      \      \"\"\"Init method definition.\"\"\"\n        self._model = model_invoker\n\
      \        self._extraction_prompt = extraction_prompt or COMMUNITY_REPORT_PROMPT\n\
      \        self._on_error = on_error or (lambda _e, _s, _d: None)\n        self._max_report_length\
      \ = max_report_length or 1500"
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n        max_report_length: int | None = None,\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__init__
    called_by: []
  - name: __call__
    start_line: 72
    end_line: 96
    code: "async def __call__(self, input_text: str):\n        \"\"\"Call method definition.\"\
      \"\"\n        output = None\n        try:\n            prompt = self._extraction_prompt.format(**{\n\
      \                INPUT_TEXT_KEY: input_text,\n                MAX_LENGTH_KEY:\
      \ str(self._max_report_length),\n            })\n            response = await\
      \ self._model.achat(\n                prompt,\n                json=True,  #\
      \ Leaving this as True to avoid creating new cache entries\n               \
      \ name=\"create_community_report\",\n                json_model=CommunityReportResponse,\
      \  # A model is required when using json mode\n            )\n\n           \
      \ output = response.parsed_response\n        except Exception as e:\n      \
      \      logger.exception(\"error generating community report\")\n           \
      \ self._on_error(e, traceback.format_exc(), None)\n\n        text_output = self._get_text_output(output)\
      \ if output else \"\"\n        return CommunityReportsResult(\n            structured_output=output,\n\
      \            output=text_output,\n        )"
    signature: 'def __call__(self, input_text: str)'
    decorators: []
    raises: []
    calls:
    - target: self._extraction_prompt.format
      type: instance
    - target: str
      type: builtin
    - target: self._model.achat
      type: instance
    - target: logger.exception
      type: unresolved
    - target: self._on_error
      type: instance
    - target: traceback::format_exc
      type: stdlib
    - target: graphrag/index/operations/summarize_communities/community_reports_extractor.py::_get_text_output
      type: internal
    - target: CommunityReportsResult
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__call__
    called_by: []
  - name: _get_text_output
    start_line: 98
    end_line: 102
    code: "def _get_text_output(self, report: CommunityReportResponse) -> str:\n \
      \       report_sections = \"\\n\\n\".join(\n            f\"## {f.summary}\\\
      n\\n{f.explanation}\" for f in report.findings\n        )\n        return f\"\
      # {report.title}\\n\\n{report.summary}\\n\\n{report_sections}\""
    signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
    decorators: []
    raises: []
    calls:
    - target: '"\n\n".join'
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor._get_text_output
    called_by: []
- file_name: graphrag/index/operations/summarize_communities/explode_communities.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: COMMUNITY_ID
    alias: null
  functions:
  - name: explode_communities
    start_line: 13
    end_line: 23
    code: "def explode_communities(\n    communities: pd.DataFrame, entities: pd.DataFrame\n\
      ) -> pd.DataFrame:\n    \"\"\"Explode a list of communities into nodes for filtering.\"\
      \"\"\n    community_join = communities.explode(\"entity_ids\").loc[\n      \
      \  :, [\"community\", \"level\", \"entity_ids\"]\n    ]\n    nodes = entities.merge(\n\
      \        community_join, left_on=\"id\", right_on=\"entity_ids\", how=\"left\"\
      \n    )\n    return nodes.loc[nodes.loc[:, COMMUNITY_ID] != -1]"
    signature: "def explode_communities(\n    communities: pd.DataFrame, entities:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: communities.explode
      type: unresolved
    - target: entities.merge
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
- file_name: graphrag/index/operations/summarize_communities/graph_context/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.operations.summarize_communities.build_mixed_context
    name: build_mixed_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.graph_context.sort_context
    name: parallel_sort_context_batch
    alias: null
  - module: graphrag.index.operations.summarize_communities.graph_context.sort_context
    name: sort_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.utils
    name: get_levels
    alias: null
  - module: graphrag.index.utils.dataframes
    name: antijoin
    alias: null
  - module: graphrag.index.utils.dataframes
    name: drop_columns
    alias: null
  - module: graphrag.index.utils.dataframes
    name: join
    alias: null
  - module: graphrag.index.utils.dataframes
    name: select
    alias: null
  - module: graphrag.index.utils.dataframes
    name: transform_series
    alias: null
  - module: graphrag.index.utils.dataframes
    name: union
    alias: null
  - module: graphrag.index.utils.dataframes
    name: where_column_equals
    alias: null
  - module: graphrag.logger.progress
    name: progress_iterable
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_local_context
    start_line: 38
    end_line: 60
    code: "def build_local_context(\n    nodes,\n    edges,\n    claims,\n    tokenizer:\
      \ Tokenizer,\n    callbacks: WorkflowCallbacks,\n    max_context_tokens: int\
      \ = 16_000,\n):\n    \"\"\"Prep communities for report generation.\"\"\"\n \
      \   levels = get_levels(nodes, schemas.COMMUNITY_LEVEL)\n\n    dfs = []\n\n\
      \    for level in progress_iterable(levels, callbacks.progress, len(levels)):\n\
      \        communities_at_level_df = _prepare_reports_at_level(\n            nodes,\
      \ edges, claims, tokenizer, level, max_context_tokens\n        )\n\n       \
      \ communities_at_level_df.loc[:, schemas.COMMUNITY_LEVEL] = level\n        dfs.append(communities_at_level_df)\n\
      \n    # build initial local context for all communities\n    return pd.concat(dfs)"
    signature: "def build_local_context(\n    nodes,\n    edges,\n    claims,\n  \
      \  tokenizer: Tokenizer,\n    callbacks: WorkflowCallbacks,\n    max_context_tokens:\
      \ int = 16_000,\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/utils.py::get_levels
      type: internal
    - target: graphrag/logger/progress.py::progress_iterable
      type: internal
    - target: len
      type: builtin
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
      type: internal
    - target: dfs.append
      type: unresolved
    - target: pandas::concat
      type: external
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
  - name: _prepare_reports_at_level
    start_line: 63
    end_line: 188
    code: "def _prepare_reports_at_level(\n    node_df: pd.DataFrame,\n    edge_df:\
      \ pd.DataFrame,\n    claim_df: pd.DataFrame | None,\n    tokenizer: Tokenizer,\n\
      \    level: int,\n    max_context_tokens: int = 16_000,\n) -> pd.DataFrame:\n\
      \    \"\"\"Prepare reports at a given level.\"\"\"\n    # Filter and prepare\
      \ node details\n    level_node_df = node_df[node_df[schemas.COMMUNITY_LEVEL]\
      \ == level]\n    logger.info(\"Number of nodes at level=%s => %s\", level, len(level_node_df))\n\
      \    nodes_set = set(level_node_df[schemas.TITLE])\n\n    # Filter and prepare\
      \ edge details\n    level_edge_df = edge_df[\n        edge_df.loc[:, schemas.EDGE_SOURCE].isin(nodes_set)\n\
      \        & edge_df.loc[:, schemas.EDGE_TARGET].isin(nodes_set)\n    ]\n    level_edge_df.loc[:,\
      \ schemas.EDGE_DETAILS] = level_edge_df.loc[\n        :,\n        [\n      \
      \      schemas.SHORT_ID,\n            schemas.EDGE_SOURCE,\n            schemas.EDGE_TARGET,\n\
      \            schemas.DESCRIPTION,\n            schemas.EDGE_DEGREE,\n      \
      \  ],\n    ].to_dict(orient=\"records\")\n\n    level_claim_df = pd.DataFrame()\n\
      \    if claim_df is not None:\n        level_claim_df = claim_df[\n        \
      \    claim_df.loc[:, schemas.CLAIM_SUBJECT].isin(nodes_set)\n        ]\n\n \
      \   # Merge node and edge details\n    # Group edge details by node and aggregate\
      \ into lists\n    source_edges = (\n        level_edge_df.groupby(schemas.EDGE_SOURCE)\n\
      \        .agg({schemas.EDGE_DETAILS: \"first\"})\n        .reset_index()\n \
      \       .rename(columns={schemas.EDGE_SOURCE: schemas.TITLE})\n    )\n\n   \
      \ target_edges = (\n        level_edge_df.groupby(schemas.EDGE_TARGET)\n   \
      \     .agg({schemas.EDGE_DETAILS: \"first\"})\n        .reset_index()\n    \
      \    .rename(columns={schemas.EDGE_TARGET: schemas.TITLE})\n    )\n\n    # Merge\
      \ aggregated edges into the node DataFrame\n    merged_node_df = level_node_df.merge(\n\
      \        source_edges, on=schemas.TITLE, how=\"left\"\n    ).merge(target_edges,\
      \ on=schemas.TITLE, how=\"left\")\n\n    # Combine source and target edge details\
      \ into a single column\n    merged_node_df.loc[:, schemas.EDGE_DETAILS] = merged_node_df.loc[\n\
      \        :, f\"{schemas.EDGE_DETAILS}_x\"\n    ].combine_first(merged_node_df.loc[:,\
      \ f\"{schemas.EDGE_DETAILS}_y\"])\n\n    # Drop intermediate columns\n    merged_node_df.drop(\n\
      \        columns=[f\"{schemas.EDGE_DETAILS}_x\", f\"{schemas.EDGE_DETAILS}_y\"\
      ], inplace=True\n    )\n\n    # Aggregate node and edge details\n    merged_node_df\
      \ = (\n        merged_node_df.groupby([\n            schemas.TITLE,\n      \
      \      schemas.COMMUNITY_ID,\n            schemas.COMMUNITY_LEVEL,\n       \
      \     schemas.NODE_DEGREE,\n        ])\n        .agg({\n            schemas.NODE_DETAILS:\
      \ \"first\",\n            schemas.EDGE_DETAILS: lambda x: list(x.dropna()),\n\
      \        })\n        .reset_index()\n    )\n\n    # Add ALL_CONTEXT column\n\
      \    # Ensure schemas.CLAIM_DETAILS exists with the correct length\n    # Merge\
      \ claim details if available\n    if claim_df is not None:\n        merged_node_df\
      \ = merged_node_df.merge(\n            level_claim_df.loc[\n               \
      \ :, [schemas.CLAIM_SUBJECT, schemas.CLAIM_DETAILS]\n            ].rename(columns={schemas.CLAIM_SUBJECT:\
      \ schemas.TITLE}),\n            on=schemas.TITLE,\n            how=\"left\"\
      ,\n        )\n\n    # Create the ALL_CONTEXT column\n    merged_node_df[schemas.ALL_CONTEXT]\
      \ = (\n        merged_node_df.loc[\n            :,\n            [\n        \
      \        schemas.TITLE,\n                schemas.NODE_DEGREE,\n            \
      \    schemas.NODE_DETAILS,\n                schemas.EDGE_DETAILS,\n        \
      \    ],\n        ]\n        .assign(\n            **{schemas.CLAIM_DETAILS:\
      \ merged_node_df[schemas.CLAIM_DETAILS]}\n            if claim_df is not None\n\
      \            else {}\n        )\n        .to_dict(orient=\"records\")\n    )\n\
      \n    # group all node details by community\n    community_df = (\n        merged_node_df.groupby(schemas.COMMUNITY_ID)\n\
      \        .agg({schemas.ALL_CONTEXT: list})\n        .reset_index()\n    )\n\n\
      \    # Generate community-level context strings using vectorized batch processing\n\
      \    return parallel_sort_context_batch(\n        community_df,\n        tokenizer=tokenizer,\n\
      \        max_context_tokens=max_context_tokens,\n    )"
    signature: "def _prepare_reports_at_level(\n    node_df: pd.DataFrame,\n    edge_df:\
      \ pd.DataFrame,\n    claim_df: pd.DataFrame | None,\n    tokenizer: Tokenizer,\n\
      \    level: int,\n    max_context_tokens: int = 16_000,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: len
      type: builtin
    - target: set
      type: builtin
    - target: edge_df.loc[:, schemas.EDGE_SOURCE].isin
      type: unresolved
    - target: edge_df.loc[:, schemas.EDGE_TARGET].isin
      type: unresolved
    - target: "level_edge_df.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n\
        \            schemas.EDGE_SOURCE,\n            schemas.EDGE_TARGET,\n    \
        \        schemas.DESCRIPTION,\n            schemas.EDGE_DEGREE,\n        ],\n\
        \    ].to_dict"
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: claim_df.loc[:, schemas.CLAIM_SUBJECT].isin
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_SOURCE)\n        .agg({schemas.EDGE_DETAILS:\
        \ \"first\"})\n        .reset_index()\n        .rename"
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_SOURCE)\n        .agg({schemas.EDGE_DETAILS:\
        \ \"first\"})\n        .reset_index"
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_SOURCE)\n        .agg"
      type: unresolved
    - target: level_edge_df.groupby
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_TARGET)\n        .agg({schemas.EDGE_DETAILS:\
        \ \"first\"})\n        .reset_index()\n        .rename"
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_TARGET)\n        .agg({schemas.EDGE_DETAILS:\
        \ \"first\"})\n        .reset_index"
      type: unresolved
    - target: "level_edge_df.groupby(schemas.EDGE_TARGET)\n        .agg"
      type: unresolved
    - target: "level_node_df.merge(\n        source_edges, on=schemas.TITLE, how=\"\
        left\"\n    ).merge"
      type: unresolved
    - target: level_node_df.merge
      type: unresolved
    - target: "merged_node_df.loc[\n        :, f\"{schemas.EDGE_DETAILS}_x\"\n   \
        \ ].combine_first"
      type: unresolved
    - target: merged_node_df.drop
      type: unresolved
    - target: "merged_node_df.groupby([\n            schemas.TITLE,\n            schemas.COMMUNITY_ID,\n\
        \            schemas.COMMUNITY_LEVEL,\n            schemas.NODE_DEGREE,\n\
        \        ])\n        .agg({\n            schemas.NODE_DETAILS: \"first\",\n\
        \            schemas.EDGE_DETAILS: lambda x: list(x.dropna()),\n        })\n\
        \        .reset_index"
      type: unresolved
    - target: "merged_node_df.groupby([\n            schemas.TITLE,\n            schemas.COMMUNITY_ID,\n\
        \            schemas.COMMUNITY_LEVEL,\n            schemas.NODE_DEGREE,\n\
        \        ])\n        .agg"
      type: unresolved
    - target: merged_node_df.groupby
      type: unresolved
    - target: list
      type: builtin
    - target: x.dropna
      type: unresolved
    - target: merged_node_df.merge
      type: unresolved
    - target: "level_claim_df.loc[\n                :, [schemas.CLAIM_SUBJECT, schemas.CLAIM_DETAILS]\n\
        \            ].rename"
      type: unresolved
    - target: "merged_node_df.loc[\n            :,\n            [\n              \
        \  schemas.TITLE,\n                schemas.NODE_DEGREE,\n                schemas.NODE_DETAILS,\n\
        \                schemas.EDGE_DETAILS,\n            ],\n        ]\n      \
        \  .assign(\n            **{schemas.CLAIM_DETAILS: merged_node_df[schemas.CLAIM_DETAILS]}\n\
        \            if claim_df is not None\n            else {}\n        )\n   \
        \     .to_dict"
      type: unresolved
    - target: "merged_node_df.loc[\n            :,\n            [\n              \
        \  schemas.TITLE,\n                schemas.NODE_DEGREE,\n                schemas.NODE_DETAILS,\n\
        \                schemas.EDGE_DETAILS,\n            ],\n        ]\n      \
        \  .assign"
      type: unresolved
    - target: "merged_node_df.groupby(schemas.COMMUNITY_ID)\n        .agg({schemas.ALL_CONTEXT:\
        \ list})\n        .reset_index"
      type: unresolved
    - target: "merged_node_df.groupby(schemas.COMMUNITY_ID)\n        .agg"
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
      type: internal
  - name: build_level_context
    start_line: 191
    end_line: 261
    code: "def build_level_context(\n    report_df: pd.DataFrame | None,\n    community_hierarchy_df:\
      \ pd.DataFrame,\n    local_context_df: pd.DataFrame,\n    tokenizer: Tokenizer,\n\
      \    level: int,\n    max_context_tokens: int,\n) -> pd.DataFrame:\n    \"\"\
      \"\n    Prep context for each community in a given level.\n\n    For each community:\n\
      \    - Check if local context fits within the limit, if yes use local context\n\
      \    - If local context exceeds the limit, iteratively replace local context\
      \ with sub-community reports, starting from the biggest sub-community\n    \"\
      \"\"\n    # Filter by community level\n    level_context_df = local_context_df.loc[\n\
      \        local_context_df.loc[:, schemas.COMMUNITY_LEVEL] == level\n    ]\n\n\
      \    # Filter valid and invalid contexts using boolean logic\n    valid_context_df\
      \ = level_context_df.loc[\n        ~level_context_df.loc[:, schemas.CONTEXT_EXCEED_FLAG]\n\
      \    ]\n    invalid_context_df = level_context_df.loc[\n        level_context_df.loc[:,\
      \ schemas.CONTEXT_EXCEED_FLAG]\n    ]\n\n    # there is no report to substitute\
      \ with, so we just trim the local context of the invalid context records\n \
      \   # this case should only happen at the bottom level of the community hierarchy\
      \ where there are no sub-communities\n    if invalid_context_df.empty:\n   \
      \     return valid_context_df\n\n    if report_df is None or report_df.empty:\n\
      \        invalid_context_df.loc[:, schemas.CONTEXT_STRING] = _sort_and_trim_context(\n\
      \            invalid_context_df, tokenizer, max_context_tokens\n        )\n\
      \        invalid_context_df[schemas.CONTEXT_SIZE] = invalid_context_df.loc[\n\
      \            :, schemas.CONTEXT_STRING\n        ].map(tokenizer.num_tokens)\n\
      \        invalid_context_df[schemas.CONTEXT_EXCEED_FLAG] = False\n        return\
      \ union(valid_context_df, invalid_context_df)\n\n    level_context_df = _antijoin_reports(level_context_df,\
      \ report_df)\n\n    # for each invalid context, we will try to substitute with\
      \ sub-community reports\n    # first get local context and report (if available)\
      \ for each sub-community\n    sub_context_df = _get_subcontext_df(level + 1,\
      \ report_df, local_context_df)\n    community_df = _get_community_df(\n    \
      \    level,\n        invalid_context_df,\n        sub_context_df,\n        community_hierarchy_df,\n\
      \        tokenizer,\n        max_context_tokens,\n    )\n\n    # handle any\
      \ remaining invalid records that can't be subsituted with sub-community reports\n\
      \    # this should be rare, but if it happens, we will just trim the local context\
      \ to fit the limit\n    remaining_df = _antijoin_reports(invalid_context_df,\
      \ community_df)\n    remaining_df.loc[:, schemas.CONTEXT_STRING] = _sort_and_trim_context(\n\
      \        remaining_df, tokenizer, max_context_tokens\n    )\n\n    result =\
      \ union(valid_context_df, community_df, remaining_df)\n    result[schemas.CONTEXT_SIZE]\
      \ = result.loc[:, schemas.CONTEXT_STRING].map(\n        tokenizer.num_tokens\n\
      \    )\n\n    result[schemas.CONTEXT_EXCEED_FLAG] = False\n    return result"
    signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n  \
      \  community_hierarchy_df: pd.DataFrame,\n    local_context_df: pd.DataFrame,\n\
      \    tokenizer: Tokenizer,\n    level: int,\n    max_context_tokens: int,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
      type: internal
    - target: "invalid_context_df.loc[\n            :, schemas.CONTEXT_STRING\n  \
        \      ].map"
      type: unresolved
    - target: graphrag/index/utils/dataframes.py::union
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
    - target: result.loc[:, schemas.CONTEXT_STRING].map
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
    called_by: []
  - name: _drop_community_level
    start_line: 264
    end_line: 266
    code: "def _drop_community_level(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\
      Drop the community level column from the dataframe.\"\"\"\n    return drop_columns(df,\
      \ schemas.COMMUNITY_LEVEL)"
    signature: 'def _drop_community_level(df: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/dataframes.py::drop_columns
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
  - name: _at_level
    start_line: 269
    end_line: 271
    code: "def _at_level(level: int, df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\
      Return records at the given level.\"\"\"\n    return where_column_equals(df,\
      \ schemas.COMMUNITY_LEVEL, level)"
    signature: 'def _at_level(level: int, df: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/dataframes.py::where_column_equals
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
  - name: _antijoin_reports
    start_line: 274
    end_line: 276
    code: "def _antijoin_reports(df: pd.DataFrame, reports: pd.DataFrame) -> pd.DataFrame:\n\
      \    \"\"\"Return records in df that are not in reports.\"\"\"\n    return antijoin(df,\
      \ reports, schemas.COMMUNITY_ID)"
    signature: 'def _antijoin_reports(df: pd.DataFrame, reports: pd.DataFrame) ->
      pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/dataframes.py::antijoin
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
      type: internal
  - name: _sort_and_trim_context
    start_line: 279
    end_line: 289
    code: "def _sort_and_trim_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series:\n    \"\"\"Sort and trim context\
      \ to fit the limit.\"\"\"\n    series = cast(\"pd.Series\", df[schemas.ALL_CONTEXT])\n\
      \    return transform_series(\n        series,\n        lambda x: sort_context(\n\
      \            x, tokenizer=tokenizer, max_context_tokens=max_context_tokens\n\
      \        ),\n    )"
    signature: "def _sort_and_trim_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: graphrag/index/utils/dataframes.py::transform_series
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
      type: internal
  - name: _build_mixed_context
    start_line: 292
    end_line: 302
    code: "def _build_mixed_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series:\n    \"\"\"Sort and trim context\
      \ to fit the limit.\"\"\"\n    series = cast(\"pd.Series\", df[schemas.ALL_CONTEXT])\n\
      \    return transform_series(\n        series,\n        lambda x: build_mixed_context(\n\
      \            x, tokenizer, max_context_tokens=max_context_tokens\n        ),\n\
      \    )"
    signature: "def _build_mixed_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: graphrag/index/utils/dataframes.py::transform_series
      type: internal
    - target: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
  - name: _get_subcontext_df
    start_line: 305
    end_line: 315
    code: "def _get_subcontext_df(\n    level: int, report_df: pd.DataFrame, local_context_df:\
      \ pd.DataFrame\n) -> pd.DataFrame:\n    \"\"\"Get sub-community context for\
      \ each community.\"\"\"\n    sub_report_df = _drop_community_level(_at_level(level,\
      \ report_df))\n    sub_context_df = _at_level(level, local_context_df)\n   \
      \ sub_context_df = join(sub_context_df, sub_report_df, schemas.COMMUNITY_ID)\n\
      \    sub_context_df.rename(\n        columns={schemas.COMMUNITY_ID: schemas.SUB_COMMUNITY},\
      \ inplace=True\n    )\n    return sub_context_df"
    signature: "def _get_subcontext_df(\n    level: int, report_df: pd.DataFrame,\
      \ local_context_df: pd.DataFrame\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
      type: internal
    - target: graphrag/index/utils/dataframes.py::join
      type: internal
    - target: sub_context_df.rename
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
      type: internal
  - name: _get_community_df
    start_line: 318
    end_line: 362
    code: "def _get_community_df(\n    level: int,\n    invalid_context_df: pd.DataFrame,\n\
      \    sub_context_df: pd.DataFrame,\n    community_hierarchy_df: pd.DataFrame,\n\
      \    tokenizer: Tokenizer,\n    max_context_tokens: int,\n) -> pd.DataFrame:\n\
      \    \"\"\"Get community context for each community.\"\"\"\n    # collect all\
      \ sub communities' contexts for each community\n    community_df = _drop_community_level(_at_level(level,\
      \ community_hierarchy_df))\n    invalid_community_ids = select(invalid_context_df,\
      \ schemas.COMMUNITY_ID)\n    subcontext_selection = select(\n        sub_context_df,\n\
      \        schemas.SUB_COMMUNITY,\n        schemas.FULL_CONTENT,\n        schemas.ALL_CONTEXT,\n\
      \        schemas.CONTEXT_SIZE,\n    )\n\n    invalid_communities = join(\n \
      \       community_df, invalid_community_ids, schemas.COMMUNITY_ID, \"inner\"\
      \n    )\n    community_df = join(\n        invalid_communities, subcontext_selection,\
      \ schemas.SUB_COMMUNITY\n    )\n    community_df[schemas.ALL_CONTEXT] = community_df.apply(\n\
      \        lambda x: {\n            schemas.SUB_COMMUNITY: x[schemas.SUB_COMMUNITY],\n\
      \            schemas.ALL_CONTEXT: x[schemas.ALL_CONTEXT],\n            schemas.FULL_CONTENT:\
      \ x[schemas.FULL_CONTENT],\n            schemas.CONTEXT_SIZE: x[schemas.CONTEXT_SIZE],\n\
      \        },\n        axis=1,\n    )\n    community_df = (\n        community_df.groupby(schemas.COMMUNITY_ID)\n\
      \        .agg({schemas.ALL_CONTEXT: list})\n        .reset_index()\n    )\n\
      \    community_df[schemas.CONTEXT_STRING] = _build_mixed_context(\n        community_df,\
      \ tokenizer, max_context_tokens\n    )\n    community_df[schemas.COMMUNITY_LEVEL]\
      \ = level\n    return community_df"
    signature: "def _get_community_df(\n    level: int,\n    invalid_context_df: pd.DataFrame,\n\
      \    sub_context_df: pd.DataFrame,\n    community_hierarchy_df: pd.DataFrame,\n\
      \    tokenizer: Tokenizer,\n    max_context_tokens: int,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
      type: internal
    - target: graphrag/index/utils/dataframes.py::select
      type: internal
    - target: graphrag/index/utils/dataframes.py::join
      type: internal
    - target: community_df.apply
      type: unresolved
    - target: "community_df.groupby(schemas.COMMUNITY_ID)\n        .agg({schemas.ALL_CONTEXT:\
        \ list})\n        .reset_index"
      type: unresolved
    - target: "community_df.groupby(schemas.COMMUNITY_ID)\n        .agg"
      type: unresolved
    - target: community_df.groupby
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
      type: internal
- file_name: graphrag/index/operations/summarize_communities/graph_context/sort_context.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  - module: concurrent.futures
    name: ThreadPoolExecutor
    alias: null
  functions:
  - name: sort_context
    start_line: 11
    end_line: 126
    code: "def sort_context(\n    local_context: list[dict],\n    tokenizer: Tokenizer,\n\
      \    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n    node_name_column: str = schemas.TITLE,\n    node_details_column:\
      \ str = schemas.NODE_DETAILS,\n    edge_id_column: str = schemas.SHORT_ID,\n\
      \    edge_details_column: str = schemas.EDGE_DETAILS,\n    edge_degree_column:\
      \ str = schemas.EDGE_DEGREE,\n    edge_source_column: str = schemas.EDGE_SOURCE,\n\
      \    edge_target_column: str = schemas.EDGE_TARGET,\n    claim_details_column:\
      \ str = schemas.CLAIM_DETAILS,\n) -> str:\n    \"\"\"Sort context by degree\
      \ in descending order, optimizing for performance.\"\"\"\n\n    def _get_context_string(\n\
      \        entities: list[dict],\n        edges: list[dict],\n        claims:\
      \ list[dict],\n        sub_community_reports: list[dict] | None = None,\n  \
      \  ) -> str:\n        \"\"\"Concatenate structured data into a context string.\"\
      \"\"\n        contexts = []\n        if sub_community_reports:\n           \
      \ report_df = pd.DataFrame(sub_community_reports)\n            if not report_df.empty:\n\
      \                contexts.append(\n                    f\"----Reports-----\\\
      n{report_df.to_csv(index=False, sep=',')}\"\n                )\n\n        for\
      \ label, data in [\n            (\"Entities\", entities),\n            (\"Claims\"\
      , claims),\n            (\"Relationships\", edges),\n        ]:\n          \
      \  if data:\n                data_df = pd.DataFrame(data)\n                if\
      \ not data_df.empty:\n                    contexts.append(\n               \
      \         f\"-----{label}-----\\n{data_df.to_csv(index=False, sep=',')}\"\n\
      \                    )\n\n        return \"\\n\\n\".join(contexts)\n\n    #\
      \ Preprocess local context\n    edges = [\n        {**e, schemas.SHORT_ID: int(e[schemas.SHORT_ID])}\n\
      \        for record in local_context\n        for e in record.get(edge_details_column,\
      \ [])\n        if isinstance(e, dict)\n    ]\n\n    node_details = {\n     \
      \   record[node_name_column]: {\n            **record[node_details_column],\n\
      \            schemas.SHORT_ID: int(record[node_details_column][schemas.SHORT_ID]),\n\
      \        }\n        for record in local_context\n    }\n\n    claim_details\
      \ = {\n        record[node_name_column]: [\n            {**c, schemas.SHORT_ID:\
      \ int(c[schemas.SHORT_ID])}\n            for c in record.get(claim_details_column,\
      \ [])\n            if isinstance(c, dict) and c.get(schemas.SHORT_ID) is not\
      \ None\n        ]\n        for record in local_context\n        if isinstance(record.get(claim_details_column),\
      \ list)\n    }\n\n    # Sort edges by degree (desc) and ID (asc)\n    edges.sort(key=lambda\
      \ x: (-x.get(edge_degree_column, 0), x.get(edge_id_column, \"\")))\n\n    #\
      \ Deduplicate and build context incrementally\n    edge_ids, nodes_ids, claims_ids\
      \ = set(), set(), set()\n    sorted_edges, sorted_nodes, sorted_claims = [],\
      \ [], []\n    context_string = \"\"\n\n    for edge in edges:\n        source,\
      \ target = edge[edge_source_column], edge[edge_target_column]\n\n        # Add\
      \ source and target node details\n        for node in [node_details.get(source),\
      \ node_details.get(target)]:\n            if node and node[schemas.SHORT_ID]\
      \ not in nodes_ids:\n                nodes_ids.add(node[schemas.SHORT_ID])\n\
      \                sorted_nodes.append(node)\n\n        # Add claims related to\
      \ source and target\n        for claims in [claim_details.get(source), claim_details.get(target)]:\n\
      \            if claims:\n                for claim in claims:\n            \
      \        if claim[schemas.SHORT_ID] not in claims_ids:\n                   \
      \     claims_ids.add(claim[schemas.SHORT_ID])\n                        sorted_claims.append(claim)\n\
      \n        # Add the edge\n        if edge[schemas.SHORT_ID] not in edge_ids:\n\
      \            edge_ids.add(edge[schemas.SHORT_ID])\n            sorted_edges.append(edge)\n\
      \n        # Generate new context string\n        new_context_string = _get_context_string(\n\
      \            sorted_nodes, sorted_edges, sorted_claims, sub_community_reports\n\
      \        )\n        if (\n            max_context_tokens\n            and tokenizer.num_tokens(new_context_string)\
      \ > max_context_tokens\n        ):\n            break\n        context_string\
      \ = new_context_string\n\n    # Return the final context string\n    return\
      \ context_string or _get_context_string(\n        sorted_nodes, sorted_edges,\
      \ sorted_claims, sub_community_reports\n    )"
    signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer:\
      \ Tokenizer,\n    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n    node_name_column: str = schemas.TITLE,\n    node_details_column:\
      \ str = schemas.NODE_DETAILS,\n    edge_id_column: str = schemas.SHORT_ID,\n\
      \    edge_details_column: str = schemas.EDGE_DETAILS,\n    edge_degree_column:\
      \ str = schemas.EDGE_DEGREE,\n    edge_source_column: str = schemas.EDGE_SOURCE,\n\
      \    edge_target_column: str = schemas.EDGE_TARGET,\n    claim_details_column:\
      \ str = schemas.CLAIM_DETAILS,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: int
      type: builtin
    - target: record.get
      type: unresolved
    - target: isinstance
      type: builtin
    - target: c.get
      type: unresolved
    - target: edges.sort
      type: unresolved
    - target: x.get
      type: unresolved
    - target: set
      type: builtin
    - target: node_details.get
      type: unresolved
    - target: nodes_ids.add
      type: unresolved
    - target: sorted_nodes.append
      type: unresolved
    - target: claim_details.get
      type: unresolved
    - target: claims_ids.add
      type: unresolved
    - target: sorted_claims.append
      type: unresolved
    - target: edge_ids.add
      type: unresolved
    - target: sorted_edges.append
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::_get_context_string
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
    called_by:
    - source: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
      type: internal
    - source: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
      type: internal
    - source: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
      type: internal
  - name: _get_context_string
    start_line: 27
    end_line: 54
    code: "def _get_context_string(\n        entities: list[dict],\n        edges:\
      \ list[dict],\n        claims: list[dict],\n        sub_community_reports: list[dict]\
      \ | None = None,\n    ) -> str:\n        \"\"\"Concatenate structured data into\
      \ a context string.\"\"\"\n        contexts = []\n        if sub_community_reports:\n\
      \            report_df = pd.DataFrame(sub_community_reports)\n            if\
      \ not report_df.empty:\n                contexts.append(\n                 \
      \   f\"----Reports-----\\n{report_df.to_csv(index=False, sep=',')}\"\n     \
      \           )\n\n        for label, data in [\n            (\"Entities\", entities),\n\
      \            (\"Claims\", claims),\n            (\"Relationships\", edges),\n\
      \        ]:\n            if data:\n                data_df = pd.DataFrame(data)\n\
      \                if not data_df.empty:\n                    contexts.append(\n\
      \                        f\"-----{label}-----\\n{data_df.to_csv(index=False,\
      \ sep=',')}\"\n                    )\n\n        return \"\\n\\n\".join(contexts)"
    signature: "def _get_context_string(\n        entities: list[dict],\n        edges:\
      \ list[dict],\n        claims: list[dict],\n        sub_community_reports: list[dict]\
      \ | None = None,\n    ) -> str"
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: contexts.append
      type: unresolved
    - target: report_df.to_csv
      type: unresolved
    - target: data_df.to_csv
      type: unresolved
    - target: '"\n\n".join'
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::_get_context_string
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
  - name: parallel_sort_context_batch
    start_line: 129
    end_line: 164
    code: "def parallel_sort_context_batch(\n    community_df, tokenizer: Tokenizer,\
      \ max_context_tokens, parallel=False\n):\n    \"\"\"Calculate context using\
      \ parallelization if enabled.\"\"\"\n    if parallel:\n        # Use ThreadPoolExecutor\
      \ for parallel execution\n        from concurrent.futures import ThreadPoolExecutor\n\
      \n        with ThreadPoolExecutor(max_workers=None) as executor:\n         \
      \   context_strings = list(\n                executor.map(\n               \
      \     lambda x: sort_context(\n                        x, tokenizer, max_context_tokens=max_context_tokens\n\
      \                    ),\n                    community_df[schemas.ALL_CONTEXT],\n\
      \                )\n            )\n        community_df[schemas.CONTEXT_STRING]\
      \ = context_strings\n\n    else:\n        # Assign context strings directly\
      \ to the DataFrame\n        community_df[schemas.CONTEXT_STRING] = community_df[schemas.ALL_CONTEXT].apply(\n\
      \            lambda context_list: sort_context(\n                context_list,\
      \ tokenizer, max_context_tokens=max_context_tokens\n            )\n        )\n\
      \n    # Calculate other columns\n    community_df[schemas.CONTEXT_SIZE] = community_df[schemas.CONTEXT_STRING].apply(\n\
      \        tokenizer.num_tokens\n    )\n    community_df[schemas.CONTEXT_EXCEED_FLAG]\
      \ = (\n        community_df[schemas.CONTEXT_SIZE] > max_context_tokens\n   \
      \ )\n\n    return community_df"
    signature: "def parallel_sort_context_batch(\n    community_df, tokenizer: Tokenizer,\
      \ max_context_tokens, parallel=False\n)"
    decorators: []
    raises: []
    calls:
    - target: concurrent.futures::ThreadPoolExecutor
      type: stdlib
    - target: list
      type: builtin
    - target: executor.map
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
    - target: community_df[schemas.ALL_CONTEXT].apply
      type: unresolved
    - target: community_df[schemas.CONTEXT_STRING].apply
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
      type: internal
- file_name: graphrag/index/operations/summarize_communities/strategies.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.summarize_communities.community_reports_extractor
    name: CommunityReportsExtractor
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: CommunityReport
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: Finding
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: StrategyConfig
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  functions:
  - name: run_graph_intelligence
    start_line: 25
    end_line: 43
    code: "async def run_graph_intelligence(\n    community: str | int,\n    input:\
      \ str,\n    level: int,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    args: StrategyConfig,\n) -> CommunityReport | None:\n    \"\"\"Run the\
      \ graph intelligence entity extraction strategy.\"\"\"\n    llm_config = LanguageModelConfig(**args[\"\
      llm\"])\n    llm = ModelManager().get_or_create_chat_model(\n        name=\"\
      community_reporting\",\n        model_type=llm_config.type,\n        config=llm_config,\n\
      \        callbacks=callbacks,\n        cache=cache,\n    )\n\n    return await\
      \ _run_extractor(llm, community, input, level, args)"
    signature: "def run_graph_intelligence(\n    community: str | int,\n    input:\
      \ str,\n    level: int,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    args: StrategyConfig,\n) -> CommunityReport | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/index/operations/summarize_communities/strategies.py::_run_extractor
      type: internal
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/strategies.py::run_graph_intelligence
    called_by: []
  - name: _run_extractor
    start_line: 46
    end_line: 85
    code: "async def _run_extractor(\n    model: ChatModel,\n    community: str |\
      \ int,\n    input: str,\n    level: int,\n    args: StrategyConfig,\n) -> CommunityReport\
      \ | None:\n    extractor = CommunityReportsExtractor(\n        model,\n    \
      \    extraction_prompt=args.get(\"extraction_prompt\", None),\n        max_report_length=args.get(\"\
      max_report_length\", None),\n        on_error=lambda e, stack, _data: logger.error(\n\
      \            \"Community Report Extraction Error\", exc_info=e, extra={\"stack\"\
      : stack}\n        ),\n    )\n\n    try:\n        results = await extractor(input)\n\
      \        report = results.structured_output\n        if report is None:\n  \
      \          logger.warning(\"No report found for community: %s\", community)\n\
      \            return None\n\n        return CommunityReport(\n            community=community,\n\
      \            full_content=results.output,\n            level=level,\n      \
      \      rank=report.rating,\n            title=report.title,\n            rating_explanation=report.rating_explanation,\n\
      \            summary=report.summary,\n            findings=[\n             \
      \   Finding(explanation=f.explanation, summary=f.summary)\n                for\
      \ f in report.findings\n            ],\n            full_content_json=report.model_dump_json(indent=4),\n\
      \        )\n    except Exception:\n        logger.exception(\"Error processing\
      \ community: %s\", community)\n        return None"
    signature: "def _run_extractor(\n    model: ChatModel,\n    community: str | int,\n\
      \    input: str,\n    level: int,\n    args: StrategyConfig,\n) -> CommunityReport\
      \ | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor
      type: internal
    - target: args.get
      type: unresolved
    - target: logger.error
      type: unresolved
    - target: extractor
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/typing.py::CommunityReport
      type: internal
    - target: graphrag/index/operations/summarize_communities/typing.py::Finding
      type: internal
    - target: report.model_dump_json
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/strategies.py::_run_extractor
    called_by:
    - source: graphrag/index/operations/summarize_communities/strategies.py::run_graph_intelligence
      type: internal
- file_name: graphrag/index/operations/summarize_communities/summarize_communities.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: CommunityReport
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: CommunityReportsStrategy
    alias: null
  - module: graphrag.index.operations.summarize_communities.typing
    name: CreateCommunityReportsStrategyType
    alias: null
  - module: graphrag.index.operations.summarize_communities.utils
    name: get_levels
    alias: null
  - module: graphrag.index.utils.derive_from_rows
    name: derive_from_rows
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  - module: graphrag.index.operations.summarize_communities.strategies
    name: run_graph_intelligence
    alias: null
  functions:
  - name: summarize_communities
    start_line: 31
    end_line: 94
    code: "async def summarize_communities(\n    nodes: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    local_contexts,\n    level_context_builder: Callable,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy:\
      \ dict,\n    tokenizer: Tokenizer,\n    max_input_length: int,\n    async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    num_threads: int = 4,\n):\n    \"\"\"\
      Generate community summaries.\"\"\"\n    reports: list[CommunityReport | None]\
      \ = []\n    tick = progress_ticker(callbacks.progress, len(local_contexts))\n\
      \    strategy_exec = load_strategy(strategy[\"type\"])\n    strategy_config\
      \ = {**strategy}\n    community_hierarchy = (\n        communities.explode(\"\
      children\")\n        .rename({\"children\": \"sub_community\"}, axis=1)\n  \
      \      .loc[:, [\"community\", \"level\", \"sub_community\"]]\n    ).dropna()\n\
      \n    levels = get_levels(nodes)\n\n    level_contexts = []\n    for level in\
      \ levels:\n        level_context = level_context_builder(\n            pd.DataFrame(reports),\n\
      \            community_hierarchy_df=community_hierarchy,\n            local_context_df=local_contexts,\n\
      \            level=level,\n            tokenizer=tokenizer,\n            max_context_tokens=max_input_length,\n\
      \        )\n        level_contexts.append(level_context)\n\n    for i, level_context\
      \ in enumerate(level_contexts):\n\n        async def run_generate(record):\n\
      \            result = await _generate_report(\n                strategy_exec,\n\
      \                community_id=record[schemas.COMMUNITY_ID],\n              \
      \  community_level=record[schemas.COMMUNITY_LEVEL],\n                community_context=record[schemas.CONTEXT_STRING],\n\
      \                callbacks=callbacks,\n                cache=cache,\n      \
      \          strategy=strategy_config,\n            )\n            tick()\n  \
      \          return result\n\n        local_reports = await derive_from_rows(\n\
      \            level_context,\n            run_generate,\n            callbacks=NoopWorkflowCallbacks(),\n\
      \            num_threads=num_threads,\n            async_type=async_mode,\n\
      \            progress_msg=f\"level {levels[i]} summarize communities progress:\
      \ \",\n        )\n        reports.extend([lr for lr in local_reports if lr is\
      \ not None])\n\n    return pd.DataFrame(reports)"
    signature: "def summarize_communities(\n    nodes: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    local_contexts,\n    level_context_builder: Callable,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy:\
      \ dict,\n    tokenizer: Tokenizer,\n    max_input_length: int,\n    async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    num_threads: int = 4,\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: len
      type: builtin
    - target: graphrag/index/operations/summarize_communities/summarize_communities.py::load_strategy
      type: internal
    - target: "(\n        communities.explode(\"children\")\n        .rename({\"children\"\
        : \"sub_community\"}, axis=1)\n        .loc[:, [\"community\", \"level\",\
        \ \"sub_community\"]]\n    ).dropna"
      type: unresolved
    - target: "communities.explode(\"children\")\n        .rename"
      type: unresolved
    - target: communities.explode
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/utils.py::get_levels
      type: internal
    - target: level_context_builder
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: level_contexts.append
      type: unresolved
    - target: enumerate
      type: builtin
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: reports.extend
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
  - name: run_generate
    start_line: 71
    end_line: 82
    code: "async def run_generate(record):\n            result = await _generate_report(\n\
      \                strategy_exec,\n                community_id=record[schemas.COMMUNITY_ID],\n\
      \                community_level=record[schemas.COMMUNITY_LEVEL],\n        \
      \        community_context=record[schemas.CONTEXT_STRING],\n               \
      \ callbacks=callbacks,\n                cache=cache,\n                strategy=strategy_config,\n\
      \            )\n            tick()\n            return result"
    signature: def run_generate(record)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/summarize_communities.py::_generate_report
      type: internal
    - target: tick
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::run_generate
    called_by: []
  - name: _generate_report
    start_line: 97
    end_line: 114
    code: "async def _generate_report(\n    runner: CommunityReportsStrategy,\n  \
      \  callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy: dict,\n\
      \    community_id: int,\n    community_level: int,\n    community_context: str,\n\
      ) -> CommunityReport | None:\n    \"\"\"Generate a report for a single community.\"\
      \"\"\n    return await runner(\n        community_id,\n        community_context,\n\
      \        community_level,\n        callbacks,\n        cache,\n        strategy,\n\
      \    )"
    signature: "def _generate_report(\n    runner: CommunityReportsStrategy,\n   \
      \ callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy: dict,\n\
      \    community_id: int,\n    community_level: int,\n    community_context: str,\n\
      ) -> CommunityReport | None"
    decorators: []
    raises: []
    calls:
    - target: runner
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::_generate_report
    called_by:
    - source: graphrag/index/operations/summarize_communities/summarize_communities.py::run_generate
      type: internal
  - name: load_strategy
    start_line: 117
    end_line: 130
    code: "def load_strategy(\n    strategy: CreateCommunityReportsStrategyType,\n\
      ) -> CommunityReportsStrategy:\n    \"\"\"Load strategy method definition.\"\
      \"\"\n    match strategy:\n        case CreateCommunityReportsStrategyType.graph_intelligence:\n\
      \            from graphrag.index.operations.summarize_communities.strategies\
      \ import (\n                run_graph_intelligence,\n            )\n\n     \
      \       return run_graph_intelligence\n        case _:\n            msg = f\"\
      Unknown strategy: {strategy}\"\n            raise ValueError(msg)"
    signature: "def load_strategy(\n    strategy: CreateCommunityReportsStrategyType,\n\
      ) -> CommunityReportsStrategy"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::load_strategy
    called_by:
    - source: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
- file_name: graphrag/index/operations/summarize_communities/text_unit_context/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.index.operations.summarize_communities.build_mixed_context
    name: build_mixed_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.text_unit_context.prep_text_units
    name: prep_text_units
    alias: null
  - module: graphrag.index.operations.summarize_communities.text_unit_context.sort_context
    name: sort_context
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_local_context
    start_line: 26
    end_line: 82
    code: "def build_local_context(\n    community_membership_df: pd.DataFrame,\n\
      \    text_units_df: pd.DataFrame,\n    node_df: pd.DataFrame,\n    tokenizer:\
      \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame:\n   \
      \ \"\"\"\n    Prep context data for community report generation using text unit\
      \ data.\n\n    Community membership has columns [COMMUNITY_ID, COMMUNITY_LEVEL,\
      \ ENTITY_IDS, RELATIONSHIP_IDS, TEXT_UNIT_IDS]\n    \"\"\"\n    # get text unit\
      \ details, include short_id, text, and entity degree (sum of degrees of the\
      \ text unit's nodes that belong to a community)\n    prepped_text_units_df =\
      \ prep_text_units(text_units_df, node_df)\n    prepped_text_units_df = prepped_text_units_df.rename(\n\
      \        columns={\n            schemas.ID: schemas.TEXT_UNIT_IDS,\n       \
      \     schemas.COMMUNITY_ID: schemas.COMMUNITY_ID,\n        }\n    )\n\n    #\
      \ merge text unit details with community membership\n    context_df = community_membership_df.loc[\n\
      \        :, [schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL, schemas.TEXT_UNIT_IDS]\n\
      \    ]\n    context_df = context_df.explode(schemas.TEXT_UNIT_IDS)\n    context_df\
      \ = context_df.merge(\n        prepped_text_units_df,\n        on=[schemas.TEXT_UNIT_IDS,\
      \ schemas.COMMUNITY_ID],\n        how=\"left\",\n    )\n\n    context_df[schemas.ALL_CONTEXT]\
      \ = context_df.apply(\n        lambda x: {\n            \"id\": x[schemas.ALL_DETAILS][schemas.SHORT_ID],\n\
      \            \"text\": x[schemas.ALL_DETAILS][schemas.TEXT],\n            \"\
      entity_degree\": x[schemas.ALL_DETAILS][schemas.ENTITY_DEGREE],\n        },\n\
      \        axis=1,\n    )\n\n    context_df = (\n        context_df.groupby([schemas.COMMUNITY_ID,\
      \ schemas.COMMUNITY_LEVEL])\n        .agg({schemas.ALL_CONTEXT: list})\n   \
      \     .reset_index()\n    )\n    context_df[schemas.CONTEXT_STRING] = context_df[schemas.ALL_CONTEXT].apply(\n\
      \        lambda x: sort_context(x, tokenizer)\n    )\n    context_df[schemas.CONTEXT_SIZE]\
      \ = context_df[schemas.CONTEXT_STRING].apply(\n        lambda x: tokenizer.num_tokens(x)\n\
      \    )\n    context_df[schemas.CONTEXT_EXCEED_FLAG] = context_df[schemas.CONTEXT_SIZE].apply(\n\
      \        lambda x: x > max_context_tokens\n    )\n\n    return context_df"
    signature: "def build_local_context(\n    community_membership_df: pd.DataFrame,\n\
      \    text_units_df: pd.DataFrame,\n    node_df: pd.DataFrame,\n    tokenizer:\
      \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py::prep_text_units
      type: internal
    - target: prepped_text_units_df.rename
      type: unresolved
    - target: context_df.explode
      type: unresolved
    - target: context_df.merge
      type: unresolved
    - target: context_df.apply
      type: unresolved
    - target: "context_df.groupby([schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL])\n\
        \        .agg({schemas.ALL_CONTEXT: list})\n        .reset_index"
      type: unresolved
    - target: "context_df.groupby([schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL])\n\
        \        .agg"
      type: unresolved
    - target: context_df.groupby
      type: unresolved
    - target: context_df[schemas.ALL_CONTEXT].apply
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
      type: internal
    - target: context_df[schemas.CONTEXT_STRING].apply
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: context_df[schemas.CONTEXT_SIZE].apply
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
    called_by:
    - source: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
  - name: build_level_context
    start_line: 85
    end_line: 235
    code: "def build_level_context(\n    report_df: pd.DataFrame | None,\n    community_hierarchy_df:\
      \ pd.DataFrame,\n    local_context_df: pd.DataFrame,\n    level: int,\n    tokenizer:\
      \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame:\n   \
      \ \"\"\"\n    Prep context for each community in a given level.\n\n    For each\
      \ community:\n    - Check if local context fits within the limit, if yes use\
      \ local context\n    - If local context exceeds the limit, iteratively replace\
      \ local context with sub-community reports, starting from the biggest sub-community\n\
      \    \"\"\"\n    if report_df is None or report_df.empty:\n        # there is\
      \ no report to substitute with, so we just trim the local context of the invalid\
      \ context records\n        # this case should only happen at the bottom level\
      \ of the community hierarchy where there are no sub-communities\n        level_context_df\
      \ = local_context_df[\n            local_context_df[schemas.COMMUNITY_LEVEL]\
      \ == level\n        ]\n\n        valid_context_df = cast(\n            \"pd.DataFrame\"\
      ,\n            level_context_df[~level_context_df[schemas.CONTEXT_EXCEED_FLAG]],\n\
      \        )\n        invalid_context_df = cast(\n            \"pd.DataFrame\"\
      ,\n            level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG]],\n\
      \        )\n\n        if invalid_context_df.empty:\n            return valid_context_df\n\
      \n        invalid_context_df.loc[:, [schemas.CONTEXT_STRING]] = invalid_context_df[\n\
      \            schemas.ALL_CONTEXT\n        ].apply(\n            lambda x: sort_context(x,\
      \ tokenizer, max_context_tokens=max_context_tokens)\n        )\n        invalid_context_df.loc[:,\
      \ [schemas.CONTEXT_SIZE]] = invalid_context_df[\n            schemas.CONTEXT_STRING\n\
      \        ].apply(lambda x: tokenizer.num_tokens(x))\n        invalid_context_df.loc[:,\
      \ [schemas.CONTEXT_EXCEED_FLAG]] = False\n\n        return pd.concat([valid_context_df,\
      \ invalid_context_df])\n\n    level_context_df = local_context_df[\n       \
      \ local_context_df[schemas.COMMUNITY_LEVEL] == level\n    ]\n\n    # exclude\
      \ those that already have reports\n    level_context_df = level_context_df.merge(\n\
      \        report_df[[schemas.COMMUNITY_ID]],\n        on=schemas.COMMUNITY_ID,\n\
      \        how=\"outer\",\n        indicator=True,\n    )\n    level_context_df\
      \ = level_context_df[level_context_df[\"_merge\"] == \"left_only\"].drop(\n\
      \        \"_merge\", axis=1\n    )\n    valid_context_df = cast(\n        \"\
      pd.DataFrame\",\n        level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG]\
      \ is False],\n    )\n    invalid_context_df = cast(\n        \"pd.DataFrame\"\
      ,\n        level_context_df[level_context_df[schemas.CONTEXT_EXCEED_FLAG] is\
      \ True],\n    )\n\n    if invalid_context_df.empty:\n        return valid_context_df\n\
      \n    # for each invalid context, we will try to substitute with sub-community\
      \ reports\n    # first get local context and report (if available) for each\
      \ sub-community\n    sub_report_df = report_df[report_df[schemas.COMMUNITY_LEVEL]\
      \ == level + 1].drop(\n        [schemas.COMMUNITY_LEVEL], axis=1\n    )\n  \
      \  sub_context_df = local_context_df[\n        local_context_df[schemas.COMMUNITY_LEVEL]\
      \ == level + 1\n    ]\n    sub_context_df = sub_context_df.merge(\n        sub_report_df,\
      \ on=schemas.COMMUNITY_ID, how=\"left\"\n    )\n    sub_context_df.rename(\n\
      \        columns={schemas.COMMUNITY_ID: schemas.SUB_COMMUNITY}, inplace=True\n\
      \    )\n\n    # collect all sub communities' contexts for each community\n \
      \   community_df = community_hierarchy_df[\n        community_hierarchy_df[schemas.COMMUNITY_LEVEL]\
      \ == level\n    ].drop([schemas.COMMUNITY_LEVEL], axis=1)\n    community_df\
      \ = community_df.merge(\n        invalid_context_df[[schemas.COMMUNITY_ID]],\
      \ on=schemas.COMMUNITY_ID, how=\"inner\"\n    )\n    community_df = community_df.merge(\n\
      \        sub_context_df[\n            [\n                schemas.SUB_COMMUNITY,\n\
      \                schemas.FULL_CONTENT,\n                schemas.ALL_CONTEXT,\n\
      \                schemas.CONTEXT_SIZE,\n            ]\n        ],\n        on=schemas.SUB_COMMUNITY,\n\
      \        how=\"left\",\n    )\n    community_df[schemas.ALL_CONTEXT] = community_df.apply(\n\
      \        lambda x: {\n            schemas.SUB_COMMUNITY: x[schemas.SUB_COMMUNITY],\n\
      \            schemas.ALL_CONTEXT: x[schemas.ALL_CONTEXT],\n            schemas.FULL_CONTENT:\
      \ x[schemas.FULL_CONTENT],\n            schemas.CONTEXT_SIZE: x[schemas.CONTEXT_SIZE],\n\
      \        },\n        axis=1,\n    )\n    community_df = (\n        community_df.groupby(schemas.COMMUNITY_ID)\n\
      \        .agg({schemas.ALL_CONTEXT: list})\n        .reset_index()\n    )\n\
      \    community_df[schemas.CONTEXT_STRING] = community_df[schemas.ALL_CONTEXT].apply(\n\
      \        lambda x: build_mixed_context(x, tokenizer, max_context_tokens)\n \
      \   )\n    community_df[schemas.CONTEXT_SIZE] = community_df[schemas.CONTEXT_STRING].apply(\n\
      \        lambda x: tokenizer.num_tokens(x)\n    )\n    community_df[schemas.CONTEXT_EXCEED_FLAG]\
      \ = False\n    community_df[schemas.COMMUNITY_LEVEL] = level\n\n    # handle\
      \ any remaining invalid records that can't be subsituted with sub-community\
      \ reports\n    # this should be rare, but if it happens, we will just trim the\
      \ local context to fit the limit\n    remaining_df = invalid_context_df.merge(\n\
      \        community_df[[schemas.COMMUNITY_ID]],\n        on=schemas.COMMUNITY_ID,\n\
      \        how=\"outer\",\n        indicator=True,\n    )\n    remaining_df =\
      \ remaining_df[remaining_df[\"_merge\"] == \"left_only\"].drop(\n        \"\
      _merge\", axis=1\n    )\n    remaining_df[schemas.CONTEXT_STRING] = cast(\n\
      \        \"pd.DataFrame\", remaining_df[schemas.ALL_CONTEXT]\n    ).apply(lambda\
      \ x: sort_context(x, tokenizer, max_context_tokens=max_context_tokens))\n  \
      \  remaining_df[schemas.CONTEXT_SIZE] = cast(\n        \"pd.DataFrame\", remaining_df[schemas.CONTEXT_STRING]\n\
      \    ).apply(lambda x: tokenizer.num_tokens(x))\n    remaining_df[schemas.CONTEXT_EXCEED_FLAG]\
      \ = False\n\n    return cast(\n        \"pd.DataFrame\", pd.concat([valid_context_df,\
      \ community_df, remaining_df])\n    )"
    signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n  \
      \  community_hierarchy_df: pd.DataFrame,\n    local_context_df: pd.DataFrame,\n\
      \    level: int,\n    tokenizer: Tokenizer,\n    max_context_tokens: int = 16000,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: "invalid_context_df[\n            schemas.ALL_CONTEXT\n        ].apply"
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
      type: internal
    - target: "invalid_context_df[\n            schemas.CONTEXT_STRING\n        ].apply"
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: pandas::concat
      type: external
    - target: level_context_df.merge
      type: unresolved
    - target: level_context_df[level_context_df["_merge"] == "left_only"].drop
      type: unresolved
    - target: report_df[report_df[schemas.COMMUNITY_LEVEL] == level + 1].drop
      type: unresolved
    - target: sub_context_df.merge
      type: unresolved
    - target: sub_context_df.rename
      type: unresolved
    - target: "community_hierarchy_df[\n        community_hierarchy_df[schemas.COMMUNITY_LEVEL]\
        \ == level\n    ].drop"
      type: unresolved
    - target: community_df.merge
      type: unresolved
    - target: community_df.apply
      type: unresolved
    - target: "community_df.groupby(schemas.COMMUNITY_ID)\n        .agg({schemas.ALL_CONTEXT:\
        \ list})\n        .reset_index"
      type: unresolved
    - target: "community_df.groupby(schemas.COMMUNITY_ID)\n        .agg"
      type: unresolved
    - target: community_df.groupby
      type: unresolved
    - target: community_df[schemas.ALL_CONTEXT].apply
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
      type: internal
    - target: community_df[schemas.CONTEXT_STRING].apply
      type: unresolved
    - target: invalid_context_df.merge
      type: unresolved
    - target: remaining_df[remaining_df["_merge"] == "left_only"].drop
      type: unresolved
    - target: "cast(\n        \"pd.DataFrame\", remaining_df[schemas.ALL_CONTEXT]\n\
        \    ).apply"
      type: unresolved
    - target: "cast(\n        \"pd.DataFrame\", remaining_df[schemas.CONTEXT_STRING]\n\
        \    ).apply"
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
    called_by: []
- file_name: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  functions:
  - name: prep_text_units
    start_line: 15
    end_line: 45
    code: "def prep_text_units(\n    text_unit_df: pd.DataFrame,\n    node_df: pd.DataFrame,\n\
      ) -> pd.DataFrame:\n    \"\"\"\n    Calculate text unit degree  and concatenate\
      \ text unit details.\n\n    Returns : dataframe with columns [COMMUNITY_ID,\
      \ TEXT_UNIT_ID, ALL_DETAILS]\n    \"\"\"\n    node_df.drop(columns=[\"id\"],\
      \ inplace=True)\n    node_to_text_ids = node_df.explode(schemas.TEXT_UNIT_IDS).rename(\n\
      \        columns={schemas.TEXT_UNIT_IDS: schemas.ID}\n    )\n    node_to_text_ids\
      \ = node_to_text_ids[\n        [schemas.TITLE, schemas.COMMUNITY_ID, schemas.NODE_DEGREE,\
      \ schemas.ID]\n    ]\n    text_unit_degrees = (\n        node_to_text_ids.groupby([schemas.COMMUNITY_ID,\
      \ schemas.ID])\n        .agg({schemas.NODE_DEGREE: \"sum\"})\n        .reset_index()\n\
      \    )\n    result_df = text_unit_df.merge(text_unit_degrees, on=schemas.ID,\
      \ how=\"left\")\n    result_df[schemas.ALL_DETAILS] = result_df.apply(\n   \
      \     lambda x: {\n            schemas.SHORT_ID: x[schemas.SHORT_ID],\n    \
      \        schemas.TEXT: x[schemas.TEXT],\n            schemas.ENTITY_DEGREE:\
      \ x[schemas.NODE_DEGREE],\n        },\n        axis=1,\n    )\n    return result_df.loc[:,\
      \ [schemas.COMMUNITY_ID, schemas.ID, schemas.ALL_DETAILS]]"
    signature: "def prep_text_units(\n    text_unit_df: pd.DataFrame,\n    node_df:\
      \ pd.DataFrame,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: node_df.drop
      type: unresolved
    - target: node_df.explode(schemas.TEXT_UNIT_IDS).rename
      type: unresolved
    - target: node_df.explode
      type: unresolved
    - target: "node_to_text_ids.groupby([schemas.COMMUNITY_ID, schemas.ID])\n    \
        \    .agg({schemas.NODE_DEGREE: \"sum\"})\n        .reset_index"
      type: unresolved
    - target: "node_to_text_ids.groupby([schemas.COMMUNITY_ID, schemas.ID])\n    \
        \    .agg"
      type: unresolved
    - target: node_to_text_ids.groupby
      type: unresolved
    - target: text_unit_df.merge
      type: unresolved
    - target: result_df.apply
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py::prep_text_units
    called_by:
    - source: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
      type: internal
- file_name: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: get_context_string
    start_line: 16
    end_line: 55
    code: "def get_context_string(\n    text_units: list[dict],\n    sub_community_reports:\
      \ list[dict] | None = None,\n) -> str:\n    \"\"\"Concatenate structured data\
      \ into a context string.\"\"\"\n    contexts = []\n    if sub_community_reports:\n\
      \        sub_community_reports = [\n            report\n            for report\
      \ in sub_community_reports\n            if schemas.COMMUNITY_ID in report\n\
      \            and report[schemas.COMMUNITY_ID]\n            and str(report[schemas.COMMUNITY_ID]).strip()\
      \ != \"\"\n        ]\n        report_df = pd.DataFrame(sub_community_reports).drop_duplicates()\n\
      \        if not report_df.empty:\n            if report_df[schemas.COMMUNITY_ID].dtype\
      \ == float:\n                report_df[schemas.COMMUNITY_ID] = report_df[\n\
      \                    schemas.COMMUNITY_ID\n                ].astype(int)\n \
      \           report_string = (\n                f\"----REPORTS-----\\n{report_df.to_csv(index=False,\
      \ sep=',')}\"\n            )\n            contexts.append(report_string)\n\n\
      \    text_units = [\n        unit\n        for unit in text_units\n        if\
      \ \"id\" in unit and unit[\"id\"] and str(unit[\"id\"]).strip() != \"\"\n  \
      \  ]\n    text_units_df = pd.DataFrame(text_units).drop_duplicates()\n    if\
      \ not text_units_df.empty:\n        if text_units_df[\"id\"].dtype == float:\n\
      \            text_units_df[\"id\"] = text_units_df[\"id\"].astype(int)\n   \
      \     text_unit_string = (\n            f\"-----SOURCES-----\\n{text_units_df.to_csv(index=False,\
      \ sep=',')}\"\n        )\n        contexts.append(text_unit_string)\n\n    return\
      \ \"\\n\\n\".join(contexts)"
    signature: "def get_context_string(\n    text_units: list[dict],\n    sub_community_reports:\
      \ list[dict] | None = None,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: str(report[schemas.COMMUNITY_ID]).strip
      type: unresolved
    - target: str
      type: builtin
    - target: pandas::DataFrame(sub_community_reports).drop_duplicates
      type: external
    - target: pandas::DataFrame
      type: external
    - target: "report_df[\n                    schemas.COMMUNITY_ID\n            \
        \    ].astype"
      type: unresolved
    - target: report_df.to_csv
      type: unresolved
    - target: contexts.append
      type: unresolved
    - target: str(unit["id"]).strip
      type: unresolved
    - target: pandas::DataFrame(text_units).drop_duplicates
      type: external
    - target: text_units_df["id"].astype
      type: unresolved
    - target: text_units_df.to_csv
      type: unresolved
    - target: '"\n\n".join'
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::get_context_string
    called_by:
    - source: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
      type: internal
  - name: sort_context
    start_line: 58
    end_line: 85
    code: "def sort_context(\n    local_context: list[dict],\n    tokenizer: Tokenizer,\n\
      \    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n) -> str:\n    \"\"\"Sort local context (list of text\
      \ units) by total degree of associated nodes in descending order.\"\"\"\n  \
      \  sorted_text_units = sorted(\n        local_context, key=lambda x: x[schemas.ENTITY_DEGREE],\
      \ reverse=True\n    )\n\n    current_text_units = []\n    context_string = \"\
      \"\n    for record in sorted_text_units:\n        current_text_units.append(record)\n\
      \        if max_context_tokens:\n            new_context_string = get_context_string(\n\
      \                current_text_units, sub_community_reports\n            )\n\
      \            if tokenizer.num_tokens(new_context_string) > max_context_tokens:\n\
      \                break\n\n            context_string = new_context_string\n\n\
      \    if context_string == \"\":\n        return get_context_string(sorted_text_units,\
      \ sub_community_reports)\n\n    return context_string"
    signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer:\
      \ Tokenizer,\n    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: sorted
      type: builtin
    - target: current_text_units.append
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::get_context_string
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
    called_by:
    - source: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
      type: internal
- file_name: graphrag/index/operations/summarize_communities/typing.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing_extensions
    name: TypedDict
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  functions:
  - name: __repr__
    start_line: 61
    end_line: 63
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType.__repr__
    called_by: []
- file_name: graphrag/index/operations/summarize_communities/utils.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  functions:
  - name: get_levels
    start_line: 11
    end_line: 17
    code: "def get_levels(\n    df: pd.DataFrame, level_column: str = schemas.COMMUNITY_LEVEL\n\
      ) -> list[int]:\n    \"\"\"Get the levels of the communities.\"\"\"\n    levels\
      \ = df[level_column].dropna().unique()\n    levels = [int(lvl) for lvl in levels\
      \ if lvl != -1]\n    return sorted(levels, reverse=True)"
    signature: "def get_levels(\n    df: pd.DataFrame, level_column: str = schemas.COMMUNITY_LEVEL\n\
      ) -> list[int]"
    decorators: []
    raises: []
    calls:
    - target: df[level_column].dropna().unique
      type: unresolved
    - target: df[level_column].dropna
      type: unresolved
    - target: int
      type: builtin
    - target: sorted
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/summarize_communities/utils.py::get_levels
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
- file_name: graphrag/index/operations/summarize_descriptions/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  imports:
  - module: json
    name: null
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.index.summarize_descriptions
    name: SUMMARIZE_PROMPT
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 37
    end_line: 52
    code: "def __init__(\n        self,\n        model_invoker: ChatModel,\n     \
      \   max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
      \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    ):\n\
      \        \"\"\"Init method definition.\"\"\"\n        # TODO: streamline construction\n\
      \        self._model = model_invoker\n        self._tokenizer = get_tokenizer(model_invoker.config)\n\
      \        self._summarization_prompt = summarization_prompt or SUMMARIZE_PROMPT\n\
      \        self._on_error = on_error or (lambda _e, _s, _d: None)\n        self._max_summary_length\
      \ = max_summary_length\n        self._max_input_tokens = max_input_tokens"
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
      \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
    called_by: []
  - name: __call__
    start_line: 54
    end_line: 71
    code: "async def __call__(\n        self,\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n    ) -> SummarizationResult:\n        \"\
      \"\"Call method definition.\"\"\"\n        result = \"\"\n        if len(descriptions)\
      \ == 0:\n            result = \"\"\n        elif len(descriptions) == 1:\n \
      \           result = descriptions[0]\n        else:\n            result = await\
      \ self._summarize_descriptions(id, descriptions)\n\n        return SummarizationResult(\n\
      \            id=id,\n            description=result or \"\",\n        )"
    signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n    ) -> SummarizationResult"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::_summarize_descriptions
      type: internal
    - target: SummarizationResult
      type: unresolved
    visibility: protected
    node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__call__
    called_by: []
  - name: _summarize_descriptions
    start_line: 73
    end_line: 116
    code: "async def _summarize_descriptions(\n        self, id: str | tuple[str,\
      \ str], descriptions: list[str]\n    ) -> str:\n        \"\"\"Summarize descriptions\
      \ into a single description.\"\"\"\n        sorted_id = sorted(id) if isinstance(id,\
      \ list) else id\n\n        # Safety check, should always be a list\n       \
      \ if not isinstance(descriptions, list):\n            descriptions = [descriptions]\n\
      \n        # Sort description lists\n        if len(descriptions) > 1:\n    \
      \        descriptions = sorted(descriptions)\n\n        # Iterate over descriptions,\
      \ adding all until the max input tokens is reached\n        usable_tokens =\
      \ self._max_input_tokens - self._tokenizer.num_tokens(\n            self._summarization_prompt\n\
      \        )\n        descriptions_collected = []\n        result = \"\"\n\n \
      \       for i, description in enumerate(descriptions):\n            usable_tokens\
      \ -= self._tokenizer.num_tokens(description)\n            descriptions_collected.append(description)\n\
      \n            # If buffer is full, or all descriptions have been added, summarize\n\
      \            if (usable_tokens < 0 and len(descriptions_collected) > 1) or (\n\
      \                i == len(descriptions) - 1\n            ):\n              \
      \  # Calculate result (final or partial)\n                result = await self._summarize_descriptions_with_llm(\n\
      \                    sorted_id, descriptions_collected\n                )\n\n\
      \                # If we go for another loop, reset values to new\n        \
      \        if i != len(descriptions) - 1:\n                    descriptions_collected\
      \ = [result]\n                    usable_tokens = (\n                      \
      \  self._max_input_tokens\n                        - self._tokenizer.num_tokens(self._summarization_prompt)\n\
      \                        - self._tokenizer.num_tokens(result)\n            \
      \        )\n\n        return result"
    signature: "def _summarize_descriptions(\n        self, id: str | tuple[str, str],\
      \ descriptions: list[str]\n    ) -> str"
    decorators: []
    raises: []
    calls:
    - target: sorted
      type: builtin
    - target: isinstance
      type: builtin
    - target: len
      type: builtin
    - target: self._tokenizer.num_tokens
      type: instance
    - target: enumerate
      type: builtin
    - target: descriptions_collected.append
      type: unresolved
    - target: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::_summarize_descriptions_with_llm
      type: internal
    visibility: protected
    node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions
    called_by: []
  - name: _summarize_descriptions_with_llm
    start_line: 118
    end_line: 133
    code: "async def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
      \ str] | list[str], descriptions: list[str]\n    ):\n        \"\"\"Summarize\
      \ descriptions using the LLM.\"\"\"\n        response = await self._model.achat(\n\
      \            self._summarization_prompt.format(**{\n                ENTITY_NAME_KEY:\
      \ json.dumps(id, ensure_ascii=False),\n                DESCRIPTION_LIST_KEY:\
      \ json.dumps(\n                    sorted(descriptions), ensure_ascii=False\n\
      \                ),\n                MAX_LENGTH_KEY: self._max_summary_length,\n\
      \            }),\n            name=\"summarize\",\n        )\n        # Calculate\
      \ result\n        return str(response.output.content)"
    signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
      \ str] | list[str], descriptions: list[str]\n    )"
    decorators: []
    raises: []
    calls:
    - target: self._model.achat
      type: instance
    - target: self._summarization_prompt.format
      type: instance
    - target: json::dumps
      type: stdlib
    - target: sorted
      type: builtin
    - target: str
      type: builtin
    visibility: protected
    node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions_with_llm
    called_by: []
- file_name: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.description_summary_extractor
    name: SummarizeExtractor
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.typing
    name: StrategyConfig
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.typing
    name: SummarizedDescriptionResult
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  functions:
  - name: run_graph_intelligence
    start_line: 23
    end_line: 38
    code: "async def run_graph_intelligence(\n    id: str | tuple[str, str],\n   \
      \ descriptions: list[str],\n    cache: PipelineCache,\n    args: StrategyConfig,\n\
      ) -> SummarizedDescriptionResult:\n    \"\"\"Run the graph intelligence entity\
      \ extraction strategy.\"\"\"\n    llm_config = LanguageModelConfig(**args[\"\
      llm\"])\n    llm = ModelManager().get_or_create_chat_model(\n        name=\"\
      summarize_descriptions\",\n        model_type=llm_config.type,\n        config=llm_config,\n\
      \        cache=cache,\n    )\n\n    return await run_summarize_descriptions(llm,\
      \ id, descriptions, args)"
    signature: "def run_graph_intelligence(\n    id: str | tuple[str, str],\n    descriptions:\
      \ list[str],\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> SummarizedDescriptionResult"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_summarize_descriptions
      type: internal
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_graph_intelligence
    called_by: []
  - name: run_summarize_descriptions
    start_line: 41
    end_line: 65
    code: "async def run_summarize_descriptions(\n    model: ChatModel,\n    id: str\
      \ | tuple[str, str],\n    descriptions: list[str],\n    args: StrategyConfig,\n\
      ) -> SummarizedDescriptionResult:\n    \"\"\"Run the entity extraction chain.\"\
      \"\"\n    # Extraction Arguments\n    summarize_prompt = args.get(\"summarize_prompt\"\
      , None)\n    max_input_tokens = args[\"max_input_tokens\"]\n    max_summary_length\
      \ = args[\"max_summary_length\"]\n    extractor = SummarizeExtractor(\n    \
      \    model_invoker=model,\n        summarization_prompt=summarize_prompt,\n\
      \        on_error=lambda e, stack, details: logger.error(\n            \"Entity\
      \ Extraction Error\",\n            exc_info=e,\n            extra={\"stack\"\
      : stack, \"details\": details},\n        ),\n        max_summary_length=max_summary_length,\n\
      \        max_input_tokens=max_input_tokens,\n    )\n\n    result = await extractor(id=id,\
      \ descriptions=descriptions)\n    return SummarizedDescriptionResult(id=result.id,\
      \ description=result.description)"
    signature: "def run_summarize_descriptions(\n    model: ChatModel,\n    id: str\
      \ | tuple[str, str],\n    descriptions: list[str],\n    args: StrategyConfig,\n\
      ) -> SummarizedDescriptionResult"
    decorators: []
    raises: []
    calls:
    - target: args.get
      type: unresolved
    - target: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor
      type: internal
    - target: logger.error
      type: unresolved
    - target: extractor
      type: unresolved
    - target: graphrag/index/operations/summarize_descriptions/typing.py::SummarizedDescriptionResult
      type: internal
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_summarize_descriptions
    called_by:
    - source: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_graph_intelligence
      type: internal
- file_name: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.typing
    name: SummarizationStrategy
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.typing
    name: SummarizeStrategyType
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.graph_intelligence_strategy
    name: run_graph_intelligence
    alias: null
  functions:
  - name: summarize_descriptions
    start_line: 23
    end_line: 108
    code: "async def summarize_descriptions(\n    entities_df: pd.DataFrame,\n   \
      \ relationships_df: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n    cache:\
      \ PipelineCache,\n    strategy: dict[str, Any] | None = None,\n    num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Summarize entity\
      \ and relationship descriptions from an entity graph, using a language model.\"\
      \"\"\n    logger.debug(\"summarize_descriptions strategy=%s\", strategy)\n \
      \   strategy = strategy or {}\n    strategy_exec = load_strategy(\n        strategy.get(\"\
      type\", SummarizeStrategyType.graph_intelligence)\n    )\n    strategy_config\
      \ = {**strategy}\n\n    async def get_summarized(\n        nodes: pd.DataFrame,\
      \ edges: pd.DataFrame, semaphore: asyncio.Semaphore\n    ):\n        ticker_length\
      \ = len(nodes) + len(edges)\n\n        ticker = progress_ticker(\n         \
      \   callbacks.progress,\n            ticker_length,\n            description=\"\
      Summarize entity/relationship description progress: \",\n        )\n\n     \
      \   node_futures = [\n            do_summarize_descriptions(\n             \
      \   str(row.title),  # type: ignore\n                sorted(set(row.description)),\
      \  # type: ignore\n                ticker,\n                semaphore,\n   \
      \         )\n            for row in nodes.itertuples(index=False)\n        ]\n\
      \n        node_results = await asyncio.gather(*node_futures)\n\n        node_descriptions\
      \ = [\n            {\n                \"title\": result.id,\n              \
      \  \"description\": result.description,\n            }\n            for result\
      \ in node_results\n        ]\n\n        edge_futures = [\n            do_summarize_descriptions(\n\
      \                (str(row.source), str(row.target)),  # type: ignore\n     \
      \           sorted(set(row.description)),  # type: ignore\n                ticker,\n\
      \                semaphore,\n            )\n            for row in edges.itertuples(index=False)\n\
      \        ]\n\n        edge_results = await asyncio.gather(*edge_futures)\n\n\
      \        edge_descriptions = [\n            {\n                \"source\": result.id[0],\n\
      \                \"target\": result.id[1],\n                \"description\"\
      : result.description,\n            }\n            for result in edge_results\n\
      \        ]\n\n        entity_descriptions = pd.DataFrame(node_descriptions)\n\
      \        relationship_descriptions = pd.DataFrame(edge_descriptions)\n     \
      \   return entity_descriptions, relationship_descriptions\n\n    async def do_summarize_descriptions(\n\
      \        id: str | tuple[str, str],\n        descriptions: list[str],\n    \
      \    ticker: ProgressTicker,\n        semaphore: asyncio.Semaphore,\n    ):\n\
      \        async with semaphore:\n            results = await strategy_exec(id,\
      \ descriptions, cache, strategy_config)\n            ticker(1)\n        return\
      \ results\n\n    semaphore = asyncio.Semaphore(num_threads)\n\n    return await\
      \ get_summarized(entities_df, relationships_df, semaphore)"
    signature: "def summarize_descriptions(\n    entities_df: pd.DataFrame,\n    relationships_df:\
      \ pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    strategy: dict[str, Any] | None = None,\n    num_threads: int = 4,\n) ->\
      \ tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: logger.debug
      type: unresolved
    - target: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::load_strategy
      type: internal
    - target: strategy.get
      type: unresolved
    - target: asyncio::Semaphore
      type: stdlib
    - target: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
      type: internal
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
    called_by:
    - source: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
      type: internal
  - name: get_summarized
    start_line: 39
    end_line: 93
    code: "async def get_summarized(\n        nodes: pd.DataFrame, edges: pd.DataFrame,\
      \ semaphore: asyncio.Semaphore\n    ):\n        ticker_length = len(nodes) +\
      \ len(edges)\n\n        ticker = progress_ticker(\n            callbacks.progress,\n\
      \            ticker_length,\n            description=\"Summarize entity/relationship\
      \ description progress: \",\n        )\n\n        node_futures = [\n       \
      \     do_summarize_descriptions(\n                str(row.title),  # type: ignore\n\
      \                sorted(set(row.description)),  # type: ignore\n           \
      \     ticker,\n                semaphore,\n            )\n            for row\
      \ in nodes.itertuples(index=False)\n        ]\n\n        node_results = await\
      \ asyncio.gather(*node_futures)\n\n        node_descriptions = [\n         \
      \   {\n                \"title\": result.id,\n                \"description\"\
      : result.description,\n            }\n            for result in node_results\n\
      \        ]\n\n        edge_futures = [\n            do_summarize_descriptions(\n\
      \                (str(row.source), str(row.target)),  # type: ignore\n     \
      \           sorted(set(row.description)),  # type: ignore\n                ticker,\n\
      \                semaphore,\n            )\n            for row in edges.itertuples(index=False)\n\
      \        ]\n\n        edge_results = await asyncio.gather(*edge_futures)\n\n\
      \        edge_descriptions = [\n            {\n                \"source\": result.id[0],\n\
      \                \"target\": result.id[1],\n                \"description\"\
      : result.description,\n            }\n            for result in edge_results\n\
      \        ]\n\n        entity_descriptions = pd.DataFrame(node_descriptions)\n\
      \        relationship_descriptions = pd.DataFrame(edge_descriptions)\n     \
      \   return entity_descriptions, relationship_descriptions"
    signature: "def get_summarized(\n        nodes: pd.DataFrame, edges: pd.DataFrame,\
      \ semaphore: asyncio.Semaphore\n    )"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::do_summarize_descriptions
      type: internal
    - target: str
      type: builtin
    - target: sorted
      type: builtin
    - target: set
      type: builtin
    - target: nodes.itertuples
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    - target: edges.itertuples
      type: unresolved
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
    called_by:
    - source: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
      type: internal
  - name: do_summarize_descriptions
    start_line: 95
    end_line: 104
    code: "async def do_summarize_descriptions(\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n        ticker: ProgressTicker,\n       \
      \ semaphore: asyncio.Semaphore,\n    ):\n        async with semaphore:\n   \
      \         results = await strategy_exec(id, descriptions, cache, strategy_config)\n\
      \            ticker(1)\n        return results"
    signature: "def do_summarize_descriptions(\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n        ticker: ProgressTicker,\n       \
      \ semaphore: asyncio.Semaphore,\n    )"
    decorators: []
    raises: []
    calls:
    - target: strategy_exec
      type: unresolved
    - target: ticker
      type: unresolved
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::do_summarize_descriptions
    called_by:
    - source: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
      type: internal
  - name: load_strategy
    start_line: 111
    end_line: 122
    code: "def load_strategy(strategy_type: SummarizeStrategyType) -> SummarizationStrategy:\n\
      \    \"\"\"Load strategy method definition.\"\"\"\n    match strategy_type:\n\
      \        case SummarizeStrategyType.graph_intelligence:\n            from graphrag.index.operations.summarize_descriptions.graph_intelligence_strategy\
      \ import (\n                run_graph_intelligence,\n            )\n\n     \
      \       return run_graph_intelligence\n        case _:\n            msg = f\"\
      Unknown strategy: {strategy_type}\"\n            raise ValueError(msg)"
    signature: 'def load_strategy(strategy_type: SummarizeStrategyType) -> SummarizationStrategy'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::load_strategy
    called_by:
    - source: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
      type: internal
- file_name: graphrag/index/operations/summarize_descriptions/typing.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: NamedTuple
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  functions:
  - name: __repr__
    start_line: 46
    end_line: 48
    code: "def __repr__(self):\n        \"\"\"Get a string representation.\"\"\"\n\
      \        return f'\"{self.value}\"'"
    signature: def __repr__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType.__repr__
    called_by: []
- file_name: graphrag/index/run/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/run/run_pipeline.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: AsyncIterable
    alias: null
  - module: dataclasses
    name: asdict
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: create_run_context
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.pipeline
    name: Pipeline
    alias: null
  - module: graphrag.index.typing.pipeline_run_result
    name: PipelineRunResult
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.api
    name: create_cache_from_config
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_pipeline
    start_line: 29
    end_line: 101
    code: "async def run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    callbacks: WorkflowCallbacks,\n    is_update_run: bool = False,\n    additional_context:\
      \ dict[str, Any] | None = None,\n    input_documents: pd.DataFrame | None =\
      \ None,\n) -> AsyncIterable[PipelineRunResult]:\n    \"\"\"Run all workflows\
      \ using a simplified pipeline.\"\"\"\n    root_dir = config.root_dir\n\n   \
      \ input_storage = create_storage_from_config(config.input.storage)\n    output_storage\
      \ = create_storage_from_config(config.output)\n    cache = create_cache_from_config(config.cache,\
      \ root_dir)\n\n    # load existing state in case any workflows are stateful\n\
      \    state_json = await output_storage.get(\"context.json\")\n    state = json.loads(state_json)\
      \ if state_json else {}\n\n    if additional_context:\n        state.setdefault(\"\
      additional_context\", {}).update(additional_context)\n\n    if is_update_run:\n\
      \        logger.info(\"Running incremental indexing.\")\n\n        update_storage\
      \ = create_storage_from_config(config.update_index_output)\n        # we use\
      \ this to store the new subset index, and will merge its content with the previous\
      \ index\n        update_timestamp = time.strftime(\"%Y%m%d-%H%M%S\")\n     \
      \   timestamped_storage = update_storage.child(update_timestamp)\n        delta_storage\
      \ = timestamped_storage.child(\"delta\")\n        # copy the previous output\
      \ to a backup folder, so we can replace it with the update\n        # we'll\
      \ read from this later when we merge the old and new indexes\n        previous_storage\
      \ = timestamped_storage.child(\"previous\")\n        await _copy_previous_output(output_storage,\
      \ previous_storage)\n\n        state[\"update_timestamp\"] = update_timestamp\n\
      \n        # if the user passes in a df directly, write directly to storage so\
      \ we can skip finding/parsing later\n        if input_documents is not None:\n\
      \            await write_table_to_storage(input_documents, \"documents\", delta_storage)\n\
      \            pipeline.remove(\"load_update_documents\")\n\n        context =\
      \ create_run_context(\n            input_storage=input_storage,\n          \
      \  output_storage=delta_storage,\n            previous_storage=previous_storage,\n\
      \            cache=cache,\n            callbacks=callbacks,\n            state=state,\n\
      \        )\n\n    else:\n        logger.info(\"Running standard indexing.\"\
      )\n\n        # if the user passes in a df directly, write directly to storage\
      \ so we can skip finding/parsing later\n        if input_documents is not None:\n\
      \            await write_table_to_storage(input_documents, \"documents\", output_storage)\n\
      \            pipeline.remove(\"load_input_documents\")\n\n        context =\
      \ create_run_context(\n            input_storage=input_storage,\n          \
      \  output_storage=output_storage,\n            cache=cache,\n            callbacks=callbacks,\n\
      \            state=state,\n        )\n\n    async for table in _run_pipeline(\n\
      \        pipeline=pipeline,\n        config=config,\n        context=context,\n\
      \    ):\n        yield table"
    signature: "def run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    callbacks: WorkflowCallbacks,\n    is_update_run: bool = False,\n    additional_context:\
      \ dict[str, Any] | None = None,\n    input_documents: pd.DataFrame | None =\
      \ None,\n) -> AsyncIterable[PipelineRunResult]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/utils/api.py::create_cache_from_config
      type: internal
    - target: output_storage.get
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: state.setdefault("additional_context", {}).update
      type: unresolved
    - target: state.setdefault
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: time::strftime
      type: stdlib
    - target: update_storage.child
      type: unresolved
    - target: timestamped_storage.child
      type: unresolved
    - target: graphrag/index/run/run_pipeline.py::_copy_previous_output
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: pipeline.remove
      type: unresolved
    - target: graphrag/index/run/utils.py::create_run_context
      type: internal
    - target: graphrag/index/run/run_pipeline.py::_run_pipeline
      type: internal
    visibility: public
    node_id: graphrag/index/run/run_pipeline.py::run_pipeline
    called_by:
    - source: graphrag/api/index.py::build_index
      type: internal
  - name: _run_pipeline
    start_line: 104
    end_line: 139
    code: "async def _run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> AsyncIterable[PipelineRunResult]:\n\
      \    start_time = time.time()\n\n    last_workflow = \"<startup>\"\n\n    try:\n\
      \        await _dump_json(context)\n\n        logger.info(\"Executing pipeline...\"\
      )\n        for name, workflow_function in pipeline.run():\n            last_workflow\
      \ = name\n            context.callbacks.workflow_start(name, None)\n       \
      \     work_time = time.time()\n            result = await workflow_function(config,\
      \ context)\n            context.callbacks.workflow_end(name, result)\n     \
      \       yield PipelineRunResult(\n                workflow=name, result=result.result,\
      \ state=context.state, errors=None\n            )\n            context.stats.workflows[name]\
      \ = {\"overall\": time.time() - work_time}\n            if result.stop:\n  \
      \              logger.info(\"Halting pipeline at workflow request\")\n     \
      \           break\n\n        context.stats.total_runtime = time.time() - start_time\n\
      \        logger.info(\"Indexing pipeline complete.\")\n        await _dump_json(context)\n\
      \n    except Exception as e:\n        logger.exception(\"error running workflow\
      \ %s\", last_workflow)\n        yield PipelineRunResult(\n            workflow=last_workflow,\
      \ result=None, state=context.state, errors=[e]\n        )"
    signature: "def _run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> AsyncIterable[PipelineRunResult]"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: graphrag/index/run/run_pipeline.py::_dump_json
      type: internal
    - target: logger.info
      type: unresolved
    - target: pipeline.run
      type: unresolved
    - target: context.callbacks.workflow_start
      type: unresolved
    - target: workflow_function
      type: unresolved
    - target: context.callbacks.workflow_end
      type: unresolved
    - target: graphrag/index/typing/pipeline_run_result.py::PipelineRunResult
      type: internal
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/index/run/run_pipeline.py::_run_pipeline
    called_by:
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
  - name: _dump_json
    start_line: 142
    end_line: 157
    code: "async def _dump_json(context: PipelineRunContext) -> None:\n    \"\"\"\
      Dump the stats and context state to the storage.\"\"\"\n    await context.output_storage.set(\n\
      \        \"stats.json\", json.dumps(asdict(context.stats), indent=4, ensure_ascii=False)\n\
      \    )\n    # Dump context state, excluding additional_context\n    temp_context\
      \ = context.state.pop(\n        \"additional_context\", None\n    )  # Remove\
      \ reference only, as object size is uncertain\n    try:\n        state_blob\
      \ = json.dumps(context.state, indent=4, ensure_ascii=False)\n    finally:\n\
      \        if temp_context:\n            context.state[\"additional_context\"\
      ] = temp_context\n\n    await context.output_storage.set(\"context.json\", state_blob)"
    signature: 'def _dump_json(context: PipelineRunContext) -> None'
    decorators: []
    raises: []
    calls:
    - target: context.output_storage.set
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: dataclasses::asdict
      type: stdlib
    - target: context.state.pop
      type: unresolved
    visibility: protected
    node_id: graphrag/index/run/run_pipeline.py::_dump_json
    called_by:
    - source: graphrag/index/run/run_pipeline.py::_run_pipeline
      type: internal
  - name: _copy_previous_output
    start_line: 160
    end_line: 167
    code: "async def _copy_previous_output(\n    storage: PipelineStorage,\n    copy_storage:\
      \ PipelineStorage,\n):\n    for file in storage.find(re.compile(r\"\\.parquet$\"\
      )):\n        base_name = file[0].replace(\".parquet\", \"\")\n        table\
      \ = await load_table_from_storage(base_name, storage)\n        await write_table_to_storage(table,\
      \ base_name, copy_storage)"
    signature: "def _copy_previous_output(\n    storage: PipelineStorage,\n    copy_storage:\
      \ PipelineStorage,\n)"
    decorators: []
    raises: []
    calls:
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: file[0].replace
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/run/run_pipeline.py::_copy_previous_output
    called_by:
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
- file_name: graphrag/index/run/utils.py
  imports:
  - module: graphrag.cache.memory_pipeline_cache
    name: InMemoryCache
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.callbacks.workflow_callbacks_manager
    name: WorkflowCallbacksManager
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.state
    name: PipelineState
    alias: null
  - module: graphrag.index.typing.stats
    name: PipelineRunStats
    alias: null
  - module: graphrag.storage.memory_pipeline_storage
    name: MemoryPipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  functions:
  - name: create_run_context
    start_line: 20
    end_line: 38
    code: "def create_run_context(\n    input_storage: PipelineStorage | None = None,\n\
      \    output_storage: PipelineStorage | None = None,\n    previous_storage: PipelineStorage\
      \ | None = None,\n    cache: PipelineCache | None = None,\n    callbacks: WorkflowCallbacks\
      \ | None = None,\n    stats: PipelineRunStats | None = None,\n    state: PipelineState\
      \ | None = None,\n) -> PipelineRunContext:\n    \"\"\"Create the run context\
      \ for the pipeline.\"\"\"\n    return PipelineRunContext(\n        input_storage=input_storage\
      \ or MemoryPipelineStorage(),\n        output_storage=output_storage or MemoryPipelineStorage(),\n\
      \        previous_storage=previous_storage or MemoryPipelineStorage(),\n   \
      \     cache=cache or InMemoryCache(),\n        callbacks=callbacks or NoopWorkflowCallbacks(),\n\
      \        stats=stats or PipelineRunStats(),\n        state=state or {},\n  \
      \  )"
    signature: "def create_run_context(\n    input_storage: PipelineStorage | None\
      \ = None,\n    output_storage: PipelineStorage | None = None,\n    previous_storage:\
      \ PipelineStorage | None = None,\n    cache: PipelineCache | None = None,\n\
      \    callbacks: WorkflowCallbacks | None = None,\n    stats: PipelineRunStats\
      \ | None = None,\n    state: PipelineState | None = None,\n) -> PipelineRunContext"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/typing/context.py::PipelineRunContext
      type: internal
    - target: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage
      type: internal
    - target: graphrag.cache.memory_pipeline_cache::InMemoryCache
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: graphrag/index/typing/stats.py::PipelineRunStats
      type: internal
    visibility: public
    node_id: graphrag/index/run/utils.py::create_run_context
    called_by:
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
    - source: tests/verbs/test_pipeline_state.py::test_pipeline_state
      type: internal
    - source: tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
      type: internal
    - source: tests/verbs/util.py::create_test_context
      type: internal
  - name: create_callback_chain
    start_line: 41
    end_line: 48
    code: "def create_callback_chain(\n    callbacks: list[WorkflowCallbacks] | None,\n\
      ) -> WorkflowCallbacks:\n    \"\"\"Create a callback manager that encompasses\
      \ multiple callbacks.\"\"\"\n    manager = WorkflowCallbacksManager()\n    for\
      \ callback in callbacks or []:\n        manager.register(callback)\n    return\
      \ manager"
    signature: "def create_callback_chain(\n    callbacks: list[WorkflowCallbacks]\
      \ | None,\n) -> WorkflowCallbacks"
    decorators: []
    raises: []
    calls:
    - target: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager
      type: internal
    - target: manager.register
      type: unresolved
    visibility: public
    node_id: graphrag/index/run/utils.py::create_callback_chain
    called_by:
    - source: graphrag/api/index.py::build_index
      type: internal
  - name: get_update_storages
    start_line: 51
    end_line: 61
    code: "def get_update_storages(\n    config: GraphRagConfig, timestamp: str\n\
      ) -> tuple[PipelineStorage, PipelineStorage, PipelineStorage]:\n    \"\"\"Get\
      \ storage objects for the update index run.\"\"\"\n    output_storage = create_storage_from_config(config.output)\n\
      \    update_storage = create_storage_from_config(config.update_index_output)\n\
      \    timestamped_storage = update_storage.child(timestamp)\n    delta_storage\
      \ = timestamped_storage.child(\"delta\")\n    previous_storage = timestamped_storage.child(\"\
      previous\")\n\n    return output_storage, previous_storage, delta_storage"
    signature: "def get_update_storages(\n    config: GraphRagConfig, timestamp: str\n\
      ) -> tuple[PipelineStorage, PipelineStorage, PipelineStorage]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: update_storage.child
      type: unresolved
    - target: timestamped_storage.child
      type: unresolved
    visibility: public
    node_id: graphrag/index/run/utils.py::get_update_storages
    called_by:
    - source: graphrag/index/workflows/update_communities.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_community_reports.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_covariates.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_entities_relationships.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_final_documents.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_text_units.py::run_workflow
      type: internal
- file_name: graphrag/index/text_splitting/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/text_splitting/check_token_limit.py
  imports:
  - module: graphrag.index.text_splitting.text_splitting
    name: TokenTextSplitter
    alias: null
  functions:
  - name: check_token_limit
    start_line: 9
    end_line: 15
    code: "def check_token_limit(text, max_token):\n    \"\"\"Check token limit.\"\
      \"\"\n    text_splitter = TokenTextSplitter(chunk_size=max_token, chunk_overlap=0)\n\
      \    docs = text_splitter.split_text(text)\n    if len(docs) > 1:\n        return\
      \ 0\n    return 1"
    signature: def check_token_limit(text, max_token)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: text_splitter.split_text
      type: unresolved
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/index/text_splitting/check_token_limit.py::check_token_limit
    called_by: []
- file_name: graphrag/index/text_splitting/text_splitting.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.index.operations.chunk_text.typing
    name: TextChunk
    alias: null
  - module: graphrag.logger.progress
    name: ProgressTicker
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 51
    end_line: 68
    code: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
      \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
      \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n      \
      \  length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
      \        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n\
      \    ):\n        \"\"\"Init method definition.\"\"\"\n        self._chunk_size\
      \ = chunk_size\n        self._chunk_overlap = chunk_overlap\n        self._length_function\
      \ = length_function\n        self._keep_separator = keep_separator\n       \
      \ self._add_start_index = add_start_index\n        self._strip_whitespace =\
      \ strip_whitespace"
    signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
      \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
      \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n      \
      \  length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
      \        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n\
      \    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.__init__
    called_by: []
  - name: split_text
    start_line: 71
    end_line: 72
    code: "def split_text(self, text: str | list[str]) -> Iterable[str]:\n       \
      \ \"\"\"Split text method definition.\"\"\""
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.split_text
    called_by: []
  - name: split_text
    start_line: 78
    end_line: 80
    code: "def split_text(self, text: str | list[str]) -> Iterable[str]:\n       \
      \ \"\"\"Split text method definition.\"\"\"\n        return [text] if isinstance(text,\
      \ str) else text"
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter.split_text
    called_by: []
  - name: __init__
    start_line: 86
    end_line: 93
    code: "def __init__(\n        self,\n        tokenizer: Tokenizer | None = None,\n\
      \        **kwargs: Any,\n    ):\n        \"\"\"Init method definition.\"\"\"\
      \n        super().__init__(**kwargs)\n        self._tokenizer = tokenizer or\
      \ get_tokenizer()"
    signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None\
      \ = None,\n        **kwargs: Any,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
    called_by: []
  - name: num_tokens
    start_line: 95
    end_line: 97
    code: "def num_tokens(self, text: str) -> int:\n        \"\"\"Return the number\
      \ of tokens in a string.\"\"\"\n        return self._tokenizer.num_tokens(text)"
    signature: 'def num_tokens(self, text: str) -> int'
    decorators: []
    raises: []
    calls:
    - target: self._tokenizer.num_tokens
      type: instance
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.num_tokens
    called_by: []
  - name: split_text
    start_line: 99
    end_line: 116
    code: "def split_text(self, text: str | list[str]) -> list[str]:\n        \"\"\
      \"Split text method.\"\"\"\n        if isinstance(text, list):\n           \
      \ text = \" \".join(text)\n        elif cast(\"bool\", pd.isna(text)) or text\
      \ == \"\":\n            return []\n        if not isinstance(text, str):\n \
      \           msg = f\"Attempting to split a non-string value, actual is {type(text)}\"\
      \n            raise TypeError(msg)\n\n        token_chunker_options = TokenChunkerOptions(\n\
      \            chunk_overlap=self._chunk_overlap,\n            tokens_per_chunk=self._chunk_size,\n\
      \            decode=self._tokenizer.decode,\n            encode=self._tokenizer.encode,\n\
      \        )\n\n        return split_single_text_on_tokens(text=text, tokenizer=token_chunker_options)"
    signature: 'def split_text(self, text: str | list[str]) -> list[str]'
    decorators: []
    raises:
    - TypeError
    calls:
    - target: isinstance
      type: builtin
    - target: '" ".join'
      type: unresolved
    - target: typing::cast
      type: stdlib
    - target: pandas::isna
      type: external
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    - target: TokenChunkerOptions
      type: unresolved
    - target: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
      type: internal
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
    called_by: []
  - name: split_single_text_on_tokens
    start_line: 119
    end_line: 137
    code: "def split_single_text_on_tokens(text: str, tokenizer: TokenChunkerOptions)\
      \ -> list[str]:\n    \"\"\"Split a single text and return chunks using the tokenizer.\"\
      \"\"\n    result = []\n    input_ids = tokenizer.encode(text)\n\n    start_idx\
      \ = 0\n    cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n\
      \    chunk_ids = input_ids[start_idx:cur_idx]\n\n    while start_idx < len(input_ids):\n\
      \        chunk_text = tokenizer.decode(list(chunk_ids))\n        result.append(chunk_text)\
      \  # Append chunked text as string\n        if cur_idx == len(input_ids):\n\
      \            break\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n\
      \        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n\
      \        chunk_ids = input_ids[start_idx:cur_idx]\n\n    return result"
    signature: 'def split_single_text_on_tokens(text: str, tokenizer: TokenChunkerOptions)
      -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: tokenizer.encode
      type: unresolved
    - target: min
      type: builtin
    - target: len
      type: builtin
    - target: tokenizer.decode
      type: unresolved
    - target: list
      type: builtin
    - target: result.append
      type: unresolved
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
    called_by:
    - source: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
      type: internal
    - source: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens
      type: internal
    - source: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
      type: internal
  - name: split_multiple_texts_on_tokens
    start_line: 142
    end_line: 173
    code: "def split_multiple_texts_on_tokens(\n    texts: list[str], tokenizer: TokenChunkerOptions,\
      \ tick: ProgressTicker\n) -> list[TextChunk]:\n    \"\"\"Split multiple texts\
      \ and return chunks with metadata using the tokenizer.\"\"\"\n    result = []\n\
      \    mapped_ids = []\n\n    for source_doc_idx, text in enumerate(texts):\n\
      \        encoded = tokenizer.encode(text)\n        if tick:\n            tick(1)\
      \  # Track progress if tick callback is provided\n        mapped_ids.append((source_doc_idx,\
      \ encoded))\n\n    input_ids = [\n        (source_doc_idx, id) for source_doc_idx,\
      \ ids in mapped_ids for id in ids\n    ]\n\n    start_idx = 0\n    cur_idx =\
      \ min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n    chunk_ids\
      \ = input_ids[start_idx:cur_idx]\n\n    while start_idx < len(input_ids):\n\
      \        chunk_text = tokenizer.decode([id for _, id in chunk_ids])\n      \
      \  doc_indices = list({doc_idx for doc_idx, _ in chunk_ids})\n        result.append(TextChunk(chunk_text,\
      \ doc_indices, len(chunk_ids)))\n        if cur_idx == len(input_ids):\n   \
      \         break\n        start_idx += tokenizer.tokens_per_chunk - tokenizer.chunk_overlap\n\
      \        cur_idx = min(start_idx + tokenizer.tokens_per_chunk, len(input_ids))\n\
      \        chunk_ids = input_ids[start_idx:cur_idx]\n\n    return result"
    signature: "def split_multiple_texts_on_tokens(\n    texts: list[str], tokenizer:\
      \ TokenChunkerOptions, tick: ProgressTicker\n) -> list[TextChunk]"
    decorators: []
    raises: []
    calls:
    - target: enumerate
      type: builtin
    - target: tokenizer.encode
      type: unresolved
    - target: tick
      type: unresolved
    - target: mapped_ids.append
      type: unresolved
    - target: min
      type: builtin
    - target: len
      type: builtin
    - target: tokenizer.decode
      type: unresolved
    - target: list
      type: builtin
    - target: result.append
      type: unresolved
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    visibility: public
    node_id: graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
    called_by:
    - source: graphrag/index/operations/chunk_text/strategies.py::run_tokens
      type: internal
    - source: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_multiple_texts_on_tokens
      type: internal
- file_name: graphrag/index/typing/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/typing/context.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.index.typing.state
    name: PipelineState
    alias: null
  - module: graphrag.index.typing.stats
    name: PipelineRunStats
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions: []
- file_name: graphrag/index/typing/error_handler.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  functions: []
- file_name: graphrag/index/typing/pipeline.py
  imports:
  - module: collections.abc
    name: Generator
    alias: null
  - module: graphrag.index.typing.workflow
    name: Workflow
    alias: null
  functions:
  - name: __init__
    start_line: 14
    end_line: 15
    code: "def __init__(self, workflows: list[Workflow]):\n        self.workflows\
      \ = workflows"
    signature: 'def __init__(self, workflows: list[Workflow])'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/typing/pipeline.py::Pipeline.__init__
    called_by: []
  - name: run
    start_line: 17
    end_line: 19
    code: "def run(self) -> Generator[Workflow]:\n        \"\"\"Return a Generator\
      \ over the pipeline workflows.\"\"\"\n        yield from self.workflows"
    signature: def run(self) -> Generator[Workflow]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/typing/pipeline.py::Pipeline.run
    called_by: []
  - name: names
    start_line: 21
    end_line: 23
    code: "def names(self) -> list[str]:\n        \"\"\"Return the names of the workflows\
      \ in the pipeline.\"\"\"\n        return [name for name, _ in self.workflows]"
    signature: def names(self) -> list[str]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/typing/pipeline.py::Pipeline.names
    called_by: []
  - name: remove
    start_line: 25
    end_line: 27
    code: "def remove(self, name: str) -> None:\n        \"\"\"Remove a workflow from\
      \ the pipeline by name.\"\"\"\n        self.workflows = [w for w in self.workflows\
      \ if w[0] != name]"
    signature: 'def remove(self, name: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/typing/pipeline.py::Pipeline.remove
    called_by: []
- file_name: graphrag/index/typing/pipeline_run_result.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.index.typing.state
    name: PipelineState
    alias: null
  functions: []
- file_name: graphrag/index/typing/state.py
  imports:
  - module: typing
    name: Any
    alias: null
  functions: []
- file_name: graphrag/index/typing/stats.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: dataclasses
    name: field
    alias: null
  functions: []
- file_name: graphrag/index/typing/workflow.py
  imports:
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  functions: []
- file_name: graphrag/index/update/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/update/communities.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: COMMUNITIES_FINAL_COLUMNS
    alias: null
  - module: graphrag.data_model.schemas
    name: COMMUNITY_REPORTS_FINAL_COLUMNS
    alias: null
  functions:
  - name: _update_and_merge_communities
    start_line: 14
    end_line: 86
    code: "def _update_and_merge_communities(\n    old_communities: pd.DataFrame,\n\
      \    delta_communities: pd.DataFrame,\n) -> tuple[pd.DataFrame, dict]:\n   \
      \ \"\"\"Update and merge communities.\n\n    Parameters\n    ----------\n  \
      \  old_communities : pd.DataFrame\n        The old communities.\n    delta_communities\
      \ : pd.DataFrame\n        The delta communities.\n    community_id_mapping :\
      \ dict\n        The community id mapping.\n\n    Returns\n    -------\n    pd.DataFrame\n\
      \        The updated communities.\n    \"\"\"\n    # Check if size and period\
      \ columns exist in the old_communities. If not, add them\n    if \"size\" not\
      \ in old_communities.columns:\n        old_communities[\"size\"] = None\n  \
      \  if \"period\" not in old_communities.columns:\n        old_communities[\"\
      period\"] = None\n\n    # Same for delta_communities\n    if \"size\" not in\
      \ delta_communities.columns:\n        delta_communities[\"size\"] = None\n \
      \   if \"period\" not in delta_communities.columns:\n        delta_communities[\"\
      period\"] = None\n\n    # Increment all community ids by the max of the old\
      \ communities\n    old_max_community_id = old_communities[\"community\"].fillna(0).astype(int).max()\n\
      \    # Increment only the non-NaN values in delta_communities[\"community\"\
      ]\n    community_id_mapping = {\n        v: v + old_max_community_id + 1\n \
      \       for k, v in delta_communities[\"community\"].dropna().astype(int).items()\n\
      \    }\n    community_id_mapping.update({-1: -1})\n\n    # Look for community\
      \ ids in community and replace them with the corresponding id in the mapping\n\
      \    delta_communities[\"community\"] = (\n        delta_communities[\"community\"\
      ]\n        .astype(int)\n        .apply(lambda x: community_id_mapping.get(x,\
      \ x))\n    )\n\n    delta_communities[\"parent\"] = (\n        delta_communities[\"\
      parent\"]\n        .astype(int)\n        .apply(lambda x: community_id_mapping.get(x,\
      \ x))\n    )\n\n    old_communities[\"community\"] = old_communities[\"community\"\
      ].astype(int)\n\n    # Merge the final communities\n    merged_communities =\
      \ pd.concat(\n        [old_communities, delta_communities], ignore_index=True,\
      \ copy=False\n    )\n\n    # Rename title\n    merged_communities[\"title\"\
      ] = \"Community \" + merged_communities[\"community\"].astype(\n        str\n\
      \    )\n    # Re-assign the human_readable_id\n    merged_communities[\"human_readable_id\"\
      ] = merged_communities[\"community\"]\n\n    merged_communities = merged_communities.loc[\n\
      \        :,\n        COMMUNITIES_FINAL_COLUMNS,\n    ]\n    return merged_communities,\
      \ community_id_mapping"
    signature: "def _update_and_merge_communities(\n    old_communities: pd.DataFrame,\n\
      \    delta_communities: pd.DataFrame,\n) -> tuple[pd.DataFrame, dict]"
    decorators: []
    raises: []
    calls:
    - target: old_communities["community"].fillna(0).astype(int).max
      type: unresolved
    - target: old_communities["community"].fillna(0).astype
      type: unresolved
    - target: old_communities["community"].fillna
      type: unresolved
    - target: delta_communities["community"].dropna().astype(int).items
      type: unresolved
    - target: delta_communities["community"].dropna().astype
      type: unresolved
    - target: delta_communities["community"].dropna
      type: unresolved
    - target: community_id_mapping.update
      type: unresolved
    - target: "delta_communities[\"community\"]\n        .astype(int)\n        .apply"
      type: unresolved
    - target: "delta_communities[\"community\"]\n        .astype"
      type: unresolved
    - target: community_id_mapping.get
      type: unresolved
    - target: "delta_communities[\"parent\"]\n        .astype(int)\n        .apply"
      type: unresolved
    - target: "delta_communities[\"parent\"]\n        .astype"
      type: unresolved
    - target: old_communities["community"].astype
      type: unresolved
    - target: pandas::concat
      type: external
    - target: merged_communities["community"].astype
      type: unresolved
    visibility: protected
    node_id: graphrag/index/update/communities.py::_update_and_merge_communities
    called_by:
    - source: graphrag/index/workflows/update_communities.py::_update_communities
      type: internal
  - name: _update_and_merge_community_reports
    start_line: 89
    end_line: 151
    code: "def _update_and_merge_community_reports(\n    old_community_reports: pd.DataFrame,\n\
      \    delta_community_reports: pd.DataFrame,\n    community_id_mapping: dict,\n\
      ) -> pd.DataFrame:\n    \"\"\"Update and merge community reports.\n\n    Parameters\n\
      \    ----------\n    old_community_reports : pd.DataFrame\n        The old community\
      \ reports.\n    delta_community_reports : pd.DataFrame\n        The delta community\
      \ reports.\n    community_id_mapping : dict\n        The community id mapping.\n\
      \n    Returns\n    -------\n    pd.DataFrame\n        The updated community\
      \ reports.\n    \"\"\"\n    # Check if size and period columns exist in the\
      \ old_community_reports. If not, add them\n    if \"size\" not in old_community_reports.columns:\n\
      \        old_community_reports[\"size\"] = None\n    if \"period\" not in old_community_reports.columns:\n\
      \        old_community_reports[\"period\"] = None\n\n    # Same for delta_community_reports\n\
      \    if \"size\" not in delta_community_reports.columns:\n        delta_community_reports[\"\
      size\"] = None\n    if \"period\" not in delta_community_reports.columns:\n\
      \        delta_community_reports[\"period\"] = None\n\n    # Look for community\
      \ ids in community and replace them with the corresponding id in the mapping\n\
      \    delta_community_reports[\"community\"] = (\n        delta_community_reports[\"\
      community\"]\n        .astype(int)\n        .apply(lambda x: community_id_mapping.get(x,\
      \ x))\n    )\n\n    delta_community_reports[\"parent\"] = (\n        delta_community_reports[\"\
      parent\"]\n        .astype(int)\n        .apply(lambda x: community_id_mapping.get(x,\
      \ x))\n    )\n\n    old_community_reports[\"community\"] = old_community_reports[\"\
      community\"].astype(int)\n\n    # Merge the final community reports\n    merged_community_reports\
      \ = pd.concat(\n        [old_community_reports, delta_community_reports], ignore_index=True,\
      \ copy=False\n    )\n\n    # Maintain type compat with query\n    merged_community_reports[\"\
      community\"] = merged_community_reports[\n        \"community\"\n    ].astype(int)\n\
      \    # Re-assign the human_readable_id\n    merged_community_reports[\"human_readable_id\"\
      ] = merged_community_reports[\n        \"community\"\n    ]\n\n    return merged_community_reports.loc[:,\
      \ COMMUNITY_REPORTS_FINAL_COLUMNS]"
    signature: "def _update_and_merge_community_reports(\n    old_community_reports:\
      \ pd.DataFrame,\n    delta_community_reports: pd.DataFrame,\n    community_id_mapping:\
      \ dict,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: "delta_community_reports[\"community\"]\n        .astype(int)\n    \
        \    .apply"
      type: unresolved
    - target: "delta_community_reports[\"community\"]\n        .astype"
      type: unresolved
    - target: community_id_mapping.get
      type: unresolved
    - target: "delta_community_reports[\"parent\"]\n        .astype(int)\n       \
        \ .apply"
      type: unresolved
    - target: "delta_community_reports[\"parent\"]\n        .astype"
      type: unresolved
    - target: old_community_reports["community"].astype
      type: unresolved
    - target: pandas::concat
      type: external
    - target: "merged_community_reports[\n        \"community\"\n    ].astype"
      type: unresolved
    visibility: protected
    node_id: graphrag/index/update/communities.py::_update_and_merge_community_reports
    called_by:
    - source: graphrag/index/workflows/update_community_reports.py::_update_community_reports
      type: internal
- file_name: graphrag/index/update/entities.py
  imports:
  - module: itertools
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: ENTITIES_FINAL_COLUMNS
    alias: null
  functions:
  - name: _group_and_resolve_entities
    start_line: 14
    end_line: 78
    code: "def _group_and_resolve_entities(\n    old_entities_df: pd.DataFrame, delta_entities_df:\
      \ pd.DataFrame\n) -> tuple[pd.DataFrame, dict]:\n    \"\"\"Group and resolve\
      \ entities.\n\n    Parameters\n    ----------\n    old_entities_df : pd.DataFrame\n\
      \        The first dataframe.\n    delta_entities_df : pd.DataFrame\n      \
      \  The delta dataframe.\n\n    Returns\n    -------\n    pd.DataFrame\n    \
      \    The resolved dataframe.\n    dict\n        The id mapping for existing\
      \ entities. In the form of {df_b.id: df_a.id}.\n    \"\"\"\n    # If a title\
      \ exists in A and B, make a dictionary for {B.id : A.id}\n    merged = delta_entities_df[[\"\
      id\", \"title\"]].merge(\n        old_entities_df[[\"id\", \"title\"]],\n  \
      \      on=\"title\",\n        suffixes=(\"_B\", \"_A\"),\n        copy=False,\n\
      \    )\n    id_mapping = dict(zip(merged[\"id_B\"], merged[\"id_A\"], strict=True))\n\
      \n    # Increment human readable id in b by the max of a\n    initial_id = old_entities_df[\"\
      human_readable_id\"].max() + 1\n    delta_entities_df[\"human_readable_id\"\
      ] = np.arange(\n        initial_id, initial_id + len(delta_entities_df)\n  \
      \  )\n    # Concat A and B\n    combined = pd.concat(\n        [old_entities_df,\
      \ delta_entities_df], ignore_index=True, copy=False\n    )\n\n    # Group by\
      \ title and resolve conflicts\n    aggregated = (\n        combined.groupby(\"\
      title\")\n        .agg({\n            \"id\": \"first\",\n            \"type\"\
      : \"first\",\n            \"human_readable_id\": \"first\",\n            \"\
      description\": lambda x: list(x.astype(str)),  # Ensure str\n            # Concatenate\
      \ nd.array into a single list\n            \"text_unit_ids\": lambda x: list(itertools.chain(*x.tolist())),\n\
      \            \"degree\": \"first\",  # todo: we could probably re-compute this\
      \ with the entire new graph\n            \"x\": \"first\",\n            \"y\"\
      : \"first\",\n        })\n        .reset_index()\n    )\n\n    # recompute frequency\
      \ to include new text units\n    aggregated[\"frequency\"] = aggregated[\"text_unit_ids\"\
      ].apply(len)\n\n    # Force the result into a DataFrame\n    resolved: pd.DataFrame\
      \ = pd.DataFrame(aggregated)\n\n    # Modify column order to keep consistency\n\
      \    resolved = resolved.loc[:, ENTITIES_FINAL_COLUMNS]\n\n    return resolved,\
      \ id_mapping"
    signature: "def _group_and_resolve_entities(\n    old_entities_df: pd.DataFrame,\
      \ delta_entities_df: pd.DataFrame\n) -> tuple[pd.DataFrame, dict]"
    decorators: []
    raises: []
    calls:
    - target: delta_entities_df[["id", "title"]].merge
      type: unresolved
    - target: dict
      type: builtin
    - target: zip
      type: builtin
    - target: old_entities_df["human_readable_id"].max
      type: unresolved
    - target: numpy::arange
      type: external
    - target: len
      type: builtin
    - target: pandas::concat
      type: external
    - target: "combined.groupby(\"title\")\n        .agg({\n            \"id\": \"\
        first\",\n            \"type\": \"first\",\n            \"human_readable_id\"\
        : \"first\",\n            \"description\": lambda x: list(x.astype(str)),\
        \  # Ensure str\n            # Concatenate nd.array into a single list\n \
        \           \"text_unit_ids\": lambda x: list(itertools.chain(*x.tolist())),\n\
        \            \"degree\": \"first\",  # todo: we could probably re-compute\
        \ this with the entire new graph\n            \"x\": \"first\",\n        \
        \    \"y\": \"first\",\n        })\n        .reset_index"
      type: unresolved
    - target: "combined.groupby(\"title\")\n        .agg"
      type: unresolved
    - target: combined.groupby
      type: unresolved
    - target: list
      type: builtin
    - target: x.astype
      type: unresolved
    - target: itertools::chain
      type: stdlib
    - target: x.tolist
      type: unresolved
    - target: aggregated["text_unit_ids"].apply
      type: unresolved
    - target: pandas::DataFrame
      type: external
    visibility: protected
    node_id: graphrag/index/update/entities.py::_group_and_resolve_entities
    called_by:
    - source: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
- file_name: graphrag/index/update/incremental_index.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: get_delta_docs
    start_line: 34
    end_line: 63
    code: "async def get_delta_docs(\n    input_dataset: pd.DataFrame, storage: PipelineStorage\n\
      ) -> InputDelta:\n    \"\"\"Get the delta between the input dataset and the\
      \ final documents.\n\n    Parameters\n    ----------\n    input_dataset : pd.DataFrame\n\
      \        The input dataset.\n    storage : PipelineStorage\n        The Pipeline\
      \ storage.\n\n    Returns\n    -------\n    InputDelta\n        The input delta.\
      \ With new inputs and deleted inputs.\n    \"\"\"\n    final_docs = await load_table_from_storage(\"\
      documents\", storage)\n\n    # Select distinct title from final docs and from\
      \ dataset\n    previous_docs: list[str] = final_docs[\"title\"].unique().tolist()\n\
      \    dataset_docs: list[str] = input_dataset[\"title\"].unique().tolist()\n\n\
      \    # Get the new documents (using loc to ensure DataFrame)\n    new_docs =\
      \ input_dataset.loc[~input_dataset[\"title\"].isin(previous_docs)]\n\n    #\
      \ Get the deleted documents (again using loc to ensure DataFrame)\n    deleted_docs\
      \ = final_docs.loc[~final_docs[\"title\"].isin(dataset_docs)]\n\n    return\
      \ InputDelta(new_docs, deleted_docs)"
    signature: "def get_delta_docs(\n    input_dataset: pd.DataFrame, storage: PipelineStorage\n\
      ) -> InputDelta"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: final_docs["title"].unique().tolist
      type: unresolved
    - target: final_docs["title"].unique
      type: unresolved
    - target: input_dataset["title"].unique().tolist
      type: unresolved
    - target: input_dataset["title"].unique
      type: unresolved
    - target: input_dataset["title"].isin
      type: unresolved
    - target: final_docs["title"].isin
      type: unresolved
    - target: InputDelta
      type: unresolved
    visibility: public
    node_id: graphrag/index/update/incremental_index.py::get_delta_docs
    called_by:
    - source: graphrag/index/workflows/load_update_documents.py::load_update_documents
      type: internal
  - name: concat_dataframes
    start_line: 66
    end_line: 83
    code: "async def concat_dataframes(\n    name: str,\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> pd.DataFrame:\n    \"\"\"Concatenate dataframes.\"\"\"\n    old_df = await\
      \ load_table_from_storage(name, previous_storage)\n    delta_df = await load_table_from_storage(name,\
      \ delta_storage)\n\n    # Merge the final documents\n    initial_id = old_df[\"\
      human_readable_id\"].max() + 1\n    delta_df[\"human_readable_id\"] = np.arange(initial_id,\
      \ initial_id + len(delta_df))\n    final_df = pd.concat([old_df, delta_df],\
      \ ignore_index=True, copy=False)\n\n    await write_table_to_storage(final_df,\
      \ name, output_storage)\n\n    return final_df"
    signature: "def concat_dataframes(\n    name: str,\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: old_df["human_readable_id"].max
      type: unresolved
    - target: numpy::arange
      type: external
    - target: len
      type: builtin
    - target: pandas::concat
      type: external
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: public
    node_id: graphrag/index/update/incremental_index.py::concat_dataframes
    called_by:
    - source: graphrag/index/workflows/update_final_documents.py::run_workflow
      type: internal
- file_name: graphrag/index/update/relationships.py
  imports:
  - module: itertools
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: RELATIONSHIPS_FINAL_COLUMNS
    alias: null
  functions:
  - name: _update_and_merge_relationships
    start_line: 14
    end_line: 85
    code: "def _update_and_merge_relationships(\n    old_relationships: pd.DataFrame,\
      \ delta_relationships: pd.DataFrame\n) -> pd.DataFrame:\n    \"\"\"Update and\
      \ merge relationships.\n\n    Parameters\n    ----------\n    old_relationships\
      \ : pd.DataFrame\n        The old relationships.\n    delta_relationships :\
      \ pd.DataFrame\n        The delta relationships.\n\n    Returns\n    -------\n\
      \    pd.DataFrame\n        The updated relationships.\n    \"\"\"\n    # Increment\
      \ the human readable id in b by the max of a\n    # Ensure both columns are\
      \ integers\n    delta_relationships[\"human_readable_id\"] = delta_relationships[\n\
      \        \"human_readable_id\"\n    ].astype(int)\n    old_relationships[\"\
      human_readable_id\"] = old_relationships[\n        \"human_readable_id\"\n \
      \   ].astype(int)\n\n    # Adjust delta_relationships IDs to be greater than\
      \ any in old_relationships\n    initial_id = old_relationships[\"human_readable_id\"\
      ].max() + 1\n    delta_relationships[\"human_readable_id\"] = np.arange(\n \
      \       initial_id, initial_id + len(delta_relationships)\n    )\n\n    # Merge\
      \ the DataFrames without copying if possible\n    merged_relationships = pd.concat(\n\
      \        [old_relationships, delta_relationships], ignore_index=True, copy=False\n\
      \    )\n\n    # Group by title and resolve conflicts\n    aggregated = (\n \
      \       merged_relationships.groupby([\"source\", \"target\"])\n        .agg({\n\
      \            \"id\": \"first\",\n            \"human_readable_id\": \"first\"\
      ,\n            \"description\": lambda x: list(x.astype(str)),  # Ensure str\n\
      \            # Concatenate nd.array into a single list\n            \"text_unit_ids\"\
      : lambda x: list(itertools.chain(*x.tolist())),\n            \"weight\": \"\
      mean\",\n            \"combined_degree\": \"sum\",\n        })\n        .reset_index()\n\
      \    )\n\n    # Force the result into a DataFrame\n    final_relationships:\
      \ pd.DataFrame = pd.DataFrame(aggregated)\n\n    # Recalculate target and source\
      \ degrees\n    final_relationships[\"source_degree\"] = final_relationships.groupby(\"\
      source\")[\n        \"target\"\n    ].transform(\"count\")\n    final_relationships[\"\
      target_degree\"] = final_relationships.groupby(\"target\")[\n        \"source\"\
      \n    ].transform(\"count\")\n\n    # Recalculate the combined_degree of the\
      \ relationships (source degree + target degree)\n    final_relationships[\"\
      combined_degree\"] = (\n        final_relationships[\"source_degree\"] + final_relationships[\"\
      target_degree\"]\n    )\n\n    return final_relationships.loc[\n        :,\n\
      \        RELATIONSHIPS_FINAL_COLUMNS,\n    ]"
    signature: "def _update_and_merge_relationships(\n    old_relationships: pd.DataFrame,\
      \ delta_relationships: pd.DataFrame\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: "delta_relationships[\n        \"human_readable_id\"\n    ].astype"
      type: unresolved
    - target: "old_relationships[\n        \"human_readable_id\"\n    ].astype"
      type: unresolved
    - target: old_relationships["human_readable_id"].max
      type: unresolved
    - target: numpy::arange
      type: external
    - target: len
      type: builtin
    - target: pandas::concat
      type: external
    - target: "merged_relationships.groupby([\"source\", \"target\"])\n        .agg({\n\
        \            \"id\": \"first\",\n            \"human_readable_id\": \"first\"\
        ,\n            \"description\": lambda x: list(x.astype(str)),  # Ensure str\n\
        \            # Concatenate nd.array into a single list\n            \"text_unit_ids\"\
        : lambda x: list(itertools.chain(*x.tolist())),\n            \"weight\": \"\
        mean\",\n            \"combined_degree\": \"sum\",\n        })\n        .reset_index"
      type: unresolved
    - target: "merged_relationships.groupby([\"source\", \"target\"])\n        .agg"
      type: unresolved
    - target: merged_relationships.groupby
      type: unresolved
    - target: list
      type: builtin
    - target: x.astype
      type: unresolved
    - target: itertools::chain
      type: stdlib
    - target: x.tolist
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: "final_relationships.groupby(\"source\")[\n        \"target\"\n    ].transform"
      type: unresolved
    - target: final_relationships.groupby
      type: unresolved
    - target: "final_relationships.groupby(\"target\")[\n        \"source\"\n    ].transform"
      type: unresolved
    visibility: protected
    node_id: graphrag/index/update/relationships.py::_update_and_merge_relationships
    called_by:
    - source: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
- file_name: graphrag/index/utils/__init__.py
  imports: []
  functions: []
- file_name: graphrag/index/utils/dataframes.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: pandas._typing
    name: MergeHow
    alias: null
  functions:
  - name: drop_columns
    start_line: 13
    end_line: 15
    code: "def drop_columns(df: pd.DataFrame, *column: str) -> pd.DataFrame:\n   \
      \ \"\"\"Drop columns from a dataframe.\"\"\"\n    return df.drop(list(column),\
      \ axis=1)"
    signature: 'def drop_columns(df: pd.DataFrame, *column: str) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: df.drop
      type: unresolved
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::drop_columns
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
      type: internal
  - name: where_column_equals
    start_line: 18
    end_line: 20
    code: "def where_column_equals(df: pd.DataFrame, column: str, value: Any) -> pd.DataFrame:\n\
      \    \"\"\"Return a filtered DataFrame where a column equals a value.\"\"\"\n\
      \    return cast(\"pd.DataFrame\", df[df[column] == value])"
    signature: 'def where_column_equals(df: pd.DataFrame, column: str, value: Any)
      -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::where_column_equals
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
      type: internal
  - name: antijoin
    start_line: 23
    end_line: 31
    code: "def antijoin(df: pd.DataFrame, exclude: pd.DataFrame, column: str) -> pd.DataFrame:\n\
      \    \"\"\"Return an anti-joined dataframe.\n\n    Arguments:\n    * df: The\
      \ DataFrame to apply the exclusion to\n    * exclude: The DataFrame containing\
      \ rows to remove.\n    * column: The join-on column.\n    \"\"\"\n    return\
      \ df.loc[~df.loc[:, column].isin(exclude.loc[:, column])]"
    signature: 'def antijoin(df: pd.DataFrame, exclude: pd.DataFrame, column: str)
      -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: df.loc[:, column].isin
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::antijoin
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
      type: internal
  - name: transform_series
    start_line: 34
    end_line: 36
    code: "def transform_series(series: pd.Series, fn: Callable[[Any], Any]) -> pd.Series:\n\
      \    \"\"\"Apply a transformation function to a series.\"\"\"\n    return cast(\"\
      pd.Series\", series.apply(fn))"
    signature: 'def transform_series(series: pd.Series, fn: Callable[[Any], Any])
      -> pd.Series'
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: series.apply
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::transform_series
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
      type: internal
  - name: join
    start_line: 39
    end_line: 43
    code: "def join(\n    left: pd.DataFrame, right: pd.DataFrame, key: str, strategy:\
      \ MergeHow = \"left\"\n) -> pd.DataFrame:\n    \"\"\"Perform a table join.\"\
      \"\"\n    return left.merge(right, on=key, how=strategy)"
    signature: "def join(\n    left: pd.DataFrame, right: pd.DataFrame, key: str,\
      \ strategy: MergeHow = \"left\"\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: left.merge
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::join
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
      type: internal
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
  - name: union
    start_line: 46
    end_line: 48
    code: "def union(*frames: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Perform a\
      \ union operation on the given set of dataframes.\"\"\"\n    return pd.concat(list(frames))"
    signature: 'def union(*frames: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: pandas::concat
      type: external
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::union
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
      type: internal
  - name: select
    start_line: 51
    end_line: 53
    code: "def select(df: pd.DataFrame, *columns: str) -> pd.DataFrame:\n    \"\"\"\
      Select columns from a dataframe.\"\"\"\n    return cast(\"pd.DataFrame\", df[list(columns)])"
    signature: 'def select(df: pd.DataFrame, *columns: str) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/index/utils/dataframes.py::select
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
      type: internal
- file_name: graphrag/index/utils/derive_from_rows.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: inspect
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: traceback
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: collections.abc
    name: Coroutine
    alias: null
  - module: collections.abc
    name: Hashable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.logger.progress
    name: progress_ticker
    alias: null
  functions:
  - name: __init__
    start_line: 27
    end_line: 31
    code: "def __init__(self, num_errors: int, example: str | None = None):\n    \
      \    msg = f\"{num_errors} Errors occurred while running parallel transformation,\
      \ could not complete!\"\n        if example:\n            msg += f\"\\nExample\
      \ error: {example}\"\n        super().__init__(msg)"
    signature: 'def __init__(self, num_errors: int, example: str | None = None)'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError.__init__
    called_by: []
  - name: derive_from_rows
    start_line: 34
    end_line: 55
    code: "async def derive_from_rows(\n    input: pd.DataFrame,\n    transform: Callable[[pd.Series],\
      \ Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks | None = None,\n \
      \   num_threads: int = 4,\n    async_type: AsyncType = AsyncType.AsyncIO,\n\
      \    progress_msg: str = \"\",\n) -> list[ItemType | None]:\n    \"\"\"Apply\
      \ a generic transform function to each row. Any errors will be reported and\
      \ thrown.\"\"\"\n    callbacks = callbacks or NoopWorkflowCallbacks()\n    match\
      \ async_type:\n        case AsyncType.AsyncIO:\n            return await derive_from_rows_asyncio(\n\
      \                input, transform, callbacks, num_threads, progress_msg\n  \
      \          )\n        case AsyncType.Threaded:\n            return await derive_from_rows_asyncio_threads(\n\
      \                input, transform, callbacks, num_threads, progress_msg\n  \
      \          )\n        case _:\n            msg = f\"Unsupported scheduling type\
      \ {async_type}\"\n            raise ValueError(msg)"
    signature: "def derive_from_rows(\n    input: pd.DataFrame,\n    transform: Callable[[pd.Series],\
      \ Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks | None = None,\n \
      \   num_threads: int = 4,\n    async_type: AsyncType = AsyncType.AsyncIO,\n\
      \    progress_msg: str = \"\",\n) -> list[ItemType | None]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
      type: internal
    - target: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
      type: internal
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows
    called_by:
    - source: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
      type: internal
    - source: graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
      type: internal
    - source: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
      type: internal
    - source: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
  - name: derive_from_rows_asyncio_threads
    start_line: 61
    end_line: 88
    code: "async def derive_from_rows_asyncio_threads(\n    input: pd.DataFrame,\n\
      \    transform: Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks:\
      \ WorkflowCallbacks,\n    num_threads: int | None = 4,\n    progress_msg: str\
      \ = \"\",\n) -> list[ItemType | None]:\n    \"\"\"\n    Derive from rows asynchronously.\n\
      \n    This is useful for IO bound operations.\n    \"\"\"\n    semaphore = asyncio.Semaphore(num_threads\
      \ or 4)\n\n    async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType\
      \ | None]:\n        tasks = [asyncio.to_thread(execute, row) for row in input.iterrows()]\n\
      \n        async def execute_task(task: Coroutine) -> ItemType | None:\n    \
      \        async with semaphore:\n                # fire off the thread\n    \
      \            thread = await task\n                return await thread\n\n  \
      \      return await asyncio.gather(*[execute_task(task) for task in tasks])\n\
      \n    return await _derive_from_rows_base(\n        input, transform, callbacks,\
      \ gather, progress_msg\n    )"
    signature: "def derive_from_rows_asyncio_threads(\n    input: pd.DataFrame,\n\
      \    transform: Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks:\
      \ WorkflowCallbacks,\n    num_threads: int | None = 4,\n    progress_msg: str\
      \ = \"\",\n) -> list[ItemType | None]"
    decorators: []
    raises: []
    calls:
    - target: asyncio::Semaphore
      type: stdlib
    - target: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
      type: internal
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
  - name: gather
    start_line: 75
    end_line: 84
    code: "async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]:\n\
      \        tasks = [asyncio.to_thread(execute, row) for row in input.iterrows()]\n\
      \n        async def execute_task(task: Coroutine) -> ItemType | None:\n    \
      \        async with semaphore:\n                # fire off the thread\n    \
      \            thread = await task\n                return await thread\n\n  \
      \      return await asyncio.gather(*[execute_task(task) for task in tasks])"
    signature: 'def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]'
    decorators: []
    raises: []
    calls:
    - target: asyncio::to_thread
      type: stdlib
    - target: input.iterrows
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    - target: graphrag/index/utils/derive_from_rows.py::execute_task
      type: internal
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::gather
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
      type: internal
  - name: execute_task
    start_line: 78
    end_line: 82
    code: "async def execute_task(task: Coroutine) -> ItemType | None:\n         \
      \   async with semaphore:\n                # fire off the thread\n         \
      \       thread = await task\n                return await thread"
    signature: 'def execute_task(task: Coroutine) -> ItemType | None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::execute_task
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::gather
      type: internal
  - name: derive_from_rows_asyncio
    start_line: 94
    end_line: 122
    code: "async def derive_from_rows_asyncio(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    num_threads: int = 4,\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]:\n    \"\"\"\n    Derive from rows asynchronously.\n\n    This is\
      \ useful for IO bound operations.\n    \"\"\"\n    semaphore = asyncio.Semaphore(num_threads\
      \ or 4)\n\n    async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType\
      \ | None]:\n        async def execute_row_protected(\n            row: tuple[Hashable,\
      \ pd.Series],\n        ) -> ItemType | None:\n            async with semaphore:\n\
      \                return await execute(row)\n\n        tasks = [\n          \
      \  asyncio.create_task(execute_row_protected(row)) for row in input.iterrows()\n\
      \        ]\n        return await asyncio.gather(*tasks)\n\n    return await\
      \ _derive_from_rows_base(\n        input, transform, callbacks, gather, progress_msg\n\
      \    )"
    signature: "def derive_from_rows_asyncio(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    num_threads: int = 4,\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]"
    decorators: []
    raises: []
    calls:
    - target: asyncio::Semaphore
      type: stdlib
    - target: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
      type: internal
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::derive_from_rows
      type: internal
  - name: gather
    start_line: 108
    end_line: 118
    code: "async def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]:\n\
      \        async def execute_row_protected(\n            row: tuple[Hashable,\
      \ pd.Series],\n        ) -> ItemType | None:\n            async with semaphore:\n\
      \                return await execute(row)\n\n        tasks = [\n          \
      \  asyncio.create_task(execute_row_protected(row)) for row in input.iterrows()\n\
      \        ]\n        return await asyncio.gather(*tasks)"
    signature: 'def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]'
    decorators: []
    raises: []
    calls:
    - target: asyncio::create_task
      type: stdlib
    - target: graphrag/index/utils/derive_from_rows.py::execute_row_protected
      type: internal
    - target: input.iterrows
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::gather
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
      type: internal
  - name: execute_row_protected
    start_line: 109
    end_line: 113
    code: "async def execute_row_protected(\n            row: tuple[Hashable, pd.Series],\n\
      \        ) -> ItemType | None:\n            async with semaphore:\n        \
      \        return await execute(row)"
    signature: "def execute_row_protected(\n            row: tuple[Hashable, pd.Series],\n\
      \        ) -> ItemType | None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/derive_from_rows.py::execute
      type: internal
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::execute_row_protected
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::gather
      type: internal
  - name: _derive_from_rows_base
    start_line: 131
    end_line: 173
    code: "async def _derive_from_rows_base(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    gather: GatherFn[ItemType],\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]:\n    \"\"\"\n    Derive from rows asynchronously.\n\n    This is\
      \ useful for IO bound operations.\n    \"\"\"\n    tick = progress_ticker(\n\
      \        callbacks.progress, num_total=len(input), description=progress_msg\n\
      \    )\n    errors: list[tuple[BaseException, str]] = []\n\n    async def execute(row:\
      \ tuple[Any, pd.Series]) -> ItemType | None:\n        try:\n            result\
      \ = transform(row[1])\n            if inspect.iscoroutine(result):\n       \
      \         result = await result\n        except Exception as e:  # noqa: BLE001\n\
      \            errors.append((e, traceback.format_exc()))\n            return\
      \ None\n        else:\n            return cast(\"ItemType\", result)\n     \
      \   finally:\n            tick(1)\n\n    result = await gather(execute)\n\n\
      \    tick.done()\n\n    for error, stack in errors:\n        logger.error(\n\
      \            \"parallel transformation error\", exc_info=error, extra={\"stack\"\
      : stack}\n        )\n\n    if len(errors) > 0:\n        raise ParallelizationError(len(errors),\
      \ errors[0][1])\n\n    return result"
    signature: "def _derive_from_rows_base(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    gather: GatherFn[ItemType],\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]"
    decorators: []
    raises:
    - ParallelizationError
    calls:
    - target: graphrag/logger/progress.py::progress_ticker
      type: internal
    - target: len
      type: builtin
    - target: graphrag/index/utils/derive_from_rows.py::gather
      type: internal
    - target: tick.done
      type: unresolved
    - target: logger.error
      type: unresolved
    - target: ParallelizationError
      type: unresolved
    visibility: protected
    node_id: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
      type: internal
    - source: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
      type: internal
  - name: execute
    start_line: 148
    end_line: 159
    code: "async def execute(row: tuple[Any, pd.Series]) -> ItemType | None:\n   \
      \     try:\n            result = transform(row[1])\n            if inspect.iscoroutine(result):\n\
      \                result = await result\n        except Exception as e:  # noqa:\
      \ BLE001\n            errors.append((e, traceback.format_exc()))\n         \
      \   return None\n        else:\n            return cast(\"ItemType\", result)\n\
      \        finally:\n            tick(1)"
    signature: 'def execute(row: tuple[Any, pd.Series]) -> ItemType | None'
    decorators: []
    raises: []
    calls:
    - target: transform
      type: unresolved
    - target: inspect::iscoroutine
      type: stdlib
    - target: errors.append
      type: unresolved
    - target: traceback::format_exc
      type: stdlib
    - target: typing::cast
      type: stdlib
    - target: tick
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/derive_from_rows.py::execute
    called_by:
    - source: graphrag/index/utils/derive_from_rows.py::execute_row_protected
      type: internal
- file_name: graphrag/index/utils/dicts.py
  imports: []
  functions:
  - name: dict_has_keys_with_types
    start_line: 7
    end_line: 22
    code: "def dict_has_keys_with_types(\n    data: dict, expected_fields: list[tuple[str,\
      \ type]], inplace: bool = False\n) -> bool:\n    \"\"\"Return True if the given\
      \ dictionary has the given keys with the given types.\"\"\"\n    for field,\
      \ field_type in expected_fields:\n        if field not in data:\n          \
      \  return False\n\n        value = data[field]\n        try:\n            cast_value\
      \ = field_type(value)\n            if inplace:\n                data[field]\
      \ = cast_value\n        except (TypeError, ValueError):\n            return\
      \ False\n    return True"
    signature: "def dict_has_keys_with_types(\n    data: dict, expected_fields: list[tuple[str,\
      \ type]], inplace: bool = False\n) -> bool"
    decorators: []
    raises: []
    calls:
    - target: field_type
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/dicts.py::dict_has_keys_with_types
    called_by: []
- file_name: graphrag/index/utils/graphs.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graspologic.partition
    name: hierarchical_leiden
    alias: null
  - module: graspologic.partition
    name: modularity
    alias: null
  - module: graspologic.utils
    name: largest_connected_component
    alias: null
  - module: graphrag.config.enums
    name: ModularityMetric
    alias: null
  functions:
  - name: calculate_root_modularity
    start_line: 20
    end_line: 30
    code: "def calculate_root_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float:\n    \"\"\"Calculate\
      \ distance between the modularity of the graph's root clusters and the target\
      \ modularity.\"\"\"\n    hcs = hierarchical_leiden(\n        graph, max_cluster_size=max_cluster_size,\
      \ random_seed=random_seed\n    )\n    root_clusters = hcs.first_level_hierarchical_clustering()\n\
      \    return modularity(graph, root_clusters)"
    signature: "def calculate_root_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: graspologic.partition::hierarchical_leiden
      type: external
    - target: hcs.first_level_hierarchical_clustering
      type: unresolved
    - target: graspologic.partition::modularity
      type: external
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_root_modularity
    called_by:
    - source: graphrag/index/utils/graphs.py::calculate_graph_modularity
      type: internal
    - source: graphrag/index/utils/graphs.py::calculate_lcc_modularity
      type: internal
    - source: graphrag/index/utils/graphs.py::calculate_weighted_modularity
      type: internal
  - name: calculate_leaf_modularity
    start_line: 33
    end_line: 43
    code: "def calculate_leaf_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float:\n    \"\"\"Calculate\
      \ distance between the modularity of the graph's leaf clusters and the target\
      \ modularity.\"\"\"\n    hcs = hierarchical_leiden(\n        graph, max_cluster_size=max_cluster_size,\
      \ random_seed=random_seed\n    )\n    leaf_clusters = hcs.final_level_hierarchical_clustering()\n\
      \    return modularity(graph, leaf_clusters)"
    signature: "def calculate_leaf_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: graspologic.partition::hierarchical_leiden
      type: external
    - target: hcs.final_level_hierarchical_clustering
      type: unresolved
    - target: graspologic.partition::modularity
      type: external
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_leaf_modularity
    called_by:
    - source: graphrag/index/utils/graphs.py::calculate_graph_modularity
      type: internal
    - source: graphrag/index/utils/graphs.py::calculate_lcc_modularity
      type: internal
    - source: graphrag/index/utils/graphs.py::calculate_weighted_modularity
      type: internal
  - name: calculate_graph_modularity
    start_line: 46
    end_line: 59
    code: "def calculate_graph_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float:\n    \"\"\"Calculate modularity of the whole graph.\"\
      \"\"\n    if use_root_modularity:\n        return calculate_root_modularity(\n\
      \            graph, max_cluster_size=max_cluster_size, random_seed=random_seed\n\
      \        )\n    return calculate_leaf_modularity(\n        graph, max_cluster_size=max_cluster_size,\
      \ random_seed=random_seed\n    )"
    signature: "def calculate_graph_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/graphs.py::calculate_root_modularity
      type: internal
    - target: graphrag/index/utils/graphs.py::calculate_leaf_modularity
      type: internal
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_graph_modularity
    called_by:
    - source: graphrag/index/utils/graphs.py::calculate_modularity
      type: internal
  - name: calculate_lcc_modularity
    start_line: 62
    end_line: 76
    code: "def calculate_lcc_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float:\n    \"\"\"Calculate modularity of the largest connected\
      \ component of the graph.\"\"\"\n    lcc = cast(\"nx.Graph\", largest_connected_component(graph))\n\
      \    if use_root_modularity:\n        return calculate_root_modularity(\n  \
      \          lcc, max_cluster_size=max_cluster_size, random_seed=random_seed\n\
      \        )\n    return calculate_leaf_modularity(\n        lcc, max_cluster_size=max_cluster_size,\
      \ random_seed=random_seed\n    )"
    signature: "def calculate_lcc_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    - target: graspologic.utils::largest_connected_component
      type: external
    - target: graphrag/index/utils/graphs.py::calculate_root_modularity
      type: internal
    - target: graphrag/index/utils/graphs.py::calculate_leaf_modularity
      type: internal
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_lcc_modularity
    called_by:
    - source: graphrag/index/utils/graphs.py::calculate_modularity
      type: internal
  - name: calculate_weighted_modularity
    start_line: 79
    end_line: 114
    code: "def calculate_weighted_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    min_connected_component_size:\
      \ int = 10,\n    use_root_modularity: bool = True,\n) -> float:\n    \"\"\"\n\
      \    Calculate weighted modularity of all connected components with size greater\
      \ than min_connected_component_size.\n\n    Modularity = sum(component_modularity\
      \ * component_size) / total_nodes.\n    \"\"\"\n    connected_components: list[set]\
      \ = list(nx.connected_components(graph))\n    filtered_components = [\n    \
      \    component\n        for component in connected_components\n        if len(component)\
      \ > min_connected_component_size\n    ]\n    if len(filtered_components) ==\
      \ 0:\n        filtered_components = [graph]\n\n    total_nodes = sum(len(component)\
      \ for component in filtered_components)\n    total_modularity = 0\n    for component\
      \ in filtered_components:\n        if len(component) > min_connected_component_size:\n\
      \            subgraph = graph.subgraph(component)\n            if use_root_modularity:\n\
      \                modularity = calculate_root_modularity(\n                 \
      \   subgraph, max_cluster_size=max_cluster_size, random_seed=random_seed\n \
      \               )\n            else:\n                modularity = calculate_leaf_modularity(\n\
      \                    subgraph, max_cluster_size=max_cluster_size, random_seed=random_seed\n\
      \                )\n            total_modularity += modularity * len(component)\
      \ / total_nodes\n    return total_modularity"
    signature: "def calculate_weighted_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    min_connected_component_size:\
      \ int = 10,\n    use_root_modularity: bool = True,\n) -> float"
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: networkx::connected_components
      type: external
    - target: len
      type: builtin
    - target: sum
      type: builtin
    - target: graph.subgraph
      type: unresolved
    - target: graphrag/index/utils/graphs.py::calculate_root_modularity
      type: internal
    - target: graphrag/index/utils/graphs.py::calculate_leaf_modularity
      type: internal
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_weighted_modularity
    called_by:
    - source: graphrag/index/utils/graphs.py::calculate_modularity
      type: internal
  - name: calculate_modularity
    start_line: 117
    end_line: 152
    code: "def calculate_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n    modularity_metric: ModularityMetric = ModularityMetric.WeightedComponents,\n\
      ) -> float:\n    \"\"\"Calculate modularity of the graph based on the modularity\
      \ metric type.\"\"\"\n    match modularity_metric:\n        case ModularityMetric.Graph:\n\
      \            logger.info(\"Calculating graph modularity\")\n            return\
      \ calculate_graph_modularity(\n                graph,\n                max_cluster_size=max_cluster_size,\n\
      \                random_seed=random_seed,\n                use_root_modularity=use_root_modularity,\n\
      \            )\n        case ModularityMetric.LCC:\n            logger.info(\"\
      Calculating LCC modularity\")\n            return calculate_lcc_modularity(\n\
      \                graph,\n                max_cluster_size=max_cluster_size,\n\
      \                random_seed=random_seed,\n                use_root_modularity=use_root_modularity,\n\
      \            )\n        case ModularityMetric.WeightedComponents:\n        \
      \    logger.info(\"Calculating weighted-components modularity\")\n         \
      \   return calculate_weighted_modularity(\n                graph,\n        \
      \        max_cluster_size=max_cluster_size,\n                random_seed=random_seed,\n\
      \                use_root_modularity=use_root_modularity,\n            )\n \
      \       case _:\n            msg = f\"Unknown modularity metric type: {modularity_metric}\"\
      \n            raise ValueError(msg)"
    signature: "def calculate_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n    modularity_metric: ModularityMetric = ModularityMetric.WeightedComponents,\n\
      ) -> float"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/utils/graphs.py::calculate_graph_modularity
      type: internal
    - target: graphrag/index/utils/graphs.py::calculate_lcc_modularity
      type: internal
    - target: graphrag/index/utils/graphs.py::calculate_weighted_modularity
      type: internal
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_modularity
    called_by: []
  - name: calculate_pmi_edge_weights
    start_line: 155
    end_line: 201
    code: "def calculate_pmi_edge_weights(\n    nodes_df: pd.DataFrame,\n    edges_df:\
      \ pd.DataFrame,\n    node_name_col: str = \"title\",\n    node_freq_col: str\
      \ = \"frequency\",\n    edge_weight_col: str = \"weight\",\n    edge_source_col:\
      \ str = \"source\",\n    edge_target_col: str = \"target\",\n) -> pd.DataFrame:\n\
      \    \"\"\"\n    Calculate pointwise mutual information (PMI) edge weights.\n\
      \n    Uses a variant of PMI that accounts for bias towards low-frequency events.\n\
      \    pmi(x,y) = p(x,y) * log2(p(x,y)/ (p(x)*p(y))\n    p(x,y) = edge_weight(x,y)\
      \ / total_edge_weights\n    p(x) = freq_occurrence(x) / total_freq_occurrences.\n\
      \n    \"\"\"\n    copied_nodes_df = nodes_df[[node_name_col, node_freq_col]]\n\
      \n    total_edge_weights = edges_df[edge_weight_col].sum()\n    total_freq_occurrences\
      \ = nodes_df[node_freq_col].sum()\n    copied_nodes_df[\"prop_occurrence\"]\
      \ = (\n        copied_nodes_df[node_freq_col] / total_freq_occurrences\n   \
      \ )\n    copied_nodes_df = copied_nodes_df.loc[:, [node_name_col, \"prop_occurrence\"\
      ]]\n\n    edges_df[\"prop_weight\"] = edges_df[edge_weight_col] / total_edge_weights\n\
      \    edges_df = (\n        edges_df.merge(\n            copied_nodes_df, left_on=edge_source_col,\
      \ right_on=node_name_col, how=\"left\"\n        )\n        .drop(columns=[node_name_col])\n\
      \        .rename(columns={\"prop_occurrence\": \"source_prop\"})\n    )\n  \
      \  edges_df = (\n        edges_df.merge(\n            copied_nodes_df, left_on=edge_target_col,\
      \ right_on=node_name_col, how=\"left\"\n        )\n        .drop(columns=[node_name_col])\n\
      \        .rename(columns={\"prop_occurrence\": \"target_prop\"})\n    )\n  \
      \  edges_df[edge_weight_col] = edges_df[\"prop_weight\"] * np.log2(\n      \
      \  edges_df[\"prop_weight\"] / (edges_df[\"source_prop\"] * edges_df[\"target_prop\"\
      ])\n    )\n\n    return edges_df.drop(columns=[\"prop_weight\", \"source_prop\"\
      , \"target_prop\"])"
    signature: "def calculate_pmi_edge_weights(\n    nodes_df: pd.DataFrame,\n   \
      \ edges_df: pd.DataFrame,\n    node_name_col: str = \"title\",\n    node_freq_col:\
      \ str = \"frequency\",\n    edge_weight_col: str = \"weight\",\n    edge_source_col:\
      \ str = \"source\",\n    edge_target_col: str = \"target\",\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: edges_df[edge_weight_col].sum
      type: unresolved
    - target: nodes_df[node_freq_col].sum
      type: unresolved
    - target: "edges_df.merge(\n            copied_nodes_df, left_on=edge_source_col,\
        \ right_on=node_name_col, how=\"left\"\n        )\n        .drop(columns=[node_name_col])\n\
        \        .rename"
      type: unresolved
    - target: "edges_df.merge(\n            copied_nodes_df, left_on=edge_source_col,\
        \ right_on=node_name_col, how=\"left\"\n        )\n        .drop"
      type: unresolved
    - target: edges_df.merge
      type: unresolved
    - target: "edges_df.merge(\n            copied_nodes_df, left_on=edge_target_col,\
        \ right_on=node_name_col, how=\"left\"\n        )\n        .drop(columns=[node_name_col])\n\
        \        .rename"
      type: unresolved
    - target: "edges_df.merge(\n            copied_nodes_df, left_on=edge_target_col,\
        \ right_on=node_name_col, how=\"left\"\n        )\n        .drop"
      type: unresolved
    - target: numpy::log2
      type: external
    - target: edges_df.drop
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
    called_by:
    - source: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
      type: internal
    - source: graphrag/index/utils/graphs.py::calculate_rrf_edge_weights
      type: internal
  - name: calculate_rrf_edge_weights
    start_line: 204
    end_line: 235
    code: "def calculate_rrf_edge_weights(\n    nodes_df: pd.DataFrame,\n    edges_df:\
      \ pd.DataFrame,\n    node_name_col=\"title\",\n    node_freq_col=\"freq\",\n\
      \    edge_weight_col=\"weight\",\n    edge_source_col=\"source\",\n    edge_target_col=\"\
      target\",\n    rrf_smoothing_factor: int = 60,\n) -> pd.DataFrame:\n    \"\"\
      \"Calculate reciprocal rank fusion (RRF) edge weights as a combination of PMI\
      \ weight and combined freq of source and target.\"\"\"\n    edges_df = calculate_pmi_edge_weights(\n\
      \        nodes_df,\n        edges_df,\n        node_name_col,\n        node_freq_col,\n\
      \        edge_weight_col,\n        edge_source_col,\n        edge_target_col,\n\
      \    )\n\n    edges_df[\"pmi_rank\"] = edges_df[edge_weight_col].rank(method=\"\
      min\", ascending=False)\n    edges_df[\"raw_weight_rank\"] = edges_df[edge_weight_col].rank(\n\
      \        method=\"min\", ascending=False\n    )\n    edges_df[edge_weight_col]\
      \ = edges_df.apply(\n        lambda x: (1 / (rrf_smoothing_factor + x[\"pmi_rank\"\
      ]))\n        + (1 / (rrf_smoothing_factor + x[\"raw_weight_rank\"])),\n    \
      \    axis=1,\n    )\n\n    return edges_df.drop(columns=[\"pmi_rank\", \"raw_weight_rank\"\
      ])"
    signature: "def calculate_rrf_edge_weights(\n    nodes_df: pd.DataFrame,\n   \
      \ edges_df: pd.DataFrame,\n    node_name_col=\"title\",\n    node_freq_col=\"\
      freq\",\n    edge_weight_col=\"weight\",\n    edge_source_col=\"source\",\n\
      \    edge_target_col=\"target\",\n    rrf_smoothing_factor: int = 60,\n) ->\
      \ pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
      type: internal
    - target: edges_df[edge_weight_col].rank
      type: unresolved
    - target: edges_df.apply
      type: unresolved
    - target: edges_df.drop
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/graphs.py::calculate_rrf_edge_weights
    called_by: []
  - name: get_upper_threshold_by_std
    start_line: 238
    end_line: 242
    code: "def get_upper_threshold_by_std(data: list[float] | list[int], std_trim:\
      \ float) -> float:\n    \"\"\"Get upper threshold by standard deviation.\"\"\
      \"\n    mean = np.mean(data)\n    std = np.std(data)\n    return cast(\"float\"\
      , mean + std_trim * std)"
    signature: 'def get_upper_threshold_by_std(data: list[float] | list[int], std_trim:
      float) -> float'
    decorators: []
    raises: []
    calls:
    - target: numpy::mean
      type: external
    - target: numpy::std
      type: external
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/graphs.py::get_upper_threshold_by_std
    called_by: []
- file_name: graphrag/index/utils/hashing.py
  imports:
  - module: collections.abc
    name: Iterable
    alias: null
  - module: hashlib
    name: sha512
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: gen_sha512_hash
    start_line: 11
    end_line: 14
    code: "def gen_sha512_hash(item: dict[str, Any], hashcode: Iterable[str]):\n \
      \   \"\"\"Generate a SHA512 hash.\"\"\"\n    hashed = \"\".join([str(item[column])\
      \ for column in hashcode])\n    return f\"{sha512(hashed.encode('utf-8'), usedforsecurity=False).hexdigest()}\""
    signature: 'def gen_sha512_hash(item: dict[str, Any], hashcode: Iterable[str])'
    decorators: []
    raises: []
    calls:
    - target: '"".join'
      type: unresolved
    - target: str
      type: builtin
    - target: sha512(hashed.encode('utf-8'), usedforsecurity=False).hexdigest
      type: unresolved
    - target: hashlib::sha512
      type: stdlib
    - target: hashed.encode
      type: unresolved
    visibility: public
    node_id: graphrag/index/utils/hashing.py::gen_sha512_hash
    called_by:
    - source: graphrag/index/input/text.py::load_file
      type: internal
    - source: graphrag/index/input/util.py::process_data_columns
      type: internal
    - source: graphrag/index/operations/build_noun_graph/build_noun_graph.py::extract
      type: internal
    - source: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
      type: internal
- file_name: graphrag/index/utils/is_null.py
  imports:
  - module: math
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: is_null
    start_line: 10
    end_line: 19
    code: "def is_null(value: Any) -> bool:\n    \"\"\"Check if value is null or is\
      \ nan.\"\"\"\n\n    def is_none() -> bool:\n        return value is None\n\n\
      \    def is_nan() -> bool:\n        return isinstance(value, float) and math.isnan(value)\n\
      \n    return is_none() or is_nan()"
    signature: 'def is_null(value: Any) -> bool'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/utils/is_null.py::is_none
      type: internal
    - target: graphrag/index/utils/is_null.py::is_nan
      type: internal
    visibility: public
    node_id: graphrag/index/utils/is_null.py::is_null
    called_by:
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
  - name: is_none
    start_line: 13
    end_line: 14
    code: "def is_none() -> bool:\n        return value is None"
    signature: def is_none() -> bool
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/utils/is_null.py::is_none
    called_by:
    - source: graphrag/index/utils/is_null.py::is_null
      type: internal
  - name: is_nan
    start_line: 16
    end_line: 17
    code: "def is_nan() -> bool:\n        return isinstance(value, float) and math.isnan(value)"
    signature: def is_nan() -> bool
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: math::isnan
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/is_null.py::is_nan
    called_by:
    - source: graphrag/index/utils/is_null.py::is_null
      type: internal
- file_name: graphrag/index/utils/stable_lcc.py
  imports:
  - module: html
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graspologic.utils
    name: largest_connected_component
    alias: null
  functions:
  - name: stable_largest_connected_component
    start_line: 12
    end_line: 20
    code: "def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph:\n\
      \    \"\"\"Return the largest connected component of the graph, with nodes and\
      \ edges sorted in a stable way.\"\"\"\n    # NOTE: The import is done here to\
      \ reduce the initial import time of the module\n    from graspologic.utils import\
      \ largest_connected_component\n\n    graph = graph.copy()\n    graph = cast(\"\
      nx.Graph\", largest_connected_component(graph))\n    graph = normalize_node_names(graph)\n\
      \    return _stabilize_graph(graph)"
    signature: 'def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph'
    decorators: []
    raises: []
    calls:
    - target: graph.copy
      type: unresolved
    - target: typing::cast
      type: stdlib
    - target: graspologic.utils::largest_connected_component
      type: external
    - target: graphrag/index/utils/stable_lcc.py::normalize_node_names
      type: internal
    - target: graphrag/index/utils/stable_lcc.py::_stabilize_graph
      type: internal
    visibility: public
    node_id: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
    called_by:
    - source: graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
      type: internal
    - source: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
      type: internal
    - source: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
      type: internal
    - source: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
      type: internal
    - source: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
      type: internal
  - name: _stabilize_graph
    start_line: 23
    end_line: 61
    code: "def _stabilize_graph(graph: nx.Graph) -> nx.Graph:\n    \"\"\"Ensure an\
      \ undirected graph with the same relationships will always be read the same\
      \ way.\"\"\"\n    fixed_graph = nx.DiGraph() if graph.is_directed() else nx.Graph()\n\
      \n    sorted_nodes = graph.nodes(data=True)\n    sorted_nodes = sorted(sorted_nodes,\
      \ key=lambda x: x[0])\n\n    fixed_graph.add_nodes_from(sorted_nodes)\n    edges\
      \ = list(graph.edges(data=True))\n\n    # If the graph is undirected, we create\
      \ the edges in a stable way, so we get the same results\n    # for example:\n\
      \    # A -> B\n    # in graph theory is the same as\n    # B -> A\n    # in\
      \ an undirected graph\n    # however, this can lead to downstream issues because\
      \ sometimes\n    # consumers read graph.nodes() which ends up being [A, B] and\
      \ sometimes it's [B, A]\n    # but they base some of their logic on the order\
      \ of the nodes, so the order ends up being important\n    # so we sort the nodes\
      \ in the edge in a stable way, so that we always get the same order\n    if\
      \ not graph.is_directed():\n\n        def _sort_source_target(edge):\n     \
      \       source, target, edge_data = edge\n            if source > target:\n\
      \                temp = source\n                source = target\n          \
      \      target = temp\n            return source, target, edge_data\n\n     \
      \   edges = [_sort_source_target(edge) for edge in edges]\n\n    def _get_edge_key(source:\
      \ Any, target: Any) -> str:\n        return f\"{source} -> {target}\"\n\n  \
      \  edges = sorted(edges, key=lambda x: _get_edge_key(x[0], x[1]))\n\n    fixed_graph.add_edges_from(edges)\n\
      \    return fixed_graph"
    signature: 'def _stabilize_graph(graph: nx.Graph) -> nx.Graph'
    decorators: []
    raises: []
    calls:
    - target: networkx::DiGraph
      type: external
    - target: graph.is_directed
      type: unresolved
    - target: networkx::Graph
      type: external
    - target: graph.nodes
      type: unresolved
    - target: sorted
      type: builtin
    - target: fixed_graph.add_nodes_from
      type: unresolved
    - target: list
      type: builtin
    - target: graph.edges
      type: unresolved
    - target: graphrag/index/utils/stable_lcc.py::_sort_source_target
      type: internal
    - target: graphrag/index/utils/stable_lcc.py::_get_edge_key
      type: internal
    - target: fixed_graph.add_edges_from
      type: unresolved
    visibility: protected
    node_id: graphrag/index/utils/stable_lcc.py::_stabilize_graph
    called_by:
    - source: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
  - name: _sort_source_target
    start_line: 45
    end_line: 51
    code: "def _sort_source_target(edge):\n            source, target, edge_data =\
      \ edge\n            if source > target:\n                temp = source\n   \
      \             source = target\n                target = temp\n            return\
      \ source, target, edge_data"
    signature: def _sort_source_target(edge)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/utils/stable_lcc.py::_sort_source_target
    called_by:
    - source: graphrag/index/utils/stable_lcc.py::_stabilize_graph
      type: internal
  - name: _get_edge_key
    start_line: 55
    end_line: 56
    code: "def _get_edge_key(source: Any, target: Any) -> str:\n        return f\"\
      {source} -> {target}\""
    signature: 'def _get_edge_key(source: Any, target: Any) -> str'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/index/utils/stable_lcc.py::_get_edge_key
    called_by:
    - source: graphrag/index/utils/stable_lcc.py::_stabilize_graph
      type: internal
  - name: normalize_node_names
    start_line: 64
    end_line: 67
    code: "def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph | nx.DiGraph:\n\
      \    \"\"\"Normalize node names.\"\"\"\n    node_mapping = {node: html.unescape(node.upper().strip())\
      \ for node in graph.nodes()}  # type: ignore\n    return nx.relabel_nodes(graph,\
      \ node_mapping)"
    signature: 'def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph
      | nx.DiGraph'
    decorators: []
    raises: []
    calls:
    - target: html::unescape
      type: stdlib
    - target: node.upper().strip
      type: unresolved
    - target: node.upper
      type: unresolved
    - target: graph.nodes
      type: unresolved
    - target: networkx::relabel_nodes
      type: external
    visibility: public
    node_id: graphrag/index/utils/stable_lcc.py::normalize_node_names
    called_by:
    - source: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
- file_name: graphrag/index/utils/string.py
  imports:
  - module: html
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: clean_str
    start_line: 11
    end_line: 19
    code: "def clean_str(input: Any) -> str:\n    \"\"\"Clean an input string by removing\
      \ HTML escapes, control characters, and other unwanted characters.\"\"\"\n \
      \   # If we get non-string input, just give it back\n    if not isinstance(input,\
      \ str):\n        return input\n\n    result = html.unescape(input.strip())\n\
      \    # https://stackoverflow.com/questions/4324790/removing-control-characters-from-a-string-in-python\n\
      \    return re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \"\", result)"
    signature: 'def clean_str(input: Any) -> str'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: html::unescape
      type: stdlib
    - target: input.strip
      type: unresolved
    - target: re::sub
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/string.py::clean_str
    called_by:
    - source: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
      type: internal
- file_name: graphrag/index/utils/uuid.py
  imports:
  - module: uuid
    name: null
    alias: null
  - module: random
    name: Random
    alias: null
  - module: random
    name: getrandbits
    alias: null
  functions:
  - name: gen_uuid
    start_line: 10
    end_line: 14
    code: "def gen_uuid(rd: Random | None = None):\n    \"\"\"Generate a random UUID\
      \ v4.\"\"\"\n    return uuid.UUID(\n        int=rd.getrandbits(128) if rd is\
      \ not None else getrandbits(128), version=4\n    ).hex"
    signature: 'def gen_uuid(rd: Random | None = None)'
    decorators: []
    raises: []
    calls:
    - target: uuid::UUID
      type: stdlib
    - target: rd.getrandbits
      type: unresolved
    - target: random::getrandbits
      type: stdlib
    visibility: public
    node_id: graphrag/index/utils/uuid.py::gen_uuid
    called_by: []
- file_name: graphrag/index/validate_config.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: sys
    name: null
    alias: null
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  functions:
  - name: validate_config_names
    start_line: 17
    end_line: 53
    code: "def validate_config_names(parameters: GraphRagConfig) -> None:\n    \"\"\
      \"Validate config file for model deployment name typos, by running a quick test\
      \ message for each.\"\"\"\n    for id, config in parameters.models.items():\n\
      \        if config.type in [\"chat\", \"azure_openai\", \"openai\"]:\n     \
      \       llm = ModelManager().register_chat(\n                name=\"test-llm\"\
      ,\n                model_type=config.type,\n                config=config,\n\
      \                callbacks=NoopWorkflowCallbacks(),\n                cache=None,\n\
      \            )\n            try:\n                asyncio.run(\n           \
      \         llm.achat(\"This is an LLM connectivity test. Say Hello World\")\n\
      \                )\n                logger.info(\"LLM Config Params Validated\"\
      )\n            except Exception as e:  # noqa: BLE001\n                logger.error(f\"\
      LLM configuration error detected.\\n{e}\")  # noqa\n                print(f\"\
      Failed to validate language model ({id}) params\", e)  # noqa: T201\n      \
      \          sys.exit(1)\n        elif config.type in [\"embedding\", \"azure_openai_embedding\"\
      , \"openai_embedding\"]:\n            embed_llm = ModelManager().register_embedding(\n\
      \                name=\"test-embed-llm\",\n                model_type=config.type,\n\
      \                config=config,\n                callbacks=NoopWorkflowCallbacks(),\n\
      \                cache=None,\n            )\n            try:\n            \
      \    asyncio.run(\n                    embed_llm.aembed_batch([\"This is an\
      \ LLM Embedding Test String\"])\n                )\n                logger.info(\"\
      Embedding LLM Config Params Validated\")\n            except Exception as e:\
      \  # noqa: BLE001\n                logger.error(f\"Embedding configuration error\
      \ detected.\\n{e}\")  # noqa\n                print(f\"Failed to validate embedding\
      \ model ({id}) params\", e)  # noqa: T201\n                sys.exit(1)"
    signature: 'def validate_config_names(parameters: GraphRagConfig) -> None'
    decorators: []
    raises: []
    calls:
    - target: parameters.models.items
      type: unresolved
    - target: ModelManager().register_chat
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: asyncio::run
      type: stdlib
    - target: llm.achat
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: logger.error
      type: unresolved
    - target: print
      type: builtin
    - target: sys::exit
      type: stdlib
    - target: ModelManager().register_embedding
      type: unresolved
    - target: embed_llm.aembed_batch
      type: unresolved
    visibility: public
    node_id: graphrag/index/validate_config.py::validate_config_names
    called_by:
    - source: graphrag/cli/index.py::_run_index
      type: internal
- file_name: graphrag/index/workflows/__init__.py
  imports:
  - module: graphrag.index.workflows.factory
    name: PipelineFactory
    alias: null
  functions: []
- file_name: graphrag/index/workflows/create_base_text_units.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkStrategyType
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.operations.chunk_text.chunk_text
    name: chunk_text
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: get_encoding_fn
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.utils.hashing
    name: gen_sha512_hash
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 25
    end_line: 50
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform base text_units.\"\
      \"\"\n    logger.info(\"Workflow started: create_base_text_units\")\n    documents\
      \ = await load_table_from_storage(\"documents\", context.output_storage)\n\n\
      \    chunks = config.chunks\n\n    output = create_base_text_units(\n      \
      \  documents,\n        context.callbacks,\n        chunks.group_by_columns,\n\
      \        chunks.size,\n        chunks.overlap,\n        chunks.encoding_model,\n\
      \        strategy=chunks.strategy,\n        prepend_metadata=chunks.prepend_metadata,\n\
      \        chunk_size_includes_metadata=chunks.chunk_size_includes_metadata,\n\
      \    )\n\n    await write_table_to_storage(output, \"text_units\", context.output_storage)\n\
      \n    logger.info(\"Workflow completed: create_base_text_units\")\n    return\
      \ WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_base_text_units.py::run_workflow
    called_by:
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
  - name: create_base_text_units
    start_line: 53
    end_line: 163
    code: "def create_base_text_units(\n    documents: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    group_by_columns: list[str],\n    size: int,\n   \
      \ overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    prepend_metadata: bool = False,\n    chunk_size_includes_metadata: bool\
      \ = False,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform base text_units.\"\
      \"\"\n    sort = documents.sort_values(by=[\"id\"], ascending=[True])\n\n  \
      \  sort[\"text_with_ids\"] = list(\n        zip(*[sort[col] for col in [\"id\"\
      , \"text\"]], strict=True)\n    )\n\n    agg_dict = {\"text_with_ids\": list}\n\
      \    if \"metadata\" in documents:\n        agg_dict[\"metadata\"] = \"first\"\
      \  # type: ignore\n\n    aggregated = (\n        (\n            sort.groupby(group_by_columns,\
      \ sort=False)\n            if len(group_by_columns) > 0\n            else sort.groupby(lambda\
      \ _x: True)\n        )\n        .agg(agg_dict)\n        .reset_index()\n   \
      \ )\n    aggregated.rename(columns={\"text_with_ids\": \"texts\"}, inplace=True)\n\
      \n    def chunker(row: pd.Series) -> Any:\n        line_delimiter = \".\\n\"\
      \n        metadata_str = \"\"\n        metadata_tokens = 0\n\n        if prepend_metadata\
      \ and \"metadata\" in row:\n            metadata = row[\"metadata\"]\n     \
      \       if isinstance(metadata, str):\n                metadata = json.loads(metadata)\n\
      \            if isinstance(metadata, dict):\n                metadata_str =\
      \ (\n                    line_delimiter.join(f\"{k}: {v}\" for k, v in metadata.items())\n\
      \                    + line_delimiter\n                )\n\n            if chunk_size_includes_metadata:\n\
      \                encode, _ = get_encoding_fn(encoding_model)\n             \
      \   metadata_tokens = len(encode(metadata_str))\n                if metadata_tokens\
      \ >= size:\n                    message = \"Metadata tokens exceeds the maximum\
      \ tokens per chunk. Please increase the tokens per chunk.\"\n              \
      \      raise ValueError(message)\n\n        chunked = chunk_text(\n        \
      \    pd.DataFrame([row]).reset_index(drop=True),\n            column=\"texts\"\
      ,\n            size=size - metadata_tokens,\n            overlap=overlap,\n\
      \            encoding_model=encoding_model,\n            strategy=strategy,\n\
      \            callbacks=callbacks,\n        )[0]\n\n        if prepend_metadata:\n\
      \            for index, chunk in enumerate(chunked):\n                if isinstance(chunk,\
      \ str):\n                    chunked[index] = metadata_str + chunk\n       \
      \         else:\n                    chunked[index] = (\n                  \
      \      (chunk[0], metadata_str + chunk[1], chunk[2]) if chunk else None\n  \
      \                  )\n\n        row[\"chunks\"] = chunked\n        return row\n\
      \n    # Track progress of row-wise apply operation\n    total_rows = len(aggregated)\n\
      \    logger.info(\"Starting chunking process for %d documents\", total_rows)\n\
      \n    def chunker_with_logging(row: pd.Series, row_index: int) -> Any:\n   \
      \     \"\"\"Add logging to chunker execution.\"\"\"\n        result = chunker(row)\n\
      \        logger.info(\"chunker progress:  %d/%d\", row_index + 1, total_rows)\n\
      \        return result\n\n    aggregated = aggregated.apply(\n        lambda\
      \ row: chunker_with_logging(row, row.name), axis=1\n    )\n\n    aggregated\
      \ = cast(\"pd.DataFrame\", aggregated[[*group_by_columns, \"chunks\"]])\n  \
      \  aggregated = aggregated.explode(\"chunks\")\n    aggregated.rename(\n   \
      \     columns={\n            \"chunks\": \"chunk\",\n        },\n        inplace=True,\n\
      \    )\n    aggregated[\"id\"] = aggregated.apply(\n        lambda row: gen_sha512_hash(row,\
      \ [\"chunk\"]), axis=1\n    )\n    aggregated[[\"document_ids\", \"chunk\",\
      \ \"n_tokens\"]] = pd.DataFrame(\n        aggregated[\"chunk\"].tolist(), index=aggregated.index\n\
      \    )\n    # rename for downstream consumption\n    aggregated.rename(columns={\"\
      chunk\": \"text\"}, inplace=True)\n\n    return cast(\n        \"pd.DataFrame\"\
      , aggregated[aggregated[\"text\"].notna()].reset_index(drop=True)\n    )"
    signature: "def create_base_text_units(\n    documents: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    group_by_columns: list[str],\n    size: int,\n   \
      \ overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    prepend_metadata: bool = False,\n    chunk_size_includes_metadata: bool\
      \ = False,\n) -> pd.DataFrame"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: documents.sort_values
      type: unresolved
    - target: list
      type: builtin
    - target: zip
      type: builtin
    - target: "(\n            sort.groupby(group_by_columns, sort=False)\n       \
        \     if len(group_by_columns) > 0\n            else sort.groupby(lambda _x:\
        \ True)\n        )\n        .agg(agg_dict)\n        .reset_index"
      type: unresolved
    - target: "(\n            sort.groupby(group_by_columns, sort=False)\n       \
        \     if len(group_by_columns) > 0\n            else sort.groupby(lambda _x:\
        \ True)\n        )\n        .agg"
      type: unresolved
    - target: sort.groupby
      type: unresolved
    - target: len
      type: builtin
    - target: aggregated.rename
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: aggregated.apply
      type: unresolved
    - target: graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
      type: internal
    - target: typing::cast
      type: stdlib
    - target: aggregated.explode
      type: unresolved
    - target: graphrag/index/utils/hashing.py::gen_sha512_hash
      type: internal
    - target: pandas::DataFrame
      type: external
    - target: aggregated["chunk"].tolist
      type: unresolved
    - target: aggregated[aggregated["text"].notna()].reset_index
      type: unresolved
    - target: aggregated["text"].notna
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
    called_by:
    - source: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - source: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
      type: internal
  - name: chunker
    start_line: 86
    end_line: 128
    code: "def chunker(row: pd.Series) -> Any:\n        line_delimiter = \".\\n\"\n\
      \        metadata_str = \"\"\n        metadata_tokens = 0\n\n        if prepend_metadata\
      \ and \"metadata\" in row:\n            metadata = row[\"metadata\"]\n     \
      \       if isinstance(metadata, str):\n                metadata = json.loads(metadata)\n\
      \            if isinstance(metadata, dict):\n                metadata_str =\
      \ (\n                    line_delimiter.join(f\"{k}: {v}\" for k, v in metadata.items())\n\
      \                    + line_delimiter\n                )\n\n            if chunk_size_includes_metadata:\n\
      \                encode, _ = get_encoding_fn(encoding_model)\n             \
      \   metadata_tokens = len(encode(metadata_str))\n                if metadata_tokens\
      \ >= size:\n                    message = \"Metadata tokens exceeds the maximum\
      \ tokens per chunk. Please increase the tokens per chunk.\"\n              \
      \      raise ValueError(message)\n\n        chunked = chunk_text(\n        \
      \    pd.DataFrame([row]).reset_index(drop=True),\n            column=\"texts\"\
      ,\n            size=size - metadata_tokens,\n            overlap=overlap,\n\
      \            encoding_model=encoding_model,\n            strategy=strategy,\n\
      \            callbacks=callbacks,\n        )[0]\n\n        if prepend_metadata:\n\
      \            for index, chunk in enumerate(chunked):\n                if isinstance(chunk,\
      \ str):\n                    chunked[index] = metadata_str + chunk\n       \
      \         else:\n                    chunked[index] = (\n                  \
      \      (chunk[0], metadata_str + chunk[1], chunk[2]) if chunk else None\n  \
      \                  )\n\n        row[\"chunks\"] = chunked\n        return row"
    signature: 'def chunker(row: pd.Series) -> Any'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: isinstance
      type: builtin
    - target: json::loads
      type: stdlib
    - target: line_delimiter.join
      type: unresolved
    - target: metadata.items
      type: unresolved
    - target: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
      type: internal
    - target: len
      type: builtin
    - target: encode
      type: ambiguous
      candidates: &id002
      - graphrag/index/operations/chunk_text/strategies.py::encode
      - graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.encode
      - graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.encode
      - graphrag/tokenizer/tokenizer.py::Tokenizer.encode
      - tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.encode
      - tests/unit/indexing/text_splitting/test_text_splitting.py::encode
    - target: ValueError
      type: builtin
    - target: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - target: pandas::DataFrame([row]).reset_index
      type: external
    - target: pandas::DataFrame
      type: external
    - target: enumerate
      type: builtin
    visibility: public
    node_id: graphrag/index/workflows/create_base_text_units.py::chunker
    called_by:
    - source: graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
      type: internal
  - name: chunker_with_logging
    start_line: 134
    end_line: 138
    code: "def chunker_with_logging(row: pd.Series, row_index: int) -> Any:\n    \
      \    \"\"\"Add logging to chunker execution.\"\"\"\n        result = chunker(row)\n\
      \        logger.info(\"chunker progress:  %d/%d\", row_index + 1, total_rows)\n\
      \        return result"
    signature: 'def chunker_with_logging(row: pd.Series, row_index: int) -> Any'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/create_base_text_units.py::chunker
      type: internal
    - target: logger.info
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
    called_by:
    - source: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
      type: internal
- file_name: graphrag/index/workflows/create_communities.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: datetime
    name: timezone
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: uuid
    name: uuid4
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.schemas
    name: COMMUNITIES_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.operations.cluster_graph
    name: cluster_graph
    alias: null
  - module: graphrag.index.operations.create_graph
    name: create_graph
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 25
    end_line: 51
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform final communities.\"\
      \"\"\n    logger.info(\"Workflow started: create_communities\")\n    entities\
      \ = await load_table_from_storage(\"entities\", context.output_storage)\n  \
      \  relationships = await load_table_from_storage(\n        \"relationships\"\
      , context.output_storage\n    )\n\n    max_cluster_size = config.cluster_graph.max_cluster_size\n\
      \    use_lcc = config.cluster_graph.use_lcc\n    seed = config.cluster_graph.seed\n\
      \n    output = create_communities(\n        entities,\n        relationships,\n\
      \        max_cluster_size=max_cluster_size,\n        use_lcc=use_lcc,\n    \
      \    seed=seed,\n    )\n\n    await write_table_to_storage(output, \"communities\"\
      , context.output_storage)\n\n    logger.info(\"Workflow completed: create_communities\"\
      )\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/create_communities.py::create_communities
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_communities.py::run_workflow
    called_by:
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
  - name: create_communities
    start_line: 54
    end_line: 156
    code: "def create_communities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int\
      \ | None = None,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform final\
      \ communities.\"\"\"\n    graph = create_graph(relationships, edge_attr=[\"\
      weight\"])\n\n    clusters = cluster_graph(\n        graph,\n        max_cluster_size,\n\
      \        use_lcc,\n        seed=seed,\n    )\n\n    communities = pd.DataFrame(\n\
      \        clusters, columns=pd.Index([\"level\", \"community\", \"parent\", \"\
      title\"])\n    ).explode(\"title\")\n    communities[\"community\"] = communities[\"\
      community\"].astype(int)\n\n    # aggregate entity ids for each community\n\
      \    entity_ids = communities.merge(entities, on=\"title\", how=\"inner\")\n\
      \    entity_ids = (\n        entity_ids.groupby(\"community\").agg(entity_ids=(\"\
      id\", list)).reset_index()\n    )\n\n    # aggregate relationships ids for each\
      \ community\n    # these are limited to only those where the source and target\
      \ are in the same community\n    max_level = communities[\"level\"].max()\n\
      \    all_grouped = pd.DataFrame(\n        columns=[\"community\", \"level\"\
      , \"relationship_ids\", \"text_unit_ids\"]  # type: ignore\n    )\n    for level\
      \ in range(max_level + 1):\n        communities_at_level = communities.loc[communities[\"\
      level\"] == level]\n        sources = relationships.merge(\n            communities_at_level,\
      \ left_on=\"source\", right_on=\"title\", how=\"inner\"\n        )\n       \
      \ targets = sources.merge(\n            communities_at_level, left_on=\"target\"\
      , right_on=\"title\", how=\"inner\"\n        )\n        matched = targets.loc[targets[\"\
      community_x\"] == targets[\"community_y\"]]\n        text_units = matched.explode(\"\
      text_unit_ids\")\n        grouped = (\n            text_units.groupby([\"community_x\"\
      , \"level_x\", \"parent_x\"])\n            .agg(relationship_ids=(\"id\", list),\
      \ text_unit_ids=(\"text_unit_ids\", list))\n            .reset_index()\n   \
      \     )\n        grouped.rename(\n            columns={\n                \"\
      community_x\": \"community\",\n                \"level_x\": \"level\",\n   \
      \             \"parent_x\": \"parent\",\n            },\n            inplace=True,\n\
      \        )\n        all_grouped = pd.concat([\n            all_grouped,\n  \
      \          grouped.loc[\n                :, [\"community\", \"level\", \"parent\"\
      , \"relationship_ids\", \"text_unit_ids\"]\n            ],\n        ])\n\n \
      \   # deduplicate the lists\n    all_grouped[\"relationship_ids\"] = all_grouped[\"\
      relationship_ids\"].apply(\n        lambda x: sorted(set(x))\n    )\n    all_grouped[\"\
      text_unit_ids\"] = all_grouped[\"text_unit_ids\"].apply(\n        lambda x:\
      \ sorted(set(x))\n    )\n\n    # join it all up and add some new fields\n  \
      \  final_communities = all_grouped.merge(entity_ids, on=\"community\", how=\"\
      inner\")\n    final_communities[\"id\"] = [str(uuid4()) for _ in range(len(final_communities))]\n\
      \    final_communities[\"human_readable_id\"] = final_communities[\"community\"\
      ]\n    final_communities[\"title\"] = \"Community \" + final_communities[\"\
      community\"].astype(\n        str\n    )\n    final_communities[\"parent\"]\
      \ = final_communities[\"parent\"].astype(int)\n    # collect the children so\
      \ we have a tree going both ways\n    parent_grouped = cast(\n        \"pd.DataFrame\"\
      ,\n        final_communities.groupby(\"parent\").agg(children=(\"community\"\
      , \"unique\")),\n    )\n    final_communities = final_communities.merge(\n \
      \       parent_grouped,\n        left_on=\"community\",\n        right_on=\"\
      parent\",\n        how=\"left\",\n    )\n    # replace NaN children with empty\
      \ list\n    final_communities[\"children\"] = final_communities[\"children\"\
      ].apply(\n        lambda x: x if isinstance(x, np.ndarray) else []  # type:\
      \ ignore\n    )\n    # add fields for incremental update tracking\n    final_communities[\"\
      period\"] = datetime.now(timezone.utc).date().isoformat()\n    final_communities[\"\
      size\"] = final_communities.loc[:, \"entity_ids\"].apply(len)\n\n    return\
      \ final_communities.loc[\n        :,\n        COMMUNITIES_FINAL_COLUMNS,\n \
      \   ]"
    signature: "def create_communities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int\
      \ | None = None,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/create_graph.py::create_graph
      type: internal
    - target: graphrag/index/operations/cluster_graph.py::cluster_graph
      type: internal
    - target: "pandas::DataFrame(\n        clusters, columns=pd.Index([\"level\",\
        \ \"community\", \"parent\", \"title\"])\n    ).explode"
      type: external
    - target: pandas::DataFrame
      type: external
    - target: pandas::Index
      type: external
    - target: communities["community"].astype
      type: unresolved
    - target: communities.merge
      type: unresolved
    - target: entity_ids.groupby("community").agg(entity_ids=("id", list)).reset_index
      type: unresolved
    - target: entity_ids.groupby("community").agg
      type: unresolved
    - target: entity_ids.groupby
      type: unresolved
    - target: communities["level"].max
      type: unresolved
    - target: range
      type: builtin
    - target: relationships.merge
      type: unresolved
    - target: sources.merge
      type: unresolved
    - target: matched.explode
      type: unresolved
    - target: "text_units.groupby([\"community_x\", \"level_x\", \"parent_x\"])\n\
        \            .agg(relationship_ids=(\"id\", list), text_unit_ids=(\"text_unit_ids\"\
        , list))\n            .reset_index"
      type: unresolved
    - target: "text_units.groupby([\"community_x\", \"level_x\", \"parent_x\"])\n\
        \            .agg"
      type: unresolved
    - target: text_units.groupby
      type: unresolved
    - target: grouped.rename
      type: unresolved
    - target: pandas::concat
      type: external
    - target: all_grouped["relationship_ids"].apply
      type: unresolved
    - target: sorted
      type: builtin
    - target: set
      type: builtin
    - target: all_grouped["text_unit_ids"].apply
      type: unresolved
    - target: all_grouped.merge
      type: unresolved
    - target: str
      type: builtin
    - target: uuid::uuid4
      type: stdlib
    - target: len
      type: builtin
    - target: final_communities["community"].astype
      type: unresolved
    - target: final_communities["parent"].astype
      type: unresolved
    - target: typing::cast
      type: stdlib
    - target: final_communities.groupby("parent").agg
      type: unresolved
    - target: final_communities.groupby
      type: unresolved
    - target: final_communities.merge
      type: unresolved
    - target: final_communities["children"].apply
      type: unresolved
    - target: isinstance
      type: builtin
    - target: datetime::datetime::now(timezone.utc).date().isoformat
      type: external
    - target: datetime::datetime::now(timezone.utc).date
      type: external
    - target: datetime::datetime::now
      type: external
    - target: final_communities.loc[:, "entity_ids"].apply
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/create_communities.py::create_communities
    called_by:
    - source: graphrag/index/workflows/create_communities.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/create_community_reports.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.schemas
    name: null
    alias: schemas
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.finalize_community_reports
    name: finalize_community_reports
    alias: null
  - module: graphrag.index.operations.summarize_communities.explode_communities
    name: explode_communities
    alias: null
  - module: graphrag.index.operations.summarize_communities.graph_context.context_builder
    name: build_level_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.graph_context.context_builder
    name: build_local_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.summarize_communities
    name: summarize_communities
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: storage_has_table
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 42
    end_line: 81
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform community\
      \ reports.\"\"\"\n    logger.info(\"Workflow started: create_community_reports\"\
      )\n    edges = await load_table_from_storage(\"relationships\", context.output_storage)\n\
      \    entities = await load_table_from_storage(\"entities\", context.output_storage)\n\
      \    communities = await load_table_from_storage(\"communities\", context.output_storage)\n\
      \    claims = None\n    if config.extract_claims.enabled and await storage_has_table(\n\
      \        \"covariates\", context.output_storage\n    ):\n        claims = await\
      \ load_table_from_storage(\"covariates\", context.output_storage)\n\n    community_reports_llm_settings\
      \ = config.get_language_model_config(\n        config.community_reports.model_id\n\
      \    )\n    async_mode = community_reports_llm_settings.async_mode\n    num_threads\
      \ = community_reports_llm_settings.concurrent_requests\n    summarization_strategy\
      \ = config.community_reports.resolved_strategy(\n        config.root_dir, community_reports_llm_settings\n\
      \    )\n\n    output = await create_community_reports(\n        edges_input=edges,\n\
      \        entities=entities,\n        communities=communities,\n        claims_input=claims,\n\
      \        callbacks=context.callbacks,\n        cache=context.cache,\n      \
      \  summarization_strategy=summarization_strategy,\n        async_mode=async_mode,\n\
      \        num_threads=num_threads,\n    )\n\n    await write_table_to_storage(output,\
      \ \"community_reports\", context.output_storage)\n\n    logger.info(\"Workflow\
      \ completed: create_community_reports\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/utils/storage.py::storage_has_table
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: config.community_reports.resolved_strategy
      type: unresolved
    - target: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_community_reports.py::run_workflow
    called_by:
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
  - name: create_community_reports
    start_line: 84
    end_line: 137
    code: "async def create_community_reports(\n    edges_input: pd.DataFrame,\n \
      \   entities: pd.DataFrame,\n    communities: pd.DataFrame,\n    claims_input:\
      \ pd.DataFrame | None,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    summarization_strategy: dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    num_threads: int = 4,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform\
      \ community reports.\"\"\"\n    nodes = explode_communities(communities, entities)\n\
      \n    nodes = _prep_nodes(nodes)\n    edges = _prep_edges(edges_input)\n\n \
      \   claims = None\n    if claims_input is not None:\n        claims = _prep_claims(claims_input)\n\
      \n    summarization_strategy[\"extraction_prompt\"] = summarization_strategy[\"\
      graph_prompt\"]\n\n    model_config = LanguageModelConfig(**summarization_strategy[\"\
      llm\"])\n    tokenizer = get_tokenizer(model_config)\n\n    max_input_length\
      \ = summarization_strategy.get(\n        \"max_input_length\", graphrag_config_defaults.community_reports.max_input_length\n\
      \    )\n\n    local_contexts = build_local_context(\n        nodes,\n      \
      \  edges,\n        claims,\n        tokenizer,\n        callbacks,\n       \
      \ max_input_length,\n    )\n\n    community_reports = await summarize_communities(\n\
      \        nodes,\n        communities,\n        local_contexts,\n        build_level_context,\n\
      \        callbacks,\n        cache,\n        summarization_strategy,\n     \
      \   tokenizer=tokenizer,\n        max_input_length=max_input_length,\n     \
      \   async_mode=async_mode,\n        num_threads=num_threads,\n    )\n\n    return\
      \ finalize_community_reports(community_reports, communities)"
    signature: "def create_community_reports(\n    edges_input: pd.DataFrame,\n  \
      \  entities: pd.DataFrame,\n    communities: pd.DataFrame,\n    claims_input:\
      \ pd.DataFrame | None,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    summarization_strategy: dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    num_threads: int = 4,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
      type: internal
    - target: graphrag/index/workflows/create_community_reports.py::_prep_nodes
      type: internal
    - target: graphrag/index/workflows/create_community_reports.py::_prep_edges
      type: internal
    - target: graphrag/index/workflows/create_community_reports.py::_prep_claims
      type: internal
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: summarization_strategy.get
      type: unresolved
    - target: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
      type: internal
    - target: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
    - target: graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_community_reports.py::create_community_reports
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::run_workflow
      type: internal
  - name: _prep_nodes
    start_line: 140
    end_line: 158
    code: "def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Prepare\
      \ nodes by filtering, filling missing descriptions, and creating NODE_DETAILS.\"\
      \"\"\n    # Fill missing values in DESCRIPTION\n    input.loc[:, schemas.DESCRIPTION]\
      \ = input.loc[:, schemas.DESCRIPTION].fillna(\n        \"No Description\"\n\
      \    )\n\n    # Create NODE_DETAILS column\n    input.loc[:, schemas.NODE_DETAILS]\
      \ = input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n     \
      \       schemas.TITLE,\n            schemas.DESCRIPTION,\n            schemas.NODE_DEGREE,\n\
      \        ],\n    ].to_dict(orient=\"records\")\n\n    return input"
    signature: 'def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: input.loc[:, schemas.DESCRIPTION].fillna
      type: unresolved
    - target: "input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n\
        \            schemas.TITLE,\n            schemas.DESCRIPTION,\n          \
        \  schemas.NODE_DEGREE,\n        ],\n    ].to_dict"
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_community_reports.py::_prep_nodes
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
  - name: _prep_edges
    start_line: 161
    end_line: 177
    code: "def _prep_edges(input: pd.DataFrame) -> pd.DataFrame:\n    # Fill missing\
      \ DESCRIPTION\n    input.fillna(value={schemas.DESCRIPTION: \"No Description\"\
      }, inplace=True)\n\n    # Create EDGE_DETAILS column\n    input.loc[:, schemas.EDGE_DETAILS]\
      \ = input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n     \
      \       schemas.EDGE_SOURCE,\n            schemas.EDGE_TARGET,\n           \
      \ schemas.DESCRIPTION,\n            schemas.EDGE_DEGREE,\n        ],\n    ].to_dict(orient=\"\
      records\")\n\n    return input"
    signature: 'def _prep_edges(input: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: input.fillna
      type: unresolved
    - target: "input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n\
        \            schemas.EDGE_SOURCE,\n            schemas.EDGE_TARGET,\n    \
        \        schemas.DESCRIPTION,\n            schemas.EDGE_DEGREE,\n        ],\n\
        \    ].to_dict"
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_community_reports.py::_prep_edges
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
  - name: _prep_claims
    start_line: 180
    end_line: 196
    code: "def _prep_claims(input: pd.DataFrame) -> pd.DataFrame:\n    # Fill missing\
      \ DESCRIPTION\n    input.fillna(value={schemas.DESCRIPTION: \"No Description\"\
      }, inplace=True)\n\n    # Create CLAIM_DETAILS column\n    input.loc[:, schemas.CLAIM_DETAILS]\
      \ = input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n     \
      \       schemas.CLAIM_SUBJECT,\n            schemas.TYPE,\n            schemas.CLAIM_STATUS,\n\
      \            schemas.DESCRIPTION,\n        ],\n    ].to_dict(orient=\"records\"\
      )\n\n    return input"
    signature: 'def _prep_claims(input: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: input.fillna
      type: unresolved
    - target: "input.loc[\n        :,\n        [\n            schemas.SHORT_ID,\n\
        \            schemas.CLAIM_SUBJECT,\n            schemas.TYPE,\n         \
        \   schemas.CLAIM_STATUS,\n            schemas.DESCRIPTION,\n        ],\n\
        \    ].to_dict"
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_community_reports.py::_prep_claims
    called_by:
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
- file_name: graphrag/index/workflows/create_community_reports_text.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.defaults
    name: graphrag_config_defaults
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.operations.finalize_community_reports
    name: finalize_community_reports
    alias: null
  - module: graphrag.index.operations.summarize_communities.explode_communities
    name: explode_communities
    alias: null
  - module: graphrag.index.operations.summarize_communities.summarize_communities
    name: summarize_communities
    alias: null
  - module: graphrag.index.operations.summarize_communities.text_unit_context.context_builder
    name: build_level_context
    alias: null
  - module: graphrag.index.operations.summarize_communities.text_unit_context.context_builder
    name: build_local_context
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 37
    end_line: 71
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform community\
      \ reports.\"\"\"\n    logger.info(\"Workflow started: create_community_reports_text\"\
      )\n    entities = await load_table_from_storage(\"entities\", context.output_storage)\n\
      \    communities = await load_table_from_storage(\"communities\", context.output_storage)\n\
      \n    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \n    community_reports_llm_settings = config.get_language_model_config(\n \
      \       config.community_reports.model_id\n    )\n    async_mode = community_reports_llm_settings.async_mode\n\
      \    num_threads = community_reports_llm_settings.concurrent_requests\n    summarization_strategy\
      \ = config.community_reports.resolved_strategy(\n        config.root_dir, community_reports_llm_settings\n\
      \    )\n\n    output = await create_community_reports_text(\n        entities,\n\
      \        communities,\n        text_units,\n        context.callbacks,\n   \
      \     context.cache,\n        summarization_strategy,\n        async_mode=async_mode,\n\
      \        num_threads=num_threads,\n    )\n\n    await write_table_to_storage(output,\
      \ \"community_reports\", context.output_storage)\n\n    logger.info(\"Workflow\
      \ completed: create_community_reports_text\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: config.community_reports.resolved_strategy
      type: unresolved
    - target: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_community_reports_text.py::run_workflow
    called_by: []
  - name: create_community_reports_text
    start_line: 74
    end_line: 114
    code: "async def create_community_reports_text(\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n    num_threads: int\
      \ = 4,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform community reports.\"\
      \"\"\n    nodes = explode_communities(communities, entities)\n\n    summarization_strategy[\"\
      extraction_prompt\"] = summarization_strategy[\"text_prompt\"]\n\n    max_input_length\
      \ = summarization_strategy.get(\n        \"max_input_length\", graphrag_config_defaults.community_reports.max_input_length\n\
      \    )\n\n    model_config = LanguageModelConfig(**summarization_strategy[\"\
      llm\"])\n    tokenizer = get_tokenizer(model_config)\n\n    local_contexts =\
      \ build_local_context(\n        communities, text_units, nodes, tokenizer, max_input_length\n\
      \    )\n\n    community_reports = await summarize_communities(\n        nodes,\n\
      \        communities,\n        local_contexts,\n        build_level_context,\n\
      \        callbacks,\n        cache,\n        summarization_strategy,\n     \
      \   tokenizer=tokenizer,\n        max_input_length=max_input_length,\n     \
      \   async_mode=async_mode,\n        num_threads=num_threads,\n    )\n\n    return\
      \ finalize_community_reports(community_reports, communities)"
    signature: "def create_community_reports_text(\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n    num_threads: int\
      \ = 4,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
      type: internal
    - target: summarization_strategy.get
      type: unresolved
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
      type: internal
    - target: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
    - target: graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
    called_by:
    - source: graphrag/index/workflows/create_community_reports_text.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/create_final_documents.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.schemas
    name: DOCUMENTS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 19
    end_line: 33
    code: "async def run_workflow(\n    _config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform final documents.\"\
      \"\"\n    logger.info(\"Workflow started: create_final_documents\")\n    documents\
      \ = await load_table_from_storage(\"documents\", context.output_storage)\n \
      \   text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \n    output = create_final_documents(documents, text_units)\n\n    await write_table_to_storage(output,\
      \ \"documents\", context.output_storage)\n\n    logger.info(\"Workflow completed:\
      \ create_final_documents\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    _config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/create_final_documents.py::create_final_documents
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_final_documents.py::run_workflow
    called_by:
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
  - name: create_final_documents
    start_line: 36
    end_line: 77
    code: "def create_final_documents(\n    documents: pd.DataFrame, text_units: pd.DataFrame\n\
      ) -> pd.DataFrame:\n    \"\"\"All the steps to transform final documents.\"\"\
      \"\n    exploded = (\n        text_units.explode(\"document_ids\")\n       \
      \ .loc[:, [\"id\", \"document_ids\", \"text\"]]\n        .rename(\n        \
      \    columns={\n                \"document_ids\": \"chunk_doc_id\",\n      \
      \          \"id\": \"chunk_id\",\n                \"text\": \"chunk_text\",\n\
      \            }\n        )\n    )\n\n    joined = exploded.merge(\n        documents,\n\
      \        left_on=\"chunk_doc_id\",\n        right_on=\"id\",\n        how=\"\
      inner\",\n        copy=False,\n    )\n\n    docs_with_text_units = joined.groupby(\"\
      id\", sort=False).agg(\n        text_unit_ids=(\"chunk_id\", list)\n    )\n\n\
      \    rejoined = docs_with_text_units.merge(\n        documents,\n        on=\"\
      id\",\n        how=\"right\",\n        copy=False,\n    ).reset_index(drop=True)\n\
      \n    rejoined[\"id\"] = rejoined[\"id\"].astype(str)\n    rejoined[\"human_readable_id\"\
      ] = rejoined.index\n\n    if \"metadata\" not in rejoined.columns:\n       \
      \ rejoined[\"metadata\"] = pd.Series(dtype=\"object\")\n\n    return rejoined.loc[:,\
      \ DOCUMENTS_FINAL_COLUMNS]"
    signature: "def create_final_documents(\n    documents: pd.DataFrame, text_units:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: "text_units.explode(\"document_ids\")\n        .loc[:, [\"id\", \"document_ids\"\
        , \"text\"]]\n        .rename"
      type: unresolved
    - target: text_units.explode
      type: unresolved
    - target: exploded.merge
      type: unresolved
    - target: joined.groupby("id", sort=False).agg
      type: unresolved
    - target: joined.groupby
      type: unresolved
    - target: "docs_with_text_units.merge(\n        documents,\n        on=\"id\"\
        ,\n        how=\"right\",\n        copy=False,\n    ).reset_index"
      type: unresolved
    - target: docs_with_text_units.merge
      type: unresolved
    - target: rejoined["id"].astype
      type: unresolved
    - target: pandas::Series
      type: external
    visibility: public
    node_id: graphrag/index/workflows/create_final_documents.py::create_final_documents
    called_by:
    - source: graphrag/index/workflows/create_final_documents.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/create_final_text_units.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.schemas
    name: TEXT_UNITS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: storage_has_table
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 23
    end_line: 52
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform the text\
      \ units.\"\"\"\n    logger.info(\"Workflow started: create_final_text_units\"\
      )\n    text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \    final_entities = await load_table_from_storage(\"entities\", context.output_storage)\n\
      \    final_relationships = await load_table_from_storage(\n        \"relationships\"\
      , context.output_storage\n    )\n    final_covariates = None\n    if config.extract_claims.enabled\
      \ and await storage_has_table(\n        \"covariates\", context.output_storage\n\
      \    ):\n        final_covariates = await load_table_from_storage(\n       \
      \     \"covariates\", context.output_storage\n        )\n\n    output = create_final_text_units(\n\
      \        text_units,\n        final_entities,\n        final_relationships,\n\
      \        final_covariates,\n    )\n\n    await write_table_to_storage(output,\
      \ \"text_units\", context.output_storage)\n\n    logger.info(\"Workflow completed:\
      \ create_final_text_units\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/utils/storage.py::storage_has_table
      type: internal
    - target: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/create_final_text_units.py::run_workflow
    called_by:
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
  - name: create_final_text_units
    start_line: 55
    end_line: 83
    code: "def create_final_text_units(\n    text_units: pd.DataFrame,\n    final_entities:\
      \ pd.DataFrame,\n    final_relationships: pd.DataFrame,\n    final_covariates:\
      \ pd.DataFrame | None,\n) -> pd.DataFrame:\n    \"\"\"All the steps to transform\
      \ the text units.\"\"\"\n    selected = text_units.loc[:, [\"id\", \"text\"\
      , \"document_ids\", \"n_tokens\"]]\n    selected[\"human_readable_id\"] = selected.index\n\
      \n    entity_join = _entities(final_entities)\n    relationship_join = _relationships(final_relationships)\n\
      \n    entity_joined = _join(selected, entity_join)\n    relationship_joined\
      \ = _join(entity_joined, relationship_join)\n    final_joined = relationship_joined\n\
      \n    if final_covariates is not None:\n        covariate_join = _covariates(final_covariates)\n\
      \        final_joined = _join(relationship_joined, covariate_join)\n    else:\n\
      \        final_joined[\"covariate_ids\"] = [[] for i in range(len(final_joined))]\n\
      \n    aggregated = final_joined.groupby(\"id\", sort=False).agg(\"first\").reset_index()\n\
      \n    return aggregated.loc[\n        :,\n        TEXT_UNITS_FINAL_COLUMNS,\n\
      \    ]"
    signature: "def create_final_text_units(\n    text_units: pd.DataFrame,\n    final_entities:\
      \ pd.DataFrame,\n    final_relationships: pd.DataFrame,\n    final_covariates:\
      \ pd.DataFrame | None,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/create_final_text_units.py::_entities
      type: internal
    - target: graphrag/index/workflows/create_final_text_units.py::_relationships
      type: internal
    - target: graphrag/index/workflows/create_final_text_units.py::_join
      type: internal
    - target: graphrag/index/workflows/create_final_text_units.py::_covariates
      type: internal
    - target: range
      type: builtin
    - target: len
      type: builtin
    - target: final_joined.groupby("id", sort=False).agg("first").reset_index
      type: unresolved
    - target: final_joined.groupby("id", sort=False).agg
      type: unresolved
    - target: final_joined.groupby
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
    called_by:
    - source: graphrag/index/workflows/create_final_text_units.py::run_workflow
      type: internal
  - name: _entities
    start_line: 86
    end_line: 95
    code: "def _entities(df: pd.DataFrame) -> pd.DataFrame:\n    selected = df.loc[:,\
      \ [\"id\", \"text_unit_ids\"]]\n    unrolled = selected.explode([\"text_unit_ids\"\
      ]).reset_index(drop=True)\n\n    return (\n        unrolled.groupby(\"text_unit_ids\"\
      , sort=False)\n        .agg(entity_ids=(\"id\", \"unique\"))\n        .reset_index()\n\
      \        .rename(columns={\"text_unit_ids\": \"id\"})\n    )"
    signature: 'def _entities(df: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: selected.explode(["text_unit_ids"]).reset_index
      type: unresolved
    - target: selected.explode
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg(entity_ids=(\"\
        id\", \"unique\"))\n        .reset_index()\n        .rename"
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg(entity_ids=(\"\
        id\", \"unique\"))\n        .reset_index"
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg"
      type: unresolved
    - target: unrolled.groupby
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_final_text_units.py::_entities
    called_by:
    - source: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
      type: internal
  - name: _relationships
    start_line: 98
    end_line: 107
    code: "def _relationships(df: pd.DataFrame) -> pd.DataFrame:\n    selected = df.loc[:,\
      \ [\"id\", \"text_unit_ids\"]]\n    unrolled = selected.explode([\"text_unit_ids\"\
      ]).reset_index(drop=True)\n\n    return (\n        unrolled.groupby(\"text_unit_ids\"\
      , sort=False)\n        .agg(relationship_ids=(\"id\", \"unique\"))\n       \
      \ .reset_index()\n        .rename(columns={\"text_unit_ids\": \"id\"})\n   \
      \ )"
    signature: 'def _relationships(df: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: selected.explode(["text_unit_ids"]).reset_index
      type: unresolved
    - target: selected.explode
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg(relationship_ids=(\"\
        id\", \"unique\"))\n        .reset_index()\n        .rename"
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg(relationship_ids=(\"\
        id\", \"unique\"))\n        .reset_index"
      type: unresolved
    - target: "unrolled.groupby(\"text_unit_ids\", sort=False)\n        .agg"
      type: unresolved
    - target: unrolled.groupby
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_final_text_units.py::_relationships
    called_by:
    - source: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
      type: internal
  - name: _covariates
    start_line: 110
    end_line: 118
    code: "def _covariates(df: pd.DataFrame) -> pd.DataFrame:\n    selected = df.loc[:,\
      \ [\"id\", \"text_unit_id\"]]\n\n    return (\n        selected.groupby(\"text_unit_id\"\
      , sort=False)\n        .agg(covariate_ids=(\"id\", \"unique\"))\n        .reset_index()\n\
      \        .rename(columns={\"text_unit_id\": \"id\"})\n    )"
    signature: 'def _covariates(df: pd.DataFrame) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: "selected.groupby(\"text_unit_id\", sort=False)\n        .agg(covariate_ids=(\"\
        id\", \"unique\"))\n        .reset_index()\n        .rename"
      type: unresolved
    - target: "selected.groupby(\"text_unit_id\", sort=False)\n        .agg(covariate_ids=(\"\
        id\", \"unique\"))\n        .reset_index"
      type: unresolved
    - target: "selected.groupby(\"text_unit_id\", sort=False)\n        .agg"
      type: unresolved
    - target: selected.groupby
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_final_text_units.py::_covariates
    called_by:
    - source: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
      type: internal
  - name: _join
    start_line: 121
    end_line: 127
    code: "def _join(left, right):\n    return left.merge(\n        right,\n     \
      \   on=\"id\",\n        how=\"left\",\n        suffixes=[\"_1\", \"_2\"],\n\
      \    )"
    signature: def _join(left, right)
    decorators: []
    raises: []
    calls:
    - target: left.merge
      type: unresolved
    visibility: protected
    node_id: graphrag/index/workflows/create_final_text_units.py::_join
    called_by:
    - source: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
      type: internal
- file_name: graphrag/index/workflows/extract_covariates.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: uuid
    name: uuid4
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.schemas
    name: COVARIATES_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.operations.extract_covariates.extract_covariates
    name: extract_covariates
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 27
    end_line: 61
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to extract and format\
      \ covariates.\"\"\"\n    logger.info(\"Workflow started: extract_covariates\"\
      )\n    output = None\n    if config.extract_claims.enabled:\n        text_units\
      \ = await load_table_from_storage(\"text_units\", context.output_storage)\n\n\
      \        extract_claims_llm_settings = config.get_language_model_config(\n \
      \           config.extract_claims.model_id\n        )\n        extraction_strategy\
      \ = config.extract_claims.resolved_strategy(\n            config.root_dir, extract_claims_llm_settings\n\
      \        )\n\n        async_mode = extract_claims_llm_settings.async_mode\n\
      \        num_threads = extract_claims_llm_settings.concurrent_requests\n\n \
      \       output = await extract_covariates(\n            text_units,\n      \
      \      context.callbacks,\n            context.cache,\n            \"claim\"\
      ,\n            extraction_strategy,\n            async_mode=async_mode,\n  \
      \          entity_types=None,\n            num_threads=num_threads,\n      \
      \  )\n\n        await write_table_to_storage(output, \"covariates\", context.output_storage)\n\
      \n    logger.info(\"Workflow completed: extract_covariates\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: config.extract_claims.resolved_strategy
      type: unresolved
    - target: graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/extract_covariates.py::run_workflow
    called_by:
    - source: tests/verbs/test_extract_covariates.py::test_extract_covariates
      type: internal
  - name: extract_covariates
    start_line: 64
    end_line: 93
    code: "async def extract_covariates(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    covariate_type: str,\n\
      \    extraction_strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n    num_threads:\
      \ int = 4,\n) -> pd.DataFrame:\n    \"\"\"All the steps to extract and format\
      \ covariates.\"\"\"\n    # reassign the id because it will be overwritten in\
      \ the output by a covariate one\n    # this also results in text_unit_id being\
      \ copied to the output covariate table\n    text_units[\"text_unit_id\"] = text_units[\"\
      id\"]\n    covariates = await extractor(\n        input=text_units,\n      \
      \  callbacks=callbacks,\n        cache=cache,\n        column=\"text\",\n  \
      \      covariate_type=covariate_type,\n        strategy=extraction_strategy,\n\
      \        async_mode=async_mode,\n        entity_types=entity_types,\n      \
      \  num_threads=num_threads,\n    )\n    text_units.drop(columns=[\"text_unit_id\"\
      ], inplace=True)  # don't pollute the global\n    covariates[\"id\"] = covariates[\"\
      covariate_type\"].apply(lambda _x: str(uuid4()))\n    covariates[\"human_readable_id\"\
      ] = covariates.index\n\n    return covariates.loc[:, COVARIATES_FINAL_COLUMNS]"
    signature: "def extract_covariates(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    covariate_type: str,\n\
      \    extraction_strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n    num_threads:\
      \ int = 4,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: extractor
      type: unresolved
    - target: text_units.drop
      type: unresolved
    - target: covariates["covariate_type"].apply
      type: unresolved
    - target: str
      type: builtin
    - target: uuid::uuid4
      type: stdlib
    visibility: public
    node_id: graphrag/index/workflows/extract_covariates.py::extract_covariates
    called_by: []
- file_name: graphrag/index/workflows/extract_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.enums
    name: AsyncType
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.operations.extract_graph.extract_graph
    name: extract_graph
    alias: null
  - module: graphrag.index.operations.summarize_descriptions.summarize_descriptions
    name: summarize_descriptions
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 28
    end_line: 79
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to create the base entity\
      \ graph.\"\"\"\n    logger.info(\"Workflow started: extract_graph\")\n    text_units\
      \ = await load_table_from_storage(\"text_units\", context.output_storage)\n\n\
      \    extract_graph_llm_settings = config.get_language_model_config(\n      \
      \  config.extract_graph.model_id\n    )\n    extraction_strategy = config.extract_graph.resolved_strategy(\n\
      \        config.root_dir, extract_graph_llm_settings\n    )\n\n    summarization_llm_settings\
      \ = config.get_language_model_config(\n        config.summarize_descriptions.model_id\n\
      \    )\n    summarization_strategy = config.summarize_descriptions.resolved_strategy(\n\
      \        config.root_dir, summarization_llm_settings\n    )\n\n    entities,\
      \ relationships, raw_entities, raw_relationships = await extract_graph(\n  \
      \      text_units=text_units,\n        callbacks=context.callbacks,\n      \
      \  cache=context.cache,\n        extraction_strategy=extraction_strategy,\n\
      \        extraction_num_threads=extract_graph_llm_settings.concurrent_requests,\n\
      \        extraction_async_mode=extract_graph_llm_settings.async_mode,\n    \
      \    entity_types=config.extract_graph.entity_types,\n        summarization_strategy=summarization_strategy,\n\
      \        summarization_num_threads=summarization_llm_settings.concurrent_requests,\n\
      \    )\n\n    await write_table_to_storage(entities, \"entities\", context.output_storage)\n\
      \    await write_table_to_storage(relationships, \"relationships\", context.output_storage)\n\
      \n    if config.snapshots.raw_graph:\n        await write_table_to_storage(\n\
      \            raw_entities, \"raw_entities\", context.output_storage\n      \
      \  )\n        await write_table_to_storage(\n            raw_relationships,\
      \ \"raw_relationships\", context.output_storage\n        )\n\n    logger.info(\"\
      Workflow completed: extract_graph\")\n    return WorkflowFunctionOutput(\n \
      \       result={\n            \"entities\": entities,\n            \"relationships\"\
      : relationships,\n        }\n    )"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: config.extract_graph.resolved_strategy
      type: unresolved
    - target: config.summarize_descriptions.resolved_strategy
      type: unresolved
    - target: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/extract_graph.py::run_workflow
    called_by:
    - source: tests/verbs/test_extract_graph.py::test_extract_graph
      type: internal
  - name: extract_graph
    start_line: 82
    end_line: 132
    code: "async def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    extraction_strategy: dict[str,\
      \ Any] | None = None,\n    extraction_num_threads: int = 4,\n    extraction_async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n\
      \    summarization_strategy: dict[str, Any] | None = None,\n    summarization_num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n\
      \    \"\"\"All the steps to create the base entity graph.\"\"\"\n    # this\
      \ returns a graph for each text unit, to be merged later\n    extracted_entities,\
      \ extracted_relationships = await extractor(\n        text_units=text_units,\n\
      \        callbacks=callbacks,\n        cache=cache,\n        text_column=\"\
      text\",\n        id_column=\"id\",\n        strategy=extraction_strategy,\n\
      \        async_mode=extraction_async_mode,\n        entity_types=entity_types,\n\
      \        num_threads=extraction_num_threads,\n    )\n\n    if not _validate_data(extracted_entities):\n\
      \        error_msg = \"Entity Extraction failed. No entities detected during\
      \ extraction.\"\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\
      \n    if not _validate_data(extracted_relationships):\n        error_msg = (\n\
      \            \"Entity Extraction failed. No relationships detected during extraction.\"\
      \n        )\n        logger.error(error_msg)\n        raise ValueError(error_msg)\n\
      \n    # copy these as is before any summarization\n    raw_entities = extracted_entities.copy()\n\
      \    raw_relationships = extracted_relationships.copy()\n\n    entities, relationships\
      \ = await get_summarized_entities_relationships(\n        extracted_entities=extracted_entities,\n\
      \        extracted_relationships=extracted_relationships,\n        callbacks=callbacks,\n\
      \        cache=cache,\n        summarization_strategy=summarization_strategy,\n\
      \        summarization_num_threads=summarization_num_threads,\n    )\n\n   \
      \ return (entities, relationships, raw_entities, raw_relationships)"
    signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    extraction_strategy: dict[str,\
      \ Any] | None = None,\n    extraction_num_threads: int = 4,\n    extraction_async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n\
      \    summarization_strategy: dict[str, Any] | None = None,\n    summarization_num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: extractor
      type: unresolved
    - target: graphrag/index/workflows/extract_graph.py::_validate_data
      type: internal
    - target: logger.error
      type: unresolved
    - target: ValueError
      type: builtin
    - target: extracted_entities.copy
      type: unresolved
    - target: extracted_relationships.copy
      type: unresolved
    - target: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/extract_graph.py::extract_graph
    called_by: []
  - name: get_summarized_entities_relationships
    start_line: 135
    end_line: 159
    code: "async def get_summarized_entities_relationships(\n    extracted_entities:\
      \ pd.DataFrame,\n    extracted_relationships: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict[str, Any] | None = None,\n    summarization_num_threads: int = 4,\n)\
      \ -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Summarize the entities and\
      \ relationships.\"\"\"\n    entity_summaries, relationship_summaries = await\
      \ summarize_descriptions(\n        entities_df=extracted_entities,\n       \
      \ relationships_df=extracted_relationships,\n        callbacks=callbacks,\n\
      \        cache=cache,\n        strategy=summarization_strategy,\n        num_threads=summarization_num_threads,\n\
      \    )\n\n    relationships = extracted_relationships.drop(columns=[\"description\"\
      ]).merge(\n        relationship_summaries, on=[\"source\", \"target\"], how=\"\
      left\"\n    )\n\n    extracted_entities.drop(columns=[\"description\"], inplace=True)\n\
      \    entities = extracted_entities.merge(entity_summaries, on=\"title\", how=\"\
      left\")\n    return entities, relationships"
    signature: "def get_summarized_entities_relationships(\n    extracted_entities:\
      \ pd.DataFrame,\n    extracted_relationships: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict[str, Any] | None = None,\n    summarization_num_threads: int = 4,\n)\
      \ -> tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
      type: internal
    - target: extracted_relationships.drop(columns=["description"]).merge
      type: unresolved
    - target: extracted_relationships.drop
      type: unresolved
    - target: extracted_entities.drop
      type: unresolved
    - target: extracted_entities.merge
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
    called_by:
    - source: graphrag/index/workflows/extract_graph.py::extract_graph
      type: internal
    - source: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
  - name: _validate_data
    start_line: 162
    end_line: 164
    code: "def _validate_data(df: pd.DataFrame) -> bool:\n    \"\"\"Validate that\
      \ the dataframe has data.\"\"\"\n    return len(df) > 0"
    signature: 'def _validate_data(df: pd.DataFrame) -> bool'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    visibility: protected
    node_id: graphrag/index/workflows/extract_graph.py::_validate_data
    called_by:
    - source: graphrag/index/workflows/extract_graph.py::extract_graph
      type: internal
- file_name: graphrag/index/workflows/extract_graph_nlp.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.models.extract_graph_nlp_config
    name: ExtractGraphNLPConfig
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.operations.build_noun_graph.build_noun_graph
    name: build_noun_graph
    alias: null
  - module: graphrag.index.operations.build_noun_graph.np_extractors.factory
    name: create_noun_phrase_extractor
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 24
    end_line: 48
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to create the base entity\
      \ graph.\"\"\"\n    logger.info(\"Workflow started: extract_graph_nlp\")\n \
      \   text_units = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \n    entities, relationships = await extract_graph_nlp(\n        text_units,\n\
      \        context.cache,\n        extraction_config=config.extract_graph_nlp,\n\
      \    )\n\n    await write_table_to_storage(entities, \"entities\", context.output_storage)\n\
      \    await write_table_to_storage(relationships, \"relationships\", context.output_storage)\n\
      \n    logger.info(\"Workflow completed: extract_graph_nlp\")\n\n    return WorkflowFunctionOutput(\n\
      \        result={\n            \"entities\": entities,\n            \"relationships\"\
      : relationships,\n        }\n    )"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
    called_by:
    - source: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
      type: internal
  - name: extract_graph_nlp
    start_line: 51
    end_line: 73
    code: "async def extract_graph_nlp(\n    text_units: pd.DataFrame,\n    cache:\
      \ PipelineCache,\n    extraction_config: ExtractGraphNLPConfig,\n) -> tuple[pd.DataFrame,\
      \ pd.DataFrame]:\n    \"\"\"All the steps to create the base entity graph.\"\
      \"\"\n    text_analyzer_config = extraction_config.text_analyzer\n    text_analyzer\
      \ = create_noun_phrase_extractor(text_analyzer_config)\n    extracted_nodes,\
      \ extracted_edges = await build_noun_graph(\n        text_units,\n        text_analyzer=text_analyzer,\n\
      \        normalize_edge_weights=extraction_config.normalize_edge_weights,\n\
      \        num_threads=extraction_config.concurrent_requests,\n        async_mode=extraction_config.async_mode,\n\
      \        cache=cache,\n    )\n\n    # add in any other columns required by downstream\
      \ workflows\n    extracted_nodes[\"type\"] = \"NOUN PHRASE\"\n    extracted_nodes[\"\
      description\"] = \"\"\n    extracted_edges[\"description\"] = \"\"\n\n    return\
      \ (extracted_nodes, extracted_edges)"
    signature: "def extract_graph_nlp(\n    text_units: pd.DataFrame,\n    cache:\
      \ PipelineCache,\n    extraction_config: ExtractGraphNLPConfig,\n) -> tuple[pd.DataFrame,\
      \ pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::create_noun_phrase_extractor
      type: internal
    - target: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
    called_by:
    - source: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/factory.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: IndexingMethod
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.typing.pipeline
    name: Pipeline
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunction
    alias: null
  functions:
  - name: register
    start_line: 24
    end_line: 26
    code: "def register(cls, name: str, workflow: WorkflowFunction):\n        \"\"\
      \"Register a custom workflow function.\"\"\"\n        cls.workflows[name] =\
      \ workflow"
    signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/workflows/factory.py::PipelineFactory.register
    called_by: []
  - name: register_all
    start_line: 29
    end_line: 32
    code: "def register_all(cls, workflows: dict[str, WorkflowFunction]):\n      \
      \  \"\"\"Register a dict of custom workflow functions.\"\"\"\n        for name,\
      \ workflow in workflows.items():\n            cls.register(name, workflow)"
    signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: workflows.items
      type: unresolved
    - target: cls.register
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_all
    called_by: []
  - name: register_pipeline
    start_line: 35
    end_line: 37
    code: "def register_pipeline(cls, name: str, workflows: list[str]):\n        \"\
      \"\"Register a new pipeline method as a list of workflow names.\"\"\"\n    \
      \    cls.pipelines[name] = workflows"
    signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_pipeline
    called_by: []
  - name: create_pipeline
    start_line: 40
    end_line: 48
    code: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n \
      \       method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline:\n\
      \        \"\"\"Create a pipeline generator.\"\"\"\n        workflows = config.workflows\
      \ or cls.pipelines.get(method, [])\n        logger.info(\"Creating pipeline\
      \ with workflows: %s\", workflows)\n        return Pipeline([(name, cls.workflows[name])\
      \ for name in workflows])"
    signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
      \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline"
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: cls.pipelines.get
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: graphrag/index/typing/pipeline.py::Pipeline
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/factory.py::PipelineFactory.create_pipeline
    called_by: []
- file_name: graphrag/index/workflows/finalize_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.embed_graph_config
    name: EmbedGraphConfig
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.operations.create_graph
    name: create_graph
    alias: null
  - module: graphrag.index.operations.finalize_entities
    name: finalize_entities
    alias: null
  - module: graphrag.index.operations.finalize_relationships
    name: finalize_relationships
    alias: null
  - module: graphrag.index.operations.snapshot_graphml
    name: snapshot_graphml
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 23
    end_line: 62
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to create the base entity\
      \ graph.\"\"\"\n    logger.info(\"Workflow started: finalize_graph\")\n    entities\
      \ = await load_table_from_storage(\"entities\", context.output_storage)\n  \
      \  relationships = await load_table_from_storage(\n        \"relationships\"\
      , context.output_storage\n    )\n\n    final_entities, final_relationships =\
      \ finalize_graph(\n        entities,\n        relationships,\n        embed_config=config.embed_graph,\n\
      \        layout_enabled=config.umap.enabled,\n    )\n\n    await write_table_to_storage(final_entities,\
      \ \"entities\", context.output_storage)\n    await write_table_to_storage(\n\
      \        final_relationships, \"relationships\", context.output_storage\n  \
      \  )\n\n    if config.snapshots.graphml:\n        # todo: extract graphs at\
      \ each level, and add in meta like descriptions\n        graph = create_graph(final_relationships,\
      \ edge_attr=[\"weight\"])\n\n        await snapshot_graphml(\n            graph,\n\
      \            name=\"graph\",\n            storage=context.output_storage,\n\
      \        )\n\n    logger.info(\"Workflow completed: finalize_graph\")\n    return\
      \ WorkflowFunctionOutput(\n        result={\n            \"entities\": entities,\n\
      \            \"relationships\": relationships,\n        }\n    )"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/finalize_graph.py::finalize_graph
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/operations/create_graph.py::create_graph
      type: internal
    - target: graphrag/index/operations/snapshot_graphml.py::snapshot_graphml
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/finalize_graph.py::run_workflow
    called_by:
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
      type: internal
  - name: finalize_graph
    start_line: 65
    end_line: 76
    code: "def finalize_graph(\n    entities: pd.DataFrame,\n    relationships: pd.DataFrame,\n\
      \    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled: bool\
      \ = False,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"All the steps\
      \ to finalize the entity and relationship formats.\"\"\"\n    final_entities\
      \ = finalize_entities(\n        entities, relationships, embed_config, layout_enabled\n\
      \    )\n    final_relationships = finalize_relationships(relationships)\n  \
      \  return (final_entities, final_relationships)"
    signature: "def finalize_graph(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
      \ bool = False,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/finalize_entities.py::finalize_entities
      type: internal
    - target: graphrag/index/operations/finalize_relationships.py::finalize_relationships
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/finalize_graph.py::finalize_graph
    called_by:
    - source: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/generate_text_embeddings.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.embeddings
    name: community_full_content_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: community_summary_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: community_title_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: document_text_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: entity_description_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: entity_title_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: relationship_description_embedding
    alias: null
  - module: graphrag.config.embeddings
    name: text_unit_text_embedding
    alias: null
  - module: graphrag.config.get_embedding_settings
    name: get_embedding_settings
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.operations.embed_text.embed_text
    name: embed_text
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 35
    end_line: 93
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to transform community\
      \ reports.\"\"\"\n    logger.info(\"Workflow started: generate_text_embeddings\"\
      )\n    embedded_fields = config.embed_text.names\n    logger.info(\"Embedding\
      \ the following fields: %s\", embedded_fields)\n    documents = None\n    relationships\
      \ = None\n    text_units = None\n    entities = None\n    community_reports\
      \ = None\n    if document_text_embedding in embedded_fields:\n        documents\
      \ = await load_table_from_storage(\"documents\", context.output_storage)\n \
      \   if relationship_description_embedding in embedded_fields:\n        relationships\
      \ = await load_table_from_storage(\n            \"relationships\", context.output_storage\n\
      \        )\n    if text_unit_text_embedding in embedded_fields:\n        text_units\
      \ = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \    if (\n        entity_title_embedding in embedded_fields\n        or entity_description_embedding\
      \ in embedded_fields\n    ):\n        entities = await load_table_from_storage(\"\
      entities\", context.output_storage)\n    if (\n        community_title_embedding\
      \ in embedded_fields\n        or community_summary_embedding in embedded_fields\n\
      \        or community_full_content_embedding in embedded_fields\n    ):\n  \
      \      community_reports = await load_table_from_storage(\n            \"community_reports\"\
      , context.output_storage\n        )\n\n    text_embed = get_embedding_settings(config)\n\
      \n    output = await generate_text_embeddings(\n        documents=documents,\n\
      \        relationships=relationships,\n        text_units=text_units,\n    \
      \    entities=entities,\n        community_reports=community_reports,\n    \
      \    callbacks=context.callbacks,\n        cache=context.cache,\n        text_embed_config=text_embed,\n\
      \        embedded_fields=embedded_fields,\n    )\n\n    if config.snapshots.embeddings:\n\
      \        for name, table in output.items():\n            await write_table_to_storage(\n\
      \                table,\n                f\"embeddings.{name}\",\n         \
      \       context.output_storage,\n            )\n\n    logger.info(\"Workflow\
      \ completed: generate_text_embeddings\")\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/config/get_embedding_settings.py::get_embedding_settings
      type: internal
    - target: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
      type: internal
    - target: output.items
      type: unresolved
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
    called_by:
    - source: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
      type: internal
  - name: generate_text_embeddings
    start_line: 96
    end_line: 171
    code: "async def generate_text_embeddings(\n    documents: pd.DataFrame | None,\n\
      \    relationships: pd.DataFrame | None,\n    text_units: pd.DataFrame | None,\n\
      \    entities: pd.DataFrame | None,\n    community_reports: pd.DataFrame | None,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    text_embed_config:\
      \ dict,\n    embedded_fields: list[str],\n) -> dict[str, pd.DataFrame]:\n  \
      \  \"\"\"All the steps to generate all embeddings.\"\"\"\n    embedding_param_map\
      \ = {\n        document_text_embedding: {\n            \"data\": documents.loc[:,\
      \ [\"id\", \"text\"]] if documents is not None else None,\n            \"embed_column\"\
      : \"text\",\n        },\n        relationship_description_embedding: {\n   \
      \         \"data\": relationships.loc[:, [\"id\", \"description\"]]\n      \
      \      if relationships is not None\n            else None,\n            \"\
      embed_column\": \"description\",\n        },\n        text_unit_text_embedding:\
      \ {\n            \"data\": text_units.loc[:, [\"id\", \"text\"]]\n         \
      \   if text_units is not None\n            else None,\n            \"embed_column\"\
      : \"text\",\n        },\n        entity_title_embedding: {\n            \"data\"\
      : entities.loc[:, [\"id\", \"title\"]] if entities is not None else None,\n\
      \            \"embed_column\": \"title\",\n        },\n        entity_description_embedding:\
      \ {\n            \"data\": entities.loc[:, [\"id\", \"title\", \"description\"\
      ]].assign(\n                title_description=lambda df: df[\"title\"] + \"\
      :\" + df[\"description\"]\n            )\n            if entities is not None\n\
      \            else None,\n            \"embed_column\": \"title_description\"\
      ,\n        },\n        community_title_embedding: {\n            \"data\": community_reports.loc[:,\
      \ [\"id\", \"title\"]]\n            if community_reports is not None\n     \
      \       else None,\n            \"embed_column\": \"title\",\n        },\n \
      \       community_summary_embedding: {\n            \"data\": community_reports.loc[:,\
      \ [\"id\", \"summary\"]]\n            if community_reports is not None\n   \
      \         else None,\n            \"embed_column\": \"summary\",\n        },\n\
      \        community_full_content_embedding: {\n            \"data\": community_reports.loc[:,\
      \ [\"id\", \"full_content\"]]\n            if community_reports is not None\n\
      \            else None,\n            \"embed_column\": \"full_content\",\n \
      \       },\n    }\n\n    logger.info(\"Creating embeddings\")\n    outputs =\
      \ {}\n    for field in embedded_fields:\n        if embedding_param_map[field][\"\
      data\"] is None:\n            msg = f\"Embedding {field} is specified but data\
      \ table is not in storage. This may or may not be intentional - if you expect\
      \ it to me here, please check for errors earlier in the logs.\"\n          \
      \  logger.warning(msg)\n        else:\n            outputs[field] = await _run_embeddings(\n\
      \                name=field,\n                callbacks=callbacks,\n       \
      \         cache=cache,\n                text_embed_config=text_embed_config,\n\
      \                **embedding_param_map[field],\n            )\n    return outputs"
    signature: "def generate_text_embeddings(\n    documents: pd.DataFrame | None,\n\
      \    relationships: pd.DataFrame | None,\n    text_units: pd.DataFrame | None,\n\
      \    entities: pd.DataFrame | None,\n    community_reports: pd.DataFrame | None,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    text_embed_config:\
      \ dict,\n    embedded_fields: list[str],\n) -> dict[str, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: entities.loc[:, ["id", "title", "description"]].assign
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
    called_by:
    - source: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_text_embeddings.py::run_workflow
      type: internal
  - name: _run_embeddings
    start_line: 174
    end_line: 192
    code: "async def _run_embeddings(\n    name: str,\n    data: pd.DataFrame,\n \
      \   embed_column: str,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    text_embed_config: dict,\n) -> pd.DataFrame:\n    \"\"\"All the steps to\
      \ generate single embedding.\"\"\"\n    data[\"embedding\"] = await embed_text(\n\
      \        input=data,\n        callbacks=callbacks,\n        cache=cache,\n \
      \       embed_column=embed_column,\n        embedding_name=name,\n        strategy=text_embed_config[\"\
      strategy\"],\n    )\n\n    return data.loc[:, [\"id\", \"embedding\"]]"
    signature: "def _run_embeddings(\n    name: str,\n    data: pd.DataFrame,\n  \
      \  embed_column: str,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    text_embed_config: dict,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/embed_text/embed_text.py::embed_text
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
    called_by:
    - source: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
      type: internal
- file_name: graphrag/index/workflows/load_input_documents.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 21
    end_line: 36
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Load and parse input documents into\
      \ a standard format.\"\"\"\n    output = await load_input_documents(\n     \
      \   config.input,\n        context.input_storage,\n    )\n\n    logger.info(\"\
      Final # of rows loaded: %s\", len(output))\n    context.stats.num_documents\
      \ = len(output)\n\n    await write_table_to_storage(output, \"documents\", context.output_storage)\n\
      \n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/load_input_documents.py::load_input_documents
      type: internal
    - target: logger.info
      type: unresolved
    - target: len
      type: builtin
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/load_input_documents.py::run_workflow
    called_by: []
  - name: load_input_documents
    start_line: 39
    end_line: 43
    code: "async def load_input_documents(\n    config: InputConfig, storage: PipelineStorage\n\
      ) -> pd.DataFrame:\n    \"\"\"Load and parse input documents into a standard\
      \ format.\"\"\"\n    return await create_input(config, storage)"
    signature: "def load_input_documents(\n    config: InputConfig, storage: PipelineStorage\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/load_input_documents.py::load_input_documents
    called_by:
    - source: graphrag/index/workflows/load_input_documents.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/load_update_documents.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.update.incremental_index
    name: get_delta_docs
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 22
    end_line: 42
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Load and parse update-only input documents\
      \ into a standard format.\"\"\"\n    output = await load_update_documents(\n\
      \        config.input,\n        context.input_storage,\n        context.previous_storage,\n\
      \    )\n\n    logger.info(\"Final # of update rows loaded: %s\", len(output))\n\
      \    context.stats.update_documents = len(output)\n\n    if len(output) == 0:\n\
      \        logger.warning(\"No new update documents found.\")\n        return\
      \ WorkflowFunctionOutput(result=None, stop=True)\n\n    await write_table_to_storage(output,\
      \ \"documents\", context.output_storage)\n\n    return WorkflowFunctionOutput(result=output)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/load_update_documents.py::load_update_documents
      type: internal
    - target: logger.info
      type: unresolved
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/load_update_documents.py::run_workflow
    called_by: []
  - name: load_update_documents
    start_line: 45
    end_line: 55
    code: "async def load_update_documents(\n    config: InputConfig,\n    input_storage:\
      \ PipelineStorage,\n    previous_storage: PipelineStorage,\n) -> pd.DataFrame:\n\
      \    \"\"\"Load and parse update-only input documents into a standard format.\"\
      \"\"\n    input_documents = await create_input(config, input_storage)\n    #\
      \ previous storage is the output of the previous run\n    # we'll use this to\
      \ diff the input from the prior\n    delta_documents = await get_delta_docs(input_documents,\
      \ previous_storage)\n    return delta_documents.new_inputs"
    signature: "def load_update_documents(\n    config: InputConfig,\n    input_storage:\
      \ PipelineStorage,\n    previous_storage: PipelineStorage,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    - target: graphrag/index/update/incremental_index.py::get_delta_docs
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/load_update_documents.py::load_update_documents
    called_by:
    - source: graphrag/index/workflows/load_update_documents.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/prune_graph.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.prune_graph_config
    name: PruneGraphConfig
    alias: null
  - module: graphrag.index.operations.create_graph
    name: create_graph
    alias: null
  - module: graphrag.index.operations.graph_to_dataframes
    name: graph_to_dataframes
    alias: null
  - module: graphrag.index.operations.prune_graph
    name: prune_graph
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 22
    end_line: 50
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"All the steps to create the base entity\
      \ graph.\"\"\"\n    logger.info(\"Workflow started: prune_graph\")\n    entities\
      \ = await load_table_from_storage(\"entities\", context.output_storage)\n  \
      \  relationships = await load_table_from_storage(\n        \"relationships\"\
      , context.output_storage\n    )\n\n    pruned_entities, pruned_relationships\
      \ = prune_graph(\n        entities,\n        relationships,\n        pruning_config=config.prune_graph,\n\
      \    )\n\n    await write_table_to_storage(pruned_entities, \"entities\", context.output_storage)\n\
      \    await write_table_to_storage(\n        pruned_relationships, \"relationships\"\
      , context.output_storage\n    )\n\n    logger.info(\"Workflow completed: prune_graph\"\
      )\n    return WorkflowFunctionOutput(\n        result={\n            \"entities\"\
      : pruned_entities,\n            \"relationships\": pruned_relationships,\n \
      \       }\n    )"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/operations/prune_graph.py::prune_graph
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/prune_graph.py::run_workflow
    called_by:
    - source: tests/verbs/test_prune_graph.py::test_prune_graph
      type: internal
  - name: prune_graph
    start_line: 53
    end_line: 82
    code: "def prune_graph(\n    entities: pd.DataFrame,\n    relationships: pd.DataFrame,\n\
      \    pruning_config: PruneGraphConfig,\n) -> tuple[pd.DataFrame, pd.DataFrame]:\n\
      \    \"\"\"Prune a full graph based on graph statistics.\"\"\"\n    # create\
      \ a temporary graph to prune, then turn it back into dataframes\n    graph =\
      \ create_graph(relationships, edge_attr=[\"weight\"], nodes=entities)\n    pruned\
      \ = prune_graph_operation(\n        graph,\n        min_node_freq=pruning_config.min_node_freq,\n\
      \        max_node_freq_std=pruning_config.max_node_freq_std,\n        min_node_degree=pruning_config.min_node_degree,\n\
      \        max_node_degree_std=pruning_config.max_node_degree_std,\n        min_edge_weight_pct=pruning_config.min_edge_weight_pct,\n\
      \        remove_ego_nodes=pruning_config.remove_ego_nodes,\n        lcc_only=pruning_config.lcc_only,\n\
      \    )\n\n    pruned_nodes, pruned_edges = graph_to_dataframes(\n        pruned,\
      \ node_columns=[\"title\"], edge_columns=[\"source\", \"target\"]\n    )\n\n\
      \    # subset the full nodes and edges to only include the pruned remainders\n\
      \    subset_entities = pruned_nodes.merge(entities, on=\"title\", how=\"inner\"\
      )\n    subset_relationships = pruned_edges.merge(\n        relationships, on=[\"\
      source\", \"target\"], how=\"inner\"\n    )\n\n    return (subset_entities,\
      \ subset_relationships)"
    signature: "def prune_graph(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    pruning_config: PruneGraphConfig,\n) -> tuple[pd.DataFrame,\
      \ pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/create_graph.py::create_graph
      type: internal
    - target: prune_graph_operation
      type: unresolved
    - target: graphrag/index/operations/graph_to_dataframes.py::graph_to_dataframes
      type: internal
    - target: pruned_nodes.merge
      type: unresolved
    - target: pruned_edges.merge
      type: unresolved
    visibility: public
    node_id: graphrag/index/workflows/prune_graph.py::prune_graph
    called_by: []
- file_name: graphrag/index/workflows/update_clean_state.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  functions:
  - name: run_workflow
    start_line: 15
    end_line: 31
    code: "async def run_workflow(  # noqa: RUF029\n    _config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> WorkflowFunctionOutput:\n    \"\"\"\
      Clean the state after the update.\"\"\"\n    logger.info(\"Workflow started:\
      \ update_clean_state\")\n    keys_to_delete = [\n        key_name\n        for\
      \ key_name in context.state\n        if key_name.startswith(\"incremental_update_\"\
      )\n    ]\n\n    for key_name in keys_to_delete:\n        del context.state[key_name]\n\
      \n    logger.info(\"Workflow completed: update_clean_state\")\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(  # noqa: RUF029\n    _config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: key_name.startswith
      type: unresolved
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_clean_state.py::run_workflow
    called_by: []
- file_name: graphrag/index/workflows/update_communities.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.update.communities
    name: _update_and_merge_communities
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 19
    end_line: 36
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the communities from a incremental\
      \ index run.\"\"\"\n    logger.info(\"Workflow started: update_communities\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n\n    community_id_mapping\
      \ = await _update_communities(\n        previous_storage, delta_storage, output_storage\n\
      \    )\n\n    context.state[\"incremental_update_community_id_mapping\"] = community_id_mapping\n\
      \n    logger.info(\"Workflow completed: update_communities\")\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/index/workflows/update_communities.py::_update_communities
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_communities.py::run_workflow
    called_by: []
  - name: _update_communities
    start_line: 39
    end_line: 53
    code: "async def _update_communities(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> dict:\n    \"\"\"Update the communities output.\"\"\"\n    old_communities\
      \ = await load_table_from_storage(\"communities\", previous_storage)\n    delta_communities\
      \ = await load_table_from_storage(\"communities\", delta_storage)\n    merged_communities,\
      \ community_id_mapping = _update_and_merge_communities(\n        old_communities,\
      \ delta_communities\n    )\n\n    await write_table_to_storage(merged_communities,\
      \ \"communities\", output_storage)\n\n    return community_id_mapping"
    signature: "def _update_communities(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> dict"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/update/communities.py::_update_and_merge_communities
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/update_communities.py::_update_communities
    called_by:
    - source: graphrag/index/workflows/update_communities.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/update_community_reports.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.update.communities
    name: _update_and_merge_community_reports
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 21
    end_line: 42
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the community reports from a\
      \ incremental index run.\"\"\"\n    logger.info(\"Workflow started: update_community_reports\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n\n    community_id_mapping\
      \ = context.state[\"incremental_update_community_id_mapping\"]\n\n    merged_community_reports\
      \ = await _update_community_reports(\n        previous_storage, delta_storage,\
      \ output_storage, community_id_mapping\n    )\n\n    context.state[\"incremental_update_merged_community_reports\"\
      ] = (\n        merged_community_reports\n    )\n\n    logger.info(\"Workflow\
      \ completed: update_community_reports\")\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/index/workflows/update_community_reports.py::_update_community_reports
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_community_reports.py::run_workflow
    called_by: []
  - name: _update_community_reports
    start_line: 45
    end_line: 66
    code: "async def _update_community_reports(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    community_id_mapping: dict,\n) -> pd.DataFrame:\n    \"\"\"Update the community\
      \ reports output.\"\"\"\n    old_community_reports = await load_table_from_storage(\n\
      \        \"community_reports\", previous_storage\n    )\n    delta_community_reports\
      \ = await load_table_from_storage(\n        \"community_reports\", delta_storage\n\
      \    )\n    merged_community_reports = _update_and_merge_community_reports(\n\
      \        old_community_reports, delta_community_reports, community_id_mapping\n\
      \    )\n\n    await write_table_to_storage(\n        merged_community_reports,\
      \ \"community_reports\", output_storage\n    )\n\n    return merged_community_reports"
    signature: "def _update_community_reports(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    community_id_mapping: dict,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/update/communities.py::_update_and_merge_community_reports
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/update_community_reports.py::_update_community_reports
    called_by:
    - source: graphrag/index/workflows/update_community_reports.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/update_covariates.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: storage_has_table
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 25
    end_line: 42
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the covariates from a incremental\
      \ index run.\"\"\"\n    logger.info(\"Workflow started: update_covariates\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n\n    if await\
      \ storage_has_table(\n        \"covariates\", previous_storage\n    ) and await\
      \ storage_has_table(\"covariates\", delta_storage):\n        logger.info(\"\
      Updating Covariates\")\n        await _update_covariates(previous_storage, delta_storage,\
      \ output_storage)\n\n    logger.info(\"Workflow completed: update_covariates\"\
      )\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/utils/storage.py::storage_has_table
      type: internal
    - target: graphrag/index/workflows/update_covariates.py::_update_covariates
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_covariates.py::run_workflow
    called_by: []
  - name: _update_covariates
    start_line: 45
    end_line: 55
    code: "async def _update_covariates(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> None:\n    \"\"\"Update the covariates output.\"\"\"\n    old_covariates\
      \ = await load_table_from_storage(\"covariates\", previous_storage)\n    delta_covariates\
      \ = await load_table_from_storage(\"covariates\", delta_storage)\n    merged_covariates\
      \ = _merge_covariates(old_covariates, delta_covariates)\n\n    await write_table_to_storage(merged_covariates,\
      \ \"covariates\", output_storage)"
    signature: "def _update_covariates(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/update_covariates.py::_merge_covariates
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/update_covariates.py::_update_covariates
    called_by:
    - source: graphrag/index/workflows/update_covariates.py::run_workflow
      type: internal
  - name: _merge_covariates
    start_line: 58
    end_line: 82
    code: "def _merge_covariates(\n    old_covariates: pd.DataFrame, delta_covariates:\
      \ pd.DataFrame\n) -> pd.DataFrame:\n    \"\"\"Merge the covariates.\n\n    Parameters\n\
      \    ----------\n    old_covariates : pd.DataFrame\n        The old covariates.\n\
      \    delta_covariates : pd.DataFrame\n        The delta covariates.\n\n    Returns\n\
      \    -------\n    pd.DataFrame\n        The merged covariates.\n    \"\"\"\n\
      \    # Get the max human readable id from the old covariates and update the\
      \ delta covariates\n    initial_id = old_covariates[\"human_readable_id\"].max()\
      \ + 1\n    delta_covariates[\"human_readable_id\"] = np.arange(\n        initial_id,\
      \ initial_id + len(delta_covariates)\n    )\n\n    # Concatenate the old and\
      \ delta covariates\n    return pd.concat([old_covariates, delta_covariates],\
      \ ignore_index=True, copy=False)"
    signature: "def _merge_covariates(\n    old_covariates: pd.DataFrame, delta_covariates:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: old_covariates["human_readable_id"].max
      type: unresolved
    - target: numpy::arange
      type: external
    - target: len
      type: builtin
    - target: pandas::concat
      type: external
    visibility: protected
    node_id: graphrag/index/workflows/update_covariates.py::_merge_covariates
    called_by:
    - source: graphrag/index/workflows/update_covariates.py::_update_covariates
      type: internal
- file_name: graphrag/index/workflows/update_entities_relationships.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.update.entities
    name: _group_and_resolve_entities
    alias: null
  - module: graphrag.index.update.relationships
    name: _update_and_merge_relationships
    alias: null
  - module: graphrag.index.workflows.extract_graph
    name: get_summarized_entities_relationships
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 25
    end_line: 53
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the entities and relationships\
      \ from a incremental index run.\"\"\"\n    logger.info(\"Workflow started: update_entities_relationships\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n\n    (\n     \
      \   merged_entities_df,\n        merged_relationships_df,\n        entity_id_mapping,\n\
      \    ) = await _update_entities_and_relationships(\n        previous_storage,\n\
      \        delta_storage,\n        output_storage,\n        config,\n        context.cache,\n\
      \        context.callbacks,\n    )\n\n    context.state[\"incremental_update_merged_entities\"\
      ] = merged_entities_df\n    context.state[\"incremental_update_merged_relationships\"\
      ] = merged_relationships_df\n    context.state[\"incremental_update_entity_id_mapping\"\
      ] = entity_id_mapping\n\n    logger.info(\"Workflow completed: update_entities_relationships\"\
      )\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_entities_relationships.py::run_workflow
    called_by: []
  - name: _update_entities_and_relationships
    start_line: 56
    end_line: 106
    code: "async def _update_entities_and_relationships(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    config: GraphRagConfig,\n    cache: PipelineCache,\n    callbacks: WorkflowCallbacks,\n\
      ) -> tuple[pd.DataFrame, pd.DataFrame, dict]:\n    \"\"\"Update Final Entities\
      \  and Relationships output.\"\"\"\n    old_entities = await load_table_from_storage(\"\
      entities\", previous_storage)\n    delta_entities = await load_table_from_storage(\"\
      entities\", delta_storage)\n\n    merged_entities_df, entity_id_mapping = _group_and_resolve_entities(\n\
      \        old_entities, delta_entities\n    )\n\n    # Update Relationships\n\
      \    old_relationships = await load_table_from_storage(\"relationships\", previous_storage)\n\
      \    delta_relationships = await load_table_from_storage(\"relationships\",\
      \ delta_storage)\n    merged_relationships_df = _update_and_merge_relationships(\n\
      \        old_relationships,\n        delta_relationships,\n    )\n\n    summarization_llm_settings\
      \ = config.get_language_model_config(\n        config.summarize_descriptions.model_id\n\
      \    )\n    summarization_strategy = config.summarize_descriptions.resolved_strategy(\n\
      \        config.root_dir, summarization_llm_settings\n    )\n\n    (\n     \
      \   merged_entities_df,\n        merged_relationships_df,\n    ) = await get_summarized_entities_relationships(\n\
      \        extracted_entities=merged_entities_df,\n        extracted_relationships=merged_relationships_df,\n\
      \        callbacks=callbacks,\n        cache=cache,\n        summarization_strategy=summarization_strategy,\n\
      \        summarization_num_threads=summarization_llm_settings.concurrent_requests,\n\
      \    )\n\n    # Save the updated entities back to storage\n    await write_table_to_storage(merged_entities_df,\
      \ \"entities\", output_storage)\n\n    await write_table_to_storage(\n     \
      \   merged_relationships_df, \"relationships\", output_storage\n    )\n\n  \
      \  return merged_entities_df, merged_relationships_df, entity_id_mapping"
    signature: "def _update_entities_and_relationships(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    config: GraphRagConfig,\n    cache: PipelineCache,\n    callbacks: WorkflowCallbacks,\n\
      ) -> tuple[pd.DataFrame, pd.DataFrame, dict]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/update/entities.py::_group_and_resolve_entities
      type: internal
    - target: graphrag/index/update/relationships.py::_update_and_merge_relationships
      type: internal
    - target: config.get_language_model_config
      type: unresolved
    - target: config.summarize_descriptions.resolved_strategy
      type: unresolved
    - target: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
    called_by:
    - source: graphrag/index/workflows/update_entities_relationships.py::run_workflow
      type: internal
- file_name: graphrag/index/workflows/update_final_documents.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.update.incremental_index
    name: concat_dataframes
    alias: null
  functions:
  - name: run_workflow
    start_line: 17
    end_line: 34
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the documents from a incremental\
      \ index run.\"\"\"\n    logger.info(\"Workflow started: update_final_documents\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n\n    final_documents\
      \ = await concat_dataframes(\n        \"documents\", previous_storage, delta_storage,\
      \ output_storage\n    )\n\n    context.state[\"incremental_update_final_documents\"\
      ] = final_documents\n\n    logger.info(\"Workflow completed: update_final_documents\"\
      )\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/index/update/incremental_index.py::concat_dataframes
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_final_documents.py::run_workflow
    called_by: []
- file_name: graphrag/index/workflows/update_text_embeddings.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: graphrag.config.get_embedding_settings
    name: get_embedding_settings
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.workflows.generate_text_embeddings
    name: generate_text_embeddings
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 19
    end_line: 59
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the text embeddings from a incremental\
      \ index run.\"\"\"\n    logger.info(\"Workflow started: update_text_embeddings\"\
      )\n    output_storage, _, _ = get_update_storages(\n        config, context.state[\"\
      update_timestamp\"]\n    )\n\n    final_documents_df = context.state[\"incremental_update_final_documents\"\
      ]\n    merged_relationships_df = context.state[\"incremental_update_merged_relationships\"\
      ]\n    merged_text_units = context.state[\"incremental_update_merged_text_units\"\
      ]\n    merged_entities_df = context.state[\"incremental_update_merged_entities\"\
      ]\n    merged_community_reports = context.state[\n        \"incremental_update_merged_community_reports\"\
      \n    ]\n\n    embedded_fields = config.embed_text.names\n    text_embed = get_embedding_settings(config)\n\
      \    result = await generate_text_embeddings(\n        documents=final_documents_df,\n\
      \        relationships=merged_relationships_df,\n        text_units=merged_text_units,\n\
      \        entities=merged_entities_df,\n        community_reports=merged_community_reports,\n\
      \        callbacks=context.callbacks,\n        cache=context.cache,\n      \
      \  text_embed_config=text_embed,\n        embedded_fields=embedded_fields,\n\
      \    )\n    if config.snapshots.embeddings:\n        for name, table in result.items():\n\
      \            await write_table_to_storage(\n                table,\n       \
      \         f\"embeddings.{name}\",\n                output_storage,\n       \
      \     )\n\n    logger.info(\"Workflow completed: update_text_embeddings\")\n\
      \    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/config/get_embedding_settings.py::get_embedding_settings
      type: internal
    - target: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
      type: internal
    - target: result.items
      type: unresolved
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_text_embeddings.py::run_workflow
    called_by: []
- file_name: graphrag/index/workflows/update_text_units.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: get_update_storages
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: run_workflow
    start_line: 21
    end_line: 39
    code: "async def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput:\n    \"\"\"Update the text units from a incremental\
      \ index run.\"\"\"\n    logger.info(\"Workflow started: update_text_units\"\
      )\n    output_storage, previous_storage, delta_storage = get_update_storages(\n\
      \        config, context.state[\"update_timestamp\"]\n    )\n    entity_id_mapping\
      \ = context.state[\"incremental_update_entity_id_mapping\"]\n\n    merged_text_units\
      \ = await _update_text_units(\n        previous_storage, delta_storage, output_storage,\
      \ entity_id_mapping\n    )\n\n    context.state[\"incremental_update_merged_text_units\"\
      ] = merged_text_units\n\n    logger.info(\"Workflow completed: update_text_units\"\
      )\n    return WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - target: graphrag/index/workflows/update_text_units.py::_update_text_units
      type: internal
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: graphrag/index/workflows/update_text_units.py::run_workflow
    called_by: []
  - name: _update_text_units
    start_line: 42
    end_line: 57
    code: "async def _update_text_units(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    entity_id_mapping: dict,\n) -> pd.DataFrame:\n    \"\"\"Update the text\
      \ units output.\"\"\"\n    old_text_units = await load_table_from_storage(\"\
      text_units\", previous_storage)\n    delta_text_units = await load_table_from_storage(\"\
      text_units\", delta_storage)\n    merged_text_units = _update_and_merge_text_units(\n\
      \        old_text_units, delta_text_units, entity_id_mapping\n    )\n\n    await\
      \ write_table_to_storage(merged_text_units, \"text_units\", output_storage)\n\
      \n    return merged_text_units"
    signature: "def _update_text_units(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    entity_id_mapping: dict,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/update_text_units.py::_update_and_merge_text_units
      type: internal
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: protected
    node_id: graphrag/index/workflows/update_text_units.py::_update_text_units
    called_by:
    - source: graphrag/index/workflows/update_text_units.py::run_workflow
      type: internal
  - name: _update_and_merge_text_units
    start_line: 60
    end_line: 92
    code: "def _update_and_merge_text_units(\n    old_text_units: pd.DataFrame,\n\
      \    delta_text_units: pd.DataFrame,\n    entity_id_mapping: dict,\n) -> pd.DataFrame:\n\
      \    \"\"\"Update and merge text units.\n\n    Parameters\n    ----------\n\
      \    old_text_units : pd.DataFrame\n        The old text units.\n    delta_text_units\
      \ : pd.DataFrame\n        The delta text units.\n    entity_id_mapping : dict\n\
      \        The entity id mapping.\n\n    Returns\n    -------\n    pd.DataFrame\n\
      \        The updated text units.\n    \"\"\"\n    # Look for entity ids in entity_ids\
      \ and replace them with the corresponding id in the mapping\n    if entity_id_mapping:\n\
      \        delta_text_units[\"entity_ids\"] = delta_text_units[\"entity_ids\"\
      ].apply(\n            lambda x: [entity_id_mapping.get(i, i) for i in x] if\
      \ x is not None else x\n        )\n\n    initial_id = old_text_units[\"human_readable_id\"\
      ].max() + 1\n    delta_text_units[\"human_readable_id\"] = np.arange(\n    \
      \    initial_id, initial_id + len(delta_text_units)\n    )\n    # Merge the\
      \ final text units\n    return pd.concat([old_text_units, delta_text_units],\
      \ ignore_index=True, copy=False)"
    signature: "def _update_and_merge_text_units(\n    old_text_units: pd.DataFrame,\n\
      \    delta_text_units: pd.DataFrame,\n    entity_id_mapping: dict,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: delta_text_units["entity_ids"].apply
      type: unresolved
    - target: entity_id_mapping.get
      type: unresolved
    - target: old_text_units["human_readable_id"].max
      type: unresolved
    - target: numpy::arange
      type: external
    - target: len
      type: builtin
    - target: pandas::concat
      type: external
    visibility: protected
    node_id: graphrag/index/workflows/update_text_units.py::_update_and_merge_text_units
    called_by:
    - source: graphrag/index/workflows/update_text_units.py::_update_text_units
      type: internal
- file_name: graphrag/language_model/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/events/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/events/base.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Protocol
    alias: null
  functions:
  - name: on_error
    start_line: 12
    end_line: 19
    code: "async def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None:\n        \"\"\"Handle an model error.\"\"\"\n\
      \        ..."
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/events/base.py::ModelEventHandler.on_error
    called_by: []
- file_name: graphrag/language_model/factory.py
  imports:
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.language_model.providers.fnllm.models
    name: AzureOpenAIChatFNLLM
    alias: null
  - module: graphrag.language_model.providers.fnllm.models
    name: AzureOpenAIEmbeddingFNLLM
    alias: null
  - module: graphrag.language_model.providers.fnllm.models
    name: OpenAIChatFNLLM
    alias: null
  - module: graphrag.language_model.providers.fnllm.models
    name: OpenAIEmbeddingFNLLM
    alias: null
  - module: graphrag.language_model.providers.litellm.chat_model
    name: LitellmChatModel
    alias: null
  - module: graphrag.language_model.providers.litellm.embedding_model
    name: LitellmEmbeddingModel
    alias: null
  functions:
  - name: register_chat
    start_line: 30
    end_line: 32
    code: "def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])\
      \ -> None:\n        \"\"\"Register a ChatModel implementation.\"\"\"\n     \
      \   cls._chat_registry[model_type] = creator"
    signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
      -> None'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.register_chat
    called_by: []
  - name: register_embedding
    start_line: 35
    end_line: 39
    code: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
      \ EmbeddingModel]\n    ) -> None:\n        \"\"\"Register an EmbeddingModel\
      \ implementation.\"\"\"\n        cls._embedding_registry[model_type] = creator"
    signature: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
      \ EmbeddingModel]\n    ) -> None"
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.register_embedding
    called_by: []
  - name: create_chat_model
    start_line: 42
    end_line: 57
    code: "def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel:\n\
      \        \"\"\"\n        Create a ChatModel instance.\n\n        Args:\n   \
      \         model_type: The type of ChatModel to create.\n            **kwargs:\
      \ Additional keyword arguments for the ChatModel constructor.\n\n        Returns\n\
      \        -------\n            A ChatModel instance.\n        \"\"\"\n      \
      \  if model_type not in cls._chat_registry:\n            msg = f\"ChatMOdel\
      \ implementation '{model_type}' is not registered.\"\n            raise ValueError(msg)\n\
      \        return cls._chat_registry[model_type](**kwargs)"
    signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.create_chat_model
    called_by: []
  - name: create_embedding_model
    start_line: 60
    end_line: 75
    code: "def create_embedding_model(cls, model_type: str, **kwargs: Any) -> EmbeddingModel:\n\
      \        \"\"\"\n        Create an EmbeddingModel instance.\n\n        Args:\n\
      \            model_type: The type of EmbeddingModel to create.\n           \
      \ **kwargs: Additional keyword arguments for the EmbeddingLLM constructor.\n\
      \n        Returns\n        -------\n            An EmbeddingLLM instance.\n\
      \        \"\"\"\n        if model_type not in cls._embedding_registry:\n   \
      \         msg = f\"EmbeddingModel implementation '{model_type}' is not registered.\"\
      \n            raise ValueError(msg)\n        return cls._embedding_registry[model_type](**kwargs)"
    signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any) ->
      EmbeddingModel'
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.create_embedding_model
    called_by: []
  - name: get_chat_models
    start_line: 78
    end_line: 80
    code: "def get_chat_models(cls) -> list[str]:\n        \"\"\"Get the registered\
      \ ChatModel implementations.\"\"\"\n        return list(cls._chat_registry.keys())"
    signature: def get_chat_models(cls) -> list[str]
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: list
      type: builtin
    - target: cls._chat_registry.keys
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.get_chat_models
    called_by: []
  - name: get_embedding_models
    start_line: 83
    end_line: 85
    code: "def get_embedding_models(cls) -> list[str]:\n        \"\"\"Get the registered\
      \ EmbeddingModel implementations.\"\"\"\n        return list(cls._embedding_registry.keys())"
    signature: def get_embedding_models(cls) -> list[str]
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: list
      type: builtin
    - target: cls._embedding_registry.keys
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.get_embedding_models
    called_by: []
  - name: is_supported_chat_model
    start_line: 88
    end_line: 90
    code: "def is_supported_chat_model(cls, model_type: str) -> bool:\n        \"\"\
      \"Check if the given model type is supported.\"\"\"\n        return model_type\
      \ in cls._chat_registry"
    signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_chat_model
    called_by: []
  - name: is_supported_embedding_model
    start_line: 93
    end_line: 95
    code: "def is_supported_embedding_model(cls, model_type: str) -> bool:\n     \
      \   \"\"\"Check if the given model type is supported.\"\"\"\n        return\
      \ model_type in cls._embedding_registry"
    signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_embedding_model
    called_by: []
  - name: is_supported_model
    start_line: 98
    end_line: 102
    code: "def is_supported_model(cls, model_type: str) -> bool:\n        \"\"\"Check\
      \ if the given model type is supported.\"\"\"\n        return cls.is_supported_chat_model(\n\
      \            model_type\n        ) or cls.is_supported_embedding_model(model_type)"
    signature: 'def is_supported_model(cls, model_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: cls.is_supported_chat_model
      type: unresolved
    - target: cls.is_supported_embedding_model
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_model
    called_by: []
- file_name: graphrag/language_model/manager.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: typing_extensions
    name: Self
    alias: null
  - module: graphrag.language_model.factory
    name: ModelFactory
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  functions:
  - name: __new__
    start_line: 27
    end_line: 31
    code: "def __new__(cls) -> Self:\n        \"\"\"Create a new instance of LLMManager\
      \ if it does not exist.\"\"\"\n        if cls._instance is None:\n         \
      \   cls._instance = super().__new__(cls)\n        return cls._instance  # type:\
      \ ignore[return-value]"
    signature: def __new__(cls) -> Self
    decorators: []
    raises: []
    calls:
    - target: super().__new__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/manager.py::ModelManager.__new__
    called_by: []
  - name: __init__
    start_line: 33
    end_line: 38
    code: "def __init__(self) -> None:\n        # Avoid reinitialization in the singleton.\n\
      \        if not hasattr(self, \"_initialized\"):\n            self.chat_models:\
      \ dict[str, ChatModel] = {}\n            self.embedding_models: dict[str, EmbeddingModel]\
      \ = {}\n            self._initialized = True"
    signature: def __init__(self) -> None
    decorators: []
    raises: []
    calls:
    - target: hasattr
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/manager.py::ModelManager.__init__
    called_by: []
  - name: get_instance
    start_line: 41
    end_line: 43
    code: "def get_instance(cls) -> ModelManager:\n        \"\"\"Return the singleton\
      \ instance of LLMManager.\"\"\"\n        return cls.__new__(cls)"
    signature: def get_instance(cls) -> ModelManager
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: cls.__new__
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.get_instance
    called_by: []
  - name: register_chat
    start_line: 45
    end_line: 60
    code: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
      \ Any\n    ) -> ChatModel:\n        \"\"\"\n        Register a ChatLLM instance\
      \ under a unique name.\n\n        Args:\n            name: Unique identifier\
      \ for the ChatLLM instance.\n            model_type: Key for the ChatLLM implementation\
      \ in LLMFactory.\n            **chat_kwargs: Additional parameters for instantiation.\n\
      \        \"\"\"\n        chat_kwargs[\"name\"] = name\n        self.chat_models[name]\
      \ = ModelFactory.create_chat_model(\n            model_type, **chat_kwargs\n\
      \        )\n        return self.chat_models[name]"
    signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
      \ Any\n    ) -> ChatModel"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/factory.py::ModelFactory::create_chat_model
      type: external
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.register_chat
    called_by: []
  - name: register_embedding
    start_line: 62
    end_line: 77
    code: "def register_embedding(\n        self, name: str, model_type: str, **embedding_kwargs:\
      \ Any\n    ) -> EmbeddingModel:\n        \"\"\"\n        Register an EmbeddingsLLM\
      \ instance under a unique name.\n\n        Args:\n            name: Unique identifier\
      \ for the EmbeddingsLLM instance.\n            embedding_key: Key for the EmbeddingsLLM\
      \ implementation in LLMFactory.\n            **embedding_kwargs: Additional\
      \ parameters for instantiation.\n        \"\"\"\n        embedding_kwargs[\"\
      name\"] = name\n        self.embedding_models[name] = ModelFactory.create_embedding_model(\n\
      \            model_type, **embedding_kwargs\n        )\n        return self.embedding_models[name]"
    signature: "def register_embedding(\n        self, name: str, model_type: str,\
      \ **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/factory.py::ModelFactory::create_embedding_model
      type: external
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.register_embedding
    called_by: []
  - name: get_chat_model
    start_line: 79
    end_line: 90
    code: "def get_chat_model(self, name: str) -> ChatModel | None:\n        \"\"\"\
      \n        Retrieve the ChatLLM instance registered under the given name.\n\n\
      \        Raises\n        ------\n            ValueError: If no ChatLLM is registered\
      \ under the name.\n        \"\"\"\n        if name not in self.chat_models:\n\
      \            msg = f\"No ChatLLM registered under the name '{name}'.\"\n   \
      \         raise ValueError(msg)\n        return self.chat_models[name]"
    signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.get_chat_model
    called_by: []
  - name: get_embedding_model
    start_line: 92
    end_line: 103
    code: "def get_embedding_model(self, name: str) -> EmbeddingModel | None:\n  \
      \      \"\"\"\n        Retrieve the EmbeddingsLLM instance registered under\
      \ the given name.\n\n        Raises\n        ------\n            ValueError:\
      \ If no EmbeddingsLLM is registered under the name.\n        \"\"\"\n      \
      \  if name not in self.embedding_models:\n            msg = f\"No EmbeddingsLLM\
      \ registered under the name '{name}'.\"\n            raise ValueError(msg)\n\
      \        return self.embedding_models[name]"
    signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.get_embedding_model
    called_by: []
  - name: get_or_create_chat_model
    start_line: 105
    end_line: 120
    code: "def get_or_create_chat_model(\n        self, name: str, model_type: str,\
      \ **chat_kwargs: Any\n    ) -> ChatModel:\n        \"\"\"\n        Retrieve\
      \ the ChatLLM instance registered under the given name.\n\n        If the ChatLLM\
      \ does not exist, it is created and registered.\n\n        Args:\n         \
      \   name: Unique identifier for the ChatLLM instance.\n            model_type:\
      \ Key for the ChatModel implementation in LLMFactory.\n            **chat_kwargs:\
      \ Additional parameters for instantiation.\n        \"\"\"\n        if name\
      \ not in self.chat_models:\n            return self.register_chat(name, model_type,\
      \ **chat_kwargs)\n        return self.chat_models[name]"
    signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
      \ str, **chat_kwargs: Any\n    ) -> ChatModel"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/manager.py::register_chat
      type: internal
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_chat_model
    called_by: []
  - name: get_or_create_embedding_model
    start_line: 122
    end_line: 137
    code: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
      \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel:\n        \"\"\"\n \
      \       Retrieve the EmbeddingsLLM instance registered under the given name.\n\
      \n        If the EmbeddingsLLM does not exist, it is created and registered.\n\
      \n        Args:\n            name: Unique identifier for the EmbeddingsLLM instance.\n\
      \            model_type: Key for the EmbeddingsLLM implementation in LLMFactory.\n\
      \            **embedding_kwargs: Additional parameters for instantiation.\n\
      \        \"\"\"\n        if name not in self.embedding_models:\n           \
      \ return self.register_embedding(name, model_type, **embedding_kwargs)\n   \
      \     return self.embedding_models[name]"
    signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
      \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/manager.py::register_embedding
      type: internal
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_embedding_model
    called_by: []
  - name: remove_chat
    start_line: 139
    end_line: 141
    code: "def remove_chat(self, name: str) -> None:\n        \"\"\"Remove the ChatLLM\
      \ instance registered under the given name.\"\"\"\n        self.chat_models.pop(name,\
      \ None)"
    signature: 'def remove_chat(self, name: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: self.chat_models.pop
      type: instance
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.remove_chat
    called_by: []
  - name: remove_embedding
    start_line: 143
    end_line: 145
    code: "def remove_embedding(self, name: str) -> None:\n        \"\"\"Remove the\
      \ EmbeddingsLLM instance registered under the given name.\"\"\"\n        self.embedding_models.pop(name,\
      \ None)"
    signature: 'def remove_embedding(self, name: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: self.embedding_models.pop
      type: instance
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.remove_embedding
    called_by: []
  - name: list_chat_models
    start_line: 147
    end_line: 149
    code: "def list_chat_models(self) -> dict[str, ChatModel]:\n        \"\"\"Return\
      \ a copy of all registered ChatLLM instances.\"\"\"\n        return dict(self.chat_models)"
    signature: def list_chat_models(self) -> dict[str, ChatModel]
    decorators: []
    raises: []
    calls:
    - target: dict
      type: builtin
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.list_chat_models
    called_by: []
  - name: list_embedding_models
    start_line: 151
    end_line: 153
    code: "def list_embedding_models(self) -> dict[str, EmbeddingModel]:\n       \
      \ \"\"\"Return a copy of all registered EmbeddingsLLM instances.\"\"\"\n   \
      \     return dict(self.embedding_models)"
    signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
    decorators: []
    raises: []
    calls:
    - target: dict
      type: builtin
    visibility: public
    node_id: graphrag/language_model/manager.py::ModelManager.list_embedding_models
    called_by: []
- file_name: graphrag/language_model/protocol/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/protocol/base.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Protocol
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: collections.abc
    name: Generator
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.language_model.response.base
    name: ModelResponse
    alias: null
  functions:
  - name: aembed_batch
    start_line: 27
    end_line: 41
    code: "async def aembed_batch(\n        self, text_list: list[str], **kwargs:\
      \ Any\n    ) -> list[list[float]]:\n        \"\"\"\n        Generate an embedding\
      \ vector for the given list of strings.\n\n        Args:\n            text:\
      \ The text to generate an embedding for.\n            **kwargs: Additional keyword\
      \ arguments (e.g., model parameters).\n\n        Returns\n        -------\n\
      \            A collections of list of floats representing the embedding vector\
      \ for each item in the batch.\n        \"\"\"\n        ..."
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed_batch
    called_by: []
  - name: aembed
    start_line: 43
    end_line: 55
    code: "async def aembed(self, text: str, **kwargs: Any) -> list[float]:\n    \
      \    \"\"\"\n        Generate an embedding vector for the given text.\n\n  \
      \      Args:\n            text: The text to generate an embedding for.\n   \
      \         **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \n        Returns\n        -------\n            A list of floats representing\
      \ the embedding vector.\n        \"\"\"\n        ..."
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed
    called_by: []
  - name: embed_batch
    start_line: 57
    end_line: 69
    code: "def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]:\n\
      \        \"\"\"\n        Generate an embedding vector for the given list of\
      \ strings.\n\n        Args:\n            text: The text to generate an embedding\
      \ for.\n            **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \n        Returns\n        -------\n            A collections of list of floats\
      \ representing the embedding vector for each item in the batch.\n        \"\"\
      \"\n        ..."
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed_batch
    called_by: []
  - name: embed
    start_line: 71
    end_line: 83
    code: "def embed(self, text: str, **kwargs: Any) -> list[float]:\n        \"\"\
      \"\n        Generate an embedding vector for the given text.\n\n        Args:\n\
      \            text: The text to generate an embedding for.\n            **kwargs:\
      \ Additional keyword arguments (e.g., model parameters).\n\n        Returns\n\
      \        -------\n            A list of floats representing the embedding vector.\n\
      \        \"\"\"\n        ..."
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed
    called_by: []
  - name: achat
    start_line: 97
    end_line: 113
    code: "async def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse:\n        \"\"\"\n        Generate a\
      \ response for the given text.\n\n        Args:\n            prompt: The text\
      \ to generate a response for.\n            history: The conversation history.\n\
      \            **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \n        Returns\n        -------\n            A string representing the response.\n\
      \n        \"\"\"\n        ..."
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::ChatModel.achat
    called_by: []
  - name: achat_stream
    start_line: 115
    end_line: 131
    code: "async def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\
      \n        Generate a response for the given text using a streaming interface.\n\
      \n        Args:\n            prompt: The text to generate a response for.\n\
      \            history: The conversation history.\n            **kwargs: Additional\
      \ keyword arguments (e.g., model parameters).\n\n        Returns\n        -------\n\
      \            A generator that yields strings representing the response.\n  \
      \      \"\"\"\n        yield \"\"  # Yield an empty string so that the function\
      \ is recognized as a generator\n        ..."
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::ChatModel.achat_stream
    called_by: []
  - name: chat
    start_line: 133
    end_line: 149
    code: "def chat(\n        self, prompt: str, history: list | None = None, **kwargs:\
      \ Any\n    ) -> ModelResponse:\n        \"\"\"\n        Generate a response\
      \ for the given text.\n\n        Args:\n            prompt: The text to generate\
      \ a response for.\n            history: The conversation history.\n        \
      \    **kwargs: Additional keyword arguments (e.g., model parameters).\n\n  \
      \      Returns\n        -------\n            A string representing the response.\n\
      \n        \"\"\"\n        ..."
    signature: "def chat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::ChatModel.chat
    called_by: []
  - name: chat_stream
    start_line: 151
    end_line: 166
    code: "def chat_stream(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> Generator[str, None]:\n        \"\"\"\n        Generate\
      \ a response for the given text using a streaming interface.\n\n        Args:\n\
      \            prompt: The text to generate a response for.\n            history:\
      \ The conversation history.\n            **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\n        Returns\n        -------\n          \
      \  A generator that yields strings representing the response.\n        \"\"\"\
      \n        ..."
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/protocol/base.py::ChatModel.chat_stream
    called_by: []
- file_name: graphrag/language_model/providers/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/fnllm/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/fnllm/cache.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: fnllm.caching
    name: Cache
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  functions:
  - name: __init__
    start_line: 16
    end_line: 17
    code: "def __init__(self, cache: PipelineCache):\n        self._cache = cache"
    signature: 'def __init__(self, cache: PipelineCache)'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.__init__
    called_by: []
  - name: has
    start_line: 19
    end_line: 21
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Check if the cache\
      \ has a value.\"\"\"\n        return await self._cache.has(key)"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: self._cache.has
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.has
    called_by: []
  - name: get
    start_line: 23
    end_line: 25
    code: "async def get(self, key: str) -> Any | None:\n        \"\"\"Retrieve a\
      \ value from the cache.\"\"\"\n        return await self._cache.get(key)"
    signature: 'def get(self, key: str) -> Any | None'
    decorators: []
    raises: []
    calls:
    - target: self._cache.get
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.get
    called_by: []
  - name: set
    start_line: 27
    end_line: 31
    code: "async def set(\n        self, key: str, value: Any, metadata: dict[str,\
      \ Any] | None = None\n    ) -> None:\n        \"\"\"Write a value into the cache.\"\
      \"\"\n        await self._cache.set(key, value, metadata)"
    signature: "def set(\n        self, key: str, value: Any, metadata: dict[str,\
      \ Any] | None = None\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: self._cache.set
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.set
    called_by: []
  - name: remove
    start_line: 33
    end_line: 35
    code: "async def remove(self, key: str) -> None:\n        \"\"\"Remove a value\
      \ from the cache.\"\"\"\n        await self._cache.delete(key)"
    signature: 'def remove(self, key: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: self._cache.delete
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.remove
    called_by: []
  - name: clear
    start_line: 37
    end_line: 39
    code: "async def clear(self) -> None:\n        \"\"\"Clear the cache.\"\"\"\n\
      \        await self._cache.clear()"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._cache.clear
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.clear
    called_by: []
  - name: child
    start_line: 41
    end_line: 44
    code: "def child(self, key: str) -> \"FNLLMCacheProvider\":\n        \"\"\"Create\
      \ a child cache.\"\"\"\n        child_cache = self._cache.child(key)\n     \
      \   return FNLLMCacheProvider(child_cache)"
    signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
    decorators: []
    raises: []
    calls:
    - target: self._cache.child
      type: instance
    - target: FNLLMCacheProvider
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.child
    called_by: []
- file_name: graphrag/language_model/providers/fnllm/events.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: fnllm.events
    name: LLMEvents
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  functions:
  - name: __init__
    start_line: 16
    end_line: 17
    code: "def __init__(self, on_error: ErrorHandlerFn):\n        self._on_error =\
      \ on_error"
    signature: 'def __init__(self, on_error: ErrorHandlerFn)'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.__init__
    called_by: []
  - name: on_error
    start_line: 19
    end_line: 26
    code: "async def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None:\n        \"\"\"Handle an fnllm error.\"\"\"\n\
      \        self._on_error(error, traceback, arguments)"
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: self._on_error
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.on_error
    called_by: []
- file_name: graphrag/language_model/providers/fnllm/models.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: fnllm.openai
    name: create_openai_chat_llm
    alias: null
  - module: fnllm.openai
    name: create_openai_client
    alias: null
  - module: fnllm.openai
    name: create_openai_embeddings_llm
    alias: null
  - module: graphrag.language_model.providers.fnllm.events
    name: FNLLMEvents
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: _create_cache
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: _create_error_handler
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: _create_openai_config
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: run_coroutine_sync
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelOutput
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelResponse
    alias: null
  - module: graphrag.language_model.response.base
    name: ModelResponse
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: collections.abc
    name: Generator
    alias: null
  - module: fnllm.openai.types.client
    name: OpenAIChatLLM
    alias: null
  - module: fnllm.openai.types.client
    name: OpenAIEmbeddingsLLM
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: __init__
    start_line: 47
    end_line: 65
    code: "def __init__(\n        self,\n        *,\n        name: str,\n        config:\
      \ LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None = None,\n\
      \        cache: PipelineCache | None = None,\n    ) -> None:\n        model_config\
      \ = _create_openai_config(config, azure=False)\n        error_handler = _create_error_handler(callbacks)\
      \ if callbacks else None\n        model_cache = _create_cache(cache, name)\n\
      \        client = create_openai_client(model_config)\n        self.model = create_openai_chat_llm(\n\
      \            model_config,\n            client=client,\n            cache=model_cache,\n\
      \            events=FNLLMEvents(error_handler) if error_handler else None,\n\
      \        )\n        self.config = config"
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_cache
      type: internal
    - target: fnllm.openai::create_openai_client
      type: external
    - target: fnllm.openai::create_openai_chat_llm
      type: external
    - target: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
    called_by: []
  - name: achat
    start_line: 67
    end_line: 95
    code: "async def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse:\n        \"\"\"\n        Chat with the Model\
      \ using the given prompt.\n\n        Args:\n            prompt: The prompt to\
      \ chat with.\n            kwargs: Additional arguments to pass to the Model.\n\
      \n        Returns\n        -------\n            The response from the Model.\n\
      \        \"\"\"\n        if history is None:\n            response = await self.model(prompt,\
      \ **kwargs)\n        else:\n            response = await self.model(prompt,\
      \ history=history, **kwargs)\n        return BaseModelResponse(\n          \
      \  output=BaseModelOutput(\n                content=response.output.content,\n\
      \                full_response=response.output.raw_model.to_dict(),\n      \
      \      ),\n            parsed_response=response.parsed_json,\n            history=response.history,\n\
      \            cache_hit=response.cache_hit,\n            tool_calls=response.tool_calls,\n\
      \            metrics=response.metrics,\n        )"
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: self.model
      type: instance
    - target: graphrag/language_model/response/base.py::BaseModelResponse
      type: internal
    - target: graphrag/language_model/response/base.py::BaseModelOutput
      type: internal
    - target: response.output.raw_model.to_dict
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat
    called_by: []
  - name: achat_stream
    start_line: 97
    end_line: 117
    code: "async def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n  \
      \      Stream Chat with the Model using the given prompt.\n\n        Args:\n\
      \            prompt: The prompt to chat with.\n            kwargs: Additional\
      \ arguments to pass to the Model.\n\n        Returns\n        -------\n    \
      \        A generator that yields strings representing the response.\n      \
      \  \"\"\"\n        if history is None:\n            response = await self.model(prompt,\
      \ stream=True, **kwargs)\n        else:\n            response = await self.model(prompt,\
      \ history=history, stream=True, **kwargs)\n        async for chunk in response.output.content:\n\
      \            if chunk is not None:\n                yield chunk"
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: self.model
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat_stream
    called_by: []
  - name: chat
    start_line: 119
    end_line: 131
    code: "def chat(self, prompt: str, history: list | None = None, **kwargs) -> ModelResponse:\n\
      \        \"\"\"\n        Chat with the Model using the given prompt.\n\n   \
      \     Args:\n            prompt: The prompt to chat with.\n            kwargs:\
      \ Additional arguments to pass to the Model.\n\n        Returns\n        -------\n\
      \            The response from the Model.\n        \"\"\"\n        return run_coroutine_sync(self.achat(prompt,\
      \ history=history, **kwargs))"
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::achat
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
    called_by: []
  - name: chat_stream
    start_line: 133
    end_line: 148
    code: "def chat_stream(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> Generator[str, None]:\n        \"\"\"\n        Stream Chat\
      \ with the Model using the given prompt.\n\n        Args:\n            prompt:\
      \ The prompt to chat with.\n            kwargs: Additional arguments to pass\
      \ to the Model.\n\n        Returns\n        -------\n            A generator\
      \ that yields strings representing the response.\n        \"\"\"\n        msg\
      \ = \"chat_stream is not supported for synchronous execution\"\n        raise\
      \ NotImplementedError(msg)"
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat_stream
    called_by: []
  - name: __init__
    start_line: 156
    end_line: 174
    code: "def __init__(\n        self,\n        *,\n        name: str,\n        config:\
      \ LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None = None,\n\
      \        cache: PipelineCache | None = None,\n    ) -> None:\n        model_config\
      \ = _create_openai_config(config, azure=False)\n        error_handler = _create_error_handler(callbacks)\
      \ if callbacks else None\n        model_cache = _create_cache(cache, name)\n\
      \        client = create_openai_client(model_config)\n        self.model = create_openai_embeddings_llm(\n\
      \            model_config,\n            client=client,\n            cache=model_cache,\n\
      \            events=FNLLMEvents(error_handler) if error_handler else None,\n\
      \        )\n        self.config = config"
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_cache
      type: internal
    - target: fnllm.openai::create_openai_client
      type: external
    - target: fnllm.openai::create_openai_embeddings_llm
      type: external
    - target: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
    called_by: []
  - name: aembed_batch
    start_line: 176
    end_line: 193
    code: "async def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]:\n\
      \        \"\"\"\n        Embed the given text using the Model.\n\n        Args:\n\
      \            text: The text to embed.\n            kwargs: Additional arguments\
      \ to pass to the LLM.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        response = await self.model(text_list,\
      \ **kwargs)\n        if response.output.embeddings is None:\n            msg\
      \ = \"No embeddings found in response\"\n            raise ValueError(msg)\n\
      \        embeddings: list[list[float]] = response.output.embeddings\n      \
      \  return embeddings"
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.model
      type: instance
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed_batch
    called_by: []
  - name: aembed
    start_line: 195
    end_line: 212
    code: "async def aembed(self, text: str, **kwargs) -> list[float]:\n        \"\
      \"\"\n        Embed the given text using the Model.\n\n        Args:\n     \
      \       text: The text to embed.\n            kwargs: Additional arguments to\
      \ pass to the Model.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        response = await self.model([text],\
      \ **kwargs)\n        if response.output.embeddings is None:\n            msg\
      \ = \"No embeddings found in response\"\n            raise ValueError(msg)\n\
      \        embeddings: list[float] = response.output.embeddings[0]\n        return\
      \ embeddings"
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.model
      type: instance
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed
    called_by: []
  - name: embed_batch
    start_line: 214
    end_line: 226
    code: "def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]:\n\
      \        \"\"\"\n        Embed the given text using the Model.\n\n        Args:\n\
      \            text: The text to embed.\n            kwargs: Additional arguments\
      \ to pass to the LLM.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        return run_coroutine_sync(self.aembed_batch(text_list,\
      \ **kwargs))"
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::aembed_batch
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
    called_by: []
  - name: embed
    start_line: 228
    end_line: 240
    code: "def embed(self, text: str, **kwargs) -> list[float]:\n        \"\"\"\n\
      \        Embed the given text using the Model.\n\n        Args:\n          \
      \  text: The text to embed.\n            kwargs: Additional arguments to pass\
      \ to the Model.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        return run_coroutine_sync(self.aembed(text,\
      \ **kwargs))"
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::aembed
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
    called_by: []
  - name: __init__
    start_line: 248
    end_line: 266
    code: "def __init__(\n        self,\n        *,\n        name: str,\n        config:\
      \ LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None = None,\n\
      \        cache: PipelineCache | None = None,\n    ) -> None:\n        model_config\
      \ = _create_openai_config(config, azure=True)\n        error_handler = _create_error_handler(callbacks)\
      \ if callbacks else None\n        model_cache = _create_cache(cache, name)\n\
      \        client = create_openai_client(model_config)\n        self.model = create_openai_chat_llm(\n\
      \            model_config,\n            client=client,\n            cache=model_cache,\n\
      \            events=FNLLMEvents(error_handler) if error_handler else None,\n\
      \        )\n        self.config = config"
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_cache
      type: internal
    - target: fnllm.openai::create_openai_client
      type: external
    - target: fnllm.openai::create_openai_chat_llm
      type: external
    - target: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
    called_by: []
  - name: achat
    start_line: 268
    end_line: 297
    code: "async def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse:\n        \"\"\"\n        Chat with the Model\
      \ using the given prompt.\n\n        Args:\n            prompt: The prompt to\
      \ chat with.\n            history: The conversation history.\n            kwargs:\
      \ Additional arguments to pass to the Model.\n\n        Returns\n        -------\n\
      \            The response from the Model.\n        \"\"\"\n        if history\
      \ is None:\n            response = await self.model(prompt, **kwargs)\n    \
      \    else:\n            response = await self.model(prompt, history=history,\
      \ **kwargs)\n        return BaseModelResponse(\n            output=BaseModelOutput(\n\
      \                content=response.output.content,\n                full_response=response.output.raw_model.to_dict(),\n\
      \            ),\n            parsed_response=response.parsed_json,\n       \
      \     history=response.history,\n            cache_hit=response.cache_hit,\n\
      \            tool_calls=response.tool_calls,\n            metrics=response.metrics,\n\
      \        )"
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: self.model
      type: instance
    - target: graphrag/language_model/response/base.py::BaseModelResponse
      type: internal
    - target: graphrag/language_model/response/base.py::BaseModelOutput
      type: internal
    - target: response.output.raw_model.to_dict
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat
    called_by: []
  - name: achat_stream
    start_line: 299
    end_line: 320
    code: "async def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\n  \
      \      Stream Chat with the Model using the given prompt.\n\n        Args:\n\
      \            prompt: The prompt to chat with.\n            history: The conversation\
      \ history.\n            kwargs: Additional arguments to pass to the Model.\n\
      \n        Returns\n        -------\n            A generator that yields strings\
      \ representing the response.\n        \"\"\"\n        if history is None:\n\
      \            response = await self.model(prompt, stream=True, **kwargs)\n  \
      \      else:\n            response = await self.model(prompt, history=history,\
      \ stream=True, **kwargs)\n        async for chunk in response.output.content:\n\
      \            if chunk is not None:\n                yield chunk"
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: self.model
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat_stream
    called_by: []
  - name: chat
    start_line: 322
    end_line: 334
    code: "def chat(self, prompt: str, history: list | None = None, **kwargs) -> ModelResponse:\n\
      \        \"\"\"\n        Chat with the Model using the given prompt.\n\n   \
      \     Args:\n            prompt: The prompt to chat with.\n            kwargs:\
      \ Additional arguments to pass to the Model.\n\n        Returns\n        -------\n\
      \            The response from the Model.\n        \"\"\"\n        return run_coroutine_sync(self.achat(prompt,\
      \ history=history, **kwargs))"
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::achat
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
    called_by: []
  - name: chat_stream
    start_line: 336
    end_line: 351
    code: "def chat_stream(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> Generator[str, None]:\n        \"\"\"\n        Stream Chat\
      \ with the Model using the given prompt.\n\n        Args:\n            prompt:\
      \ The prompt to chat with.\n            kwargs: Additional arguments to pass\
      \ to the Model.\n\n        Returns\n        -------\n            A generator\
      \ that yields strings representing the response.\n        \"\"\"\n        msg\
      \ = \"chat_stream is not supported for synchronous execution\"\n        raise\
      \ NotImplementedError(msg)"
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat_stream
    called_by: []
  - name: __init__
    start_line: 359
    end_line: 377
    code: "def __init__(\n        self,\n        *,\n        name: str,\n        config:\
      \ LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None = None,\n\
      \        cache: PipelineCache | None = None,\n    ) -> None:\n        model_config\
      \ = _create_openai_config(config, azure=True)\n        error_handler = _create_error_handler(callbacks)\
      \ if callbacks else None\n        model_cache = _create_cache(cache, name)\n\
      \        client = create_openai_client(model_config)\n        self.model = create_openai_embeddings_llm(\n\
      \            model_config,\n            client=client,\n            cache=model_cache,\n\
      \            events=FNLLMEvents(error_handler) if error_handler else None,\n\
      \        )\n        self.config = config"
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::_create_cache
      type: internal
    - target: fnllm.openai::create_openai_client
      type: external
    - target: fnllm.openai::create_openai_embeddings_llm
      type: external
    - target: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
    called_by: []
  - name: aembed_batch
    start_line: 379
    end_line: 396
    code: "async def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]:\n\
      \        \"\"\"\n        Embed the given text using the Model.\n\n        Args:\n\
      \            text: The text to embed.\n            kwargs: Additional arguments\
      \ to pass to the Model.\n\n        Returns\n        -------\n            The\
      \ embeddings of the text.\n        \"\"\"\n        response = await self.model(text_list,\
      \ **kwargs)\n        if response.output.embeddings is None:\n            msg\
      \ = \"No embeddings found in response\"\n            raise ValueError(msg)\n\
      \        embeddings: list[list[float]] = response.output.embeddings\n      \
      \  return embeddings"
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.model
      type: instance
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed_batch
    called_by: []
  - name: aembed
    start_line: 398
    end_line: 415
    code: "async def aembed(self, text: str, **kwargs) -> list[float]:\n        \"\
      \"\"\n        Embed the given text using the Model.\n\n        Args:\n     \
      \       text: The text to embed.\n            kwargs: Additional arguments to\
      \ pass to the Model.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        response = await self.model([text],\
      \ **kwargs)\n        if response.output.embeddings is None:\n            msg\
      \ = \"No embeddings found in response\"\n            raise ValueError(msg)\n\
      \        embeddings: list[float] = response.output.embeddings[0]\n        return\
      \ embeddings"
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: self.model
      type: instance
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed
    called_by: []
  - name: embed_batch
    start_line: 417
    end_line: 429
    code: "def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]:\n\
      \        \"\"\"\n        Embed the given text using the Model.\n\n        Args:\n\
      \            text: The text to embed.\n            kwargs: Additional arguments\
      \ to pass to the Model.\n\n        Returns\n        -------\n            The\
      \ embeddings of the text.\n        \"\"\"\n        return run_coroutine_sync(self.aembed_batch(text_list,\
      \ **kwargs))"
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::aembed_batch
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
    called_by: []
  - name: embed
    start_line: 431
    end_line: 443
    code: "def embed(self, text: str, **kwargs) -> list[float]:\n        \"\"\"\n\
      \        Embed the given text using the Model.\n\n        Args:\n          \
      \  text: The text to embed.\n            kwargs: Additional arguments to pass\
      \ to the Model.\n\n        Returns\n        -------\n            The embeddings\
      \ of the text.\n        \"\"\"\n        return run_coroutine_sync(self.aembed(text,\
      \ **kwargs))"
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
      type: internal
    - target: graphrag/language_model/providers/fnllm/models.py::aembed
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
    called_by: []
- file_name: graphrag/language_model/providers/fnllm/utils.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: threading
    name: null
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: fnllm.base.config
    name: JsonStrategy
    alias: null
  - module: fnllm.base.config
    name: RetryStrategy
    alias: null
  - module: fnllm.openai
    name: AzureOpenAIConfig
    alias: null
  - module: fnllm.openai
    name: OpenAIConfig
    alias: null
  - module: fnllm.openai
    name: PublicOpenAIConfig
    alias: null
  - module: fnllm.openai.types.chat.parameters
    name: OpenAIChatParameters
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.language_model.providers.fnllm.cache
    name: FNLLMCacheProvider
    alias: null
  - module: collections.abc
    name: Coroutine
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.callbacks.workflow_callbacks
    name: WorkflowCallbacks
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.index.typing.error_handler
    name: ErrorHandlerFn
    alias: null
  functions:
  - name: _create_cache
    start_line: 33
    end_line: 37
    code: "def _create_cache(cache: PipelineCache | None, name: str) -> FNLLMCacheProvider\
      \ | None:\n    \"\"\"Create an FNLLM cache from a pipeline cache.\"\"\"\n  \
      \  if cache is None:\n        return None\n    return FNLLMCacheProvider(cache).child(name)"
    signature: 'def _create_cache(cache: PipelineCache | None, name: str) -> FNLLMCacheProvider
      | None'
    decorators: []
    raises: []
    calls:
    - target: FNLLMCacheProvider(cache).child
      type: unresolved
    - target: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/utils.py::_create_cache
    called_by:
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
      type: internal
  - name: _create_error_handler
    start_line: 40
    end_line: 54
    code: "def _create_error_handler(callbacks: WorkflowCallbacks) -> ErrorHandlerFn:\
      \  # noqa: ARG001\n    \"\"\"Create an error handler from a WorkflowCallbacks.\"\
      \"\"\n\n    def on_error(\n        error: BaseException | None = None,\n   \
      \     stack: str | None = None,\n        details: dict | None = None,\n    )\
      \ -> None:\n        logger.error(\n            \"Error Invoking LLM\",\n   \
      \         exc_info=error,\n            extra={\"stack\": stack, \"details\"\
      : details},\n        )\n\n    return on_error"
    signature: 'def _create_error_handler(callbacks: WorkflowCallbacks) -> ErrorHandlerFn'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
    called_by:
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
      type: internal
  - name: on_error
    start_line: 43
    end_line: 52
    code: "def on_error(\n        error: BaseException | None = None,\n        stack:\
      \ str | None = None,\n        details: dict | None = None,\n    ) -> None:\n\
      \        logger.error(\n            \"Error Invoking LLM\",\n            exc_info=error,\n\
      \            extra={\"stack\": stack, \"details\": details},\n        )"
    signature: "def on_error(\n        error: BaseException | None = None,\n     \
      \   stack: str | None = None,\n        details: dict | None = None,\n    ) ->\
      \ None"
    decorators: []
    raises: []
    calls:
    - target: logger.error
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/utils.py::on_error
    called_by: []
  - name: _create_openai_config
    start_line: 57
    end_line: 107
    code: "def _create_openai_config(config: LanguageModelConfig, azure: bool) ->\
      \ OpenAIConfig:\n    \"\"\"Create an OpenAIConfig from a LanguageModelConfig.\"\
      \"\"\n    encoding_model = config.encoding_model\n    json_strategy = (\n  \
      \      JsonStrategy.VALID if config.model_supports_json else JsonStrategy.LOOSE\n\
      \    )\n    chat_parameters = OpenAIChatParameters(\n        **get_openai_model_parameters_from_config(config)\n\
      \    )\n\n    if azure:\n        if config.api_base is None:\n            msg\
      \ = \"Azure OpenAI Chat LLM requires an API base\"\n            raise ValueError(msg)\n\
      \n        audience = config.audience or defs.COGNITIVE_SERVICES_AUDIENCE\n \
      \       return AzureOpenAIConfig(\n            api_key=config.api_key,\n   \
      \         endpoint=config.api_base,\n            json_strategy=json_strategy,\n\
      \            api_version=config.api_version,\n            organization=config.organization,\n\
      \            max_retries=config.max_retries,\n            max_retry_wait=config.max_retry_wait,\n\
      \            requests_per_minute=config.requests_per_minute,\n            tokens_per_minute=config.tokens_per_minute,\n\
      \            audience=audience,\n            retry_strategy=RetryStrategy(config.retry_strategy),\n\
      \            timeout=config.request_timeout,\n            max_concurrency=config.concurrent_requests,\n\
      \            model=config.model,\n            encoding=encoding_model,\n   \
      \         deployment=config.deployment_name,\n            chat_parameters=chat_parameters,\n\
      \        )\n    return PublicOpenAIConfig(\n        api_key=config.api_key,\n\
      \        base_url=config.api_base,\n        json_strategy=json_strategy,\n \
      \       organization=config.organization,\n        retry_strategy=RetryStrategy(config.retry_strategy),\n\
      \        max_retries=config.max_retries,\n        max_retry_wait=config.max_retry_wait,\n\
      \        requests_per_minute=config.requests_per_minute,\n        tokens_per_minute=config.tokens_per_minute,\n\
      \        timeout=config.request_timeout,\n        max_concurrency=config.concurrent_requests,\n\
      \        model=config.model,\n        encoding=encoding_model,\n        chat_parameters=chat_parameters,\n\
      \    )"
    signature: 'def _create_openai_config(config: LanguageModelConfig, azure: bool)
      -> OpenAIConfig'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: fnllm.openai.types.chat.parameters::OpenAIChatParameters
      type: external
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
      type: internal
    - target: ValueError
      type: builtin
    - target: fnllm.openai::AzureOpenAIConfig
      type: external
    - target: fnllm.base.config::RetryStrategy
      type: external
    - target: fnllm.openai::PublicOpenAIConfig
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
    called_by:
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
      type: internal
  - name: run_coroutine_sync
    start_line: 118
    end_line: 132
    code: "def run_coroutine_sync(coroutine: Coroutine[Any, Any, T]) -> T:\n    \"\
      \"\"\n    Run a coroutine synchronously.\n\n    Args:\n        coroutine: The\
      \ coroutine to run.\n\n    Returns\n    -------\n        The result of the coroutine.\n\
      \    \"\"\"\n    if not _thr.is_alive():\n        _thr.start()\n    future =\
      \ asyncio.run_coroutine_threadsafe(coroutine, _loop)\n    return future.result()"
    signature: 'def run_coroutine_sync(coroutine: Coroutine[Any, Any, T]) -> T'
    decorators: []
    raises: []
    calls:
    - target: _thr.is_alive
      type: unresolved
    - target: _thr.start
      type: unresolved
    - target: asyncio::run_coroutine_threadsafe
      type: stdlib
    - target: future.result
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
    called_by:
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
      type: internal
    - source: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
      type: internal
  - name: is_reasoning_model
    start_line: 135
    end_line: 137
    code: "def is_reasoning_model(model: str) -> bool:\n    \"\"\"Return whether the\
      \ model uses a known OpenAI reasoning model.\"\"\"\n    return model.lower()\
      \ in {\"o1\", \"o1-mini\", \"o3-mini\"}"
    signature: 'def is_reasoning_model(model: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: model.lower
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/utils.py::is_reasoning_model
    called_by:
    - source: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
      type: internal
  - name: get_openai_model_parameters_from_config
    start_line: 140
    end_line: 144
    code: "def get_openai_model_parameters_from_config(\n    config: LanguageModelConfig,\n\
      ) -> dict[str, Any]:\n    \"\"\"Get the model parameters for a given config,\
      \ adjusting for reasoning API differences.\"\"\"\n    return get_openai_model_parameters_from_dict(config.model_dump())"
    signature: "def get_openai_model_parameters_from_config(\n    config: LanguageModelConfig,\n\
      ) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
      type: internal
    - target: config.model_dump
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
    called_by:
    - source: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
      type: internal
    - source: graphrag/query/factory.py::get_local_search_engine
      type: internal
    - source: graphrag/query/factory.py::get_global_search_engine
      type: internal
    - source: graphrag/query/factory.py::get_basic_search_engine
      type: internal
  - name: get_openai_model_parameters_from_dict
    start_line: 147
    end_line: 165
    code: "def get_openai_model_parameters_from_dict(config: dict[str, Any]) -> dict[str,\
      \ Any]:\n    \"\"\"Get the model parameters for a given config, adjusting for\
      \ reasoning API differences.\"\"\"\n    params = {\n        \"n\": config.get(\"\
      n\"),\n    }\n    if is_reasoning_model(config[\"model\"]):\n        params[\"\
      max_completion_tokens\"] = config.get(\"max_completion_tokens\")\n        params[\"\
      reasoning_effort\"] = config.get(\"reasoning_effort\")\n    else:\n        params[\"\
      max_tokens\"] = config.get(\"max_tokens\")\n        params[\"temperature\"]\
      \ = config.get(\"temperature\")\n        params[\"frequency_penalty\"] = config.get(\"\
      frequency_penalty\")\n        params[\"presence_penalty\"] = config.get(\"presence_penalty\"\
      )\n        params[\"top_p\"] = config.get(\"top_p\")\n\n    if config.get(\"\
      response_format\"):\n        params[\"response_format\"] = config[\"response_format\"\
      ]\n\n    return params"
    signature: 'def get_openai_model_parameters_from_dict(config: dict[str, Any])
      -> dict[str, Any]'
    decorators: []
    raises: []
    calls:
    - target: config.get
      type: unresolved
    - target: graphrag/language_model/providers/fnllm/utils.py::is_reasoning_model
      type: internal
    visibility: public
    node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
    called_by:
    - source: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
      type: internal
    - source: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
      type: internal
    - source: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
      type: internal
    - source: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
      type: internal
- file_name: graphrag/language_model/providers/litellm/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/litellm/chat_model.py
  imports:
  - module: inspect
    name: null
    alias: null
  - module: json
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: collections.abc
    name: Generator
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: litellm
    name: null
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.identity
    name: get_bearer_token_provider
    alias: null
  - module: litellm
    name: CustomStreamWrapper
    alias: null
  - module: litellm
    name: ModelResponse
    alias: null
  - module: litellm
    name: acompletion
    alias: null
  - module: litellm
    name: completion
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  - module: graphrag.config.defaults
    name: COGNITIVE_SERVICES_AUDIENCE
    alias: null
  - module: graphrag.config.enums
    name: AuthType
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_cache
    name: with_cache
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_logging
    name: with_logging
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_rate_limiter
    name: with_rate_limiter
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_retries
    name: with_retries
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AFixedModelCompletion
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: FixedModelCompletion
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.language_model.response.base
    name: ModelResponse
    alias: null
  functions:
  - name: _create_base_completions
    start_line: 48
    end_line: 111
    code: "def _create_base_completions(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelCompletion, AFixedModelCompletion]:\n    \"\"\"Wrap\
      \ the base litellm completion function with the model configuration.\n\n   \
      \ Args\n    ----\n        model_config: The configuration for the language model.\n\
      \n    Returns\n    -------\n        A tuple containing the synchronous and asynchronous\
      \ completion functions.\n    \"\"\"\n    model_provider = model_config.model_provider\n\
      \    model = model_config.deployment_name or model_config.model\n\n    base_args:\
      \ dict[str, Any] = {\n        \"drop_params\": True,  # LiteLLM drop unsupported\
      \ params for selected model.\n        \"model\": f\"{model_provider}/{model}\"\
      ,\n        \"timeout\": model_config.request_timeout,\n        \"top_p\": model_config.top_p,\n\
      \        \"n\": model_config.n,\n        \"temperature\": model_config.temperature,\n\
      \        \"frequency_penalty\": model_config.frequency_penalty,\n        \"\
      presence_penalty\": model_config.presence_penalty,\n        \"api_base\": model_config.api_base,\n\
      \        \"api_version\": model_config.api_version,\n        \"api_key\": model_config.api_key,\n\
      \        \"organization\": model_config.organization,\n        \"proxy\": model_config.proxy,\n\
      \        \"audience\": model_config.audience,\n        \"max_tokens\": model_config.max_tokens,\n\
      \        \"max_completion_tokens\": model_config.max_completion_tokens,\n  \
      \      \"reasoning_effort\": model_config.reasoning_effort,\n    }\n\n    if\
      \ model_config.auth_type == AuthType.AzureManagedIdentity:\n        if model_config.model_provider\
      \ != \"azure\":\n            msg = \"Azure Managed Identity authentication is\
      \ only supported for Azure models.\"\n            raise ValueError(msg)\n\n\
      \        base_args[\"azure_scope\"] = base_args.pop(\"audience\")\n        base_args[\"\
      azure_ad_token_provider\"] = get_bearer_token_provider(\n            DefaultAzureCredential(),\n\
      \            model_config.audience or COGNITIVE_SERVICES_AUDIENCE,\n       \
      \ )\n\n    def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n\
      \        new_args = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n\
      \            new_args.pop(\"name\")\n\n        return completion(**new_args)\n\
      \n    async def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n\
      \        new_args = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n\
      \            new_args.pop(\"name\")\n\n        return await acompletion(**new_args)\n\
      \n    return (_base_completion, _base_acompletion)"
    signature: "def _create_base_completions(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelCompletion, AFixedModelCompletion]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: base_args.pop
      type: unresolved
    - target: azure.identity::get_bearer_token_provider
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_base_completions
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
  - name: _base_completion
    start_line: 95
    end_line: 101
    code: "def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n\
      \        new_args = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n\
      \            new_args.pop(\"name\")\n\n        return completion(**new_args)"
    signature: 'def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
    decorators: []
    raises: []
    calls:
    - target: new_args.pop
      type: unresolved
    - target: litellm::completion
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_completion
    called_by: []
  - name: _base_acompletion
    start_line: 103
    end_line: 109
    code: "async def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper:\n\
      \        new_args = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n\
      \            new_args.pop(\"name\")\n\n        return await acompletion(**new_args)"
    signature: 'def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
    decorators: []
    raises: []
    calls:
    - target: new_args.pop
      type: unresolved
    - target: litellm::acompletion
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_acompletion
    called_by: []
  - name: _create_completions
    start_line: 114
    end_line: 186
    code: "def _create_completions(\n    model_config: \"LanguageModelConfig\",\n\
      \    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelCompletion,\
      \ AFixedModelCompletion]:\n    \"\"\"Wrap the base litellm completion function\
      \ with the model configuration and additional features.\n\n    Wrap the base\
      \ litellm completion function with instance variables based on the model configuration.\n\
      \    Then wrap additional features such as rate limiting, retries, and caching,\
      \ if enabled.\n\n    Final function composition order:\n    - Logging(Cache(Retries(RateLimiter(ModelCompletion()))))\n\
      \n    Args\n    ----\n        model_config: The configuration for the language\
      \ model.\n        cache: Optional cache for storing responses.\n        cache_key_prefix:\
      \ Prefix for cache keys.\n\n    Returns\n    -------\n        A tuple containing\
      \ the synchronous and asynchronous completion functions.\n\n    \"\"\"\n   \
      \ completion, acompletion = _create_base_completions(model_config)\n\n    #\
      \ TODO: For v2.x release, rpm/tpm can be int or str (auto) for backwards compatibility\
      \ with fnllm.\n    # LiteLLM does not support \"auto\", so we have to check\
      \ those values here.\n    # For v3 release, force rpm/tpm to be int and remove\
      \ the type checks below\n    # and just check if rate_limit_strategy is enabled.\n\
      \    if model_config.rate_limit_strategy is not None:\n        rpm = (\n   \
      \         model_config.requests_per_minute\n            if type(model_config.requests_per_minute)\
      \ is int\n            else None\n        )\n        tpm = (\n            model_config.tokens_per_minute\n\
      \            if type(model_config.tokens_per_minute) is int\n            else\
      \ None\n        )\n        if rpm is not None or tpm is not None:\n        \
      \    completion, acompletion = with_rate_limiter(\n                sync_fn=completion,\n\
      \                async_fn=acompletion,\n                model_config=model_config,\n\
      \                rpm=rpm,\n                tpm=tpm,\n            )\n\n    if\
      \ model_config.retry_strategy != \"none\":\n        completion, acompletion\
      \ = with_retries(\n            sync_fn=completion,\n            async_fn=acompletion,\n\
      \            model_config=model_config,\n        )\n\n    if cache is not None:\n\
      \        completion, acompletion = with_cache(\n            sync_fn=completion,\n\
      \            async_fn=acompletion,\n            model_config=model_config,\n\
      \            cache=cache,\n            request_type=\"chat\",\n            cache_key_prefix=cache_key_prefix,\n\
      \        )\n\n    completion, acompletion = with_logging(\n        sync_fn=completion,\n\
      \        async_fn=acompletion,\n    )\n\n    return (completion, acompletion)"
    signature: "def _create_completions(\n    model_config: \"LanguageModelConfig\"\
      ,\n    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelCompletion,\
      \ AFixedModelCompletion]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/chat_model.py::_create_base_completions
      type: internal
    - target: type
      type: builtin
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
      type: internal
  - name: __init__
    start_line: 214
    end_line: 226
    code: "def __init__(\n        self,\n        name: str,\n        config: \"LanguageModelConfig\"\
      ,\n        cache: \"PipelineCache | None\" = None,\n        **kwargs: Any,\n\
      \    ):\n        self.name = name\n        self.config = config\n        self.cache\
      \ = cache.child(self.name) if cache else None\n        self.completion, self.acompletion\
      \ = _create_completions(\n            config, self.cache, \"chat\"\n       \
      \ )"
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    decorators: []
    raises: []
    calls:
    - target: cache.child
      type: unresolved
    - target: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
    called_by: []
  - name: _get_kwargs
    start_line: 228
    end_line: 264
    code: "def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]:\n        \"\"\"\
      Get model arguments supported by litellm.\"\"\"\n        args_to_include = [\n\
      \            \"name\",\n            \"modalities\",\n            \"prediction\"\
      ,\n            \"audio\",\n            \"logit_bias\",\n            \"metadata\"\
      ,\n            \"user\",\n            \"response_format\",\n            \"seed\"\
      ,\n            \"tools\",\n            \"tool_choice\",\n            \"logprobs\"\
      ,\n            \"top_logprobs\",\n            \"parallel_tool_calls\",\n   \
      \         \"web_search_options\",\n            \"extra_headers\",\n        \
      \    \"functions\",\n            \"function_call\",\n            \"thinking\"\
      ,\n        ]\n        new_args = {k: v for k, v in kwargs.items() if k in args_to_include}\n\
      \n        # If using JSON, check if response_format should be a Pydantic model\
      \ or just a general JSON object\n        if kwargs.get(\"json\"):\n        \
      \    new_args[\"response_format\"] = {\"type\": \"json_object\"}\n\n       \
      \     if (\n                \"json_model\" in kwargs\n                and inspect.isclass(kwargs[\"\
      json_model\"])\n                and issubclass(kwargs[\"json_model\"], BaseModel)\n\
      \            ):\n                new_args[\"response_format\"] = kwargs[\"json_model\"\
      ]\n\n        return new_args"
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    decorators: []
    raises: []
    calls:
    - target: kwargs.items
      type: unresolved
    - target: kwargs.get
      type: unresolved
    - target: inspect::isclass
      type: stdlib
    - target: issubclass
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel._get_kwargs
    called_by: []
  - name: achat
    start_line: 266
    end_line: 314
    code: "async def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> \"MR\":\n        \"\"\"\n        Generate a response\
      \ for the given prompt and history.\n\n        Args\n        ----\n        \
      \    prompt: The prompt to generate a response for.\n            history: Optional\
      \ chat history.\n            **kwargs: Additional keyword arguments (e.g., model\
      \ parameters).\n\n        Returns\n        -------\n            LitellmModelResponse:\
      \ The generated model response.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        messages: list[dict[str, str]] = history or []\n        messages.append({\"\
      role\": \"user\", \"content\": prompt})\n\n        response = await self.acompletion(messages=messages,\
      \ stream=False, **new_kwargs)  # type: ignore\n\n        messages.append({\n\
      \            \"role\": \"assistant\",\n            \"content\": response.choices[0].message.content\
      \ or \"\",  # type: ignore\n        })\n\n        parsed_response: BaseModel\
      \ | None = None\n        if \"response_format\" in new_kwargs:\n           \
      \ parsed_dict: dict[str, Any] = json.loads(\n                response.choices[0].message.content\
      \ or \"{}\"  # type: ignore\n            )\n            parsed_response = parsed_dict\
      \  # type: ignore\n            if inspect.isclass(new_kwargs[\"response_format\"\
      ]) and issubclass(\n                new_kwargs[\"response_format\"], BaseModel\n\
      \            ):\n                # If response_format is a pydantic model, instantiate\
      \ it\n                model_initializer = cast(\n                    \"type[BaseModel]\"\
      , new_kwargs[\"response_format\"]\n                )\n                parsed_response\
      \ = model_initializer(**parsed_dict)\n\n        return LitellmModelResponse(\n\
      \            output=LitellmModelOutput(\n                content=response.choices[0].message.content\
      \ or \"\"  # type: ignore\n            ),\n            parsed_response=parsed_response,\n\
      \            history=messages,\n        )"
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> \"MR\""
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
      type: internal
    - target: messages.append
      type: unresolved
    - target: self.acompletion
      type: instance
    - target: json::loads
      type: stdlib
    - target: inspect::isclass
      type: stdlib
    - target: issubclass
      type: builtin
    - target: typing::cast
      type: stdlib
    - target: model_initializer
      type: unresolved
    - target: LitellmModelResponse
      type: unresolved
    - target: LitellmModelOutput
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat
    called_by: []
  - name: achat_stream
    start_line: 316
    end_line: 340
    code: "async def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"\
      \n        Generate a response for the given prompt and history.\n\n        Args\n\
      \        ----\n            prompt: The prompt to generate a response for.\n\
      \            history: Optional chat history.\n            **kwargs: Additional\
      \ keyword arguments (e.g., model parameters).\n\n        Returns\n        -------\n\
      \            AsyncGenerator[str, None]: The generated response as a stream of\
      \ strings.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        messages: list[dict[str, str]] = history or []\n        messages.append({\"\
      role\": \"user\", \"content\": prompt})\n\n        response = await self.acompletion(messages=messages,\
      \ stream=True, **new_kwargs)  # type: ignore\n\n        async for chunk in response:\
      \  # type: ignore\n            if chunk.choices and chunk.choices[0].delta.content:\n\
      \                yield chunk.choices[0].delta.content"
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
      type: internal
    - target: messages.append
      type: unresolved
    - target: self.acompletion
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat_stream
    called_by: []
  - name: chat
    start_line: 342
    end_line: 388
    code: "def chat(self, prompt: str, history: list | None = None, **kwargs: Any)\
      \ -> \"MR\":\n        \"\"\"\n        Generate a response for the given prompt\
      \ and history.\n\n        Args\n        ----\n            prompt: The prompt\
      \ to generate a response for.\n            history: Optional chat history.\n\
      \            **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \n        Returns\n        -------\n            LitellmModelResponse: The generated\
      \ model response.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        messages: list[dict[str, str]] = history or []\n        messages.append({\"\
      role\": \"user\", \"content\": prompt})\n\n        response = self.completion(messages=messages,\
      \ stream=False, **new_kwargs)  # type: ignore\n\n        messages.append({\n\
      \            \"role\": \"assistant\",\n            \"content\": response.choices[0].message.content\
      \ or \"\",  # type: ignore\n        })\n\n        parsed_response: BaseModel\
      \ | None = None\n        if \"response_format\" in new_kwargs:\n           \
      \ parsed_dict: dict[str, Any] = json.loads(\n                response.choices[0].message.content\
      \ or \"{}\"  # type: ignore\n            )\n            parsed_response = parsed_dict\
      \  # type: ignore\n            if inspect.isclass(new_kwargs[\"response_format\"\
      ]) and issubclass(\n                new_kwargs[\"response_format\"], BaseModel\n\
      \            ):\n                # If response_format is a pydantic model, instantiate\
      \ it\n                model_initializer = cast(\n                    \"type[BaseModel]\"\
      , new_kwargs[\"response_format\"]\n                )\n                parsed_response\
      \ = model_initializer(**parsed_dict)\n\n        return LitellmModelResponse(\n\
      \            output=LitellmModelOutput(\n                content=response.choices[0].message.content\
      \ or \"\"  # type: ignore\n            ),\n            parsed_response=parsed_response,\n\
      \            history=messages,\n        )"
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs:
      Any) -> "MR"'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
      type: internal
    - target: messages.append
      type: unresolved
    - target: self.completion
      type: instance
    - target: json::loads
      type: stdlib
    - target: inspect::isclass
      type: stdlib
    - target: issubclass
      type: builtin
    - target: typing::cast
      type: stdlib
    - target: model_initializer
      type: unresolved
    - target: LitellmModelResponse
      type: unresolved
    - target: LitellmModelOutput
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat
    called_by: []
  - name: chat_stream
    start_line: 390
    end_line: 414
    code: "def chat_stream(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> Generator[str, None]:\n        \"\"\"\n        Generate\
      \ a response for the given prompt and history.\n\n        Args\n        ----\n\
      \            prompt: The prompt to generate a response for.\n            history:\
      \ Optional chat history.\n            **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\n        Returns\n        -------\n          \
      \  Generator[str, None]: The generated response as a stream of strings.\n  \
      \      \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n        messages:\
      \ list[dict[str, str]] = history or []\n        messages.append({\"role\": \"\
      user\", \"content\": prompt})\n\n        response = self.completion(messages=messages,\
      \ stream=True, **new_kwargs)  # type: ignore\n\n        for chunk in response:\n\
      \            if chunk.choices and chunk.choices[0].delta.content:  # type: ignore\n\
      \                yield chunk.choices[0].delta.content  # type: ignore"
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
      type: internal
    - target: messages.append
      type: unresolved
    - target: self.completion
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat_stream
    called_by: []
- file_name: graphrag/language_model/providers/litellm/embedding_model.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: litellm
    name: null
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.identity
    name: get_bearer_token_provider
    alias: null
  - module: litellm
    name: EmbeddingResponse
    alias: null
  - module: litellm
    name: aembedding
    alias: null
  - module: litellm
    name: embedding
    alias: null
  - module: graphrag.config.defaults
    name: COGNITIVE_SERVICES_AUDIENCE
    alias: null
  - module: graphrag.config.enums
    name: AuthType
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_cache
    name: with_cache
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_logging
    name: with_logging
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_rate_limiter
    name: with_rate_limiter
    alias: null
  - module: graphrag.language_model.providers.litellm.request_wrappers.with_retries
    name: with_retries
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AFixedModelEmbedding
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: FixedModelEmbedding
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: _create_base_embeddings
    start_line: 42
    end_line: 97
    code: "def _create_base_embeddings(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]:\n    \"\"\"Wrap the\
      \ base litellm embedding function with the model configuration.\n\n    Args\n\
      \    ----\n        model_config: The configuration for the language model.\n\
      \n    Returns\n    -------\n        A tuple containing the synchronous and asynchronous\
      \ embedding functions.\n    \"\"\"\n    model_provider = model_config.model_provider\n\
      \    model = model_config.deployment_name or model_config.model\n\n    base_args:\
      \ dict[str, Any] = {\n        \"drop_params\": True,  # LiteLLM drop unsupported\
      \ params for selected model.\n        \"model\": f\"{model_provider}/{model}\"\
      ,\n        \"timeout\": model_config.request_timeout,\n        \"api_base\"\
      : model_config.api_base,\n        \"api_version\": model_config.api_version,\n\
      \        \"api_key\": model_config.api_key,\n        \"organization\": model_config.organization,\n\
      \        \"proxy\": model_config.proxy,\n        \"audience\": model_config.audience,\n\
      \    }\n\n    if model_config.auth_type == AuthType.AzureManagedIdentity:\n\
      \        if model_config.model_provider != \"azure\":\n            msg = \"\
      Azure Managed Identity authentication is only supported for Azure models.\"\n\
      \            raise ValueError(msg)\n\n        base_args[\"azure_scope\"] = base_args.pop(\"\
      audience\")\n        base_args[\"azure_ad_token_provider\"] = get_bearer_token_provider(\n\
      \            DefaultAzureCredential(),\n            model_config.audience or\
      \ COGNITIVE_SERVICES_AUDIENCE,\n        )\n\n    def _base_embedding(**kwargs:\
      \ Any) -> EmbeddingResponse:\n        new_args = {**base_args, **kwargs}\n\n\
      \        if \"name\" in new_args:\n            new_args.pop(\"name\")\n\n  \
      \      return embedding(**new_args)\n\n    async def _base_aembedding(**kwargs:\
      \ Any) -> EmbeddingResponse:\n        new_args = {**base_args, **kwargs}\n\n\
      \        if \"name\" in new_args:\n            new_args.pop(\"name\")\n\n  \
      \      return await aembedding(**new_args)\n\n    return (_base_embedding, _base_aembedding)"
    signature: "def _create_base_embeddings(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: base_args.pop
      type: unresolved
    - target: azure.identity::get_bearer_token_provider
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_base_embeddings
    called_by:
    - source: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
  - name: _base_embedding
    start_line: 81
    end_line: 87
    code: "def _base_embedding(**kwargs: Any) -> EmbeddingResponse:\n        new_args\
      \ = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n          \
      \  new_args.pop(\"name\")\n\n        return embedding(**new_args)"
    signature: 'def _base_embedding(**kwargs: Any) -> EmbeddingResponse'
    decorators: []
    raises: []
    calls:
    - target: new_args.pop
      type: unresolved
    - target: litellm::embedding
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_embedding
    called_by: []
  - name: _base_aembedding
    start_line: 89
    end_line: 95
    code: "async def _base_aembedding(**kwargs: Any) -> EmbeddingResponse:\n     \
      \   new_args = {**base_args, **kwargs}\n\n        if \"name\" in new_args:\n\
      \            new_args.pop(\"name\")\n\n        return await aembedding(**new_args)"
    signature: 'def _base_aembedding(**kwargs: Any) -> EmbeddingResponse'
    decorators: []
    raises: []
    calls:
    - target: new_args.pop
      type: unresolved
    - target: litellm::aembedding
      type: external
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_aembedding
    called_by: []
  - name: _create_embeddings
    start_line: 100
    end_line: 172
    code: "def _create_embeddings(\n    model_config: \"LanguageModelConfig\",\n \
      \   cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelEmbedding,\
      \ AFixedModelEmbedding]:\n    \"\"\"Wrap the base litellm embedding function\
      \ with the model configuration and additional features.\n\n    Wrap the base\
      \ litellm embedding function with instance variables based on the model configuration.\n\
      \    Then wrap additional features such as rate limiting, retries, and caching,\
      \ if enabled.\n\n    Final function composition order:\n    - Logging(Cache(Retries(RateLimiter(ModelEmbedding()))))\n\
      \n    Args\n    ----\n        model_config: The configuration for the language\
      \ model.\n        cache: Optional cache for storing responses.\n        cache_key_prefix:\
      \ Prefix for cache keys.\n\n    Returns\n    -------\n        A tuple containing\
      \ the synchronous and asynchronous embedding functions.\n\n    \"\"\"\n    embedding,\
      \ aembedding = _create_base_embeddings(model_config)\n\n    # TODO: For v2.x\
      \ release, rpm/tpm can be int or str (auto) for backwards compatibility with\
      \ fnllm.\n    # LiteLLM does not support \"auto\", so we have to check those\
      \ values here.\n    # For v3 release, force rpm/tpm to be int and remove the\
      \ type checks below\n    # and just check if rate_limit_strategy is enabled.\n\
      \    if model_config.rate_limit_strategy is not None:\n        rpm = (\n   \
      \         model_config.requests_per_minute\n            if type(model_config.requests_per_minute)\
      \ is int\n            else None\n        )\n        tpm = (\n            model_config.tokens_per_minute\n\
      \            if type(model_config.tokens_per_minute) is int\n            else\
      \ None\n        )\n        if rpm is not None or tpm is not None:\n        \
      \    embedding, aembedding = with_rate_limiter(\n                sync_fn=embedding,\n\
      \                async_fn=aembedding,\n                model_config=model_config,\n\
      \                rpm=rpm,\n                tpm=tpm,\n            )\n\n    if\
      \ model_config.retry_strategy != \"none\":\n        embedding, aembedding =\
      \ with_retries(\n            sync_fn=embedding,\n            async_fn=aembedding,\n\
      \            model_config=model_config,\n        )\n\n    if cache is not None:\n\
      \        embedding, aembedding = with_cache(\n            sync_fn=embedding,\n\
      \            async_fn=aembedding,\n            model_config=model_config,\n\
      \            cache=cache,\n            request_type=\"embedding\",\n       \
      \     cache_key_prefix=cache_key_prefix,\n        )\n\n    embedding, aembedding\
      \ = with_logging(\n        sync_fn=embedding,\n        async_fn=aembedding,\n\
      \    )\n\n    return (embedding, aembedding)"
    signature: "def _create_embeddings(\n    model_config: \"LanguageModelConfig\"\
      ,\n    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelEmbedding,\
      \ AFixedModelEmbedding]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_create_base_embeddings
      type: internal
    - target: type
      type: builtin
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
      type: internal
    - target: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
    called_by:
    - source: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
      type: internal
  - name: __init__
    start_line: 178
    end_line: 190
    code: "def __init__(\n        self,\n        name: str,\n        config: \"LanguageModelConfig\"\
      ,\n        cache: \"PipelineCache | None\" = None,\n        **kwargs: Any,\n\
      \    ):\n        self.name = name\n        self.config = config\n        self.cache\
      \ = cache.child(self.name) if cache else None\n        self.embedding, self.aembedding\
      \ = _create_embeddings(\n            config, self.cache, \"embeddings\"\n  \
      \      )"
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    decorators: []
    raises: []
    calls:
    - target: cache.child
      type: unresolved
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
    called_by: []
  - name: _get_kwargs
    start_line: 192
    end_line: 201
    code: "def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]:\n        \"\"\"\
      Get model arguments supported by litellm.\"\"\"\n        args_to_include = [\n\
      \            \"name\",\n            \"dimensions\",\n            \"encoding_format\"\
      ,\n            \"timeout\",\n            \"user\",\n        ]\n        return\
      \ {k: v for k, v in kwargs.items() if k in args_to_include}"
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    decorators: []
    raises: []
    calls:
    - target: kwargs.items
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel._get_kwargs
    called_by: []
  - name: aembed_batch
    start_line: 203
    end_line: 221
    code: "async def aembed_batch(\n        self, text_list: list[str], **kwargs:\
      \ Any\n    ) -> list[list[float]]:\n        \"\"\"\n        Batch generate embeddings.\n\
      \n        Args\n        ----\n            text_list: A batch of text inputs\
      \ to generate embeddings for.\n            **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\n        Returns\n        -------\n          \
      \  A Batch of embeddings.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        response = await self.aembedding(input=text_list, **new_kwargs)\n\n\
      \        return [emb.get(\"embedding\", []) for emb in response.data]"
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
      type: internal
    - target: self.aembedding
      type: instance
    - target: emb.get
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed_batch
    called_by: []
  - name: aembed
    start_line: 223
    end_line: 242
    code: "async def aembed(self, text: str, **kwargs: Any) -> list[float]:\n    \
      \    \"\"\"\n        Async embed.\n\n        Args:\n            text: The text\
      \ to generate an embedding for.\n            **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\n        Returns\n        -------\n          \
      \  An embedding.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        response = await self.aembedding(input=[text], **new_kwargs)\n\n  \
      \      return (\n            response.data[0].get(\"embedding\", [])\n     \
      \       if response.data and response.data[0]\n            else []\n       \
      \ )"
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
      type: internal
    - target: self.aembedding
      type: instance
    - target: response.data[0].get
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed
    called_by: []
  - name: embed_batch
    start_line: 244
    end_line: 259
    code: "def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]:\n\
      \        \"\"\"\n        Batch generate embeddings.\n\n        Args:\n     \
      \       text_list: A batch of text inputs to generate embeddings for.\n    \
      \        **kwargs: Additional keyword arguments (e.g., model parameters).\n\n\
      \        Returns\n        -------\n            A Batch of embeddings.\n    \
      \    \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n        response\
      \ = self.embedding(input=text_list, **new_kwargs)\n\n        return [emb.get(\"\
      embedding\", []) for emb in response.data]"
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
      type: internal
    - target: self.embedding
      type: instance
    - target: emb.get
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed_batch
    called_by: []
  - name: embed
    start_line: 261
    end_line: 280
    code: "def embed(self, text: str, **kwargs: Any) -> list[float]:\n        \"\"\
      \"\n        Embed a single text input.\n\n        Args:\n            text: The\
      \ text to generate an embedding for.\n            **kwargs: Additional keyword\
      \ arguments (e.g., model parameters).\n\n        Returns\n        -------\n\
      \            An embedding.\n        \"\"\"\n        new_kwargs = self._get_kwargs(**kwargs)\n\
      \        response = self.embedding(input=[text], **new_kwargs)\n\n        return\
      \ (\n            response.data[0].get(\"embedding\", [])\n            if response.data\
      \ and response.data[0]\n            else []\n        )"
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
      type: internal
    - target: self.embedding
      type: instance
    - target: response.data[0].get
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed
    called_by: []
- file_name: graphrag/language_model/providers/litellm/get_cache_key.py
  imports:
  - module: hashlib
    name: null
    alias: null
  - module: inspect
    name: null
    alias: null
  - module: json
    name: null
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: get_cache_key
    start_line: 33
    end_line: 78
    code: "def get_cache_key(\n    model_config: \"LanguageModelConfig\",\n    prefix:\
      \ str,\n    messages: str | None = None,\n    input: str | None = None,\n  \
      \  **kwargs: Any,\n) -> str:\n    \"\"\"Generate a cache key based on the model\
      \ configuration and input arguments.\n\n    Modeled after the fnllm cache key\
      \ generation.\n    https://github.com/microsoft/essex-toolkit/blob/23d3077b65c0e8f1d89c397a2968fe570a25f790/python/fnllm/fnllm/caching/base.py#L50\n\
      \n    Args\n    ____\n        model_config: The configuration of the language\
      \ model.\n        prefix: A prefix for the cache key.\n        **kwargs: Additional\
      \ model input parameters.\n\n    Returns\n    -------\n        `{prefix}_{data_hash}_v{version}`\
      \ if prefix is provided.\n    \"\"\"\n    cache_key: dict[str, Any] = {\n  \
      \      \"parameters\": _get_parameters(model_config, **kwargs),\n    }\n\n \
      \   if messages is not None and input is not None:\n        msg = \"Only one\
      \ of 'messages' or 'input' should be provided.\"\n        raise ValueError(msg)\n\
      \n    if messages is not None:\n        cache_key[\"messages\"] = messages\n\
      \    elif input is not None:\n        cache_key[\"input\"] = input\n    else:\n\
      \        msg = \"Either 'messages' or 'input' must be provided.\"\n        raise\
      \ ValueError(msg)\n\n    data_hash = _hash(json.dumps(cache_key, sort_keys=True))\n\
      \n    name = kwargs.get(\"name\")\n\n    if name:\n        prefix += f\"_{name}\"\
      \n\n    return f\"{prefix}_{data_hash}_v{_CACHE_VERSION}\""
    signature: "def get_cache_key(\n    model_config: \"LanguageModelConfig\",\n \
      \   prefix: str,\n    messages: str | None = None,\n    input: str | None =\
      \ None,\n    **kwargs: Any,\n) -> str"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/language_model/providers/litellm/get_cache_key.py::_get_parameters
      type: internal
    - target: ValueError
      type: builtin
    - target: graphrag/language_model/providers/litellm/get_cache_key.py::_hash
      type: internal
    - target: json::dumps
      type: stdlib
    - target: kwargs.get
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
    called_by:
    - source: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache
      type: internal
    - source: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache_async
      type: internal
  - name: _get_parameters
    start_line: 81
    end_line: 135
    code: "def _get_parameters(\n    model_config: \"LanguageModelConfig\",\n    **kwargs:\
      \ Any,\n) -> dict[str, Any]:\n    \"\"\"Pluck out the parameters that define\
      \ a cache key.\n\n    Use the same parameters as fnllm except request timeout.\n\
      \    - embeddings: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/embeddings/parameters.py#L12\n\
      \    - chat: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/chat/parameters.py#L25\n\
      \n    Args\n    ____\n        model_config: The configuration of the language\
      \ model.\n        **kwargs: Additional model input parameters.\n\n    Returns\n\
      \    -------\n        dict[str, Any]: A dictionary of parameters that define\
      \ the cache key.\n    \"\"\"\n    parameters = {\n        \"model\": model_config.deployment_name\
      \ or model_config.model,\n        \"frequency_penalty\": model_config.frequency_penalty,\n\
      \        \"max_tokens\": model_config.max_tokens,\n        \"max_completion_tokens\"\
      : model_config.max_completion_tokens,\n        \"n\": model_config.n,\n    \
      \    \"presence_penalty\": model_config.presence_penalty,\n        \"temperature\"\
      : model_config.temperature,\n        \"top_p\": model_config.top_p,\n      \
      \  \"reasoning_effort\": model_config.reasoning_effort,\n    }\n    keys_to_cache\
      \ = [\n        \"function_call\",\n        \"functions\",\n        \"logit_bias\"\
      ,\n        \"logprobs\",\n        \"parallel_tool_calls\",\n        \"seed\"\
      ,\n        \"service_tier\",\n        \"stop\",\n        \"tool_choice\",\n\
      \        \"tools\",\n        \"top_logprobs\",\n        \"user\",\n        \"\
      dimensions\",\n        \"encoding_format\",\n    ]\n    parameters.update({key:\
      \ kwargs.get(key) for key in keys_to_cache if key in kwargs})\n\n    response_format\
      \ = kwargs.get(\"response_format\")\n    if inspect.isclass(response_format)\
      \ and issubclass(response_format, BaseModel):\n        parameters[\"response_format\"\
      ] = str(response_format)\n    elif response_format is not None:\n        parameters[\"\
      response_format\"] = response_format\n\n    return parameters"
    signature: "def _get_parameters(\n    model_config: \"LanguageModelConfig\",\n\
      \    **kwargs: Any,\n) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: parameters.update
      type: unresolved
    - target: kwargs.get
      type: unresolved
    - target: inspect::isclass
      type: stdlib
    - target: issubclass
      type: builtin
    - target: str
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_get_parameters
    called_by:
    - source: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
      type: internal
  - name: _hash
    start_line: 138
    end_line: 140
    code: "def _hash(input: str) -> str:\n    \"\"\"Generate a hash for the input\
      \ string.\"\"\"\n    return hashlib.sha256(input.encode()).hexdigest()"
    signature: 'def _hash(input: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: hashlib::sha256(input.encode()).hexdigest
      type: stdlib
    - target: hashlib::sha256
      type: stdlib
    - target: input.encode
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_hash
    called_by:
    - source: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
      type: internal
- file_name: graphrag/language_model/providers/litellm/request_wrappers/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Literal
    alias: null
  - module: litellm
    name: EmbeddingResponse
    alias: null
  - module: litellm
    name: ModelResponse
    alias: null
  - module: graphrag.language_model.providers.litellm.get_cache_key
    name: get_cache_key
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AsyncLitellmRequestFunc
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: LitellmRequestFunc
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: with_cache
    start_line: 22
    end_line: 107
    code: "def with_cache(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n   \
      \ cache: \"PipelineCache\",\n    request_type: Literal[\"chat\", \"embedding\"\
      ],\n    cache_key_prefix: str,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n\
      \    \"\"\"\n    Wrap the synchronous and asynchronous request functions with\
      \ caching.\n\n    Args\n    ----\n        sync_fn: The synchronous chat/embedding\
      \ request function to wrap.\n        async_fn: The asynchronous chat/embedding\
      \ request function to wrap.\n        model_config: The configuration for the\
      \ language model.\n        cache: The cache to use for storing responses.\n\
      \        request_type: The type of request being made, either \"chat\" or \"\
      embedding\".\n        cache_key_prefix: The prefix to use for cache keys.\n\n\
      \    Returns\n    -------\n        A tuple containing the wrapped synchronous\
      \ and asynchronous chat/embedding request functions.\n    \"\"\"\n\n    def\
      \ _wrapped_with_cache(**kwargs: Any) -> Any:\n        is_streaming = kwargs.get(\"\
      stream\", False)\n        if is_streaming:\n            return sync_fn(**kwargs)\n\
      \        cache_key = get_cache_key(\n            model_config=model_config,\
      \ prefix=cache_key_prefix, **kwargs\n        )\n        event_loop = asyncio.get_event_loop()\n\
      \        cached_response = event_loop.run_until_complete(cache.get(cache_key))\n\
      \        if (\n            cached_response is not None\n            and isinstance(cached_response,\
      \ dict)\n            and \"response\" in cached_response\n            and cached_response[\"\
      response\"] is not None\n            and isinstance(cached_response[\"response\"\
      ], dict)\n        ):\n            try:\n                if request_type == \"\
      chat\":\n                    return ModelResponse(**cached_response[\"response\"\
      ])\n                return EmbeddingResponse(**cached_response[\"response\"\
      ])\n            except Exception:  # noqa: BLE001\n                # Try to\
      \ retrieve value from cache but if it fails, continue\n                # to\
      \ make the request.\n                ...\n        response = sync_fn(**kwargs)\n\
      \        event_loop.run_until_complete(\n            cache.set(cache_key, {\"\
      response\": response.model_dump()})\n        )\n        return response\n\n\
      \    async def _wrapped_with_cache_async(\n        **kwargs: Any,\n    ) ->\
      \ Any:\n        is_streaming = kwargs.get(\"stream\", False)\n        if is_streaming:\n\
      \            return await async_fn(**kwargs)\n        cache_key = get_cache_key(\n\
      \            model_config=model_config, prefix=cache_key_prefix, **kwargs\n\
      \        )\n        cached_response = await cache.get(cache_key)\n        if\
      \ (\n            cached_response is not None\n            and isinstance(cached_response,\
      \ dict)\n            and \"response\" in cached_response\n            and cached_response[\"\
      response\"] is not None\n            and isinstance(cached_response[\"response\"\
      ], dict)\n        ):\n            try:\n                if request_type == \"\
      chat\":\n                    return ModelResponse(**cached_response[\"response\"\
      ])\n                return EmbeddingResponse(**cached_response[\"response\"\
      ])\n            except Exception:  # noqa: BLE001\n                # Try to\
      \ retrieve value from cache but if it fails, continue\n                # to\
      \ make the request.\n                ...\n        response = await async_fn(**kwargs)\n\
      \        await cache.set(cache_key, {\"response\": response.model_dump()})\n\
      \        return response\n\n    return (_wrapped_with_cache, _wrapped_with_cache_async)"
    signature: "def with_cache(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n   \
      \ cache: \"PipelineCache\",\n    request_type: Literal[\"chat\", \"embedding\"\
      ],\n    cache_key_prefix: str,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
    - source: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
  - name: _wrapped_with_cache
    start_line: 48
    end_line: 76
    code: "def _wrapped_with_cache(**kwargs: Any) -> Any:\n        is_streaming =\
      \ kwargs.get(\"stream\", False)\n        if is_streaming:\n            return\
      \ sync_fn(**kwargs)\n        cache_key = get_cache_key(\n            model_config=model_config,\
      \ prefix=cache_key_prefix, **kwargs\n        )\n        event_loop = asyncio.get_event_loop()\n\
      \        cached_response = event_loop.run_until_complete(cache.get(cache_key))\n\
      \        if (\n            cached_response is not None\n            and isinstance(cached_response,\
      \ dict)\n            and \"response\" in cached_response\n            and cached_response[\"\
      response\"] is not None\n            and isinstance(cached_response[\"response\"\
      ], dict)\n        ):\n            try:\n                if request_type == \"\
      chat\":\n                    return ModelResponse(**cached_response[\"response\"\
      ])\n                return EmbeddingResponse(**cached_response[\"response\"\
      ])\n            except Exception:  # noqa: BLE001\n                # Try to\
      \ retrieve value from cache but if it fails, continue\n                # to\
      \ make the request.\n                ...\n        response = sync_fn(**kwargs)\n\
      \        event_loop.run_until_complete(\n            cache.set(cache_key, {\"\
      response\": response.model_dump()})\n        )\n        return response"
    signature: 'def _wrapped_with_cache(**kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: kwargs.get
      type: unresolved
    - target: sync_fn
      type: unresolved
    - target: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
      type: internal
    - target: asyncio::get_event_loop
      type: stdlib
    - target: event_loop.run_until_complete
      type: unresolved
    - target: cache.get
      type: unresolved
    - target: isinstance
      type: builtin
    - target: litellm::ModelResponse
      type: external
    - target: litellm::EmbeddingResponse
      type: external
    - target: cache.set
      type: unresolved
    - target: response.model_dump
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache
    called_by: []
  - name: _wrapped_with_cache_async
    start_line: 78
    end_line: 105
    code: "async def _wrapped_with_cache_async(\n        **kwargs: Any,\n    ) ->\
      \ Any:\n        is_streaming = kwargs.get(\"stream\", False)\n        if is_streaming:\n\
      \            return await async_fn(**kwargs)\n        cache_key = get_cache_key(\n\
      \            model_config=model_config, prefix=cache_key_prefix, **kwargs\n\
      \        )\n        cached_response = await cache.get(cache_key)\n        if\
      \ (\n            cached_response is not None\n            and isinstance(cached_response,\
      \ dict)\n            and \"response\" in cached_response\n            and cached_response[\"\
      response\"] is not None\n            and isinstance(cached_response[\"response\"\
      ], dict)\n        ):\n            try:\n                if request_type == \"\
      chat\":\n                    return ModelResponse(**cached_response[\"response\"\
      ])\n                return EmbeddingResponse(**cached_response[\"response\"\
      ])\n            except Exception:  # noqa: BLE001\n                # Try to\
      \ retrieve value from cache but if it fails, continue\n                # to\
      \ make the request.\n                ...\n        response = await async_fn(**kwargs)\n\
      \        await cache.set(cache_key, {\"response\": response.model_dump()})\n\
      \        return response"
    signature: "def _wrapped_with_cache_async(\n        **kwargs: Any,\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: kwargs.get
      type: unresolved
    - target: async_fn
      type: unresolved
    - target: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
      type: internal
    - target: cache.get
      type: unresolved
    - target: isinstance
      type: builtin
    - target: litellm::ModelResponse
      type: external
    - target: litellm::EmbeddingResponse
      type: external
    - target: cache.set
      type: unresolved
    - target: response.model_dump
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache_async
    called_by: []
- file_name: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AsyncLitellmRequestFunc
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: LitellmRequestFunc
    alias: null
  functions:
  - name: with_logging
    start_line: 17
    end_line: 56
    code: "def with_logging(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n\
      \    \"\"\"\n    Wrap the synchronous and asynchronous request functions with\
      \ retries.\n\n    Args\n    ----\n        sync_fn: The synchronous chat/embedding\
      \ request function to wrap.\n        async_fn: The asynchronous chat/embedding\
      \ request function to wrap.\n        model_config: The configuration for the\
      \ language model.\n\n    Returns\n    -------\n        A tuple containing the\
      \ wrapped synchronous and asynchronous chat/embedding request functions.\n \
      \   \"\"\"\n\n    def _wrapped_with_logging(**kwargs: Any) -> Any:\n       \
      \ try:\n            return sync_fn(**kwargs)\n        except Exception as e:\n\
      \            logger.exception(\n                f\"with_logging: Request failed\
      \ with exception={e}\",  # noqa: G004, TRY401\n            )\n            raise\n\
      \n    async def _wrapped_with_logging_async(\n        **kwargs: Any,\n    )\
      \ -> Any:\n        try:\n            return await async_fn(**kwargs)\n     \
      \   except Exception as e:\n            logger.exception(\n                f\"\
      with_logging: Async request failed with exception={e}\",  # noqa: G004, TRY401\n\
      \            )\n            raise\n\n    return (_wrapped_with_logging, _wrapped_with_logging_async)"
    signature: "def with_logging(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
    - source: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
  - name: _wrapped_with_logging
    start_line: 36
    end_line: 43
    code: "def _wrapped_with_logging(**kwargs: Any) -> Any:\n        try:\n      \
      \      return sync_fn(**kwargs)\n        except Exception as e:\n          \
      \  logger.exception(\n                f\"with_logging: Request failed with exception={e}\"\
      ,  # noqa: G004, TRY401\n            )\n            raise"
    signature: 'def _wrapped_with_logging(**kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: sync_fn
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging
    called_by: []
  - name: _wrapped_with_logging_async
    start_line: 45
    end_line: 54
    code: "async def _wrapped_with_logging_async(\n        **kwargs: Any,\n    ) ->\
      \ Any:\n        try:\n            return await async_fn(**kwargs)\n        except\
      \ Exception as e:\n            logger.exception(\n                f\"with_logging:\
      \ Async request failed with exception={e}\",  # noqa: G004, TRY401\n       \
      \     )\n            raise"
    signature: "def _wrapped_with_logging_async(\n        **kwargs: Any,\n    ) ->\
      \ Any"
    decorators: []
    raises: []
    calls:
    - target: async_fn
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging_async
    called_by: []
- file_name: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: litellm
    name: token_counter
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter_factory
    name: RateLimiterFactory
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AsyncLitellmRequestFunc
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: LitellmRequestFunc
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: with_rate_limiter
    start_line: 22
    end_line: 97
    code: "def with_rate_limiter(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n   \
      \ rpm: int | None = None,\n    tpm: int | None = None,\n) -> tuple[LitellmRequestFunc,\
      \ AsyncLitellmRequestFunc]:\n    \"\"\"\n    Wrap the synchronous and asynchronous\
      \ request functions with rate limiting.\n\n    Args\n    ----\n        sync_fn:\
      \ The synchronous chat/embedding request function to wrap.\n        async_fn:\
      \ The asynchronous chat/embedding request function to wrap.\n        model_config:\
      \ The configuration for the language model.\n        processing_event: A threading\
      \ event that can be used to pause the rate limiter.\n        rpm: An optional\
      \ requests per minute limit.\n        tpm: An optional tokens per minute limit.\n\
      \n    If `rpm` and `tpm` is set to 0 or None, rate limiting is disabled.\n\n\
      \    Returns\n    -------\n        A tuple containing the wrapped synchronous\
      \ and asynchronous chat/embedding request functions.\n    \"\"\"\n    rate_limiter_factory\
      \ = RateLimiterFactory()\n\n    if (\n        model_config.rate_limit_strategy\
      \ is None\n        or model_config.rate_limit_strategy not in rate_limiter_factory\n\
      \    ):\n        msg = f\"Rate Limiter strategy '{model_config.rate_limit_strategy}'\
      \ is none or not registered. Available strategies: {', '.join(rate_limiter_factory.keys())}\"\
      \n        raise ValueError(msg)\n\n    rate_limiter_service = rate_limiter_factory.create(\n\
      \        strategy=model_config.rate_limit_strategy, rpm=rpm, tpm=tpm\n    )\n\
      \n    max_tokens = model_config.max_completion_tokens or model_config.max_tokens\
      \ or 0\n\n    def _wrapped_with_rate_limiter(**kwargs: Any) -> Any:\n      \
      \  token_count = max_tokens\n        if \"messages\" in kwargs:\n          \
      \  token_count += token_counter(\n                model=model_config.model,\n\
      \                messages=kwargs[\"messages\"],\n            )\n        elif\
      \ \"input\" in kwargs:\n            token_count += token_counter(\n        \
      \        model=model_config.model,\n                text=kwargs[\"input\"],\n\
      \            )\n\n        with rate_limiter_service.acquire(token_count=token_count):\n\
      \            return sync_fn(**kwargs)\n\n    async def _wrapped_with_rate_limiter_async(\n\
      \        **kwargs: Any,\n    ) -> Any:\n        token_count = max_tokens\n \
      \       if \"messages\" in kwargs:\n            token_count += token_counter(\n\
      \                model=model_config.model,\n                messages=kwargs[\"\
      messages\"],\n            )\n        elif \"input\" in kwargs:\n           \
      \ token_count += token_counter(\n                model=model_config.model,\n\
      \                text=kwargs[\"input\"],\n            )\n\n        with rate_limiter_service.acquire(token_count=token_count):\n\
      \            return await async_fn(**kwargs)\n\n    return (_wrapped_with_rate_limiter,\
      \ _wrapped_with_rate_limiter_async)"
    signature: "def with_rate_limiter(\n    *,\n    sync_fn: LitellmRequestFunc,\n\
      \    async_fn: AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\"\
      ,\n    rpm: int | None = None,\n    tpm: int | None = None,\n) -> tuple[LitellmRequestFunc,\
      \ AsyncLitellmRequestFunc]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py::RateLimiterFactory
      type: internal
    - target: ''', ''.join'
      type: unresolved
    - target: rate_limiter_factory.keys
      type: unresolved
    - target: ValueError
      type: builtin
    - target: rate_limiter_factory.create
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
    - source: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
  - name: _wrapped_with_rate_limiter
    start_line: 63
    end_line: 77
    code: "def _wrapped_with_rate_limiter(**kwargs: Any) -> Any:\n        token_count\
      \ = max_tokens\n        if \"messages\" in kwargs:\n            token_count\
      \ += token_counter(\n                model=model_config.model,\n           \
      \     messages=kwargs[\"messages\"],\n            )\n        elif \"input\"\
      \ in kwargs:\n            token_count += token_counter(\n                model=model_config.model,\n\
      \                text=kwargs[\"input\"],\n            )\n\n        with rate_limiter_service.acquire(token_count=token_count):\n\
      \            return sync_fn(**kwargs)"
    signature: 'def _wrapped_with_rate_limiter(**kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: litellm::token_counter
      type: external
    - target: rate_limiter_service.acquire
      type: unresolved
    - target: sync_fn
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter
    called_by: []
  - name: _wrapped_with_rate_limiter_async
    start_line: 79
    end_line: 95
    code: "async def _wrapped_with_rate_limiter_async(\n        **kwargs: Any,\n \
      \   ) -> Any:\n        token_count = max_tokens\n        if \"messages\" in\
      \ kwargs:\n            token_count += token_counter(\n                model=model_config.model,\n\
      \                messages=kwargs[\"messages\"],\n            )\n        elif\
      \ \"input\" in kwargs:\n            token_count += token_counter(\n        \
      \        model=model_config.model,\n                text=kwargs[\"input\"],\n\
      \            )\n\n        with rate_limiter_service.acquire(token_count=token_count):\n\
      \            return await async_fn(**kwargs)"
    signature: "def _wrapped_with_rate_limiter_async(\n        **kwargs: Any,\n  \
      \  ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: litellm::token_counter
      type: external
    - target: rate_limiter_service.acquire
      type: unresolved
    - target: async_fn
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter_async
    called_by: []
- file_name: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry_factory
    name: RetryFactory
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: AsyncLitellmRequestFunc
    alias: null
  - module: graphrag.language_model.providers.litellm.types
    name: LitellmRequestFunc
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  functions:
  - name: with_retries
    start_line: 20
    end_line: 54
    code: "def with_retries(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n) ->\
      \ tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\n    \"\"\"\n    Wrap\
      \ the synchronous and asynchronous request functions with retries.\n\n    Args\n\
      \    ----\n        sync_fn: The synchronous chat/embedding request function\
      \ to wrap.\n        async_fn: The asynchronous chat/embedding request function\
      \ to wrap.\n        model_config: The configuration for the language model.\n\
      \n    Returns\n    -------\n        A tuple containing the wrapped synchronous\
      \ and asynchronous chat/embedding request functions.\n    \"\"\"\n    retry_factory\
      \ = RetryFactory()\n    retry_service = retry_factory.create(\n        strategy=model_config.retry_strategy,\n\
      \        max_retries=model_config.max_retries,\n        max_retry_wait=model_config.max_retry_wait,\n\
      \    )\n\n    def _wrapped_with_retries(**kwargs: Any) -> Any:\n        return\
      \ retry_service.retry(func=sync_fn, **kwargs)\n\n    async def _wrapped_with_retries_async(\n\
      \        **kwargs: Any,\n    ) -> Any:\n        return await retry_service.aretry(func=async_fn,\
      \ **kwargs)\n\n    return (_wrapped_with_retries, _wrapped_with_retries_async)"
    signature: "def with_retries(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n) ->\
      \ tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/litellm/services/retry/retry_factory.py::RetryFactory
      type: internal
    - target: retry_factory.create
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
    called_by:
    - source: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
      type: internal
    - source: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
      type: internal
  - name: _wrapped_with_retries
    start_line: 46
    end_line: 47
    code: "def _wrapped_with_retries(**kwargs: Any) -> Any:\n        return retry_service.retry(func=sync_fn,\
      \ **kwargs)"
    signature: 'def _wrapped_with_retries(**kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: retry_service.retry
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries
    called_by: []
  - name: _wrapped_with_retries_async
    start_line: 49
    end_line: 52
    code: "async def _wrapped_with_retries_async(\n        **kwargs: Any,\n    ) ->\
      \ Any:\n        return await retry_service.aretry(func=async_fn, **kwargs)"
    signature: "def _wrapped_with_retries_async(\n        **kwargs: Any,\n    ) ->\
      \ Any"
    decorators: []
    raises: []
    calls:
    - target: retry_service.aretry
      type: unresolved
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries_async
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/litellm/services/rate_limiter/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: contextlib
    name: contextmanager
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: __init__
    start_line: 16
    end_line: 20
    code: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n    )\
      \ -> None: ..."
    signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n\
      \    ) -> None"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.__init__
    called_by: []
  - name: acquire
    start_line: 24
    end_line: 37
    code: "def acquire(self, *, token_count: int) -> Iterator[None]:\n        \"\"\
      \"\n        Acquire Rate Limiter.\n\n        Args\n        ----\n          \
      \  token_count: The estimated number of tokens for the current request.\n\n\
      \        Yields\n        ------\n            None: This context manager does\
      \ not return any value.\n        \"\"\"\n        msg = \"RateLimiter subclasses\
      \ must implement the acquire method.\"\n        raise NotImplementedError(msg)"
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    decorators:
    - '@abstractmethod'
    - '@contextmanager'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.acquire
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py
  imports:
  - module: graphrag.config.defaults
    name: DEFAULT_RATE_LIMITER_SERVICES
    alias: null
  - module: graphrag.factory.factory
    name: Factory
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter
    name: RateLimiter
    alias: null
  functions: []
- file_name: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  imports:
  - module: threading
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections
    name: deque
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: contextlib
    name: contextmanager
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter
    name: RateLimiter
    alias: null
  functions:
  - name: __init__
    start_line: 21
    end_line: 52
    code: "def __init__(\n        self,\n        *,\n        rpm: int | None = None,\n\
      \        tpm: int | None = None,\n        default_stagger: float = 0.0,\n  \
      \      period_in_seconds: int = 60,\n        **kwargs: Any,\n    ):\n      \
      \  if rpm is None and tpm is None:\n            msg = \"Both TPM and RPM cannot\
      \ be None (disabled), one or both must be set to a positive integer.\"\n   \
      \         raise ValueError(msg)\n        if (rpm is not None and rpm <= 0) or\
      \ (tpm is not None and tpm <= 0):\n            msg = \"RPM and TPM must be either\
      \ None (disabled) or positive integers.\"\n            raise ValueError(msg)\n\
      \        if default_stagger < 0:\n            msg = \"Default stagger must be\
      \ a >= 0.\"\n            raise ValueError(msg)\n        if period_in_seconds\
      \ <= 0:\n            msg = \"Period in seconds must be a positive integer.\"\
      \n            raise ValueError(msg)\n        self.rpm = rpm\n        self.tpm\
      \ = tpm\n        self._lock = threading.Lock()\n        self.rate_queue: deque[float]\
      \ = deque()\n        self.token_queue: deque[int] = deque()\n        self.period_in_seconds\
      \ = period_in_seconds\n        self._last_time: float | None = None\n\n    \
      \    self.stagger = default_stagger\n        if self.rpm is not None and self.rpm\
      \ > 0:\n            self.stagger = self.period_in_seconds / self.rpm"
    signature: "def __init__(\n        self,\n        *,\n        rpm: int | None\
      \ = None,\n        tpm: int | None = None,\n        default_stagger: float =\
      \ 0.0,\n        period_in_seconds: int = 60,\n        **kwargs: Any,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: threading::Lock
      type: stdlib
    - target: collections::deque
      type: stdlib
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.__init__
    called_by: []
  - name: acquire
    start_line: 55
    end_line: 133
    code: "def acquire(self, *, token_count: int) -> Iterator[None]:\n        \"\"\
      \"\n        Acquire Rate Limiter.\n\n        Args\n        ----\n          \
      \  token_count: The estimated number of tokens for the current request.\n\n\
      \        Yields\n        ------\n            None: This context manager does\
      \ not return any value.\n        \"\"\"\n        while True:\n            with\
      \ self._lock:\n                current_time = time.time()\n\n              \
      \  # Use two sliding windows to keep track of #requests and tokens per period\n\
      \                # Drop old requests and tokens out of the sliding windows\n\
      \                while (\n                    len(self.rate_queue) > 0\n   \
      \                 and self.rate_queue[0] < current_time - self.period_in_seconds\n\
      \                ):\n                    self.rate_queue.popleft()\n       \
      \             self.token_queue.popleft()\n\n                # If sliding window\
      \ still exceed request limit, wait again\n                # Waiting requires\
      \ reacquiring the lock, allowing other threads\n                # to see if\
      \ their request fits within the rate limiting windows\n                # Makes\
      \ more sense for token limit than request limit\n                if (\n    \
      \                self.rpm is not None\n                    and self.rpm > 0\n\
      \                    and len(self.rate_queue) >= self.rpm\n                ):\n\
      \                    continue\n\n                # Check if current token window\
      \ exceeds token limit\n                # If it does, wait again\n          \
      \      # This does not account for the tokens from the current request\n   \
      \             # This is intentional, as we want to allow the current request\n\
      \                # to be processed if it is larger than the tpm but smaller\
      \ than context window.\n                # tpm is a rate/soft limit and not the\
      \ hard limit of context window limits.\n                if (\n             \
      \       self.tpm is not None\n                    and self.tpm > 0\n       \
      \             and sum(self.token_queue) >= self.tpm\n                ):\n  \
      \                  continue\n\n                # This check accounts for the\
      \ current request token usage\n                # is within the token limits\
      \ bound.\n                # If the current requests token limit exceeds the\
      \ token limit,\n                # Then let it be processed.\n              \
      \  if (\n                    self.tpm is not None\n                    and self.tpm\
      \ > 0\n                    and token_count <= self.tpm\n                   \
      \ and sum(self.token_queue) + token_count > self.tpm\n                ):\n \
      \                   continue\n\n                # If there was a previous call,\
      \ check if we need to stagger\n                if (\n                    self.stagger\
      \ > 0\n                    and (\n                        self._last_time  #\
      \ is None if this is the first hit to the rate limiter\n                   \
      \     and current_time - self._last_time\n                        < self.stagger\
      \  # If more time has passed than the stagger time, we can proceed\n       \
      \             )\n                ):\n                    time.sleep(self.stagger\
      \ - (current_time - self._last_time))\n                    current_time = time.time()\n\
      \n                # Add the current request to the sliding window\n        \
      \        self.rate_queue.append(current_time)\n                self.token_queue.append(token_count)\n\
      \                self._last_time = current_time\n                break\n   \
      \     yield"
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    decorators:
    - '@contextmanager'
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: len
      type: builtin
    - target: self.rate_queue.popleft
      type: instance
    - target: self.token_queue.popleft
      type: instance
    - target: sum
      type: builtin
    - target: time::sleep
      type: stdlib
    - target: self.rate_queue.append
      type: instance
    - target: self.token_queue.append
      type: instance
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.acquire
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: random
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions:
  - name: __init__
    start_line: 21
    end_line: 39
    code: "def __init__(\n        self,\n        *,\n        max_retries: int = 5,\n\
      \        base_delay: float = 2.0,\n        jitter: bool = True,\n        **kwargs:\
      \ Any,\n    ):\n        if max_retries <= 0:\n            msg = \"max_retries\
      \ must be greater than 0.\"\n            raise ValueError(msg)\n\n        if\
      \ base_delay <= 1.0:\n            msg = \"base_delay must be greater than 1.0.\"\
      \n            raise ValueError(msg)\n\n        self._max_retries = max_retries\n\
      \        self._base_delay = base_delay\n        self._jitter = jitter"
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n   \
      \     **kwargs: Any,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.__init__
    called_by: []
  - name: retry
    start_line: 41
    end_line: 59
    code: "def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n    \
      \    \"\"\"Retry a synchronous function.\"\"\"\n        retries = 0\n      \
      \  delay = 1.0  # Initial delay in seconds\n        while True:\n          \
      \  try:\n                return func(**kwargs)\n            except Exception\
      \ as e:\n                if retries >= self._max_retries:\n                \
      \    logger.exception(\n                        f\"ExponentialRetry: Max retries\
      \ exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\"\
      ,  # noqa: G004, TRY401\n                    )\n                    raise\n\
      \                retries += 1\n                delay *= self._base_delay\n \
      \               logger.exception(\n                    f\"ExponentialRetry:\
      \ Request failed, retrying, retries={retries}, delay={delay}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                )\n              \
      \  time.sleep(delay + (self._jitter * random.uniform(0, 1)))  # noqa: S311"
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: time::sleep
      type: stdlib
    - target: random::uniform
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.retry
    called_by: []
  - name: aretry
    start_line: 61
    end_line: 83
    code: "async def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Retry an asynchronous\
      \ function.\"\"\"\n        retries = 0\n        delay = 1.0  # Initial delay\
      \ in seconds\n        while True:\n            try:\n                return\
      \ await func(**kwargs)\n            except Exception as e:\n               \
      \ if retries >= self._max_retries:\n                    logger.exception(\n\
      \                        f\"ExponentialRetry: Max retries exceeded, retries={retries},\
      \ max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n\
      \                    )\n                    raise\n                retries +=\
      \ 1\n                delay *= self._base_delay\n                logger.exception(\n\
      \                    f\"ExponentialRetry: Request failed, retrying, retries={retries},\
      \ delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa:\
      \ G004, TRY401\n                )\n                await asyncio.sleep(delay\
      \ + (self._jitter * random.uniform(0, 1)))  # noqa: S311"
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: asyncio::sleep
      type: stdlib
    - target: random::uniform
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.aretry
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions:
  - name: __init__
    start_line: 20
    end_line: 37
    code: "def __init__(\n        self,\n        *,\n        max_retry_wait: float,\n\
      \        max_retries: int = 5,\n        **kwargs: Any,\n    ):\n        if max_retries\
      \ <= 0:\n            msg = \"max_retries must be greater than 0.\"\n       \
      \     raise ValueError(msg)\n\n        if max_retry_wait <= 0:\n           \
      \ msg = \"max_retry_wait must be greater than 0.\"\n            raise ValueError(msg)\n\
      \n        self._max_retries = max_retries\n        self._max_retry_wait = max_retry_wait\n\
      \        self._increment = max_retry_wait / max_retries"
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.__init__
    called_by: []
  - name: retry
    start_line: 39
    end_line: 57
    code: "def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n    \
      \    \"\"\"Retry a synchronous function.\"\"\"\n        retries = 0\n      \
      \  delay = 0.0\n        while True:\n            try:\n                return\
      \ func(**kwargs)\n            except Exception as e:\n                if retries\
      \ >= self._max_retries:\n                    logger.exception(\n           \
      \             f\"IncrementalWaitRetry: Max retries exceeded, retries={retries},\
      \ max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n\
      \                    )\n                    raise\n                retries +=\
      \ 1\n                delay += self._increment\n                logger.exception(\n\
      \                    f\"IncrementalWaitRetry: Request failed, retrying after\
      \ incremental delay, retries={retries}, delay={delay}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                )\n              \
      \  time.sleep(delay)"
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: time::sleep
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.retry
    called_by: []
  - name: aretry
    start_line: 59
    end_line: 81
    code: "async def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Retry an asynchronous\
      \ function.\"\"\"\n        retries = 0\n        delay = 0.0\n        while True:\n\
      \            try:\n                return await func(**kwargs)\n           \
      \ except Exception as e:\n                if retries >= self._max_retries:\n\
      \                    logger.exception(\n                        f\"IncrementalWaitRetry:\
      \ Max retries exceeded, retries={retries}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                    )\n          \
      \          raise\n                retries += 1\n                delay += self._increment\n\
      \                logger.exception(\n                    f\"IncrementalWaitRetry:\
      \ Request failed, retrying after incremental delay, retries={retries}, delay={delay},\
      \ max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n\
      \                )\n                await asyncio.sleep(delay)"
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: asyncio::sleep
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.aretry
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions:
  - name: __init__
    start_line: 18
    end_line: 28
    code: "def __init__(\n        self,\n        *,\n        max_retries: int = 5,\n\
      \        **kwargs: Any,\n    ):\n        if max_retries <= 0:\n            msg\
      \ = \"max_retries must be greater than 0.\"\n            raise ValueError(msg)\n\
      \n        self._max_retries = max_retries"
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        **kwargs: Any,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.__init__
    called_by: []
  - name: retry
    start_line: 30
    end_line: 45
    code: "def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n    \
      \    \"\"\"Retry a synchronous function.\"\"\"\n        retries = 0\n      \
      \  while True:\n            try:\n                return func(**kwargs)\n  \
      \          except Exception as e:\n                if retries >= self._max_retries:\n\
      \                    logger.exception(\n                        f\"NativeRetry:\
      \ Max retries exceeded, retries={retries}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                    )\n          \
      \          raise\n                retries += 1\n                logger.exception(\n\
      \                    f\"NativeRetry: Request failed, immediately retrying, retries={retries},\
      \ max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n\
      \                )"
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.retry
    called_by: []
  - name: aretry
    start_line: 47
    end_line: 66
    code: "async def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Retry an asynchronous\
      \ function.\"\"\"\n        retries = 0\n        while True:\n            try:\n\
      \                return await func(**kwargs)\n            except Exception as\
      \ e:\n                if retries >= self._max_retries:\n                   \
      \ logger.exception(\n                        f\"NativeRetry: Max retries exceeded,\
      \ retries={retries}, max_retries={self._max_retries}, exception={e}\",  # noqa:\
      \ G004, TRY401\n                    )\n                    raise\n         \
      \       retries += 1\n                logger.exception(\n                  \
      \  f\"NativeRetry: Request failed, immediately retrying, retries={retries},\
      \ max_retries={self._max_retries}, exception={e}\",  # noqa: G004, TRY401\n\
      \                )"
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.aretry
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: random
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions:
  - name: __init__
    start_line: 21
    end_line: 37
    code: "def __init__(\n        self,\n        *,\n        max_retry_wait: float,\n\
      \        max_retries: int = 5,\n        **kwargs: Any,\n    ):\n        if max_retries\
      \ <= 0:\n            msg = \"max_retries must be greater than 0.\"\n       \
      \     raise ValueError(msg)\n\n        if max_retry_wait <= 0:\n           \
      \ msg = \"max_retry_wait must be greater than 0.\"\n            raise ValueError(msg)\n\
      \n        self._max_retries = max_retries\n        self._max_retry_wait = max_retry_wait"
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.__init__
    called_by: []
  - name: retry
    start_line: 39
    end_line: 56
    code: "def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n    \
      \    \"\"\"Retry a synchronous function.\"\"\"\n        retries = 0\n      \
      \  while True:\n            try:\n                return func(**kwargs)\n  \
      \          except Exception as e:\n                if retries >= self._max_retries:\n\
      \                    logger.exception(\n                        f\"RandomWaitRetry:\
      \ Max retries exceeded, retries={retries}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                    )\n          \
      \          raise\n                retries += 1\n                delay = random.uniform(0,\
      \ self._max_retry_wait)  # noqa: S311\n                logger.exception(\n \
      \                   f\"RandomWaitRetry: Request failed, retrying after random\
      \ delay, retries={retries}, delay={delay}, max_retries={self._max_retries},\
      \ exception={e}\",  # noqa: G004, TRY401\n                )\n              \
      \  time.sleep(delay)"
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: random::uniform
      type: stdlib
    - target: time::sleep
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.retry
    called_by: []
  - name: aretry
    start_line: 58
    end_line: 79
    code: "async def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Retry an asynchronous\
      \ function.\"\"\"\n        retries = 0\n        while True:\n            try:\n\
      \                return await func(**kwargs)\n            except Exception as\
      \ e:\n                if retries >= self._max_retries:\n                   \
      \ logger.exception(\n                        f\"RandomWaitRetry: Max retries\
      \ exceeded, retries={retries}, max_retries={self._max_retries}, exception={e}\"\
      ,  # noqa: G004, TRY401\n                    )\n                    raise\n\
      \                retries += 1\n                delay = random.uniform(0, self._max_retry_wait)\
      \  # noqa: S311\n                logger.exception(\n                    f\"\
      RandomWaitRetry: Request failed, retrying after random delay, retries={retries},\
      \ delay={delay}, max_retries={self._max_retries}, exception={e}\",  # noqa:\
      \ G004, TRY401\n                )\n                await asyncio.sleep(delay)"
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: logger.exception
      type: unresolved
    - target: random::uniform
      type: stdlib
    - target: asyncio::sleep
      type: stdlib
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.aretry
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/retry.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: collections.abc
    name: Awaitable
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: __init__
    start_line: 15
    end_line: 17
    code: "def __init__(self, /, **kwargs: Any):\n        msg = \"Retry subclasses\
      \ must implement the __init__ method.\"\n        raise NotImplementedError(msg)"
    signature: 'def __init__(self, /, **kwargs: Any)'
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.__init__
    called_by: []
  - name: retry
    start_line: 20
    end_line: 23
    code: "def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any:\n    \
      \    \"\"\"Retry a synchronous function.\"\"\"\n        msg = \"Subclasses must\
      \ implement this method\"\n        raise NotImplementedError(msg)"
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.retry
    called_by: []
  - name: aretry
    start_line: 26
    end_line: 33
    code: "async def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Retry an asynchronous\
      \ function.\"\"\"\n        msg = \"Subclasses must implement this method\"\n\
      \        raise NotImplementedError(msg)"
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.aretry
    called_by: []
- file_name: graphrag/language_model/providers/litellm/services/retry/retry_factory.py
  imports:
  - module: graphrag.config.defaults
    name: DEFAULT_RETRY_SERVICES
    alias: null
  - module: graphrag.factory.factory
    name: Factory
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry
    name: Retry
    alias: null
  functions: []
- file_name: graphrag/language_model/providers/litellm/types.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Protocol
    alias: null
  - module: typing
    name: runtime_checkable
    alias: null
  - module: litellm
    name: AnthropicThinkingParam
    alias: null
  - module: litellm
    name: BaseModel
    alias: null
  - module: litellm
    name: ChatCompletionAudioParam
    alias: null
  - module: litellm
    name: ChatCompletionModality
    alias: null
  - module: litellm
    name: ChatCompletionPredictionContentParam
    alias: null
  - module: litellm
    name: CustomStreamWrapper
    alias: null
  - module: litellm
    name: EmbeddingResponse
    alias: null
  - module: litellm
    name: ModelResponse
    alias: null
  - module: litellm
    name: OpenAIWebSearchOptions
    alias: null
  - module: openai.types.chat.chat_completion
    name: ChatCompletion
    alias: null
  - module: openai.types.chat.chat_completion
    name: Choice
    alias: null
  - module: openai.types.chat.chat_completion_chunk
    name: ChatCompletionChunk
    alias: null
  - module: openai.types.chat.chat_completion_chunk
    name: ChoiceDelta
    alias: null
  - module: openai.types.chat.chat_completion_chunk
    name: Choice
    alias: null
  - module: openai.types.chat.chat_completion_message
    name: ChatCompletionMessage
    alias: null
  - module: openai.types.chat.chat_completion_message_param
    name: ChatCompletionMessageParam
    alias: null
  - module: openai.types.completion_usage
    name: CompletionTokensDetails
    alias: null
  - module: openai.types.completion_usage
    name: CompletionUsage
    alias: null
  - module: openai.types.completion_usage
    name: PromptTokensDetails
    alias: null
  - module: openai.types.create_embedding_response
    name: CreateEmbeddingResponse
    alias: null
  - module: openai.types.create_embedding_response
    name: Usage
    alias: null
  - module: openai.types.embedding
    name: Embedding
    alias: null
  functions:
  - name: __call__
    start_line: 68
    end_line: 101
    code: "def __call__(\n        self,\n        *,\n        messages: list = [],\
      \  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n     \
      \   stream_options: dict | None = None,  # type: ignore\n        stop=None,\
      \  # type: ignore\n        max_completion_tokens: int | None = None,\n     \
      \   max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
      \ | None = None,\n        prediction: ChatCompletionPredictionContentParam |\
      \ None = None,\n        audio: ChatCompletionAudioParam | None = None,\n   \
      \     logit_bias: dict | None = None,  # type: ignore\n        user: str | None\
      \ = None,\n        # openai v1.0+ new params\n        response_format: dict\
      \ | type[BaseModel] | None = None,  # type: ignore\n        seed: int | None\
      \ = None,\n        tools: list | None = None,  # type: ignore\n        tool_choice:\
      \ str | dict | None = None,  # type: ignore\n        logprobs: bool | None =\
      \ None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper:\n        \"\"\"Chat completion\
      \ function.\"\"\"\n        ..."
    signature: "def __call__(\n        self,\n        *,\n        messages: list =\
      \ [],  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n \
      \       stream_options: dict | None = None,  # type: ignore\n        stop=None,\
      \  # type: ignore\n        max_completion_tokens: int | None = None,\n     \
      \   max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
      \ | None = None,\n        prediction: ChatCompletionPredictionContentParam |\
      \ None = None,\n        audio: ChatCompletionAudioParam | None = None,\n   \
      \     logit_bias: dict | None = None,  # type: ignore\n        user: str | None\
      \ = None,\n        # openai v1.0+ new params\n        response_format: dict\
      \ | type[BaseModel] | None = None,  # type: ignore\n        seed: int | None\
      \ = None,\n        tools: list | None = None,  # type: ignore\n        tool_choice:\
      \ str | dict | None = None,  # type: ignore\n        logprobs: bool | None =\
      \ None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion.__call__
    called_by: []
  - name: __call__
    start_line: 113
    end_line: 147
    code: "async def __call__(\n        self,\n        *,\n        # Optional OpenAI\
      \ params: see https://platform.openai.com/docs/api-reference/chat/create\n \
      \       messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
      \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
      \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
      \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
      \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
      \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n \
      \       logit_bias: dict | None = None,  # type: ignore\n        user: str |\
      \ None = None,\n        # openai v1.0+ new params\n        response_format:\
      \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int |\
      \ None = None,\n        tools: list | None = None,  # type: ignore\n       \
      \ tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
      \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper:\n        \"\"\"Chat completion\
      \ function.\"\"\"\n        ..."
    signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
      \ params: see https://platform.openai.com/docs/api-reference/chat/create\n \
      \       messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
      \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
      \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
      \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
      \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
      \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n \
      \       logit_bias: dict | None = None,  # type: ignore\n        user: str |\
      \ None = None,\n        # openai v1.0+ new params\n        response_format:\
      \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int |\
      \ None = None,\n        tools: list | None = None,  # type: ignore\n       \
      \ tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
      \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion.__call__
    called_by: []
  - name: __call__
    start_line: 159
    end_line: 178
    code: "def __call__(\n        self,\n        *,\n        request_id: str | None\
      \ = None,\n        input: list = [],  # type: ignore  # noqa: B006\n       \
      \ # Optional params\n        dimensions: int | None = None,\n        encoding_format:\
      \ str | None = None,\n        timeout: int = 600,  # default to 10 minutes\n\
      \        # set api_base, api_version, api_key\n        api_base: str | None\
      \ = None,\n        api_version: str | None = None,\n        api_key: str | None\
      \ = None,\n        api_type: str | None = None,\n        caching: bool = False,\n\
      \        user: str | None = None,\n        **kwargs: Any,\n    ) -> EmbeddingResponse:\n\
      \        \"\"\"Embedding function.\"\"\"\n        ..."
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding.__call__
    called_by: []
  - name: __call__
    start_line: 190
    end_line: 209
    code: "async def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse:\n        \"\"\"Embedding function.\"\"\"\n    \
      \    ..."
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding.__call__
    called_by: []
  - name: __call__
    start_line: 220
    end_line: 222
    code: "def __call__(self, /, **kwargs: Any) -> Any:\n        \"\"\"Request function.\"\
      \"\"\n        ..."
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc.__call__
    called_by: []
  - name: __call__
    start_line: 233
    end_line: 235
    code: "async def __call__(self, /, **kwargs: Any) -> Any:\n        \"\"\"Request\
      \ function.\"\"\"\n        ..."
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc.__call__
    called_by: []
- file_name: graphrag/language_model/response/__init__.py
  imports: []
  functions: []
- file_name: graphrag/language_model/response/base.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Generic
    alias: null
  - module: typing
    name: Protocol
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: pydantic
    name: Field
    alias: null
  functions:
  - name: content
    start_line: 17
    end_line: 19
    code: "def content(self) -> str:\n        \"\"\"Return the textual content of\
      \ the output.\"\"\"\n        ..."
    signature: def content(self) -> str
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.py::ModelOutput.content
    called_by: []
  - name: full_response
    start_line: 22
    end_line: 24
    code: "def full_response(self) -> dict[str, Any] | None:\n        \"\"\"Return\
      \ the complete JSON response returned by the model.\"\"\"\n        ..."
    signature: def full_response(self) -> dict[str, Any] | None
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.py::ModelOutput.full_response
    called_by: []
  - name: output
    start_line: 31
    end_line: 33
    code: "def output(self) -> ModelOutput:\n        \"\"\"Return the output of the\
      \ response.\"\"\"\n        ..."
    signature: def output(self) -> ModelOutput
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.py::ModelResponse.output
    called_by: []
  - name: parsed_response
    start_line: 36
    end_line: 38
    code: "def parsed_response(self) -> T | None:\n        \"\"\"Return the parsed\
      \ response.\"\"\"\n        ..."
    signature: def parsed_response(self) -> T | None
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.py::ModelResponse.parsed_response
    called_by: []
  - name: history
    start_line: 41
    end_line: 43
    code: "def history(self) -> list:\n        \"\"\"Return the history of the response.\"\
      \"\"\n        ..."
    signature: def history(self) -> list
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.py::ModelResponse.history
    called_by: []
- file_name: graphrag/language_model/response/base.pyi
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Generic
    alias: null
  - module: typing
    name: Protocol
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  functions:
  - name: content
    start_line: 12
    end_line: 12
    code: 'def content(self) -> str: ...'
    signature: def content(self) -> str
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.pyi::ModelOutput.content
    called_by: []
  - name: full_response
    start_line: 14
    end_line: 14
    code: 'def full_response(self) -> dict[str, Any] | None: ...'
    signature: def full_response(self) -> dict[str, Any] | None
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.pyi::ModelOutput.full_response
    called_by: []
  - name: output
    start_line: 18
    end_line: 18
    code: 'def output(self) -> ModelOutput: ...'
    signature: def output(self) -> ModelOutput
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.pyi::ModelResponse.output
    called_by: []
  - name: parsed_response
    start_line: 20
    end_line: 20
    code: 'def parsed_response(self) -> _T | None: ...'
    signature: def parsed_response(self) -> _T | None
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.pyi::ModelResponse.parsed_response
    called_by: []
  - name: history
    start_line: 22
    end_line: 22
    code: 'def history(self) -> list[Any]: ...'
    signature: def history(self) -> list[Any]
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/language_model/response/base.pyi::ModelResponse.history
    called_by: []
  - name: __init__
    start_line: 28
    end_line: 32
    code: "def __init__(\n        self,\n        content: str,\n        full_response:\
      \ dict[str, Any] | None = None,\n    ) -> None: ..."
    signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
      \ dict[str, Any] | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/response/base.pyi::BaseModelOutput.__init__
    called_by: []
  - name: __init__
    start_line: 42
    end_line: 50
    code: "def __init__(\n        self,\n        output: BaseModelOutput,\n      \
      \  parsed_response: _T | None = None,\n        history: list[Any] = ...,  #\
      \ default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
      \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
      \ bool | None = None,\n    ) -> None: ..."
    signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n \
      \       parsed_response: _T | None = None,\n        history: list[Any] = ...,\
      \  # default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
      \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
      \ bool | None = None,\n    ) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/language_model/response/base.pyi::BaseModelResponse.__init__
    called_by: []
- file_name: graphrag/logger/__init__.py
  imports: []
  functions: []
- file_name: graphrag/logger/blob_workflow_logger.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: datetime
    name: timezone
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.storage.blob
    name: BlobServiceClient
    alias: null
  functions:
  - name: __init__
    start_line: 23
    end_line: 70
    code: "def __init__(\n        self,\n        connection_string: str | None,\n\
      \        container_name: str | None,\n        blob_name: str = \"\",\n     \
      \   base_dir: str | None = None,\n        storage_account_blob_url: str | None\
      \ = None,\n        level: int = logging.NOTSET,\n    ):\n        \"\"\"Create\
      \ a new instance of the BlobWorkflowLogger class.\"\"\"\n        super().__init__(level)\n\
      \n        if container_name is None:\n            msg = \"No container name\
      \ provided for blob storage.\"\n            raise ValueError(msg)\n        if\
      \ connection_string is None and storage_account_blob_url is None:\n        \
      \    msg = \"No storage account blob url provided for blob storage.\"\n    \
      \        raise ValueError(msg)\n\n        self._connection_string = connection_string\n\
      \        self._storage_account_blob_url = storage_account_blob_url\n\n     \
      \   if self._connection_string:\n            self._blob_service_client = BlobServiceClient.from_connection_string(\n\
      \                self._connection_string\n            )\n        else:\n   \
      \         if storage_account_blob_url is None:\n                msg = \"Either\
      \ connection_string or storage_account_blob_url must be provided.\"\n      \
      \          raise ValueError(msg)\n\n            self._blob_service_client =\
      \ BlobServiceClient(\n                storage_account_blob_url,\n          \
      \      credential=DefaultAzureCredential(),\n            )\n\n        if blob_name\
      \ == \"\":\n            blob_name = f\"report/{datetime.now(tz=timezone.utc).strftime('%Y-%m-%d-%H:%M:%S:%f')}.logs.json\"\
      \n\n        self._blob_name = str(Path(base_dir or \"\") / blob_name)\n    \
      \    self._container_name = container_name\n        self._blob_client = self._blob_service_client.get_blob_client(\n\
      \            self._container_name, self._blob_name\n        )\n        if not\
      \ self._blob_client.exists():\n            self._blob_client.create_append_blob()\n\
      \n        self._num_blocks = 0  # refresh block counter"
    signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
      \        container_name: str | None,\n        blob_name: str = \"\",\n     \
      \   base_dir: str | None = None,\n        storage_account_blob_url: str | None\
      \ = None,\n        level: int = logging.NOTSET,\n    )"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: ValueError
      type: builtin
    - target: azure.storage.blob::BlobServiceClient::from_connection_string
      type: external
    - target: azure.storage.blob::BlobServiceClient
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: datetime::datetime::now(tz=timezone.utc).strftime
      type: external
    - target: datetime::datetime::now
      type: external
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: self._blob_service_client.get_blob_client
      type: instance
    - target: self._blob_client.exists
      type: instance
    - target: self._blob_client.create_append_blob
      type: instance
    visibility: protected
    node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.__init__
    called_by: []
  - name: emit
    start_line: 72
    end_line: 91
    code: "def emit(self, record) -> None:\n        \"\"\"Emit a log record to blob\
      \ storage.\"\"\"\n        try:\n            # Create JSON structure based on\
      \ record\n            log_data = {\n                \"type\": self._get_log_type(record.levelno),\n\
      \                \"data\": record.getMessage(),\n            }\n\n         \
      \   # Add additional fields if they exist\n            if hasattr(record, \"\
      details\") and record.details:  # type: ignore[reportAttributeAccessIssue]\n\
      \                log_data[\"details\"] = record.details  # type: ignore[reportAttributeAccessIssue]\n\
      \            if record.exc_info and record.exc_info[1]:\n                log_data[\"\
      cause\"] = str(record.exc_info[1])\n            if hasattr(record, \"stack\"\
      ) and record.stack:  # type: ignore[reportAttributeAccessIssue]\n          \
      \      log_data[\"stack\"] = record.stack  # type: ignore[reportAttributeAccessIssue]\n\
      \n            self._write_log(log_data)\n        except (OSError, ValueError):\n\
      \            self.handleError(record)"
    signature: def emit(self, record) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/blob_workflow_logger.py::_get_log_type
      type: internal
    - target: record.getMessage
      type: unresolved
    - target: hasattr
      type: builtin
    - target: str
      type: builtin
    - target: graphrag/logger/blob_workflow_logger.py::_write_log
      type: internal
    - target: self.handleError
      type: instance
    visibility: public
    node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.emit
    called_by: []
  - name: _get_log_type
    start_line: 93
    end_line: 99
    code: "def _get_log_type(self, level: int) -> str:\n        \"\"\"Get log type\
      \ string based on log level.\"\"\"\n        if level >= logging.ERROR:\n   \
      \         return \"error\"\n        if level >= logging.WARNING:\n         \
      \   return \"warning\"\n        return \"log\""
    signature: 'def _get_log_type(self, level: int) -> str'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._get_log_type
    called_by: []
  - name: _write_log
    start_line: 101
    end_line: 119
    code: "def _write_log(self, log: dict[str, Any]):\n        \"\"\"Write log data\
      \ to blob storage.\"\"\"\n        # create a new file when block count hits\
      \ close 25k\n        if (\n            self._num_blocks >= self._max_block_count\n\
      \        ):  # Check if block count exceeds 25k\n            self.__init__(\n\
      \                self._connection_string,\n                self._container_name,\n\
      \                storage_account_blob_url=self._storage_account_blob_url,\n\
      \            )\n\n        blob_client = self._blob_service_client.get_blob_client(\n\
      \            self._container_name, self._blob_name\n        )\n        blob_client.append_block(json.dumps(log,\
      \ indent=4, ensure_ascii=False) + \"\\n\")\n\n        # update the blob's block\
      \ count\n        self._num_blocks += 1"
    signature: 'def _write_log(self, log: dict[str, Any])'
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/blob_workflow_logger.py::__init__
      type: internal
    - target: self._blob_service_client.get_blob_client
      type: instance
    - target: blob_client.append_block
      type: unresolved
    - target: json::dumps
      type: stdlib
    visibility: protected
    node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._write_log
    called_by: []
- file_name: graphrag/logger/factory.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: ReportingType
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: graphrag.logger.blob_workflow_logger
    name: BlobWorkflowLogger
    alias: null
  functions:
  - name: register
    start_line: 36
    end_line: 45
    code: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
      \ logging.Handler]\n    ) -> None:\n        \"\"\"Register a custom logger implementation.\n\
      \n        Args:\n            reporting_type: The type identifier for the logger.\n\
      \            creator: A class or callable that initializes logging.\n      \
      \  \"\"\"\n        cls._registry[reporting_type] = creator"
    signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
      \ logging.Handler]\n    ) -> None"
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/logger/factory.py::LoggerFactory.register
    called_by: []
  - name: create_logger
    start_line: 48
    end_line: 68
    code: "def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler:\n\
      \        \"\"\"Create a logger from the provided type.\n\n        Args:\n  \
      \          reporting_type: The type of logger to create.\n            logger:\
      \ The logger instance for the application.\n            kwargs: Additional keyword\
      \ arguments for the constructor.\n\n        Returns\n        -------\n     \
      \       A logger instance.\n\n        Raises\n        ------\n            ValueError:\
      \ If the logger type is not registered.\n        \"\"\"\n        if reporting_type\
      \ not in cls._registry:\n            msg = f\"Unknown reporting type: {reporting_type}\"\
      \n            raise ValueError(msg)\n\n        return cls._registry[reporting_type](**kwargs)"
    signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/logger/factory.py::LoggerFactory.create_logger
    called_by: []
  - name: get_logger_types
    start_line: 71
    end_line: 73
    code: "def get_logger_types(cls) -> list[str]:\n        \"\"\"Get the registered\
      \ logger implementations.\"\"\"\n        return list(cls._registry.keys())"
    signature: def get_logger_types(cls) -> list[str]
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: list
      type: builtin
    - target: cls._registry.keys
      type: unresolved
    visibility: public
    node_id: graphrag/logger/factory.py::LoggerFactory.get_logger_types
    called_by: []
  - name: is_supported_type
    start_line: 76
    end_line: 78
    code: "def is_supported_type(cls, reporting_type: str) -> bool:\n        \"\"\"\
      Check if the given logger type is supported.\"\"\"\n        return reporting_type\
      \ in cls._registry"
    signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/logger/factory.py::LoggerFactory.is_supported_type
    called_by: []
  - name: create_file_logger
    start_line: 82
    end_line: 96
    code: "def create_file_logger(**kwargs) -> logging.Handler:\n    \"\"\"Create\
      \ a file-based logger.\"\"\"\n    root_dir = kwargs[\"root_dir\"]\n    base_dir\
      \ = kwargs[\"base_dir\"]\n    filename = kwargs[\"filename\"]\n    log_dir =\
      \ Path(root_dir) / base_dir\n    log_dir.mkdir(parents=True, exist_ok=True)\n\
      \    log_file_path = log_dir / filename\n\n    handler = logging.FileHandler(str(log_file_path),\
      \ mode=\"a\")\n\n    formatter = logging.Formatter(fmt=LOG_FORMAT, datefmt=DATE_FORMAT)\n\
      \    handler.setFormatter(formatter)\n\n    return handler"
    signature: def create_file_logger(**kwargs) -> logging.Handler
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: log_dir.mkdir
      type: unresolved
    - target: logging::FileHandler
      type: stdlib
    - target: str
      type: builtin
    - target: logging::Formatter
      type: stdlib
    - target: handler.setFormatter
      type: unresolved
    visibility: public
    node_id: graphrag/logger/factory.py::create_file_logger
    called_by: []
  - name: create_blob_logger
    start_line: 99
    end_line: 108
    code: "def create_blob_logger(**kwargs) -> logging.Handler:\n    \"\"\"Create\
      \ a blob storage-based logger.\"\"\"\n    from graphrag.logger.blob_workflow_logger\
      \ import BlobWorkflowLogger\n\n    return BlobWorkflowLogger(\n        connection_string=kwargs[\"\
      connection_string\"],\n        container_name=kwargs[\"container_name\"],\n\
      \        base_dir=kwargs[\"base_dir\"],\n        storage_account_blob_url=kwargs[\"\
      storage_account_blob_url\"],\n    )"
    signature: def create_blob_logger(**kwargs) -> logging.Handler
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger
      type: internal
    visibility: public
    node_id: graphrag/logger/factory.py::create_blob_logger
    called_by: []
- file_name: graphrag/logger/progress.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  functions:
  - name: __init__
    start_line: 41
    end_line: 47
    code: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n    ):\n        self._callback = callback\n\
      \        self._description = description\n        self._num_total = num_total\n\
      \        self._num_complete = 0"
    signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/logger/progress.py::ProgressTicker.__init__
    called_by: []
  - name: __call__
    start_line: 49
    end_line: 60
    code: "def __call__(self, num_ticks: int = 1) -> None:\n        \"\"\"Emit progress.\"\
      \"\"\n        self._num_complete += num_ticks\n        if self._callback is\
      \ not None:\n            p = Progress(\n                total_items=self._num_total,\n\
      \                completed_items=self._num_complete,\n                description=self._description,\n\
      \            )\n            if p.description:\n                logger.info(\"\
      %s%s/%s\", p.description, p.completed_items, p.total_items)\n            self._callback(p)"
    signature: 'def __call__(self, num_ticks: int = 1) -> None'
    decorators: []
    raises: []
    calls:
    - target: Progress
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: self._callback
      type: instance
    visibility: protected
    node_id: graphrag/logger/progress.py::ProgressTicker.__call__
    called_by: []
  - name: done
    start_line: 62
    end_line: 71
    code: "def done(self) -> None:\n        \"\"\"Mark the progress as done.\"\"\"\
      \n        if self._callback is not None:\n            self._callback(\n    \
      \            Progress(\n                    total_items=self._num_total,\n \
      \                   completed_items=self._num_total,\n                    description=self._description,\n\
      \                )\n            )"
    signature: def done(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._callback
      type: instance
    - target: Progress
      type: unresolved
    visibility: public
    node_id: graphrag/logger/progress.py::ProgressTicker.done
    called_by: []
  - name: progress_ticker
    start_line: 74
    end_line: 78
    code: "def progress_ticker(\n    callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n) -> ProgressTicker:\n    \"\"\"Create a progress\
      \ ticker.\"\"\"\n    return ProgressTicker(callback, num_total, description=description)"
    signature: "def progress_ticker(\n    callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n) -> ProgressTicker"
    decorators: []
    raises: []
    calls:
    - target: ProgressTicker
      type: unresolved
    visibility: public
    node_id: graphrag/logger/progress.py::progress_ticker
    called_by:
    - source: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - source: graphrag/index/operations/embed_text/strategies/mock.py::run
      type: internal
    - source: graphrag/index/operations/embed_text/strategies/openai.py::run
      type: internal
    - source: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
      type: internal
    - source: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
      type: internal
    - source: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
      type: internal
  - name: progress_iterable
    start_line: 81
    end_line: 95
    code: "def progress_iterable(\n    iterable: Iterable[T],\n    progress: ProgressHandler\
      \ | None,\n    num_total: int | None = None,\n    description: str = \"\",\n\
      ) -> Iterable[T]:\n    \"\"\"Wrap an iterable with a progress handler. Every\
      \ time an item is yielded, the progress handler will be called with the current\
      \ progress.\"\"\"\n    if num_total is None:\n        num_total = len(list(iterable))\n\
      \n    tick = ProgressTicker(progress, num_total, description=description)\n\n\
      \    for item in iterable:\n        tick(1)\n        yield item"
    signature: "def progress_iterable(\n    iterable: Iterable[T],\n    progress:\
      \ ProgressHandler | None,\n    num_total: int | None = None,\n    description:\
      \ str = \"\",\n) -> Iterable[T]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: list
      type: builtin
    - target: ProgressTicker
      type: unresolved
    - target: tick
      type: unresolved
    visibility: public
    node_id: graphrag/logger/progress.py::progress_iterable
    called_by:
    - source: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
      type: internal
- file_name: graphrag/logger/standard_logging.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: graphrag.logger.factory
    name: LoggerFactory
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: init_loggers
    start_line: 50
    end_line: 83
    code: "def init_loggers(\n    config: GraphRagConfig,\n    verbose: bool = False,\n\
      \    filename: str = DEFAULT_LOG_FILENAME,\n) -> None:\n    \"\"\"Initialize\
      \ logging handlers for graphrag based on configuration.\n\n    Parameters\n\
      \    ----------\n    config : GraphRagConfig | None, default=None\n        The\
      \ GraphRAG configuration. If None, defaults to file-based reporting.\n    verbose\
      \ : bool, default=False\n        Whether to enable verbose (DEBUG) logging.\n\
      \    filename : Optional[str]\n        Log filename on disk. If unset, will\
      \ use a default name.\n    \"\"\"\n    logger = logging.getLogger(\"graphrag\"\
      )\n    log_level = logging.DEBUG if verbose else logging.INFO\n    logger.setLevel(log_level)\n\
      \n    # clear any existing handlers to avoid duplicate logs\n    if logger.hasHandlers():\n\
      \        # Close file handlers properly before removing them\n        for handler\
      \ in logger.handlers:\n            if isinstance(handler, logging.FileHandler):\n\
      \                handler.close()\n        logger.handlers.clear()\n\n    reporting_config\
      \ = config.reporting\n    config_dict = reporting_config.model_dump()\n    args\
      \ = {**config_dict, \"root_dir\": config.root_dir, \"filename\": filename}\n\
      \n    handler = LoggerFactory.create_logger(reporting_config.type, args)\n \
      \   logger.addHandler(handler)"
    signature: "def init_loggers(\n    config: GraphRagConfig,\n    verbose: bool\
      \ = False,\n    filename: str = DEFAULT_LOG_FILENAME,\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: logging::getLogger
      type: stdlib
    - target: logger.setLevel
      type: unresolved
    - target: logger.hasHandlers
      type: unresolved
    - target: isinstance
      type: builtin
    - target: handler.close
      type: unresolved
    - target: logger.handlers.clear
      type: unresolved
    - target: reporting_config.model_dump
      type: unresolved
    - target: graphrag/logger/factory.py::LoggerFactory::create_logger
      type: external
    - target: logger.addHandler
      type: unresolved
    visibility: public
    node_id: graphrag/logger/standard_logging.py::init_loggers
    called_by:
    - source: graphrag/api/index.py::build_index
      type: internal
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
    - source: graphrag/api/query.py::global_search
      type: internal
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
    - source: graphrag/api/query.py::multi_index_global_search
      type: internal
    - source: graphrag/api/query.py::local_search
      type: internal
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::multi_index_local_search
      type: internal
    - source: graphrag/api/query.py::drift_search
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
    - source: graphrag/api/query.py::multi_index_drift_search
      type: internal
    - source: graphrag/api/query.py::basic_search
      type: internal
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
    - source: graphrag/api/query.py::multi_index_basic_search
      type: internal
    - source: graphrag/cli/index.py::_run_index
      type: internal
    - source: graphrag/cli/prompt_tune.py::prompt_tune
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
      type: internal
- file_name: graphrag/prompt_tune/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/defaults.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/generator/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/generator/community_report_rating.py
  imports:
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.prompt.community_report_rating
    name: GENERATE_REPORT_RATING_PROMPT
    alias: null
  functions:
  - name: generate_community_report_rating
    start_line: 12
    end_line: 35
    code: "async def generate_community_report_rating(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str:\n    \"\"\"Generate an\
      \ LLM persona to use for GraphRAG prompts.\n\n    Parameters\n    ----------\n\
      \    - llm (CompletionLLM): The LLM to use for generation\n    - domain (str):\
      \ The domain to generate a rating for\n    - persona (str): The persona to generate\
      \ a rating for for\n    - docs (str | list[str]): Documents used to contextualize\
      \ the rating\n\n    Returns\n    -------\n    - str: The generated rating description\
      \ prompt response.\n    \"\"\"\n    docs_str = \" \".join(docs) if isinstance(docs,\
      \ list) else docs\n    domain_prompt = GENERATE_REPORT_RATING_PROMPT.format(\n\
      \        domain=domain, persona=persona, input_text=docs_str\n    )\n\n    response\
      \ = await model.achat(domain_prompt)\n\n    return str(response.output.content).strip()"
    signature: "def generate_community_report_rating(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: '" ".join'
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/prompt_tune/prompt/community_report_rating.py::GENERATE_REPORT_RATING_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: str(response.output.content).strip
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/community_report_rating.py::generate_community_report_rating
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/community_report_summarization.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.prompt_tune.template.community_report_summarization
    name: COMMUNITY_REPORT_SUMMARIZATION_PROMPT
    alias: null
  functions:
  - name: create_community_summarization_prompt
    start_line: 15
    end_line: 50
    code: "def create_community_summarization_prompt(\n    persona: str,\n    role:\
      \ str,\n    report_rating_description: str,\n    language: str,\n    output_path:\
      \ Path | None = None,\n) -> str:\n    \"\"\"Create a prompt for community summarization.\
      \ If output_path is provided, write the prompt to a file.\n\n    Parameters\n\
      \    ----------\n    - persona (str): The persona to use for the community summarization\
      \ prompt\n    - role (str): The role to use for the community summarization\
      \ prompt\n    - language (str): The language to use for the community summarization\
      \ prompt\n    - output_path (Path | None): The path to write the prompt to.\
      \ Default is None. If None, the prompt is not written to a file. Default is\
      \ None.\n\n    Returns\n    -------\n    - str: The community summarization\
      \ prompt\n    \"\"\"\n    prompt = COMMUNITY_REPORT_SUMMARIZATION_PROMPT.format(\n\
      \        persona=persona,\n        role=role,\n        report_rating_description=report_rating_description,\n\
      \        language=language,\n    )\n\n    if output_path:\n        output_path.mkdir(parents=True,\
      \ exist_ok=True)\n\n        output_path = output_path / COMMUNITY_SUMMARIZATION_FILENAME\n\
      \        # Write file to output path\n        with output_path.open(\"wb\")\
      \ as file:\n            file.write(prompt.encode(encoding=\"utf-8\", errors=\"\
      strict\"))\n\n    return prompt"
    signature: "def create_community_summarization_prompt(\n    persona: str,\n  \
      \  role: str,\n    report_rating_description: str,\n    language: str,\n   \
      \ output_path: Path | None = None,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: graphrag/prompt_tune/template/community_report_summarization.py::COMMUNITY_REPORT_SUMMARIZATION_PROMPT::format
      type: external
    - target: output_path.mkdir
      type: unresolved
    - target: output_path.open
      type: unresolved
    - target: file.write
      type: unresolved
    - target: prompt.encode
      type: unresolved
    visibility: public
    node_id: graphrag/prompt_tune/generator/community_report_summarization.py::create_community_summarization_prompt
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/community_reporter_role.py
  imports:
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.prompt.community_reporter_role
    name: GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT
    alias: null
  functions:
  - name: generate_community_reporter_role
    start_line: 12
    end_line: 35
    code: "async def generate_community_reporter_role(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str:\n    \"\"\"Generate an\
      \ LLM persona to use for GraphRAG prompts.\n\n    Parameters\n    ----------\n\
      \    - llm (CompletionLLM): The LLM to use for generation\n    - domain (str):\
      \ The domain to generate a persona for\n    - persona (str): The persona to\
      \ generate a role for\n    - docs (str | list[str]): The domain to generate\
      \ a persona for\n\n    Returns\n    -------\n    - str: The generated domain\
      \ prompt response.\n    \"\"\"\n    docs_str = \" \".join(docs) if isinstance(docs,\
      \ list) else docs\n    domain_prompt = GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT.format(\n\
      \        domain=domain, persona=persona, input_text=docs_str\n    )\n\n    response\
      \ = await model.achat(domain_prompt)\n\n    return str(response.output.content)"
    signature: "def generate_community_reporter_role(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: '" ".join'
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/prompt_tune/prompt/community_reporter_role.py::GENERATE_COMMUNITY_REPORTER_ROLE_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/community_reporter_role.py::generate_community_reporter_role
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/domain.py
  imports:
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.prompt.domain
    name: GENERATE_DOMAIN_PROMPT
    alias: null
  functions:
  - name: generate_domain
    start_line: 10
    end_line: 27
    code: "async def generate_domain(model: ChatModel, docs: str | list[str]) -> str:\n\
      \    \"\"\"Generate an LLM persona to use for GraphRAG prompts.\n\n    Parameters\n\
      \    ----------\n    - llm (CompletionLLM): The LLM to use for generation\n\
      \    - docs (str | list[str]): The domain to generate a persona for\n\n    Returns\n\
      \    -------\n    - str: The generated domain prompt response.\n    \"\"\"\n\
      \    docs_str = \" \".join(docs) if isinstance(docs, list) else docs\n    domain_prompt\
      \ = GENERATE_DOMAIN_PROMPT.format(input_text=docs_str)\n\n    response = await\
      \ model.achat(domain_prompt)\n\n    return str(response.output.content)"
    signature: 'def generate_domain(model: ChatModel, docs: str | list[str]) -> str'
    decorators: []
    raises: []
    calls:
    - target: '" ".join'
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/prompt_tune/prompt/domain.py::GENERATE_DOMAIN_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/domain.py::generate_domain
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/entity_relationship.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.prompt.entity_relationship
    name: ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT
    alias: null
  - module: graphrag.prompt_tune.prompt.entity_relationship
    name: ENTITY_RELATIONSHIPS_GENERATION_PROMPT
    alias: null
  - module: graphrag.prompt_tune.prompt.entity_relationship
    name: UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT
    alias: null
  functions:
  - name: generate_entity_relationship_examples
    start_line: 18
    end_line: 65
    code: "async def generate_entity_relationship_examples(\n    model: ChatModel,\n\
      \    persona: str,\n    entity_types: str | list[str] | None,\n    docs: str\
      \ | list[str],\n    language: str,\n    json_mode: bool = False,\n) -> list[str]:\n\
      \    \"\"\"Generate a list of entity/relationships examples for use in generating\
      \ an entity configuration.\n\n    Will return entity/relationships examples\
      \ as either JSON or in tuple_delimiter format depending\n    on the json_mode\
      \ parameter.\n    \"\"\"\n    docs_list = [docs] if isinstance(docs, str) else\
      \ docs\n    history = [{\"content\": persona, \"role\": \"system\"}]\n\n   \
      \ if entity_types:\n        entity_types_str = (\n            entity_types\n\
      \            if isinstance(entity_types, str)\n            else \", \".join(map(str,\
      \ entity_types))\n        )\n\n        messages = [\n            (\n       \
      \         ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT\n                if json_mode\n\
      \                else ENTITY_RELATIONSHIPS_GENERATION_PROMPT\n            ).format(entity_types=entity_types_str,\
      \ input_text=doc, language=language)\n            for doc in docs_list\n   \
      \     ]\n    else:\n        messages = [\n            UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT.format(\n\
      \                input_text=doc, language=language\n            )\n        \
      \    for doc in docs_list\n        ]\n\n    messages = messages[:MAX_EXAMPLES]\n\
      \n    tasks = [\n        model.achat(message, history=history, json=json_mode)\
      \ for message in messages\n    ]\n\n    responses = await asyncio.gather(*tasks)\n\
      \n    return [str(response.output.content) for response in responses]"
    signature: "def generate_entity_relationship_examples(\n    model: ChatModel,\n\
      \    persona: str,\n    entity_types: str | list[str] | None,\n    docs: str\
      \ | list[str],\n    language: str,\n    json_mode: bool = False,\n) -> list[str]"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: '", ".join'
      type: unresolved
    - target: map
      type: builtin
    - target: "(\n                ENTITY_RELATIONSHIPS_GENERATION_JSON_PROMPT\n  \
        \              if json_mode\n                else ENTITY_RELATIONSHIPS_GENERATION_PROMPT\n\
        \            ).format"
      type: unresolved
    - target: graphrag/prompt_tune/prompt/entity_relationship.py::UNTYPED_ENTITY_RELATIONSHIPS_GENERATION_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/entity_relationship.py::generate_entity_relationship_examples
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/entity_summarization_prompt.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.prompt_tune.template.entity_summarization
    name: ENTITY_SUMMARIZATION_PROMPT
    alias: null
  functions:
  - name: create_entity_summarization_prompt
    start_line: 15
    end_line: 39
    code: "def create_entity_summarization_prompt(\n    persona: str,\n    language:\
      \ str,\n    output_path: Path | None = None,\n) -> str:\n    \"\"\"\n    Create\
      \ a prompt for entity summarization.\n\n    Parameters\n    ----------\n   \
      \ - persona (str): The persona to use for the entity summarization prompt\n\
      \    - language (str): The language to use for the entity summarization prompt\n\
      \    - output_path (Path | None): The path to write the prompt to. Default is\
      \ None.\n    \"\"\"\n    prompt = ENTITY_SUMMARIZATION_PROMPT.format(persona=persona,\
      \ language=language)\n\n    if output_path:\n        output_path.mkdir(parents=True,\
      \ exist_ok=True)\n\n        output_path = output_path / ENTITY_SUMMARIZATION_FILENAME\n\
      \        # Write file to output path\n        with output_path.open(\"wb\")\
      \ as file:\n            file.write(prompt.encode(encoding=\"utf-8\", errors=\"\
      strict\"))\n\n    return prompt"
    signature: "def create_entity_summarization_prompt(\n    persona: str,\n    language:\
      \ str,\n    output_path: Path | None = None,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: graphrag/prompt_tune/template/entity_summarization.py::ENTITY_SUMMARIZATION_PROMPT::format
      type: external
    - target: output_path.mkdir
      type: unresolved
    - target: output_path.open
      type: unresolved
    - target: file.write
      type: unresolved
    - target: prompt.encode
      type: unresolved
    visibility: public
    node_id: graphrag/prompt_tune/generator/entity_summarization_prompt.py::create_entity_summarization_prompt
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/entity_types.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: DEFAULT_TASK
    alias: null
  - module: graphrag.prompt_tune.prompt.entity_types
    name: ENTITY_TYPE_GENERATION_JSON_PROMPT
    alias: null
  - module: graphrag.prompt_tune.prompt.entity_types
    name: ENTITY_TYPE_GENERATION_PROMPT
    alias: null
  functions:
  - name: generate_entity_types
    start_line: 22
    end_line: 59
    code: "async def generate_entity_types(\n    model: ChatModel,\n    domain: str,\n\
      \    persona: str,\n    docs: str | list[str],\n    task: str = DEFAULT_TASK,\n\
      \    json_mode: bool = False,\n) -> str | list[str]:\n    \"\"\"\n    Generate\
      \ entity type categories from a given set of (small) documents.\n\n    Example\
      \ Output:\n    \"entity_types\": ['military unit', 'organization', 'person',\
      \ 'location', 'event', 'date', 'equipment']\n    \"\"\"\n    formatted_task\
      \ = task.format(domain=domain)\n\n    docs_str = \"\\n\".join(docs) if isinstance(docs,\
      \ list) else docs\n\n    entity_types_prompt = (\n        ENTITY_TYPE_GENERATION_JSON_PROMPT\n\
      \        if json_mode\n        else ENTITY_TYPE_GENERATION_PROMPT\n    ).format(task=formatted_task,\
      \ input_text=docs_str)\n\n    history = [{\"role\": \"system\", \"content\"\
      : persona}]\n\n    if json_mode:\n        response = await model.achat(\n  \
      \          entity_types_prompt,\n            history=history,\n            json=json_mode,\n\
      \            json_model=EntityTypesResponse,\n        )\n        parsed_model\
      \ = response.parsed_response\n        return parsed_model.entity_types if parsed_model\
      \ else []\n\n    response = await model.achat(entity_types_prompt, history=history,\
      \ json=json_mode)\n    return str(response.output.content)"
    signature: "def generate_entity_types(\n    model: ChatModel,\n    domain: str,\n\
      \    persona: str,\n    docs: str | list[str],\n    task: str = DEFAULT_TASK,\n\
      \    json_mode: bool = False,\n) -> str | list[str]"
    decorators: []
    raises: []
    calls:
    - target: task.format
      type: unresolved
    - target: '"\n".join'
      type: unresolved
    - target: isinstance
      type: builtin
    - target: "(\n        ENTITY_TYPE_GENERATION_JSON_PROMPT\n        if json_mode\n\
        \        else ENTITY_TYPE_GENERATION_PROMPT\n    ).format"
      type: unresolved
    - target: model.achat
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/entity_types.py::generate_entity_types
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/extract_graph_prompt.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.prompt_tune.template.extract_graph
    name: EXAMPLE_EXTRACTION_TEMPLATE
    alias: null
  - module: graphrag.prompt_tune.template.extract_graph
    name: GRAPH_EXTRACTION_JSON_PROMPT
    alias: null
  - module: graphrag.prompt_tune.template.extract_graph
    name: GRAPH_EXTRACTION_PROMPT
    alias: null
  - module: graphrag.prompt_tune.template.extract_graph
    name: UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE
    alias: null
  - module: graphrag.prompt_tune.template.extract_graph
    name: UNTYPED_GRAPH_EXTRACTION_PROMPT
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: create_extract_graph_prompt
    start_line: 21
    end_line: 109
    code: "def create_extract_graph_prompt(\n    entity_types: str | list[str] | None,\n\
      \    docs: list[str],\n    examples: list[str],\n    language: str,\n    max_token_count:\
      \ int,\n    tokenizer: Tokenizer | None = None,\n    json_mode: bool = False,\n\
      \    output_path: Path | None = None,\n    min_examples_required: int = 2,\n\
      ) -> str:\n    \"\"\"\n    Create a prompt for entity extraction.\n\n    Parameters\n\
      \    ----------\n    - entity_types (str | list[str]): The entity types to extract\n\
      \    - docs (list[str]): The list of documents to extract entities from\n  \
      \  - examples (list[str]): The list of examples to use for entity extraction\n\
      \    - language (str): The language of the inputs and outputs\n    - tokenizer\
      \ (Tokenizer): The tokenizer to use for encoding and decoding text.\n    - max_token_count\
      \ (int): The maximum number of tokens to use for the prompt\n    - json_mode\
      \ (bool): Whether to use JSON mode for the prompt. Default is False\n    - output_path\
      \ (Path | None): The path to write the prompt to. Default is None.\n       \
      \ - min_examples_required (int): The minimum number of examples required. Default\
      \ is 2.\n\n    Returns\n    -------\n    - str: The entity extraction prompt\n\
      \    \"\"\"\n    prompt = (\n        (GRAPH_EXTRACTION_JSON_PROMPT if json_mode\
      \ else GRAPH_EXTRACTION_PROMPT)\n        if entity_types\n        else UNTYPED_GRAPH_EXTRACTION_PROMPT\n\
      \    )\n    if isinstance(entity_types, list):\n        entity_types = \", \"\
      .join(map(str, entity_types))\n\n    tokenizer = tokenizer or get_tokenizer()\n\
      \n    tokens_left = (\n        max_token_count\n        - tokenizer.num_tokens(prompt)\n\
      \        - tokenizer.num_tokens(entity_types)\n        if entity_types\n   \
      \     else 0\n    )\n\n    examples_prompt = \"\"\n\n    # Iterate over examples,\
      \ while we have tokens left or examples left\n    for i, output in enumerate(examples):\n\
      \        input = docs[i]\n        example_formatted = (\n            EXAMPLE_EXTRACTION_TEMPLATE.format(\n\
      \                n=i + 1, input_text=input, entity_types=entity_types, output=output\n\
      \            )\n            if entity_types\n            else UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE.format(\n\
      \                n=i + 1, input_text=input, output=output\n            )\n \
      \       )\n\n        example_tokens = tokenizer.num_tokens(example_formatted)\n\
      \n        # Ensure at least three examples are included\n        if i >= min_examples_required\
      \ and example_tokens > tokens_left:\n            break\n\n        examples_prompt\
      \ += example_formatted\n        tokens_left -= example_tokens\n\n    prompt\
      \ = (\n        prompt.format(\n            entity_types=entity_types, examples=examples_prompt,\
      \ language=language\n        )\n        if entity_types\n        else prompt.format(examples=examples_prompt,\
      \ language=language)\n    )\n\n    if output_path:\n        output_path.mkdir(parents=True,\
      \ exist_ok=True)\n\n        output_path = output_path / EXTRACT_GRAPH_FILENAME\n\
      \        # Write file to output path\n        with output_path.open(\"wb\")\
      \ as file:\n            file.write(prompt.encode(encoding=\"utf-8\", errors=\"\
      strict\"))\n\n    return prompt"
    signature: "def create_extract_graph_prompt(\n    entity_types: str | list[str]\
      \ | None,\n    docs: list[str],\n    examples: list[str],\n    language: str,\n\
      \    max_token_count: int,\n    tokenizer: Tokenizer | None = None,\n    json_mode:\
      \ bool = False,\n    output_path: Path | None = None,\n    min_examples_required:\
      \ int = 2,\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: '", ".join'
      type: unresolved
    - target: map
      type: builtin
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    - target: enumerate
      type: builtin
    - target: graphrag/prompt_tune/template/extract_graph.py::EXAMPLE_EXTRACTION_TEMPLATE::format
      type: external
    - target: graphrag/prompt_tune/template/extract_graph.py::UNTYPED_EXAMPLE_EXTRACTION_TEMPLATE::format
      type: external
    - target: prompt.format
      type: unresolved
    - target: output_path.mkdir
      type: unresolved
    - target: output_path.open
      type: unresolved
    - target: file.write
      type: unresolved
    - target: prompt.encode
      type: unresolved
    visibility: public
    node_id: graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/language.py
  imports:
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.prompt.language
    name: DETECT_LANGUAGE_PROMPT
    alias: null
  functions:
  - name: detect_language
    start_line: 10
    end_line: 27
    code: "async def detect_language(model: ChatModel, docs: str | list[str]) -> str:\n\
      \    \"\"\"Detect input language to use for GraphRAG prompts.\n\n    Parameters\n\
      \    ----------\n    - llm (CompletionLLM): The LLM to use for generation\n\
      \    - docs (str | list[str]): The docs to detect language from\n\n    Returns\n\
      \    -------\n    - str: The detected language.\n    \"\"\"\n    docs_str =\
      \ \" \".join(docs) if isinstance(docs, list) else docs\n    language_prompt\
      \ = DETECT_LANGUAGE_PROMPT.format(input_text=docs_str)\n\n    response = await\
      \ model.achat(language_prompt)\n\n    return str(response.output.content)"
    signature: 'def detect_language(model: ChatModel, docs: str | list[str]) -> str'
    decorators: []
    raises: []
    calls:
    - target: '" ".join'
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/prompt_tune/prompt/language.py::DETECT_LANGUAGE_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/language.py::detect_language
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/generator/persona.py
  imports:
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: DEFAULT_TASK
    alias: null
  - module: graphrag.prompt_tune.prompt.persona
    name: GENERATE_PERSONA_PROMPT
    alias: null
  functions:
  - name: generate_persona
    start_line: 11
    end_line: 27
    code: "async def generate_persona(\n    model: ChatModel, domain: str, task: str\
      \ = DEFAULT_TASK\n) -> str:\n    \"\"\"Generate an LLM persona to use for GraphRAG\
      \ prompts.\n\n    Parameters\n    ----------\n    - llm (CompletionLLM): The\
      \ LLM to use for generation\n    - domain (str): The domain to generate a persona\
      \ for\n    - task (str): The task to generate a persona for. Default is DEFAULT_TASK\n\
      \    \"\"\"\n    formatted_task = task.format(domain=domain)\n    persona_prompt\
      \ = GENERATE_PERSONA_PROMPT.format(sample_task=formatted_task)\n\n    response\
      \ = await model.achat(persona_prompt)\n\n    return str(response.output.content)"
    signature: "def generate_persona(\n    model: ChatModel, domain: str, task: str\
      \ = DEFAULT_TASK\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: task.format
      type: unresolved
    - target: graphrag/prompt_tune/prompt/persona.py::GENERATE_PERSONA_PROMPT::format
      type: external
    - target: model.achat
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/prompt_tune/generator/persona.py::generate_persona
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/loader/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/loader/input.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.cache.noop_pipeline_cache
    name: NoopPipelineCache
    alias: null
  - module: graphrag.callbacks.noop_workflow_callbacks
    name: NoopWorkflowCallbacks
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.index.operations.embed_text.strategies.openai
    name: run
    alias: null
  - module: graphrag.index.workflows.create_base_text_units
    name: create_base_text_units
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: LIMIT
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: N_SUBSET_MAX
    alias: null
  - module: graphrag.prompt_tune.defaults
    name: K
    alias: null
  - module: graphrag.prompt_tune.types
    name: DocSelectionType
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  functions:
  - name: _sample_chunks_from_embeddings
    start_line: 28
    end_line: 38
    code: "def _sample_chunks_from_embeddings(\n    text_chunks: pd.DataFrame,\n \
      \   embeddings: np.ndarray[float, np.dtype[np.float_]],\n    k: int = K,\n)\
      \ -> pd.DataFrame:\n    \"\"\"Sample text chunks from embeddings.\"\"\"\n  \
      \  center = np.mean(embeddings, axis=0)\n    distances = np.linalg.norm(embeddings\
      \ - center, axis=1)\n    nearest_indices = np.argsort(distances)[:k]\n\n   \
      \ return text_chunks.iloc[nearest_indices]"
    signature: "def _sample_chunks_from_embeddings(\n    text_chunks: pd.DataFrame,\n\
      \    embeddings: np.ndarray[float, np.dtype[np.float_]],\n    k: int = K,\n\
      ) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: numpy::mean
      type: external
    - target: numpy::linalg.norm
      type: external
    - target: numpy::argsort
      type: external
    visibility: protected
    node_id: graphrag/prompt_tune/loader/input.py::_sample_chunks_from_embeddings
    called_by:
    - source: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
      type: internal
  - name: load_docs_in_chunks
    start_line: 41
    end_line: 108
    code: "async def load_docs_in_chunks(\n    config: GraphRagConfig,\n    select_method:\
      \ DocSelectionType,\n    limit: int,\n    logger: logging.Logger,\n    chunk_size:\
      \ int,\n    overlap: int,\n    n_subset_max: int = N_SUBSET_MAX,\n    k: int\
      \ = K,\n) -> list[str]:\n    \"\"\"Load docs into chunks for generating prompts.\"\
      \"\"\n    embeddings_llm_settings = config.get_language_model_config(\n    \
      \    config.embed_text.model_id\n    )\n    input_storage = create_storage_from_config(config.input.storage)\n\
      \    dataset = await create_input(config.input, input_storage)\n    chunk_config\
      \ = config.chunks\n    chunks_df = create_base_text_units(\n        documents=dataset,\n\
      \        callbacks=NoopWorkflowCallbacks(),\n        group_by_columns=chunk_config.group_by_columns,\n\
      \        size=chunk_size,\n        overlap=overlap,\n        encoding_model=chunk_config.encoding_model,\n\
      \        strategy=chunk_config.strategy,\n        prepend_metadata=chunk_config.prepend_metadata,\n\
      \        chunk_size_includes_metadata=chunk_config.chunk_size_includes_metadata,\n\
      \    )\n\n    # Depending on the select method, build the dataset\n    if limit\
      \ <= 0 or limit > len(chunks_df):\n        logger.warning(f\"Limit out of range,\
      \ using default number of chunks: {LIMIT}\")  # noqa: G004\n        limit =\
      \ LIMIT\n\n    if select_method == DocSelectionType.TOP:\n        chunks_df\
      \ = chunks_df[:limit]\n    elif select_method == DocSelectionType.RANDOM:\n\
      \        chunks_df = chunks_df.sample(n=limit)\n    elif select_method == DocSelectionType.AUTO:\n\
      \        if k is None or k <= 0:\n            msg = \"k must be an integer >\
      \ 0\"\n            raise ValueError(msg)\n\n        \"\"\"Convert text chunks\
      \ into dense text embeddings.\"\"\"\n        sampled_text_chunks = chunks_df.sample(n=min(n_subset_max,\
      \ len(chunks_df)))[\n            \"text\"\n        ].tolist()\n\n        embedding_results\
      \ = await run_embed_text(\n            sampled_text_chunks,\n            callbacks=NoopWorkflowCallbacks(),\n\
      \            cache=NoopPipelineCache(),\n            args={\n              \
      \  \"llm\": embeddings_llm_settings.model_dump(),\n                \"num_threads\"\
      : embeddings_llm_settings.concurrent_requests,\n                \"batch_size\"\
      : config.embed_text.batch_size,\n                \"batch_max_tokens\": config.embed_text.batch_max_tokens,\n\
      \            },\n        )\n        embeddings = np.array(embedding_results.embeddings)\n\
      \        chunks_df = _sample_chunks_from_embeddings(chunks_df, embeddings, k=k)\n\
      \n    # Convert the dataset to list form, so we have a list of documents\n \
      \   return [\n        # need this to prevent the str.format() function from\
      \ breaking when parsing LaTeX from markdown files\n        i.replace(\"{\",\
      \ \"{{\").replace(\"}\", \"}}\")\n        for i in chunks_df[\"text\"]\n   \
      \ ]"
    signature: "def load_docs_in_chunks(\n    config: GraphRagConfig,\n    select_method:\
      \ DocSelectionType,\n    limit: int,\n    logger: logging.Logger,\n    chunk_size:\
      \ int,\n    overlap: int,\n    n_subset_max: int = N_SUBSET_MAX,\n    k: int\
      \ = K,\n) -> list[str]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: config.get_language_model_config
      type: unresolved
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    - target: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
      type: internal
    - target: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
      type: internal
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: chunks_df.sample
      type: unresolved
    - target: ValueError
      type: builtin
    - target: "chunks_df.sample(n=min(n_subset_max, len(chunks_df)))[\n          \
        \  \"text\"\n        ].tolist"
      type: unresolved
    - target: min
      type: builtin
    - target: run_embed_text
      type: unresolved
    - target: graphrag.cache.noop_pipeline_cache::NoopPipelineCache
      type: internal
    - target: embeddings_llm_settings.model_dump
      type: unresolved
    - target: numpy::array
      type: external
    - target: graphrag/prompt_tune/loader/input.py::_sample_chunks_from_embeddings
      type: internal
    - target: i.replace("{", "{{").replace
      type: unresolved
    - target: i.replace
      type: unresolved
    visibility: public
    node_id: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
- file_name: graphrag/prompt_tune/prompt/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/community_report_rating.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/community_reporter_role.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/domain.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/entity_relationship.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/entity_types.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/language.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/prompt/persona.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/template/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/template/community_report_summarization.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/template/entity_summarization.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/template/extract_graph.py
  imports: []
  functions: []
- file_name: graphrag/prompt_tune/types.py
  imports:
  - module: enum
    name: Enum
    alias: null
  functions:
  - name: __str__
    start_line: 17
    end_line: 19
    code: "def __str__(self):\n        \"\"\"Return the string representation of the\
      \ enum value.\"\"\"\n        return self.value"
    signature: def __str__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/prompt_tune/types.py::DocSelectionType.__str__
    called_by: []
- file_name: graphrag/prompts/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/community_report.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/community_report_text_units.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/extract_claims.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/extract_graph.py
  imports: []
  functions: []
- file_name: graphrag/prompts/index/summarize_descriptions.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/__init__.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/basic_search_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/drift_search_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/global_search_knowledge_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/global_search_map_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/global_search_reduce_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/local_search_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/prompts/query/question_gen_system_prompt.py
  imports: []
  functions: []
- file_name: graphrag/query/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/context_builder/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/context_builder/builders.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  functions:
  - name: build_context
    start_line: 31
    end_line: 37
    code: "async def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult:\n\
      \        \"\"\"Build the context for the global search mode.\"\"\""
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder.build_context
    called_by: []
  - name: build_context
    start_line: 44
    end_line: 50
    code: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult:\n\
      \        \"\"\"Build the context for the local search mode.\"\"\""
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/context_builder/builders.py::LocalContextBuilder.build_context
    called_by: []
  - name: build_context
    start_line: 57
    end_line: 62
    code: "async def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
      \    ) -> tuple[pd.DataFrame, dict[str, int]]:\n        \"\"\"Build the context\
      \ for the primer search actions.\"\"\""
    signature: "def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
      \    ) -> tuple[pd.DataFrame, dict[str, int]]"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder.build_context
    called_by: []
  - name: build_context
    start_line: 69
    end_line: 75
    code: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult:\n\
      \        \"\"\"Build the context for the basic search mode.\"\"\""
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/context_builder/builders.py::BasicContextBuilder.build_context
    called_by: []
- file_name: graphrag/query/context_builder/community_context.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: random
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_community_context
    start_line: 24
    end_line: 186
    code: "def build_community_context(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None = None,\n    tokenizer: Tokenizer | None\
      \ = None,\n    use_community_summary: bool = True,\n    column_delimiter: str\
      \ = \"|\",\n    shuffle_data: bool = True,\n    include_community_rank: bool\
      \ = False,\n    min_community_rank: int = 0,\n    community_rank_name: str =\
      \ \"rank\",\n    include_community_weight: bool = True,\n    community_weight_name:\
      \ str = \"occurrence weight\",\n    normalize_community_weight: bool = True,\n\
      \    max_context_tokens: int = 8000,\n    single_batch: bool = True,\n    context_name:\
      \ str = \"Reports\",\n    random_state: int = 86,\n) -> tuple[str | list[str],\
      \ dict[str, pd.DataFrame]]:\n    \"\"\"\n    Prepare community report data table\
      \ as context data for system prompt.\n\n    If entities are provided, the community\
      \ weight is calculated as the count of text units associated with entities within\
      \ the community.\n\n    The calculated weight is added as an attribute to the\
      \ community reports and added to the context data table.\n    \"\"\"\n    tokenizer\
      \ = tokenizer or get_tokenizer()\n\n    def _is_included(report: CommunityReport)\
      \ -> bool:\n        return report.rank is not None and report.rank >= min_community_rank\n\
      \n    def _get_header(attributes: list[str]) -> list[str]:\n        header =\
      \ [\"id\", \"title\"]\n        attributes = [col for col in attributes if col\
      \ not in header]\n        if not include_community_weight:\n            attributes\
      \ = [col for col in attributes if col != community_weight_name]\n        header.extend(attributes)\n\
      \        header.append(\"summary\" if use_community_summary else \"content\"\
      )\n        if include_community_rank:\n            header.append(community_rank_name)\n\
      \        return header\n\n    def _report_context_text(\n        report: CommunityReport,\
      \ attributes: list[str]\n    ) -> tuple[str, list[str]]:\n        context: list[str]\
      \ = [\n            report.short_id if report.short_id else \"\",\n         \
      \   report.title,\n            *[\n                str(report.attributes.get(field,\
      \ \"\")) if report.attributes else \"\"\n                for field in attributes\n\
      \            ],\n        ]\n        context.append(report.summary if use_community_summary\
      \ else report.full_content)\n        if include_community_rank:\n          \
      \  context.append(str(report.rank))\n        result = column_delimiter.join(context)\
      \ + \"\\n\"\n        return result, context\n\n    compute_community_weights\
      \ = (\n        entities\n        and len(community_reports) > 0\n        and\
      \ include_community_weight\n        and (\n            community_reports[0].attributes\
      \ is None\n            or community_weight_name not in community_reports[0].attributes\n\
      \        )\n    )\n    if compute_community_weights:\n        logger.debug(\"\
      Computing community weights...\")\n        community_reports = _compute_community_weights(\n\
      \            community_reports=community_reports,\n            entities=entities,\n\
      \            weight_attribute=community_weight_name,\n            normalize=normalize_community_weight,\n\
      \        )\n\n    selected_reports = [report for report in community_reports\
      \ if _is_included(report)]\n\n    if selected_reports is None or len(selected_reports)\
      \ == 0:\n        return ([], {})\n\n    if shuffle_data:\n        random.seed(random_state)\n\
      \        random.shuffle(selected_reports)\n\n    # \"global\" variables\n  \
      \  attributes = (\n        list(community_reports[0].attributes.keys())\n  \
      \      if community_reports[0].attributes\n        else []\n    )\n    header\
      \ = _get_header(attributes)\n    all_context_text: list[str] = []\n    all_context_records:\
      \ list[pd.DataFrame] = []\n\n    # batch variables\n    batch_text: str = \"\
      \"\n    batch_tokens: int = 0\n    batch_records: list[list[str]] = []\n\n \
      \   def _init_batch() -> None:\n        nonlocal batch_text, batch_tokens, batch_records\n\
      \        batch_text = (\n            f\"-----{context_name}-----\" + \"\\n\"\
      \ + column_delimiter.join(header) + \"\\n\"\n        )\n        batch_tokens\
      \ = tokenizer.num_tokens(batch_text)\n        batch_records = []\n\n    def\
      \ _cut_batch() -> None:\n        # convert the current context records to pandas\
      \ dataframe and sort by weight and rank if exist\n        record_df = _convert_report_context_to_df(\n\
      \            context_records=batch_records,\n            header=header,\n  \
      \          weight_column=(\n                community_weight_name if entities\
      \ and include_community_weight else None\n            ),\n            rank_column=community_rank_name\
      \ if include_community_rank else None,\n        )\n        if len(record_df)\
      \ == 0:\n            return\n        current_context_text = record_df.to_csv(index=False,\
      \ sep=column_delimiter)\n        if not all_context_text and single_batch:\n\
      \            current_context_text = f\"-----{context_name}-----\\n{current_context_text}\"\
      \n\n        all_context_text.append(current_context_text)\n        all_context_records.append(record_df)\n\
      \n    # initialize the first batch\n    _init_batch()\n\n    for report in selected_reports:\n\
      \        new_context_text, new_context = _report_context_text(report, attributes)\n\
      \        new_tokens = tokenizer.num_tokens(new_context_text)\n\n        if batch_tokens\
      \ + new_tokens > max_context_tokens:\n            # add the current batch to\
      \ the context data and start a new batch if we are in multi-batch mode\n   \
      \         _cut_batch()\n            if single_batch:\n                break\n\
      \            _init_batch()\n\n        # add current report to the current batch\n\
      \        batch_text += new_context_text\n        batch_tokens += new_tokens\n\
      \        batch_records.append(new_context)\n\n    # Extract the IDs from the\
      \ current batch\n    current_batch_ids = {record[0] for record in batch_records}\n\
      \n    # Extract the IDs from all previous batches in all_context_records\n \
      \   existing_ids_sets = [set(record[\"id\"].to_list()) for record in all_context_records]\n\
      \n    # Check if the current batch has been added\n    if current_batch_ids\
      \ not in existing_ids_sets:\n        _cut_batch()\n\n    if len(all_context_records)\
      \ == 0:\n        logger.warning(NO_COMMUNITY_RECORDS_WARNING)\n        return\
      \ ([], {})\n\n    return all_context_text, {\n        context_name.lower():\
      \ pd.concat(all_context_records, ignore_index=True)\n    }"
    signature: "def build_community_context(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None = None,\n    tokenizer: Tokenizer | None\
      \ = None,\n    use_community_summary: bool = True,\n    column_delimiter: str\
      \ = \"|\",\n    shuffle_data: bool = True,\n    include_community_rank: bool\
      \ = False,\n    min_community_rank: int = 0,\n    community_rank_name: str =\
      \ \"rank\",\n    include_community_weight: bool = True,\n    community_weight_name:\
      \ str = \"occurrence weight\",\n    normalize_community_weight: bool = True,\n\
      \    max_context_tokens: int = 8000,\n    single_batch: bool = True,\n    context_name:\
      \ str = \"Reports\",\n    random_state: int = 86,\n) -> tuple[str | list[str],\
      \ dict[str, pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: len
      type: builtin
    - target: logger.debug
      type: unresolved
    - target: graphrag/query/context_builder/community_context.py::_compute_community_weights
      type: internal
    - target: graphrag/query/context_builder/community_context.py::_is_included
      type: internal
    - target: random::seed
      type: stdlib
    - target: random::shuffle
      type: stdlib
    - target: list
      type: builtin
    - target: community_reports[0].attributes.keys
      type: unresolved
    - target: graphrag/query/context_builder/community_context.py::_get_header
      type: internal
    - target: graphrag/query/context_builder/community_context.py::_init_batch
      type: internal
    - target: graphrag/query/context_builder/community_context.py::_report_context_text
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    - target: graphrag/query/context_builder/community_context.py::_cut_batch
      type: internal
    - target: batch_records.append
      type: unresolved
    - target: set
      type: builtin
    - target: record["id"].to_list
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: context_name.lower
      type: unresolved
    - target: pandas::concat
      type: external
    visibility: public
    node_id: graphrag/query/context_builder/community_context.py::build_community_context
    called_by:
    - source: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
      type: internal
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
      type: internal
  - name: _is_included
    start_line: 51
    end_line: 52
    code: "def _is_included(report: CommunityReport) -> bool:\n        return report.rank\
      \ is not None and report.rank >= min_community_rank"
    signature: 'def _is_included(report: CommunityReport) -> bool'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_is_included
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _get_header
    start_line: 54
    end_line: 63
    code: "def _get_header(attributes: list[str]) -> list[str]:\n        header =\
      \ [\"id\", \"title\"]\n        attributes = [col for col in attributes if col\
      \ not in header]\n        if not include_community_weight:\n            attributes\
      \ = [col for col in attributes if col != community_weight_name]\n        header.extend(attributes)\n\
      \        header.append(\"summary\" if use_community_summary else \"content\"\
      )\n        if include_community_rank:\n            header.append(community_rank_name)\n\
      \        return header"
    signature: 'def _get_header(attributes: list[str]) -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: header.extend
      type: unresolved
    - target: header.append
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_get_header
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _report_context_text
    start_line: 65
    end_line: 80
    code: "def _report_context_text(\n        report: CommunityReport, attributes:\
      \ list[str]\n    ) -> tuple[str, list[str]]:\n        context: list[str] = [\n\
      \            report.short_id if report.short_id else \"\",\n            report.title,\n\
      \            *[\n                str(report.attributes.get(field, \"\")) if\
      \ report.attributes else \"\"\n                for field in attributes\n   \
      \         ],\n        ]\n        context.append(report.summary if use_community_summary\
      \ else report.full_content)\n        if include_community_rank:\n          \
      \  context.append(str(report.rank))\n        result = column_delimiter.join(context)\
      \ + \"\\n\"\n        return result, context"
    signature: "def _report_context_text(\n        report: CommunityReport, attributes:\
      \ list[str]\n    ) -> tuple[str, list[str]]"
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: report.attributes.get
      type: unresolved
    - target: context.append
      type: unresolved
    - target: column_delimiter.join
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_report_context_text
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _init_batch
    start_line: 124
    end_line: 130
    code: "def _init_batch() -> None:\n        nonlocal batch_text, batch_tokens,\
      \ batch_records\n        batch_text = (\n            f\"-----{context_name}-----\"\
      \ + \"\\n\" + column_delimiter.join(header) + \"\\n\"\n        )\n        batch_tokens\
      \ = tokenizer.num_tokens(batch_text)\n        batch_records = []"
    signature: def _init_batch() -> None
    decorators: []
    raises: []
    calls:
    - target: column_delimiter.join
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_init_batch
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _cut_batch
    start_line: 132
    end_line: 149
    code: "def _cut_batch() -> None:\n        # convert the current context records\
      \ to pandas dataframe and sort by weight and rank if exist\n        record_df\
      \ = _convert_report_context_to_df(\n            context_records=batch_records,\n\
      \            header=header,\n            weight_column=(\n                community_weight_name\
      \ if entities and include_community_weight else None\n            ),\n     \
      \       rank_column=community_rank_name if include_community_rank else None,\n\
      \        )\n        if len(record_df) == 0:\n            return\n        current_context_text\
      \ = record_df.to_csv(index=False, sep=column_delimiter)\n        if not all_context_text\
      \ and single_batch:\n            current_context_text = f\"-----{context_name}-----\\\
      n{current_context_text}\"\n\n        all_context_text.append(current_context_text)\n\
      \        all_context_records.append(record_df)"
    signature: def _cut_batch() -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
      type: internal
    - target: len
      type: builtin
    - target: record_df.to_csv
      type: unresolved
    - target: all_context_text.append
      type: unresolved
    - target: all_context_records.append
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_cut_batch
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _compute_community_weights
    start_line: 189
    end_line: 225
    code: "def _compute_community_weights(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None,\n    weight_attribute: str = \"occurrence\"\
      ,\n    normalize: bool = True,\n) -> list[CommunityReport]:\n    \"\"\"Calculate\
      \ a community's weight as count of text units associated with entities within\
      \ the community.\"\"\"\n    if not entities:\n        return community_reports\n\
      \n    community_text_units = {}\n    for entity in entities:\n        if entity.community_ids:\n\
      \            for community_id in entity.community_ids:\n                if community_id\
      \ not in community_text_units:\n                    community_text_units[community_id]\
      \ = []\n                community_text_units[community_id].extend(entity.text_unit_ids)\n\
      \    for report in community_reports:\n        if not report.attributes:\n \
      \           report.attributes = {}\n        report.attributes[weight_attribute]\
      \ = len(\n            set(community_text_units.get(report.community_id, []))\n\
      \        )\n    if normalize:\n        # normalize by max weight\n        all_weights\
      \ = [\n            report.attributes[weight_attribute]\n            for report\
      \ in community_reports\n            if report.attributes\n        ]\n      \
      \  max_weight = max(all_weights)\n        for report in community_reports:\n\
      \            if report.attributes:\n                report.attributes[weight_attribute]\
      \ = (\n                    report.attributes[weight_attribute] / max_weight\n\
      \                )\n    return community_reports"
    signature: "def _compute_community_weights(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None,\n    weight_attribute: str = \"occurrence\"\
      ,\n    normalize: bool = True,\n) -> list[CommunityReport]"
    decorators: []
    raises: []
    calls:
    - target: community_text_units[community_id].extend
      type: unresolved
    - target: len
      type: builtin
    - target: set
      type: builtin
    - target: community_text_units.get
      type: unresolved
    - target: max
      type: builtin
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_compute_community_weights
    called_by:
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
  - name: _rank_report_context
    start_line: 228
    end_line: 243
    code: "def _rank_report_context(\n    report_df: pd.DataFrame,\n    weight_column:\
      \ str | None = \"occurrence weight\",\n    rank_column: str | None = \"rank\"\
      ,\n) -> pd.DataFrame:\n    \"\"\"Sort report context by community weight and\
      \ rank if exist.\"\"\"\n    rank_attributes: list[str] = []\n    if weight_column:\n\
      \        rank_attributes.append(weight_column)\n        report_df[weight_column]\
      \ = report_df[weight_column].astype(float)\n    if rank_column:\n        rank_attributes.append(rank_column)\n\
      \        report_df[rank_column] = report_df[rank_column].astype(float)\n   \
      \ if len(rank_attributes) > 0:\n        report_df.sort_values(by=rank_attributes,\
      \ ascending=False, inplace=True)\n    return report_df"
    signature: "def _rank_report_context(\n    report_df: pd.DataFrame,\n    weight_column:\
      \ str | None = \"occurrence weight\",\n    rank_column: str | None = \"rank\"\
      ,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: rank_attributes.append
      type: unresolved
    - target: report_df[weight_column].astype
      type: unresolved
    - target: report_df[rank_column].astype
      type: unresolved
    - target: len
      type: builtin
    - target: report_df.sort_values
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_rank_report_context
    called_by:
    - source: graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
      type: internal
  - name: _convert_report_context_to_df
    start_line: 246
    end_line: 264
    code: "def _convert_report_context_to_df(\n    context_records: list[list[str]],\n\
      \    header: list[str],\n    weight_column: str | None = None,\n    rank_column:\
      \ str | None = None,\n) -> pd.DataFrame:\n    \"\"\"Convert report context records\
      \ to pandas dataframe and sort by weight and rank if exist.\"\"\"\n    if len(context_records)\
      \ == 0:\n        return pd.DataFrame()\n\n    record_df = pd.DataFrame(\n  \
      \      context_records,\n        columns=cast(\"Any\", header),\n    )\n   \
      \ return _rank_report_context(\n        report_df=record_df,\n        weight_column=weight_column,\n\
      \        rank_column=rank_column,\n    )"
    signature: "def _convert_report_context_to_df(\n    context_records: list[list[str]],\n\
      \    header: list[str],\n    weight_column: str | None = None,\n    rank_column:\
      \ str | None = None,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: typing::cast
      type: stdlib
    - target: graphrag/query/context_builder/community_context.py::_rank_report_context
      type: internal
    visibility: protected
    node_id: graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
    called_by:
    - source: graphrag/query/context_builder/community_context.py::_cut_batch
      type: internal
- file_name: graphrag/query/context_builder/conversation_history.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: from_string
    start_line: 27
    end_line: 37
    code: "def from_string(value: str) -> \"ConversationRole\":\n        \"\"\"Convert\
      \ string to ConversationRole.\"\"\"\n        if value == \"system\":\n     \
      \       return ConversationRole.SYSTEM\n        if value == \"user\":\n    \
      \        return ConversationRole.USER\n        if value == \"assistant\":\n\
      \            return ConversationRole.ASSISTANT\n\n        msg = f\"Invalid Role:\
      \ {value}\"\n        raise ValueError(msg)"
    signature: 'def from_string(value: str) -> "ConversationRole"'
    decorators:
    - '@staticmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.from_string
    called_by: []
  - name: __str__
    start_line: 39
    end_line: 41
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the enum value.\"\"\"\n        return self.value"
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.__str__
    called_by: []
  - name: __str__
    start_line: 56
    end_line: 58
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the conversation turn.\"\"\"\n        return f\"{self.role}: {self.content}\""
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn.__str__
    called_by: []
  - name: get_answer_text
    start_line: 72
    end_line: 78
    code: "def get_answer_text(self) -> str | None:\n        \"\"\"Get the text of\
      \ the assistant answers.\"\"\"\n        return (\n            \"\\n\".join([answer.content\
      \ for answer in self.assistant_answers])\n            if self.assistant_answers\n\
      \            else None\n        )"
    signature: def get_answer_text(self) -> str | None
    decorators: []
    raises: []
    calls:
    - target: '"\n".join'
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::QATurn.get_answer_text
    called_by: []
  - name: __str__
    start_line: 80
    end_line: 87
    code: "def __str__(self) -> str:\n        \"\"\"Return string representation of\
      \ the QA turn.\"\"\"\n        answers = self.get_answer_text()\n        return\
      \ (\n            f\"Question: {self.user_query.content}\\nAnswer: {answers}\"\
      \n            if answers\n            else f\"Question: {self.user_query.content}\"\
      \n        )"
    signature: def __str__(self) -> str
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/context_builder/conversation_history.py::get_answer_text
      type: internal
    visibility: protected
    node_id: graphrag/query/context_builder/conversation_history.py::QATurn.__str__
    called_by: []
  - name: __init__
    start_line: 95
    end_line: 96
    code: "def __init__(self):\n        self.turns = []"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.__init__
    called_by: []
  - name: from_list
    start_line: 99
    end_line: 117
    code: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
      \    ) -> \"ConversationHistory\":\n        \"\"\"\n        Create a conversation\
      \ history from a list of conversation turns.\n\n        Each turn is a dictionary\
      \ in the form of {\"role\": \"<conversation_role>\", \"content\": \"<turn content>\"\
      }\n        \"\"\"\n        history = cls()\n        for turn in conversation_turns:\n\
      \            history.turns.append(\n                ConversationTurn(\n    \
      \                role=ConversationRole.from_string(\n                      \
      \  turn.get(\"role\", ConversationRole.USER)\n                    ),\n     \
      \               content=turn.get(\"content\", \"\"),\n                )\n  \
      \          )\n        return history"
    signature: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
      \    ) -> \"ConversationHistory\""
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: cls
      type: unresolved
    - target: history.turns.append
      type: unresolved
    - target: ConversationTurn
      type: unresolved
    - target: ConversationRole.from_string
      type: unresolved
    - target: turn.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.from_list
    called_by: []
  - name: add_turn
    start_line: 119
    end_line: 121
    code: "def add_turn(self, role: ConversationRole, content: str):\n        \"\"\
      \"Add a new turn to the conversation history.\"\"\"\n        self.turns.append(ConversationTurn(role=role,\
      \ content=content))"
    signature: 'def add_turn(self, role: ConversationRole, content: str)'
    decorators: []
    raises: []
    calls:
    - target: self.turns.append
      type: instance
    - target: ConversationTurn
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.add_turn
    called_by: []
  - name: to_qa_turns
    start_line: 123
    end_line: 137
    code: "def to_qa_turns(self) -> list[QATurn]:\n        \"\"\"Convert conversation\
      \ history to a list of QA turns.\"\"\"\n        qa_turns = list[QATurn]()\n\
      \        current_qa_turn = None\n        for turn in self.turns:\n         \
      \   if turn.role == ConversationRole.USER:\n                if current_qa_turn:\n\
      \                    qa_turns.append(current_qa_turn)\n                current_qa_turn\
      \ = QATurn(user_query=turn, assistant_answers=[])\n            else:\n     \
      \           if current_qa_turn:\n                    current_qa_turn.assistant_answers.append(turn)\
      \  # type: ignore\n        if current_qa_turn:\n            qa_turns.append(current_qa_turn)\n\
      \        return qa_turns"
    signature: def to_qa_turns(self) -> list[QATurn]
    decorators: []
    raises: []
    calls:
    - target: qa_turns.append
      type: unresolved
    - target: QATurn
      type: unresolved
    - target: current_qa_turn.assistant_answers.append
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.to_qa_turns
    called_by: []
  - name: get_user_turns
    start_line: 139
    end_line: 147
    code: "def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]:\n\
      \        \"\"\"Get the last user turns in the conversation history.\"\"\"\n\
      \        user_turns = []\n        for turn in self.turns[::-1]:\n          \
      \  if turn.role == ConversationRole.USER:\n                user_turns.append(turn.content)\n\
      \                if max_user_turns and len(user_turns) >= max_user_turns:\n\
      \                    break\n        return user_turns"
    signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
    decorators: []
    raises: []
    calls:
    - target: user_turns.append
      type: unresolved
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.get_user_turns
    called_by: []
  - name: build_context
    start_line: 149
    end_line: 213
    code: "def build_context(\n        self,\n        tokenizer: Tokenizer | None\
      \ = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
      \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
      \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]:\n\
      \        \"\"\"\n        Prepare conversation history as context data for system\
      \ prompt.\n\n        Parameters\n        ----------\n            user_queries_only:\
      \ If True, only user queries (not assistant responses) will be included in the\
      \ context, default is True.\n            max_qa_turns: Maximum number of QA\
      \ turns to include in the context, default is 1.\n            recency_bias:\
      \ If True, reverse the order of the conversation history to ensure last QA got\
      \ prioritized.\n            column_delimiter: Delimiter to use for separating\
      \ columns in the context data, default is \"|\".\n            context_name:\
      \ Name of the context, default is \"Conversation History\".\n\n        \"\"\"\
      \n        tokenizer = tokenizer or get_tokenizer()\n        qa_turns = self.to_qa_turns()\n\
      \        if include_user_turns_only:\n            qa_turns = [\n           \
      \     QATurn(user_query=qa_turn.user_query, assistant_answers=None)\n      \
      \          for qa_turn in qa_turns\n            ]\n        if recency_bias:\n\
      \            qa_turns = qa_turns[::-1]\n        if max_qa_turns and len(qa_turns)\
      \ > max_qa_turns:\n            qa_turns = qa_turns[:max_qa_turns]\n\n      \
      \  # build context for qa turns\n        # add context header\n        if len(qa_turns)\
      \ == 0 or not qa_turns:\n            return (\"\", {context_name: pd.DataFrame()})\n\
      \n        # add table header\n        header = f\"-----{context_name}-----\"\
      \ + \"\\n\"\n\n        turn_list = []\n        current_context_df = pd.DataFrame()\n\
      \        for turn in qa_turns:\n            turn_list.append({\n           \
      \     \"turn\": ConversationRole.USER.__str__(),\n                \"content\"\
      : turn.user_query.content,\n            })\n            if turn.assistant_answers:\n\
      \                turn_list.append({\n                    \"turn\": ConversationRole.ASSISTANT.__str__(),\n\
      \                    \"content\": turn.get_answer_text(),\n                })\n\
      \n            context_df = pd.DataFrame(turn_list)\n            context_text\
      \ = header + context_df.to_csv(sep=column_delimiter, index=False)\n        \
      \    if tokenizer.num_tokens(context_text) > max_context_tokens:\n         \
      \       break\n\n            current_context_df = context_df\n        context_text\
      \ = header + current_context_df.to_csv(\n            sep=column_delimiter, index=False\n\
      \        )\n        return (context_text, {context_name.lower(): current_context_df})"
    signature: "def build_context(\n        self,\n        tokenizer: Tokenizer |\
      \ None = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
      \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
      \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/context_builder/conversation_history.py::to_qa_turns
      type: internal
    - target: QATurn
      type: unresolved
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: turn_list.append
      type: unresolved
    - target: ConversationRole.USER.__str__
      type: unresolved
    - target: ConversationRole.ASSISTANT.__str__
      type: unresolved
    - target: turn.get_answer_text
      type: unresolved
    - target: context_df.to_csv
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: current_context_df.to_csv
      type: unresolved
    - target: context_name.lower
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
    called_by: []
- file_name: graphrag/query/context_builder/dynamic_community_selection.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: collections
    name: Counter
    alias: null
  - module: copy
    name: deepcopy
    alias: null
  - module: time
    name: time
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.community
    name: Community
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.query.context_builder.rate_prompt
    name: RATE_QUERY
    alias: null
  - module: graphrag.query.context_builder.rate_relevancy
    name: rate_relevancy
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 29
    end_line: 68
    code: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
      \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
      \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
      \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
      \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    ):\n  \
      \      self.model = model\n        self.tokenizer = tokenizer\n        self.rate_query\
      \ = rate_query\n        self.num_repeats = num_repeats\n        self.use_summary\
      \ = use_summary\n        self.threshold = threshold\n        self.keep_parent\
      \ = keep_parent\n        self.max_level = max_level\n        self.semaphore\
      \ = asyncio.Semaphore(concurrent_coroutines)\n        self.model_params = model_params\
      \ if model_params else {}\n\n        self.reports = {report.community_id: report\
      \ for report in community_reports}\n        self.communities = {community.short_id:\
      \ community for community in communities}\n\n        # mapping from level to\
      \ communities\n        self.levels: dict[str, list[str]] = {}\n\n        for\
      \ community in communities:\n            if community.level not in self.levels:\n\
      \                self.levels[community.level] = []\n            if community.short_id\
      \ in self.reports:\n                self.levels[community.level].append(community.short_id)\n\
      \n        # start from root communities (level 0)\n        self.starting_communities\
      \ = self.levels[\"0\"]"
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
      \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
      \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
      \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
      \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: asyncio::Semaphore
      type: stdlib
    - target: self.levels[community.level].append
      type: instance
    visibility: protected
    node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.__init__
    called_by: []
  - name: select
    start_line: 70
    end_line: 171
    code: "async def select(self, query: str) -> tuple[list[CommunityReport], dict[str,\
      \ Any]]:\n        \"\"\"\n        Select relevant communities with respect to\
      \ the query.\n\n        Args:\n            query: the query to rate against\n\
      \        \"\"\"\n        start = time()\n        queue = deepcopy(self.starting_communities)\n\
      \        level = 0\n\n        ratings = {}  # store the ratings for each community\n\
      \        llm_info: dict[str, Any] = {\n            \"llm_calls\": 0,\n     \
      \       \"prompt_tokens\": 0,\n            \"output_tokens\": 0,\n        }\n\
      \        relevant_communities = set()\n\n        while queue:\n            gather_results\
      \ = await asyncio.gather(*[\n                rate_relevancy(\n             \
      \       query=query,\n                    description=(\n                  \
      \      self.reports[community].summary\n                        if self.use_summary\n\
      \                        else self.reports[community].full_content\n       \
      \             ),\n                    model=self.model,\n                  \
      \  tokenizer=self.tokenizer,\n                    rate_query=self.rate_query,\n\
      \                    num_repeats=self.num_repeats,\n                    semaphore=self.semaphore,\n\
      \                    **self.model_params,\n                )\n             \
      \   for community in queue\n            ])\n\n            communities_to_rate\
      \ = []\n            for community, result in zip(queue, gather_results, strict=True):\n\
      \                rating = result[\"rating\"]\n                logger.debug(\n\
      \                    \"dynamic community selection: community %s rating %s\"\
      ,\n                    community,\n                    rating,\n           \
      \     )\n                ratings[community] = rating\n                llm_info[\"\
      llm_calls\"] += result[\"llm_calls\"]\n                llm_info[\"prompt_tokens\"\
      ] += result[\"prompt_tokens\"]\n                llm_info[\"output_tokens\"]\
      \ += result[\"output_tokens\"]\n                if rating >= self.threshold:\n\
      \                    relevant_communities.add(community)\n                 \
      \   # find children nodes of the current node and append them to the queue\n\
      \                    # TODO check why some sub_communities are NOT in report_df\n\
      \                    if community in self.communities:\n                   \
      \     for child in self.communities[community].children:\n                 \
      \           if child in self.reports:\n                                communities_to_rate.append(child)\n\
      \                            else:\n                                logger.debug(\n\
      \                                    \"dynamic community selection: cannot find\
      \ community %s in reports\",\n                                    child,\n \
      \                               )\n                    # remove parent node\
      \ if the current node is deemed relevant\n                    if not self.keep_parent\
      \ and community in self.communities:\n                        relevant_communities.discard(self.communities[community].parent)\n\
      \            queue = communities_to_rate\n            level += 1\n         \
      \   if (\n                (len(queue) == 0)\n                and (len(relevant_communities)\
      \ == 0)\n                and (str(level) in self.levels)\n                and\
      \ (level <= self.max_level)\n            ):\n                logger.debug(\n\
      \                    \"dynamic community selection: no relevant community \"\
      \n                    \"reports, adding all reports at level %s to rate.\",\n\
      \                    level,\n                )\n                # append all\
      \ communities at the next level to queue\n                queue = self.levels[str(level)]\n\
      \n        community_reports = [\n            self.reports[community] for community\
      \ in relevant_communities\n        ]\n        end = time()\n\n        logger.debug(\n\
      \            \"dynamic community selection (took: %ss)\\n\"\n            \"\\\
      trating distribution %s\\n\"\n            \"\\t%s out of %s community reports\
      \ are relevant\\n\"\n            \"\\tprompt tokens: %s, output tokens: %s\"\
      ,\n            int(end - start),\n            dict(sorted(Counter(ratings.values()).items())),\n\
      \            len(relevant_communities),\n            len(self.reports),\n  \
      \          llm_info[\"prompt_tokens\"],\n            llm_info[\"output_tokens\"\
      ],\n        )\n\n        llm_info[\"ratings\"] = ratings\n        return community_reports,\
      \ llm_info"
    signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
      Any]]'
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: copy::deepcopy
      type: stdlib
    - target: set
      type: builtin
    - target: asyncio::gather
      type: stdlib
    - target: graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
      type: internal
    - target: zip
      type: builtin
    - target: logger.debug
      type: unresolved
    - target: relevant_communities.add
      type: unresolved
    - target: communities_to_rate.append
      type: unresolved
    - target: relevant_communities.discard
      type: unresolved
    - target: len
      type: builtin
    - target: str
      type: builtin
    - target: int
      type: builtin
    - target: dict
      type: builtin
    - target: sorted
      type: builtin
    - target: Counter(ratings.values()).items
      type: unresolved
    - target: collections::Counter
      type: stdlib
    - target: ratings.values
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
    called_by: []
- file_name: graphrag/query/context_builder/entity_extraction.py
  imports:
  - module: enum
    name: Enum
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: get_entity_by_id
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: get_entity_by_key
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: get_entity_by_name
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: from_string
    start_line: 26
    end_line: 34
    code: "def from_string(value: str) -> \"EntityVectorStoreKey\":\n        \"\"\"\
      Convert string to EntityVectorStoreKey.\"\"\"\n        if value == \"id\":\n\
      \            return EntityVectorStoreKey.ID\n        if value == \"title\":\n\
      \            return EntityVectorStoreKey.TITLE\n\n        msg = f\"Invalid EntityVectorStoreKey:\
      \ {value}\"\n        raise ValueError(msg)"
    signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
    decorators:
    - '@staticmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey.from_string
    called_by: []
  - name: map_query_to_entities
    start_line: 37
    end_line: 92
    code: "def map_query_to_entities(\n    query: str,\n    text_embedding_vectorstore:\
      \ BaseVectorStore,\n    text_embedder: EmbeddingModel,\n    all_entities_dict:\
      \ dict[str, Entity],\n    embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n\
      \    include_entity_names: list[str] | None = None,\n    exclude_entity_names:\
      \ list[str] | None = None,\n    k: int = 10,\n    oversample_scaler: int = 2,\n\
      ) -> list[Entity]:\n    \"\"\"Extract entities that match a given query using\
      \ semantic similarity of text embeddings of query and entity descriptions.\"\
      \"\"\n    if include_entity_names is None:\n        include_entity_names = []\n\
      \    if exclude_entity_names is None:\n        exclude_entity_names = []\n \
      \   all_entities = list(all_entities_dict.values())\n    matched_entities =\
      \ []\n    if query != \"\":\n        # get entities with highest semantic similarity\
      \ to query\n        # oversample to account for excluded entities\n        search_results\
      \ = text_embedding_vectorstore.similarity_search_by_text(\n            text=query,\n\
      \            text_embedder=lambda t: text_embedder.embed(t),\n            k=k\
      \ * oversample_scaler,\n        )\n        for result in search_results:\n \
      \           if embedding_vectorstore_key == EntityVectorStoreKey.ID and isinstance(\n\
      \                result.document.id, str\n            ):\n                matched\
      \ = get_entity_by_id(all_entities_dict, result.document.id)\n            else:\n\
      \                matched = get_entity_by_key(\n                    entities=all_entities,\n\
      \                    key=embedding_vectorstore_key,\n                    value=result.document.id,\n\
      \                )\n            if matched:\n                matched_entities.append(matched)\n\
      \    else:\n        all_entities.sort(key=lambda x: x.rank if x.rank else 0,\
      \ reverse=True)\n        matched_entities = all_entities[:k]\n\n    # filter\
      \ out excluded entities\n    if exclude_entity_names:\n        matched_entities\
      \ = [\n            entity\n            for entity in matched_entities\n    \
      \        if entity.title not in exclude_entity_names\n        ]\n\n    # add\
      \ entities in the include_entity list\n    included_entities = []\n    for entity_name\
      \ in include_entity_names:\n        included_entities.extend(get_entity_by_name(all_entities,\
      \ entity_name))\n    return included_entities + matched_entities"
    signature: "def map_query_to_entities(\n    query: str,\n    text_embedding_vectorstore:\
      \ BaseVectorStore,\n    text_embedder: EmbeddingModel,\n    all_entities_dict:\
      \ dict[str, Entity],\n    embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n\
      \    include_entity_names: list[str] | None = None,\n    exclude_entity_names:\
      \ list[str] | None = None,\n    k: int = 10,\n    oversample_scaler: int = 2,\n\
      ) -> list[Entity]"
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: all_entities_dict.values
      type: unresolved
    - target: text_embedding_vectorstore.similarity_search_by_text
      type: unresolved
    - target: text_embedder.embed
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/query/input/retrieval/entities.py::get_entity_by_id
      type: internal
    - target: graphrag/query/input/retrieval/entities.py::get_entity_by_key
      type: internal
    - target: matched_entities.append
      type: unresolved
    - target: all_entities.sort
      type: unresolved
    - target: included_entities.extend
      type: unresolved
    - target: graphrag/query/input/retrieval/entities.py::get_entity_by_name
      type: internal
    visibility: public
    node_id: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
      type: internal
    - source: tests/unit/query/context_builder/test_entity_extraction.py::test_map_query_to_entities
      type: internal
  - name: find_nearest_neighbors_by_entity_rank
    start_line: 95
    end_line: 121
    code: "def find_nearest_neighbors_by_entity_rank(\n    entity_name: str,\n   \
      \ all_entities: list[Entity],\n    all_relationships: list[Relationship],\n\
      \    exclude_entity_names: list[str] | None = None,\n    k: int | None = 10,\n\
      ) -> list[Entity]:\n    \"\"\"Retrieve entities that have direct connections\
      \ with the target entity, sorted by entity rank.\"\"\"\n    if exclude_entity_names\
      \ is None:\n        exclude_entity_names = []\n    entity_relationships = [\n\
      \        rel\n        for rel in all_relationships\n        if rel.source ==\
      \ entity_name or rel.target == entity_name\n    ]\n    source_entity_names =\
      \ {rel.source for rel in entity_relationships}\n    target_entity_names = {rel.target\
      \ for rel in entity_relationships}\n    related_entity_names = (source_entity_names.union(target_entity_names)).difference(\n\
      \        set(exclude_entity_names)\n    )\n    top_relations = [\n        entity\
      \ for entity in all_entities if entity.title in related_entity_names\n    ]\n\
      \    top_relations.sort(key=lambda x: x.rank if x.rank else 0, reverse=True)\n\
      \    if k:\n        return top_relations[:k]\n    return top_relations"
    signature: "def find_nearest_neighbors_by_entity_rank(\n    entity_name: str,\n\
      \    all_entities: list[Entity],\n    all_relationships: list[Relationship],\n\
      \    exclude_entity_names: list[str] | None = None,\n    k: int | None = 10,\n\
      ) -> list[Entity]"
    decorators: []
    raises: []
    calls:
    - target: (source_entity_names.union(target_entity_names)).difference
      type: unresolved
    - target: source_entity_names.union
      type: unresolved
    - target: set
      type: builtin
    - target: top_relations.sort
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/entity_extraction.py::find_nearest_neighbors_by_entity_rank
    called_by: []
- file_name: graphrag/query/context_builder/local_context.py
  imports:
  - module: collections
    name: defaultdict
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.query.input.retrieval.covariates
    name: get_candidate_covariates
    alias: null
  - module: graphrag.query.input.retrieval.covariates
    name: to_covariate_dataframe
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: to_entity_dataframe
    alias: null
  - module: graphrag.query.input.retrieval.relationships
    name: get_candidate_relationships
    alias: null
  - module: graphrag.query.input.retrieval.relationships
    name: get_entities_from_relationships
    alias: null
  - module: graphrag.query.input.retrieval.relationships
    name: get_in_network_relationships
    alias: null
  - module: graphrag.query.input.retrieval.relationships
    name: get_out_network_relationships
    alias: null
  - module: graphrag.query.input.retrieval.relationships
    name: to_relationship_dataframe
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_entity_context
    start_line: 30
    end_line: 90
    code: "def build_entity_context(\n    selected_entities: list[Entity],\n    tokenizer:\
      \ Tokenizer | None = None,\n    max_context_tokens: int = 8000,\n    include_entity_rank:\
      \ bool = True,\n    rank_description: str = \"number of relationships\",\n \
      \   column_delimiter: str = \"|\",\n    context_name=\"Entities\",\n) -> tuple[str,\
      \ pd.DataFrame]:\n    \"\"\"Prepare entity data table as context data for system\
      \ prompt.\"\"\"\n    tokenizer = tokenizer or get_tokenizer()\n\n    if len(selected_entities)\
      \ == 0:\n        return \"\", pd.DataFrame()\n\n    # add headers\n    current_context_text\
      \ = f\"-----{context_name}-----\" + \"\\n\"\n    header = [\"id\", \"entity\"\
      , \"description\"]\n    if include_entity_rank:\n        header.append(rank_description)\n\
      \    attribute_cols = (\n        list(selected_entities[0].attributes.keys())\n\
      \        if selected_entities[0].attributes\n        else []\n    )\n    header.extend(attribute_cols)\n\
      \    current_context_text += column_delimiter.join(header) + \"\\n\"\n    current_tokens\
      \ = tokenizer.num_tokens(current_context_text)\n\n    all_context_records =\
      \ [header]\n    for entity in selected_entities:\n        new_context = [\n\
      \            entity.short_id if entity.short_id else \"\",\n            entity.title,\n\
      \            entity.description if entity.description else \"\",\n        ]\n\
      \        if include_entity_rank:\n            new_context.append(str(entity.rank))\n\
      \        for field in attribute_cols:\n            field_value = (\n       \
      \         str(entity.attributes.get(field))\n                if entity.attributes\
      \ and entity.attributes.get(field)\n                else \"\"\n            )\n\
      \            new_context.append(field_value)\n        new_context_text = column_delimiter.join(new_context)\
      \ + \"\\n\"\n        new_tokens = tokenizer.num_tokens(new_context_text)\n \
      \       if current_tokens + new_tokens > max_context_tokens:\n            break\n\
      \        current_context_text += new_context_text\n        all_context_records.append(new_context)\n\
      \        current_tokens += new_tokens\n\n    if len(all_context_records) > 1:\n\
      \        record_df = pd.DataFrame(\n            all_context_records[1:], columns=cast(\"\
      Any\", all_context_records[0])\n        )\n    else:\n        record_df = pd.DataFrame()\n\
      \n    return current_context_text, record_df"
    signature: "def build_entity_context(\n    selected_entities: list[Entity],\n\
      \    tokenizer: Tokenizer | None = None,\n    max_context_tokens: int = 8000,\n\
      \    include_entity_rank: bool = True,\n    rank_description: str = \"number\
      \ of relationships\",\n    column_delimiter: str = \"|\",\n    context_name=\"\
      Entities\",\n) -> tuple[str, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: header.append
      type: unresolved
    - target: list
      type: builtin
    - target: selected_entities[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: column_delimiter.join
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: new_context.append
      type: unresolved
    - target: str
      type: builtin
    - target: entity.attributes.get
      type: unresolved
    - target: all_context_records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/context_builder/local_context.py::build_entity_context
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
      type: internal
  - name: build_covariates_context
    start_line: 93
    end_line: 155
    code: "def build_covariates_context(\n    selected_entities: list[Entity],\n \
      \   covariates: list[Covariate],\n    tokenizer: Tokenizer | None = None,\n\
      \    max_context_tokens: int = 8000,\n    column_delimiter: str = \"|\",\n \
      \   context_name: str = \"Covariates\",\n) -> tuple[str, pd.DataFrame]:\n  \
      \  \"\"\"Prepare covariate data tables as context data for system prompt.\"\"\
      \"\n    tokenizer = tokenizer or get_tokenizer()\n    # create an empty list\
      \ of covariates\n    if len(selected_entities) == 0 or len(covariates) == 0:\n\
      \        return \"\", pd.DataFrame()\n\n    selected_covariates = list[Covariate]()\n\
      \    record_df = pd.DataFrame()\n\n    # add context header\n    current_context_text\
      \ = f\"-----{context_name}-----\" + \"\\n\"\n\n    # add header\n    header\
      \ = [\"id\", \"entity\"]\n    attributes = covariates[0].attributes or {} if\
      \ len(covariates) > 0 else {}\n    attribute_cols = list(attributes.keys())\
      \ if len(covariates) > 0 else []\n    header.extend(attribute_cols)\n    current_context_text\
      \ += column_delimiter.join(header) + \"\\n\"\n    current_tokens = tokenizer.num_tokens(current_context_text)\n\
      \n    all_context_records = [header]\n    for entity in selected_entities:\n\
      \        selected_covariates.extend([\n            cov for cov in covariates\
      \ if cov.subject_id == entity.title\n        ])\n\n    for covariate in selected_covariates:\n\
      \        new_context = [\n            covariate.short_id if covariate.short_id\
      \ else \"\",\n            covariate.subject_id,\n        ]\n        for field\
      \ in attribute_cols:\n            field_value = (\n                str(covariate.attributes.get(field))\n\
      \                if covariate.attributes and covariate.attributes.get(field)\n\
      \                else \"\"\n            )\n            new_context.append(field_value)\n\
      \n        new_context_text = column_delimiter.join(new_context) + \"\\n\"\n\
      \        new_tokens = tokenizer.num_tokens(new_context_text)\n        if current_tokens\
      \ + new_tokens > max_context_tokens:\n            break\n        current_context_text\
      \ += new_context_text\n        all_context_records.append(new_context)\n   \
      \     current_tokens += new_tokens\n\n        if len(all_context_records) >\
      \ 1:\n            record_df = pd.DataFrame(\n                all_context_records[1:],\
      \ columns=cast(\"Any\", all_context_records[0])\n            )\n        else:\n\
      \            record_df = pd.DataFrame()\n\n    return current_context_text,\
      \ record_df"
    signature: "def build_covariates_context(\n    selected_entities: list[Entity],\n\
      \    covariates: list[Covariate],\n    tokenizer: Tokenizer | None = None,\n\
      \    max_context_tokens: int = 8000,\n    column_delimiter: str = \"|\",\n \
      \   context_name: str = \"Covariates\",\n) -> tuple[str, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: list
      type: builtin
    - target: attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: column_delimiter.join
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: selected_covariates.extend
      type: unresolved
    - target: str
      type: builtin
    - target: covariate.attributes.get
      type: unresolved
    - target: new_context.append
      type: unresolved
    - target: all_context_records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/context_builder/local_context.py::build_covariates_context
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
      type: internal
  - name: build_relationship_context
    start_line: 158
    end_line: 229
    code: "def build_relationship_context(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    tokenizer: Tokenizer | None = None,\n\
      \    include_relationship_weight: bool = False,\n    max_context_tokens: int\
      \ = 8000,\n    top_k_relationships: int = 10,\n    relationship_ranking_attribute:\
      \ str = \"rank\",\n    column_delimiter: str = \"|\",\n    context_name: str\
      \ = \"Relationships\",\n) -> tuple[str, pd.DataFrame]:\n    \"\"\"Prepare relationship\
      \ data tables as context data for system prompt.\"\"\"\n    tokenizer = tokenizer\
      \ or get_tokenizer()\n    selected_relationships = _filter_relationships(\n\
      \        selected_entities=selected_entities,\n        relationships=relationships,\n\
      \        top_k_relationships=top_k_relationships,\n        relationship_ranking_attribute=relationship_ranking_attribute,\n\
      \    )\n\n    if len(selected_entities) == 0 or len(selected_relationships)\
      \ == 0:\n        return \"\", pd.DataFrame()\n\n    # add headers\n    current_context_text\
      \ = f\"-----{context_name}-----\" + \"\\n\"\n    header = [\"id\", \"source\"\
      , \"target\", \"description\"]\n    if include_relationship_weight:\n      \
      \  header.append(\"weight\")\n    attribute_cols = (\n        list(selected_relationships[0].attributes.keys())\n\
      \        if selected_relationships[0].attributes\n        else []\n    )\n \
      \   attribute_cols = [col for col in attribute_cols if col not in header]\n\
      \    header.extend(attribute_cols)\n\n    current_context_text += column_delimiter.join(header)\
      \ + \"\\n\"\n    current_tokens = tokenizer.num_tokens(current_context_text)\n\
      \n    all_context_records = [header]\n    for rel in selected_relationships:\n\
      \        new_context = [\n            rel.short_id if rel.short_id else \"\"\
      ,\n            rel.source,\n            rel.target,\n            rel.description\
      \ if rel.description else \"\",\n        ]\n        if include_relationship_weight:\n\
      \            new_context.append(str(rel.weight if rel.weight else \"\"))\n \
      \       for field in attribute_cols:\n            field_value = (\n        \
      \        str(rel.attributes.get(field))\n                if rel.attributes and\
      \ rel.attributes.get(field)\n                else \"\"\n            )\n    \
      \        new_context.append(field_value)\n        new_context_text = column_delimiter.join(new_context)\
      \ + \"\\n\"\n        new_tokens = tokenizer.num_tokens(new_context_text)\n \
      \       if current_tokens + new_tokens > max_context_tokens:\n            break\n\
      \        current_context_text += new_context_text\n        all_context_records.append(new_context)\n\
      \        current_tokens += new_tokens\n\n    if len(all_context_records) > 1:\n\
      \        record_df = pd.DataFrame(\n            all_context_records[1:], columns=cast(\"\
      Any\", all_context_records[0])\n        )\n    else:\n        record_df = pd.DataFrame()\n\
      \n    return current_context_text, record_df"
    signature: "def build_relationship_context(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    tokenizer: Tokenizer | None = None,\n\
      \    include_relationship_weight: bool = False,\n    max_context_tokens: int\
      \ = 8000,\n    top_k_relationships: int = 10,\n    relationship_ranking_attribute:\
      \ str = \"rank\",\n    column_delimiter: str = \"|\",\n    context_name: str\
      \ = \"Relationships\",\n) -> tuple[str, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/context_builder/local_context.py::_filter_relationships
      type: internal
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: header.append
      type: unresolved
    - target: list
      type: builtin
    - target: selected_relationships[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: column_delimiter.join
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: new_context.append
      type: unresolved
    - target: str
      type: builtin
    - target: rel.attributes.get
      type: unresolved
    - target: all_context_records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/context_builder/local_context.py::build_relationship_context
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
      type: internal
  - name: _filter_relationships
    start_line: 232
    end_line: 317
    code: "def _filter_relationships(\n    selected_entities: list[Entity],\n    relationships:\
      \ list[Relationship],\n    top_k_relationships: int = 10,\n    relationship_ranking_attribute:\
      \ str = \"rank\",\n) -> list[Relationship]:\n    \"\"\"Filter and sort relationships\
      \ based on a set of selected entities and a ranking attribute.\"\"\"\n    #\
      \ First priority: in-network relationships (i.e. relationships between selected\
      \ entities)\n    in_network_relationships = get_in_network_relationships(\n\
      \        selected_entities=selected_entities,\n        relationships=relationships,\n\
      \        ranking_attribute=relationship_ranking_attribute,\n    )\n\n    # Second\
      \ priority -  out-of-network relationships\n    # (i.e. relationships between\
      \ selected entities and other entities that are not within the selected entities)\n\
      \    out_network_relationships = get_out_network_relationships(\n        selected_entities=selected_entities,\n\
      \        relationships=relationships,\n        ranking_attribute=relationship_ranking_attribute,\n\
      \    )\n    if len(out_network_relationships) <= 1:\n        return in_network_relationships\
      \ + out_network_relationships\n\n    # within out-of-network relationships,\
      \ prioritize mutual relationships\n    # (i.e. relationships with out-network\
      \ entities that are shared with multiple selected entities)\n    selected_entity_names\
      \ = [entity.title for entity in selected_entities]\n    out_network_source_names\
      \ = [\n        relationship.source\n        for relationship in out_network_relationships\n\
      \        if relationship.source not in selected_entity_names\n    ]\n    out_network_target_names\
      \ = [\n        relationship.target\n        for relationship in out_network_relationships\n\
      \        if relationship.target not in selected_entity_names\n    ]\n    out_network_entity_names\
      \ = list(\n        set(out_network_source_names + out_network_target_names)\n\
      \    )\n    out_network_entity_links = defaultdict(int)\n    for entity_name\
      \ in out_network_entity_names:\n        targets = [\n            relationship.target\n\
      \            for relationship in out_network_relationships\n            if relationship.source\
      \ == entity_name\n        ]\n        sources = [\n            relationship.source\n\
      \            for relationship in out_network_relationships\n            if relationship.target\
      \ == entity_name\n        ]\n        out_network_entity_links[entity_name] =\
      \ len(set(targets + sources))\n\n    # sort out-network relationships by number\
      \ of links and rank_attributes\n    for rel in out_network_relationships:\n\
      \        if rel.attributes is None:\n            rel.attributes = {}\n     \
      \   rel.attributes[\"links\"] = (\n            out_network_entity_links[rel.source]\n\
      \            if rel.source in out_network_entity_links\n            else out_network_entity_links[rel.target]\n\
      \        )\n\n    # sort by attributes[links] first, then by ranking_attribute\n\
      \    if relationship_ranking_attribute == \"rank\":\n        out_network_relationships.sort(\n\
      \            key=lambda x: (x.attributes[\"links\"], x.rank),  # type: ignore\n\
      \            reverse=True,  # type: ignore\n        )\n    elif relationship_ranking_attribute\
      \ == \"weight\":\n        out_network_relationships.sort(\n            key=lambda\
      \ x: (x.attributes[\"links\"], x.weight),  # type: ignore\n            reverse=True,\
      \  # type: ignore\n        )\n    else:\n        out_network_relationships.sort(\n\
      \            key=lambda x: (\n                x.attributes[\"links\"],  # type:\
      \ ignore\n                x.attributes[relationship_ranking_attribute],  # type:\
      \ ignore\n            ),  # type: ignore\n            reverse=True,\n      \
      \  )\n\n    relationship_budget = top_k_relationships * len(selected_entities)\n\
      \    return in_network_relationships + out_network_relationships[:relationship_budget]"
    signature: "def _filter_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    top_k_relationships: int = 10,\n\
      \    relationship_ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
      type: internal
    - target: graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
      type: internal
    - target: len
      type: builtin
    - target: list
      type: builtin
    - target: set
      type: builtin
    - target: collections::defaultdict
      type: stdlib
    - target: out_network_relationships.sort
      type: unresolved
    visibility: protected
    node_id: graphrag/query/context_builder/local_context.py::_filter_relationships
    called_by:
    - source: graphrag/query/context_builder/local_context.py::build_relationship_context
      type: internal
  - name: get_candidate_context
    start_line: 320
    end_line: 357
    code: "def get_candidate_context(\n    selected_entities: list[Entity],\n    entities:\
      \ list[Entity],\n    relationships: list[Relationship],\n    covariates: dict[str,\
      \ list[Covariate]],\n    include_entity_rank: bool = True,\n    entity_rank_description:\
      \ str = \"number of relationships\",\n    include_relationship_weight: bool\
      \ = False,\n) -> dict[str, pd.DataFrame]:\n    \"\"\"Prepare entity, relationship,\
      \ and covariate data tables as context data for system prompt.\"\"\"\n    candidate_context\
      \ = {}\n    candidate_relationships = get_candidate_relationships(\n       \
      \ selected_entities=selected_entities,\n        relationships=relationships,\n\
      \    )\n    candidate_context[\"relationships\"] = to_relationship_dataframe(\n\
      \        relationships=candidate_relationships,\n        include_relationship_weight=include_relationship_weight,\n\
      \    )\n    candidate_entities = get_entities_from_relationships(\n        relationships=candidate_relationships,\
      \ entities=entities\n    )\n    candidate_context[\"entities\"] = to_entity_dataframe(\n\
      \        entities=candidate_entities,\n        include_entity_rank=include_entity_rank,\n\
      \        rank_description=entity_rank_description,\n    )\n\n    for covariate\
      \ in covariates:\n        candidate_covariates = get_candidate_covariates(\n\
      \            selected_entities=selected_entities,\n            covariates=covariates[covariate],\n\
      \        )\n        candidate_context[covariate.lower()] = to_covariate_dataframe(\n\
      \            candidate_covariates\n        )\n\n    return candidate_context"
    signature: "def get_candidate_context(\n    selected_entities: list[Entity],\n\
      \    entities: list[Entity],\n    relationships: list[Relationship],\n    covariates:\
      \ dict[str, list[Covariate]],\n    include_entity_rank: bool = True,\n    entity_rank_description:\
      \ str = \"number of relationships\",\n    include_relationship_weight: bool\
      \ = False,\n) -> dict[str, pd.DataFrame]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/relationships.py::get_candidate_relationships
      type: internal
    - target: graphrag/query/input/retrieval/relationships.py::to_relationship_dataframe
      type: internal
    - target: graphrag/query/input/retrieval/relationships.py::get_entities_from_relationships
      type: internal
    - target: graphrag/query/input/retrieval/entities.py::to_entity_dataframe
      type: internal
    - target: graphrag/query/input/retrieval/covariates.py::get_candidate_covariates
      type: internal
    - target: covariate.lower
      type: unresolved
    - target: graphrag/query/input/retrieval/covariates.py::to_covariate_dataframe
      type: internal
    visibility: public
    node_id: graphrag/query/context_builder/local_context.py::get_candidate_context
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
      type: internal
- file_name: graphrag/query/context_builder/rate_prompt.py
  imports: []
  functions: []
- file_name: graphrag/query/context_builder/rate_relevancy.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: contextlib
    name: nullcontext
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.query.context_builder.rate_prompt
    name: RATE_QUERY
    alias: null
  - module: graphrag.query.llm.text_utils
    name: try_parse_json_object
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: rate_relevancy
    start_line: 21
    end_line: 77
    code: "async def rate_relevancy(\n    query: str,\n    description: str,\n   \
      \ model: ChatModel,\n    tokenizer: Tokenizer,\n    rate_query: str = RATE_QUERY,\n\
      \    num_repeats: int = 1,\n    semaphore: asyncio.Semaphore | None = None,\n\
      \    **model_params: Any,\n) -> dict[str, Any]:\n    \"\"\"\n    Rate the relevancy\
      \ between the query and description on a scale of 0 to 10.\n\n    Args:\n  \
      \      query: the query (or question) to rate against\n        description:\
      \ the community description to rate, it can be the community\n            title,\
      \ summary, or the full content.\n        llm: LLM model to use for rating\n\
      \        tokenizer: tokenizer\n        num_repeats: number of times to repeat\
      \ the rating process for the same community (default: 1)\n        model_params:\
      \ additional arguments to pass to the LLM model\n        semaphore: asyncio.Semaphore\
      \ to limit the number of concurrent LLM calls (default: None)\n    \"\"\"\n\
      \    llm_calls, prompt_tokens, output_tokens, ratings = 0, 0, 0, []\n    messages\
      \ = [\n        {\n            \"role\": \"system\",\n            \"content\"\
      : rate_query.format(description=description, question=query),\n        },\n\
      \    ]\n    for _ in range(num_repeats):\n        async with semaphore if semaphore\
      \ is not None else nullcontext():\n            model_response = await model.achat(\n\
      \                prompt=query, history=messages, model_parameters=model_params,\
      \ json=True\n            )\n            response = model_response.output.content\n\
      \        try:\n            _, parsed_response = try_parse_json_object(response)\n\
      \            ratings.append(parsed_response[\"rating\"])\n        except KeyError:\n\
      \            # in case of json parsing error, default to rating 1 so the report\
      \ is kept.\n            # json parsing error should rarely happen.\n       \
      \     logger.warning(\"Error parsing json response, defaulting to rating 1\"\
      )\n            ratings.append(1)\n        llm_calls += 1\n        prompt_tokens\
      \ += tokenizer.num_tokens(messages[0][\"content\"])\n        output_tokens +=\
      \ tokenizer.num_tokens(response)\n    # select the decision with the most votes\n\
      \    options, counts = np.unique(ratings, return_counts=True)\n    rating =\
      \ int(options[np.argmax(counts)])\n    return {\n        \"rating\": rating,\n\
      \        \"ratings\": ratings,\n        \"llm_calls\": llm_calls,\n        \"\
      prompt_tokens\": prompt_tokens,\n        \"output_tokens\": output_tokens,\n\
      \    }"
    signature: "def rate_relevancy(\n    query: str,\n    description: str,\n    model:\
      \ ChatModel,\n    tokenizer: Tokenizer,\n    rate_query: str = RATE_QUERY,\n\
      \    num_repeats: int = 1,\n    semaphore: asyncio.Semaphore | None = None,\n\
      \    **model_params: Any,\n) -> dict[str, Any]"
    decorators: []
    raises: []
    calls:
    - target: rate_query.format
      type: unresolved
    - target: range
      type: builtin
    - target: contextlib::nullcontext
      type: stdlib
    - target: model.achat
      type: unresolved
    - target: graphrag/query/llm/text_utils.py::try_parse_json_object
      type: internal
    - target: ratings.append
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: numpy::unique
      type: external
    - target: int
      type: builtin
    - target: numpy::argmax
      type: external
    visibility: public
    node_id: graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
    called_by:
    - source: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
      type: internal
- file_name: graphrag/query/context_builder/source_context.py
  imports:
  - module: random
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: build_text_unit_context
    start_line: 21
    end_line: 79
    code: "def build_text_unit_context(\n    text_units: list[TextUnit],\n    tokenizer:\
      \ Tokenizer | None = None,\n    column_delimiter: str = \"|\",\n    shuffle_data:\
      \ bool = True,\n    max_context_tokens: int = 8000,\n    context_name: str =\
      \ \"Sources\",\n    random_state: int = 86,\n) -> tuple[str, dict[str, pd.DataFrame]]:\n\
      \    \"\"\"Prepare text-unit data table as context data for system prompt.\"\
      \"\"\n    tokenizer = tokenizer or get_tokenizer()\n    if text_units is None\
      \ or len(text_units) == 0:\n        return (\"\", {})\n\n    if shuffle_data:\n\
      \        random.seed(random_state)\n        random.shuffle(text_units)\n\n \
      \   # add context header\n    current_context_text = f\"-----{context_name}-----\"\
      \ + \"\\n\"\n\n    # add header\n    header = [\"id\", \"text\"]\n    attribute_cols\
      \ = (\n        list(text_units[0].attributes.keys()) if text_units[0].attributes\
      \ else []\n    )\n    attribute_cols = [col for col in attribute_cols if col\
      \ not in header]\n    header.extend(attribute_cols)\n\n    current_context_text\
      \ += column_delimiter.join(header) + \"\\n\"\n    current_tokens = tokenizer.num_tokens(current_context_text)\n\
      \    all_context_records = [header]\n\n    for unit in text_units:\n       \
      \ new_context = [\n            unit.short_id,\n            unit.text,\n    \
      \        *[\n                str(unit.attributes.get(field, \"\")) if unit.attributes\
      \ else \"\"\n                for field in attribute_cols\n            ],\n \
      \       ]\n        new_context_text = column_delimiter.join(new_context) + \"\
      \\n\"\n        new_tokens = tokenizer.num_tokens(new_context_text)\n\n     \
      \   if current_tokens + new_tokens > max_context_tokens:\n            break\n\
      \n        current_context_text += new_context_text\n        all_context_records.append(new_context)\n\
      \        current_tokens += new_tokens\n\n    if len(all_context_records) > 1:\n\
      \        record_df = pd.DataFrame(\n            all_context_records[1:], columns=cast(\"\
      Any\", all_context_records[0])\n        )\n    else:\n        record_df = pd.DataFrame()\n\
      \    return current_context_text, {context_name.lower(): record_df}"
    signature: "def build_text_unit_context(\n    text_units: list[TextUnit],\n  \
      \  tokenizer: Tokenizer | None = None,\n    column_delimiter: str = \"|\",\n\
      \    shuffle_data: bool = True,\n    max_context_tokens: int = 8000,\n    context_name:\
      \ str = \"Sources\",\n    random_state: int = 86,\n) -> tuple[str, dict[str,\
      \ pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: len
      type: builtin
    - target: random::seed
      type: stdlib
    - target: random::shuffle
      type: stdlib
    - target: list
      type: builtin
    - target: text_units[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: column_delimiter.join
      type: unresolved
    - target: tokenizer.num_tokens
      type: unresolved
    - target: str
      type: builtin
    - target: unit.attributes.get
      type: unresolved
    - target: all_context_records.append
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: typing::cast
      type: stdlib
    - target: context_name.lower
      type: unresolved
    visibility: public
    node_id: graphrag/query/context_builder/source_context.py::build_text_unit_context
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
      type: internal
  - name: count_relationships
    start_line: 82
    end_line: 100
    code: "def count_relationships(\n    entity_relationships: list[Relationship],\
      \ text_unit: TextUnit\n) -> int:\n    \"\"\"Count the number of relationships\
      \ of the selected entity that are associated with the text unit.\"\"\"\n   \
      \ if not text_unit.relationship_ids:\n        # Use list comprehension to count\
      \ relationships where the text_unit.id is in rel.text_unit_ids\n        return\
      \ sum(\n            1\n            for rel in entity_relationships\n       \
      \     if rel.text_unit_ids and text_unit.id in rel.text_unit_ids\n        )\n\
      \n    # Use a set for faster lookups if entity_relationships is large\n    entity_relationship_ids\
      \ = {rel.id for rel in entity_relationships}\n\n    # Count matching relationship\
      \ ids efficiently\n    return sum(\n        1 for rel_id in text_unit.relationship_ids\
      \ if rel_id in entity_relationship_ids\n    )"
    signature: "def count_relationships(\n    entity_relationships: list[Relationship],\
      \ text_unit: TextUnit\n) -> int"
    decorators: []
    raises: []
    calls:
    - target: sum
      type: builtin
    visibility: public
    node_id: graphrag/query/context_builder/source_context.py::count_relationships
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
      type: internal
- file_name: graphrag/query/factory.py
  imports:
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.community
    name: Community
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: get_openai_model_parameters_from_config
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: EntityVectorStoreKey
    alias: null
  - module: graphrag.query.structured_search.basic_search.basic_context
    name: BasicSearchContext
    alias: null
  - module: graphrag.query.structured_search.basic_search.search
    name: BasicSearch
    alias: null
  - module: graphrag.query.structured_search.drift_search.drift_context
    name: DRIFTSearchContextBuilder
    alias: null
  - module: graphrag.query.structured_search.drift_search.search
    name: DRIFTSearch
    alias: null
  - module: graphrag.query.structured_search.global_search.community_context
    name: GlobalCommunityContext
    alias: null
  - module: graphrag.query.structured_search.global_search.search
    name: GlobalSearch
    alias: null
  - module: graphrag.query.structured_search.local_search.mixed_context
    name: LocalSearchMixedContext
    alias: null
  - module: graphrag.query.structured_search.local_search.search
    name: LocalSearch
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: get_local_search_engine
    start_line: 39
    end_line: 108
    code: "def get_local_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    covariates: dict[str, list[Covariate]],\n\
      \    response_type: str,\n    description_embedding_store: BaseVectorStore,\n\
      \    system_prompt: str | None = None,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n) -> LocalSearch:\n    \"\"\"Create a local search engine\
      \ based on data + configuration.\"\"\"\n    model_settings = config.get_language_model_config(config.local_search.chat_model_id)\n\
      \n    chat_model = ModelManager().get_or_create_chat_model(\n        name=\"\
      local_search_chat\",\n        model_type=model_settings.type,\n        config=model_settings,\n\
      \    )\n\n    embedding_settings = config.get_language_model_config(\n     \
      \   config.local_search.embedding_model_id\n    )\n\n    embedding_model = ModelManager().get_or_create_embedding_model(\n\
      \        name=\"local_search_embedding\",\n        model_type=embedding_settings.type,\n\
      \        config=embedding_settings,\n    )\n\n    tokenizer = get_tokenizer(model_config=model_settings)\n\
      \n    ls_config = config.local_search\n\n    model_params = get_openai_model_parameters_from_config(model_settings)\n\
      \n    return LocalSearch(\n        model=chat_model,\n        system_prompt=system_prompt,\n\
      \        context_builder=LocalSearchMixedContext(\n            community_reports=reports,\n\
      \            text_units=text_units,\n            entities=entities,\n      \
      \      relationships=relationships,\n            covariates=covariates,\n  \
      \          entity_text_embeddings=description_embedding_store,\n           \
      \ embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses\
      \ entity title as ids, set this to EntityVectorStoreKey.TITLE\n            text_embedder=embedding_model,\n\
      \            tokenizer=tokenizer,\n        ),\n        tokenizer=tokenizer,\n\
      \        model_params=model_params,\n        context_builder_params={\n    \
      \        \"text_unit_prop\": ls_config.text_unit_prop,\n            \"community_prop\"\
      : ls_config.community_prop,\n            \"conversation_history_max_turns\"\
      : ls_config.conversation_history_max_turns,\n            \"conversation_history_user_turns_only\"\
      : True,\n            \"top_k_mapped_entities\": ls_config.top_k_entities,\n\
      \            \"top_k_relationships\": ls_config.top_k_relationships,\n     \
      \       \"include_entity_rank\": True,\n            \"include_relationship_weight\"\
      : True,\n            \"include_community_rank\": False,\n            \"return_candidate_context\"\
      : False,\n            \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\
      \  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title\
      \ as ids\n            \"max_context_tokens\": ls_config.max_context_tokens,\
      \  # change this based on the token limit you have on your model (if you are\
      \ using a model with 8k limit, a good setting could be 5000)\n        },\n \
      \       response_type=response_type,\n        callbacks=callbacks,\n    )"
    signature: "def get_local_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    covariates: dict[str, list[Covariate]],\n\
      \    response_type: str,\n    description_embedding_store: BaseVectorStore,\n\
      \    system_prompt: str | None = None,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n) -> LocalSearch"
    decorators: []
    raises: []
    calls:
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
      type: internal
    - target: graphrag/query/structured_search/local_search/search.py::LocalSearch
      type: internal
    - target: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
      type: internal
    visibility: public
    node_id: graphrag/query/factory.py::get_local_search_engine
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
  - name: get_global_search_engine
    start_line: 111
    end_line: 192
    code: "def get_global_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    entities: list[Entity],\n    communities: list[Community],\n\
      \    response_type: str,\n    dynamic_community_selection: bool = False,\n \
      \   map_system_prompt: str | None = None,\n    reduce_system_prompt: str | None\
      \ = None,\n    general_knowledge_inclusion_prompt: str | None = None,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n) -> GlobalSearch:\n    \"\"\"Create\
      \ a global search engine based on data + configuration.\"\"\"\n    model_settings\
      \ = config.get_language_model_config(\n        config.global_search.chat_model_id\n\
      \    )\n\n    model = ModelManager().get_or_create_chat_model(\n        name=\"\
      global_search\",\n        model_type=model_settings.type,\n        config=model_settings,\n\
      \    )\n\n    model_params = get_openai_model_parameters_from_config(model_settings)\n\
      \n    # Here we get encoding based on specified encoding name\n    tokenizer\
      \ = get_tokenizer(model_config=model_settings)\n    gs_config = config.global_search\n\
      \n    dynamic_community_selection_kwargs = {}\n    if dynamic_community_selection:\n\
      \        # TODO: Allow for another llm definition only for Global Search to\
      \ leverage -mini models\n\n        dynamic_community_selection_kwargs.update({\n\
      \            \"model\": model,\n            \"tokenizer\": tokenizer,\n    \
      \        \"keep_parent\": gs_config.dynamic_search_keep_parent,\n          \
      \  \"num_repeats\": gs_config.dynamic_search_num_repeats,\n            \"use_summary\"\
      : gs_config.dynamic_search_use_summary,\n            \"concurrent_coroutines\"\
      : model_settings.concurrent_requests,\n            \"threshold\": gs_config.dynamic_search_threshold,\n\
      \            \"max_level\": gs_config.dynamic_search_max_level,\n          \
      \  \"model_params\": {**model_params},\n        })\n\n    return GlobalSearch(\n\
      \        model=model,\n        map_system_prompt=map_system_prompt,\n      \
      \  reduce_system_prompt=reduce_system_prompt,\n        general_knowledge_inclusion_prompt=general_knowledge_inclusion_prompt,\n\
      \        context_builder=GlobalCommunityContext(\n            community_reports=reports,\n\
      \            communities=communities,\n            entities=entities,\n    \
      \        tokenizer=tokenizer,\n            dynamic_community_selection=dynamic_community_selection,\n\
      \            dynamic_community_selection_kwargs=dynamic_community_selection_kwargs,\n\
      \        ),\n        tokenizer=tokenizer,\n        max_data_tokens=gs_config.data_max_tokens,\n\
      \        map_llm_params={**model_params},\n        reduce_llm_params={**model_params},\n\
      \        map_max_length=gs_config.map_max_length,\n        reduce_max_length=gs_config.reduce_max_length,\n\
      \        allow_general_knowledge=False,\n        json_mode=False,\n        context_builder_params={\n\
      \            \"use_community_summary\": False,\n            \"shuffle_data\"\
      : True,\n            \"include_community_rank\": True,\n            \"min_community_rank\"\
      : 0,\n            \"community_rank_name\": \"rank\",\n            \"include_community_weight\"\
      : True,\n            \"community_weight_name\": \"occurrence weight\",\n   \
      \         \"normalize_community_weight\": True,\n            \"max_context_tokens\"\
      : gs_config.max_context_tokens,\n            \"context_name\": \"Reports\",\n\
      \        },\n        concurrent_coroutines=model_settings.concurrent_requests,\n\
      \        response_type=response_type,\n        callbacks=callbacks,\n    )"
    signature: "def get_global_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    entities: list[Entity],\n    communities: list[Community],\n\
      \    response_type: str,\n    dynamic_community_selection: bool = False,\n \
      \   map_system_prompt: str | None = None,\n    reduce_system_prompt: str | None\
      \ = None,\n    general_knowledge_inclusion_prompt: str | None = None,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n) -> GlobalSearch"
    decorators: []
    raises: []
    calls:
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: dynamic_community_selection_kwargs.update
      type: unresolved
    - target: graphrag/query/structured_search/global_search/search.py::GlobalSearch
      type: internal
    - target: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext
      type: internal
    visibility: public
    node_id: graphrag/query/factory.py::get_global_search_engine
    called_by:
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
  - name: get_drift_search_engine
    start_line: 195
    end_line: 247
    code: "def get_drift_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    description_embedding_store: BaseVectorStore,\n\
      \    response_type: str,\n    local_system_prompt: str | None = None,\n    reduce_system_prompt:\
      \ str | None = None,\n    callbacks: list[QueryCallbacks] | None = None,\n)\
      \ -> DRIFTSearch:\n    \"\"\"Create a local search engine based on data + configuration.\"\
      \"\"\n    chat_model_settings = config.get_language_model_config(\n        config.drift_search.chat_model_id\n\
      \    )\n\n    chat_model = ModelManager().get_or_create_chat_model(\n      \
      \  name=\"drift_search_chat\",\n        model_type=chat_model_settings.type,\n\
      \        config=chat_model_settings,\n    )\n\n    embedding_model_settings\
      \ = config.get_language_model_config(\n        config.drift_search.embedding_model_id\n\
      \    )\n\n    embedding_model = ModelManager().get_or_create_embedding_model(\n\
      \        name=\"drift_search_embedding\",\n        model_type=embedding_model_settings.type,\n\
      \        config=embedding_model_settings,\n    )\n\n    tokenizer = get_tokenizer(model_config=chat_model_settings)\n\
      \n    return DRIFTSearch(\n        model=chat_model,\n        context_builder=DRIFTSearchContextBuilder(\n\
      \            model=chat_model,\n            text_embedder=embedding_model,\n\
      \            entities=entities,\n            relationships=relationships,\n\
      \            reports=reports,\n            entity_text_embeddings=description_embedding_store,\n\
      \            text_units=text_units,\n            local_system_prompt=local_system_prompt,\n\
      \            reduce_system_prompt=reduce_system_prompt,\n            config=config.drift_search,\n\
      \            response_type=response_type,\n        ),\n        tokenizer=tokenizer,\n\
      \        callbacks=callbacks,\n    )"
    signature: "def get_drift_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    description_embedding_store: BaseVectorStore,\n\
      \    response_type: str,\n    local_system_prompt: str | None = None,\n    reduce_system_prompt:\
      \ str | None = None,\n    callbacks: list[QueryCallbacks] | None = None,\n)\
      \ -> DRIFTSearch"
    decorators: []
    raises: []
    calls:
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch
      type: internal
    - target: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder
      type: internal
    visibility: public
    node_id: graphrag/query/factory.py::get_drift_search_engine
    called_by:
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
  - name: get_basic_search_engine
    start_line: 250
    end_line: 303
    code: "def get_basic_search_engine(\n    text_units: list[TextUnit],\n    text_unit_embeddings:\
      \ BaseVectorStore,\n    config: GraphRagConfig,\n    system_prompt: str | None\
      \ = None,\n    response_type: str = \"multiple paragraphs\",\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n) -> BasicSearch:\n    \"\"\"Create a\
      \ basic search engine based on data + configuration.\"\"\"\n    chat_model_settings\
      \ = config.get_language_model_config(\n        config.basic_search.chat_model_id\n\
      \    )\n\n    chat_model = ModelManager().get_or_create_chat_model(\n      \
      \  name=\"basic_search_chat\",\n        model_type=chat_model_settings.type,\n\
      \        config=chat_model_settings,\n    )\n\n    embedding_model_settings\
      \ = config.get_language_model_config(\n        config.basic_search.embedding_model_id\n\
      \    )\n\n    embedding_model = ModelManager().get_or_create_embedding_model(\n\
      \        name=\"basic_search_embedding\",\n        model_type=embedding_model_settings.type,\n\
      \        config=embedding_model_settings,\n    )\n\n    tokenizer = get_tokenizer(model_config=chat_model_settings)\n\
      \n    bs_config = config.basic_search\n\n    model_params = get_openai_model_parameters_from_config(chat_model_settings)\n\
      \n    return BasicSearch(\n        model=chat_model,\n        system_prompt=system_prompt,\n\
      \        response_type=response_type,\n        context_builder=BasicSearchContext(\n\
      \            text_embedder=embedding_model,\n            text_unit_embeddings=text_unit_embeddings,\n\
      \            text_units=text_units,\n            tokenizer=tokenizer,\n    \
      \    ),\n        tokenizer=tokenizer,\n        model_params=model_params,\n\
      \        context_builder_params={\n            \"embedding_vectorstore_key\"\
      : \"id\",\n            \"k\": bs_config.k,\n            \"max_context_tokens\"\
      : bs_config.max_context_tokens,\n        },\n        callbacks=callbacks,\n\
      \    )"
    signature: "def get_basic_search_engine(\n    text_units: list[TextUnit],\n  \
      \  text_unit_embeddings: BaseVectorStore,\n    config: GraphRagConfig,\n   \
      \ system_prompt: str | None = None,\n    response_type: str = \"multiple paragraphs\"\
      ,\n    callbacks: list[QueryCallbacks] | None = None,\n) -> BasicSearch"
    decorators: []
    raises: []
    calls:
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
      type: internal
    - target: graphrag/query/structured_search/basic_search/search.py::BasicSearch
      type: internal
    - target: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext
      type: internal
    visibility: public
    node_id: graphrag/query/factory.py::get_basic_search_engine
    called_by:
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
- file_name: graphrag/query/indexer_adapters.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.data_model.community
    name: Community
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_communities
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_community_reports
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_covariates
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_entities
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_relationships
    alias: null
  - module: graphrag.query.input.loaders.dfs
    name: read_text_units
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: read_indexer_text_units
    start_line: 36
    end_line: 42
    code: "def read_indexer_text_units(final_text_units: pd.DataFrame) -> list[TextUnit]:\n\
      \    \"\"\"Read in the Text Units from the raw indexing outputs.\"\"\"\n   \
      \ return read_text_units(\n        df=final_text_units,\n        # expects a\
      \ covariate map of type -> ids\n        covariates_col=None,\n    )"
    signature: 'def read_indexer_text_units(final_text_units: pd.DataFrame) -> list[TextUnit]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_text_units
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
  - name: read_indexer_covariates
    start_line: 45
    end_line: 60
    code: "def read_indexer_covariates(final_covariates: pd.DataFrame) -> list[Covariate]:\n\
      \    \"\"\"Read in the Claims from the raw indexing outputs.\"\"\"\n    covariate_df\
      \ = final_covariates\n    covariate_df[\"id\"] = covariate_df[\"id\"].astype(str)\n\
      \    return read_covariates(\n        df=covariate_df,\n        short_id_col=\"\
      human_readable_id\",\n        attributes_cols=[\n            \"object_id\",\n\
      \            \"status\",\n            \"start_date\",\n            \"end_date\"\
      ,\n            \"description\",\n        ],\n        text_unit_ids_col=None,\n\
      \    )"
    signature: 'def read_indexer_covariates(final_covariates: pd.DataFrame) -> list[Covariate]'
    decorators: []
    raises: []
    calls:
    - target: covariate_df["id"].astype
      type: unresolved
    - target: graphrag/query/input/loaders/dfs.py::read_covariates
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_covariates
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
  - name: read_indexer_relationships
    start_line: 63
    end_line: 71
    code: "def read_indexer_relationships(final_relationships: pd.DataFrame) -> list[Relationship]:\n\
      \    \"\"\"Read in the Relationships from the raw indexing outputs.\"\"\"\n\
      \    return read_relationships(\n        df=final_relationships,\n        short_id_col=\"\
      human_readable_id\",\n        rank_col=\"combined_degree\",\n        description_embedding_col=None,\n\
      \        attributes_cols=None,\n    )"
    signature: 'def read_indexer_relationships(final_relationships: pd.DataFrame)
      -> list[Relationship]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_relationships
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
  - name: read_indexer_reports
    start_line: 74
    end_line: 127
    code: "def read_indexer_reports(\n    final_community_reports: pd.DataFrame,\n\
      \    final_communities: pd.DataFrame,\n    community_level: int | None,\n  \
      \  dynamic_community_selection: bool = False,\n    content_embedding_col: str\
      \ = \"full_content_embedding\",\n    config: GraphRagConfig | None = None,\n\
      ) -> list[CommunityReport]:\n    \"\"\"Read in the Community Reports from the\
      \ raw indexing outputs.\n\n    If not dynamic_community_selection, then select\
      \ reports with the max community level that an entity belongs to.\n    \"\"\"\
      \n    reports_df = final_community_reports\n    nodes_df = final_communities.explode(\"\
      entity_ids\")\n\n    if community_level is not None:\n        nodes_df = _filter_under_community_level(nodes_df,\
      \ community_level)\n        reports_df = _filter_under_community_level(reports_df,\
      \ community_level)\n\n    if not dynamic_community_selection:\n        # perform\
      \ community level roll up\n        nodes_df.loc[:, \"community\"] = nodes_df[\"\
      community\"].fillna(-1)\n        nodes_df.loc[:, \"community\"] = nodes_df[\"\
      community\"].astype(int)\n\n        nodes_df = nodes_df.groupby([\"title\"]).agg({\"\
      community\": \"max\"}).reset_index()\n        filtered_community_df = nodes_df[\"\
      community\"].drop_duplicates()\n\n        reports_df = reports_df.merge(\n \
      \           filtered_community_df, on=\"community\", how=\"inner\"\n       \
      \ )\n\n    if config and (\n        content_embedding_col not in reports_df.columns\n\
      \        or reports_df.loc[:, content_embedding_col].isna().any()\n    ):\n\
      \        # TODO: Find a way to retrieve the right embedding model id.\n    \
      \    embedding_model_settings = config.get_language_model_config(\n        \
      \    \"default_embedding_model\"\n        )\n        embedder = ModelManager().get_or_create_embedding_model(\n\
      \            name=\"default_embedding\",\n            model_type=embedding_model_settings.type,\n\
      \            config=embedding_model_settings,\n        )\n        reports_df\
      \ = embed_community_reports(\n            reports_df, embedder, embedding_col=content_embedding_col\n\
      \        )\n\n    return read_community_reports(\n        df=reports_df,\n \
      \       id_col=\"id\",\n        short_id_col=\"community\",\n        content_embedding_col=content_embedding_col,\n\
      \    )"
    signature: "def read_indexer_reports(\n    final_community_reports: pd.DataFrame,\n\
      \    final_communities: pd.DataFrame,\n    community_level: int | None,\n  \
      \  dynamic_community_selection: bool = False,\n    content_embedding_col: str\
      \ = \"full_content_embedding\",\n    config: GraphRagConfig | None = None,\n\
      ) -> list[CommunityReport]"
    decorators: []
    raises: []
    calls:
    - target: final_communities.explode
      type: unresolved
    - target: graphrag/query/indexer_adapters.py::_filter_under_community_level
      type: internal
    - target: nodes_df["community"].fillna
      type: unresolved
    - target: nodes_df["community"].astype
      type: unresolved
    - target: 'nodes_df.groupby(["title"]).agg({"community": "max"}).reset_index'
      type: unresolved
    - target: nodes_df.groupby(["title"]).agg
      type: unresolved
    - target: nodes_df.groupby
      type: unresolved
    - target: nodes_df["community"].drop_duplicates
      type: unresolved
    - target: reports_df.merge
      type: unresolved
    - target: reports_df.loc[:, content_embedding_col].isna().any
      type: unresolved
    - target: reports_df.loc[:, content_embedding_col].isna
      type: unresolved
    - target: config.get_language_model_config
      type: unresolved
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: graphrag/query/indexer_adapters.py::embed_community_reports
      type: internal
    - target: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_reports
    called_by:
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
  - name: read_indexer_report_embeddings
    start_line: 130
    end_line: 136
    code: "def read_indexer_report_embeddings(\n    community_reports: list[CommunityReport],\n\
      \    embeddings_store: BaseVectorStore,\n):\n    \"\"\"Read in the Community\
      \ Reports from the raw indexing outputs.\"\"\"\n    for report in community_reports:\n\
      \        report.full_content_embedding = embeddings_store.search_by_id(report.id).vector"
    signature: "def read_indexer_report_embeddings(\n    community_reports: list[CommunityReport],\n\
      \    embeddings_store: BaseVectorStore,\n)"
    decorators: []
    raises: []
    calls:
    - target: embeddings_store.search_by_id
      type: unresolved
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_report_embeddings
    called_by:
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
  - name: read_indexer_entities
    start_line: 139
    end_line: 178
    code: "def read_indexer_entities(\n    entities: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    community_level: int | None,\n) -> list[Entity]:\n    \"\
      \"\"Read in the Entities from the raw indexing outputs.\"\"\"\n    community_join\
      \ = communities.explode(\"entity_ids\").loc[\n        :, [\"community\", \"\
      level\", \"entity_ids\"]\n    ]\n    nodes_df = entities.merge(\n        community_join,\
      \ left_on=\"id\", right_on=\"entity_ids\", how=\"left\"\n    )\n\n    if community_level\
      \ is not None:\n        nodes_df = _filter_under_community_level(nodes_df, community_level)\n\
      \n    nodes_df = nodes_df.loc[:, [\"id\", \"community\"]]\n    nodes_df[\"community\"\
      ] = nodes_df[\"community\"].fillna(-1)\n    # group entities by id and degree\
      \ and remove duplicated community IDs\n    nodes_df = nodes_df.groupby([\"id\"\
      ]).agg({\"community\": set}).reset_index()\n    nodes_df[\"community\"] = nodes_df[\"\
      community\"].apply(\n        lambda x: [str(int(i)) for i in x]\n    )\n   \
      \ final_df = nodes_df.merge(entities, on=\"id\", how=\"inner\").drop_duplicates(\n\
      \        subset=[\"id\"]\n    )\n    # read entity dataframe to knowledge model\
      \ objects\n    return read_entities(\n        df=final_df,\n        id_col=\"\
      id\",\n        title_col=\"title\",\n        type_col=\"type\",\n        short_id_col=\"\
      human_readable_id\",\n        description_col=\"description\",\n        community_col=\"\
      community\",\n        rank_col=\"degree\",\n        name_embedding_col=None,\n\
      \        description_embedding_col=\"description_embedding\",\n        text_unit_ids_col=\"\
      text_unit_ids\",\n    )"
    signature: "def read_indexer_entities(\n    entities: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    community_level: int | None,\n) -> list[Entity]"
    decorators: []
    raises: []
    calls:
    - target: communities.explode
      type: unresolved
    - target: entities.merge
      type: unresolved
    - target: graphrag/query/indexer_adapters.py::_filter_under_community_level
      type: internal
    - target: nodes_df["community"].fillna
      type: unresolved
    - target: 'nodes_df.groupby(["id"]).agg({"community": set}).reset_index'
      type: unresolved
    - target: nodes_df.groupby(["id"]).agg
      type: unresolved
    - target: nodes_df.groupby
      type: unresolved
    - target: nodes_df["community"].apply
      type: unresolved
    - target: str
      type: builtin
    - target: int
      type: builtin
    - target: nodes_df.merge(entities, on="id", how="inner").drop_duplicates
      type: unresolved
    - target: nodes_df.merge
      type: unresolved
    - target: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_entities
    called_by:
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
  - name: read_indexer_communities
    start_line: 181
    end_line: 216
    code: "def read_indexer_communities(\n    final_communities: pd.DataFrame,\n \
      \   final_community_reports: pd.DataFrame,\n) -> list[Community]:\n    \"\"\"\
      Read in the Communities from the raw indexing outputs.\n\n    Reconstruct the\
      \ community hierarchy information and add to the sub-community field.\n    \"\
      \"\"\n    communities_df = final_communities\n    nodes_df = communities_df.explode(\"\
      entity_ids\")\n    reports_df = final_community_reports\n\n    # ensure communities\
      \ matches community reports\n    missing_reports = communities_df[\n       \
      \ ~communities_df.community.isin(reports_df.community.unique())\n    ].community.to_list()\n\
      \    if len(missing_reports):\n        logger.warning(\"Missing reports for\
      \ communities: %s\", missing_reports)\n        communities_df = communities_df.loc[\n\
      \            communities_df.community.isin(reports_df.community.unique())\n\
      \        ]\n        nodes_df = nodes_df.loc[nodes_df.community.isin(reports_df.community.unique())]\n\
      \n    return read_communities(\n        communities_df,\n        id_col=\"id\"\
      ,\n        short_id_col=\"community\",\n        title_col=\"title\",\n     \
      \   level_col=\"level\",\n        entities_col=None,\n        relationships_col=None,\n\
      \        covariates_col=None,\n        parent_col=\"parent\",\n        children_col=\"\
      children\",\n        attributes_cols=None,\n    )"
    signature: "def read_indexer_communities(\n    final_communities: pd.DataFrame,\n\
      \    final_community_reports: pd.DataFrame,\n) -> list[Community]"
    decorators: []
    raises: []
    calls:
    - target: communities_df.explode
      type: unresolved
    - target: "communities_df[\n        ~communities_df.community.isin(reports_df.community.unique())\n\
        \    ].community.to_list"
      type: unresolved
    - target: communities_df.community.isin
      type: unresolved
    - target: reports_df.community.unique
      type: unresolved
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: nodes_df.community.isin
      type: unresolved
    - target: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::read_indexer_communities
    called_by:
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
  - name: embed_community_reports
    start_line: 219
    end_line: 235
    code: "def embed_community_reports(\n    reports_df: pd.DataFrame,\n    embedder:\
      \ EmbeddingModel,\n    source_col: str = \"full_content\",\n    embedding_col:\
      \ str = \"full_content_embedding\",\n) -> pd.DataFrame:\n    \"\"\"Embed a source\
      \ column of the reports dataframe using the given embedder.\"\"\"\n    if source_col\
      \ not in reports_df.columns:\n        error_msg = f\"Reports missing {source_col}\
      \ column\"\n        raise ValueError(error_msg)\n\n    if embedding_col not\
      \ in reports_df.columns:\n        reports_df[embedding_col] = reports_df.loc[:,\
      \ source_col].apply(\n            lambda x: embedder.embed(x)\n        )\n\n\
      \    return reports_df"
    signature: "def embed_community_reports(\n    reports_df: pd.DataFrame,\n    embedder:\
      \ EmbeddingModel,\n    source_col: str = \"full_content\",\n    embedding_col:\
      \ str = \"full_content_embedding\",\n) -> pd.DataFrame"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: reports_df.loc[:, source_col].apply
      type: unresolved
    - target: embedder.embed
      type: unresolved
    visibility: public
    node_id: graphrag/query/indexer_adapters.py::embed_community_reports
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
  - name: _filter_under_community_level
    start_line: 238
    end_line: 244
    code: "def _filter_under_community_level(\n    df: pd.DataFrame, community_level:\
      \ int\n) -> pd.DataFrame:\n    return cast(\n        \"pd.DataFrame\",\n   \
      \     df[df.level <= community_level],\n    )"
    signature: "def _filter_under_community_level(\n    df: pd.DataFrame, community_level:\
      \ int\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: typing::cast
      type: stdlib
    visibility: protected
    node_id: graphrag/query/indexer_adapters.py::_filter_under_community_level
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
    - source: graphrag/query/indexer_adapters.py::read_indexer_entities
      type: internal
- file_name: graphrag/query/input/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/input/loaders/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/input/loaders/dfs.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.community
    name: Community
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_list
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_optional_dict
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_optional_float
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_optional_int
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_optional_list
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_optional_str
    alias: null
  - module: graphrag.query.input.loaders.utils
    name: to_str
    alias: null
  functions:
  - name: _prepare_records
    start_line: 25
    end_line: 32
    code: "def _prepare_records(df: pd.DataFrame) -> list[dict]:\n    \"\"\"\n   \
      \ Reset index and convert the DataFrame to a list of dictionaries.\n\n    We\
      \ rename the reset index column to 'Index' for consistency.\n    \"\"\"\n  \
      \  df_reset = df.reset_index().rename(columns={\"index\": \"Index\"})\n    return\
      \ df_reset.to_dict(\"records\")"
    signature: 'def _prepare_records(df: pd.DataFrame) -> list[dict]'
    decorators: []
    raises: []
    calls:
    - target: df.reset_index().rename
      type: unresolved
    - target: df.reset_index
      type: unresolved
    - target: df_reset.to_dict
      type: unresolved
    visibility: protected
    node_id: graphrag/query/input/loaders/dfs.py::_prepare_records
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_covariates
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
  - name: read_entities
    start_line: 35
    end_line: 74
    code: "def read_entities(\n    df: pd.DataFrame,\n    id_col: str = \"id\",\n\
      \    short_id_col: str | None = \"human_readable_id\",\n    title_col: str =\
      \ \"title\",\n    type_col: str | None = \"type\",\n    description_col: str\
      \ | None = \"description\",\n    name_embedding_col: str | None = \"name_embedding\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    community_col: str | None = \"community_ids\",\n    text_unit_ids_col:\
      \ str | None = \"text_unit_ids\",\n    rank_col: str | None = \"degree\",\n\
      \    attributes_cols: list[str] | None = None,\n) -> list[Entity]:\n    \"\"\
      \"Read entities from a dataframe using pre-converted records.\"\"\"\n    records\
      \ = _prepare_records(df)\n    return [\n        Entity(\n            id=to_str(row,\
      \ id_col),\n            short_id=to_optional_str(row, short_id_col)\n      \
      \      if short_id_col\n            else str(row[\"Index\"]),\n            title=to_str(row,\
      \ title_col),\n            type=to_optional_str(row, type_col),\n          \
      \  description=to_optional_str(row, description_col),\n            name_embedding=to_optional_list(row,\
      \ name_embedding_col, item_type=float),\n            description_embedding=to_optional_list(\n\
      \                row, description_embedding_col, item_type=float\n         \
      \   ),\n            community_ids=to_optional_list(row, community_col, item_type=str),\n\
      \            text_unit_ids=to_optional_list(row, text_unit_ids_col),\n     \
      \       rank=to_optional_int(row, rank_col),\n            attributes=(\n   \
      \             {col: row.get(col) for col in attributes_cols}\n             \
      \   if attributes_cols\n                else None\n            ),\n        )\n\
      \        for row in records\n    ]"
    signature: "def read_entities(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
      ,\n    short_id_col: str | None = \"human_readable_id\",\n    title_col: str\
      \ = \"title\",\n    type_col: str | None = \"type\",\n    description_col: str\
      \ | None = \"description\",\n    name_embedding_col: str | None = \"name_embedding\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    community_col: str | None = \"community_ids\",\n    text_unit_ids_col:\
      \ str | None = \"text_unit_ids\",\n    rank_col: str | None = \"degree\",\n\
      \    attributes_cols: list[str] | None = None,\n) -> list[Entity]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/entity.py::Entity
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_int
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_entities
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_entities
      type: internal
  - name: read_relationships
    start_line: 77
    end_line: 114
    code: "def read_relationships(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
      ,\n    short_id_col: str | None = \"human_readable_id\",\n    source_col: str\
      \ = \"source\",\n    target_col: str = \"target\",\n    description_col: str\
      \ | None = \"description\",\n    rank_col: str | None = \"combined_degree\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    weight_col: str | None = \"weight\",\n    text_unit_ids_col: str | None\
      \ = \"text_unit_ids\",\n    attributes_cols: list[str] | None = None,\n) ->\
      \ list[Relationship]:\n    \"\"\"Read relationships from a dataframe using pre-converted\
      \ records.\"\"\"\n    records = _prepare_records(df)\n    return [\n       \
      \ Relationship(\n            id=to_str(row, id_col),\n            short_id=to_optional_str(row,\
      \ short_id_col)\n            if short_id_col\n            else str(row[\"Index\"\
      ]),\n            source=to_str(row, source_col),\n            target=to_str(row,\
      \ target_col),\n            description=to_optional_str(row, description_col),\n\
      \            description_embedding=to_optional_list(\n                row, description_embedding_col,\
      \ item_type=float\n            ),\n            weight=to_optional_float(row,\
      \ weight_col),\n            text_unit_ids=to_optional_list(row, text_unit_ids_col,\
      \ item_type=str),\n            rank=to_optional_int(row, rank_col),\n      \
      \      attributes=(\n                {col: row.get(col) for col in attributes_cols}\n\
      \                if attributes_cols\n                else None\n           \
      \ ),\n        )\n        for row in records\n    ]"
    signature: "def read_relationships(\n    df: pd.DataFrame,\n    id_col: str =\
      \ \"id\",\n    short_id_col: str | None = \"human_readable_id\",\n    source_col:\
      \ str = \"source\",\n    target_col: str = \"target\",\n    description_col:\
      \ str | None = \"description\",\n    rank_col: str | None = \"combined_degree\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    weight_col: str | None = \"weight\",\n    text_unit_ids_col: str | None\
      \ = \"text_unit_ids\",\n    attributes_cols: list[str] | None = None,\n) ->\
      \ list[Relationship]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/relationship.py::Relationship
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_float
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_int
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_relationships
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_relationships
      type: internal
  - name: read_covariates
    start_line: 117
    end_line: 146
    code: "def read_covariates(\n    df: pd.DataFrame,\n    id_col: str = \"id\",\n\
      \    short_id_col: str | None = \"human_readable_id\",\n    subject_col: str\
      \ = \"subject_id\",\n    covariate_type_col: str | None = \"type\",\n    text_unit_ids_col:\
      \ str | None = \"text_unit_ids\",\n    attributes_cols: list[str] | None = None,\n\
      ) -> list[Covariate]:\n    \"\"\"Read covariates from a dataframe using pre-converted\
      \ records.\"\"\"\n    records = _prepare_records(df)\n    return [\n       \
      \ Covariate(\n            id=to_str(row, id_col),\n            short_id=to_optional_str(row,\
      \ short_id_col)\n            if short_id_col\n            else str(row[\"Index\"\
      ]),\n            subject_id=to_str(row, subject_col),\n            covariate_type=(\n\
      \                to_str(row, covariate_type_col) if covariate_type_col else\
      \ \"claim\"\n            ),\n            text_unit_ids=to_optional_list(row,\
      \ text_unit_ids_col, item_type=str),\n            attributes=(\n           \
      \     {col: row.get(col) for col in attributes_cols}\n                if attributes_cols\n\
      \                else None\n            ),\n        )\n        for row in records\n\
      \    ]"
    signature: "def read_covariates(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    short_id_col: str | None = \"human_readable_id\",\n    subject_col:\
      \ str = \"subject_id\",\n    covariate_type_col: str | None = \"type\",\n  \
      \  text_unit_ids_col: str | None = \"text_unit_ids\",\n    attributes_cols:\
      \ list[str] | None = None,\n) -> list[Covariate]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/covariate.py::Covariate
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_covariates
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_covariates
      type: internal
  - name: read_communities
    start_line: 149
    end_line: 188
    code: "def read_communities(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
      ,\n    short_id_col: str | None = \"community\",\n    title_col: str = \"title\"\
      ,\n    level_col: str = \"level\",\n    entities_col: str | None = \"entity_ids\"\
      ,\n    relationships_col: str | None = \"relationship_ids\",\n    text_units_col:\
      \ str | None = \"text_unit_ids\",\n    covariates_col: str | None = \"covariate_ids\"\
      ,\n    parent_col: str | None = \"parent\",\n    children_col: str | None =\
      \ \"children\",\n    attributes_cols: list[str] | None = None,\n) -> list[Community]:\n\
      \    \"\"\"Read communities from a dataframe using pre-converted records.\"\"\
      \"\n    records = _prepare_records(df)\n    return [\n        Community(\n \
      \           id=to_str(row, id_col),\n            short_id=to_optional_str(row,\
      \ short_id_col)\n            if short_id_col\n            else str(row[\"Index\"\
      ]),\n            title=to_str(row, title_col),\n            level=to_str(row,\
      \ level_col),\n            entity_ids=to_optional_list(row, entities_col, item_type=str),\n\
      \            relationship_ids=to_optional_list(row, relationships_col, item_type=str),\n\
      \            text_unit_ids=to_optional_list(row, text_units_col, item_type=str),\n\
      \            covariate_ids=to_optional_dict(\n                row, covariates_col,\
      \ key_type=str, value_type=str\n            ),\n            parent=to_str(row,\
      \ parent_col),\n            children=to_list(row, children_col),\n         \
      \   attributes=(\n                {col: row.get(col) for col in attributes_cols}\n\
      \                if attributes_cols\n                else None\n           \
      \ ),\n        )\n        for row in records\n    ]"
    signature: "def read_communities(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    short_id_col: str | None = \"community\",\n    title_col: str = \"\
      title\",\n    level_col: str = \"level\",\n    entities_col: str | None = \"\
      entity_ids\",\n    relationships_col: str | None = \"relationship_ids\",\n \
      \   text_units_col: str | None = \"text_unit_ids\",\n    covariates_col: str\
      \ | None = \"covariate_ids\",\n    parent_col: str | None = \"parent\",\n  \
      \  children_col: str | None = \"children\",\n    attributes_cols: list[str]\
      \ | None = None,\n) -> list[Community]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/community.py::Community
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_dict
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_list
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_communities
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_communities
      type: internal
  - name: read_community_reports
    start_line: 191
    end_line: 226
    code: "def read_community_reports(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    short_id_col: str | None = \"community\",\n    title_col: str = \"\
      title\",\n    community_col: str = \"community\",\n    summary_col: str = \"\
      summary\",\n    content_col: str = \"full_content\",\n    rank_col: str | None\
      \ = \"rank\",\n    content_embedding_col: str | None = \"full_content_embedding\"\
      ,\n    attributes_cols: list[str] | None = None,\n) -> list[CommunityReport]:\n\
      \    \"\"\"Read community reports from a dataframe using pre-converted records.\"\
      \"\"\n    records = _prepare_records(df)\n    return [\n        CommunityReport(\n\
      \            id=to_str(row, id_col),\n            short_id=to_optional_str(row,\
      \ short_id_col)\n            if short_id_col\n            else str(row[\"Index\"\
      ]),\n            title=to_str(row, title_col),\n            community_id=to_str(row,\
      \ community_col),\n            summary=to_str(row, summary_col),\n         \
      \   full_content=to_str(row, content_col),\n            rank=to_optional_float(row,\
      \ rank_col),\n            full_content_embedding=to_optional_list(\n       \
      \         row, content_embedding_col, item_type=float\n            ),\n    \
      \        attributes=(\n                {col: row.get(col) for col in attributes_cols}\n\
      \                if attributes_cols\n                else None\n           \
      \ ),\n        )\n        for row in records\n    ]"
    signature: "def read_community_reports(\n    df: pd.DataFrame,\n    id_col: str\
      \ = \"id\",\n    short_id_col: str | None = \"community\",\n    title_col: str\
      \ = \"title\",\n    community_col: str = \"community\",\n    summary_col: str\
      \ = \"summary\",\n    content_col: str = \"full_content\",\n    rank_col: str\
      \ | None = \"rank\",\n    content_embedding_col: str | None = \"full_content_embedding\"\
      ,\n    attributes_cols: list[str] | None = None,\n) -> list[CommunityReport]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/community_report.py::CommunityReport
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_float
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_community_reports
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_reports
      type: internal
  - name: read_text_units
    start_line: 229
    end_line: 261
    code: "def read_text_units(\n    df: pd.DataFrame,\n    id_col: str = \"id\",\n\
      \    text_col: str = \"text\",\n    entities_col: str | None = \"entity_ids\"\
      ,\n    relationships_col: str | None = \"relationship_ids\",\n    covariates_col:\
      \ str | None = \"covariate_ids\",\n    tokens_col: str | None = \"n_tokens\"\
      ,\n    document_ids_col: str | None = \"document_ids\",\n    attributes_cols:\
      \ list[str] | None = None,\n) -> list[TextUnit]:\n    \"\"\"Read text units\
      \ from a dataframe using pre-converted records.\"\"\"\n    records = _prepare_records(df)\n\
      \    return [\n        TextUnit(\n            id=to_str(row, id_col),\n    \
      \        short_id=str(row[\"Index\"]),\n            text=to_str(row, text_col),\n\
      \            entity_ids=to_optional_list(row, entities_col, item_type=str),\n\
      \            relationship_ids=to_optional_list(row, relationships_col, item_type=str),\n\
      \            covariate_ids=to_optional_dict(\n                row, covariates_col,\
      \ key_type=str, value_type=str\n            ),\n            n_tokens=to_optional_int(row,\
      \ tokens_col),\n            document_ids=to_optional_list(row, document_ids_col,\
      \ item_type=str),\n            attributes=(\n                {col: row.get(col)\
      \ for col in attributes_cols}\n                if attributes_cols\n        \
      \        else None\n            ),\n        )\n        for row in records\n\
      \    ]"
    signature: "def read_text_units(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    text_col: str = \"text\",\n    entities_col: str | None = \"entity_ids\"\
      ,\n    relationships_col: str | None = \"relationship_ids\",\n    covariates_col:\
      \ str | None = \"covariate_ids\",\n    tokens_col: str | None = \"n_tokens\"\
      ,\n    document_ids_col: str | None = \"document_ids\",\n    attributes_cols:\
      \ list[str] | None = None,\n) -> list[TextUnit]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/dfs.py::_prepare_records
      type: internal
    - target: graphrag/data_model/text_unit.py::TextUnit
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - target: str
      type: builtin
    - target: graphrag/query/input/loaders/utils.py::to_optional_list
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_dict
      type: internal
    - target: graphrag/query/input/loaders/utils.py::to_optional_int
      type: internal
    - target: row.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/dfs.py::read_text_units
    called_by:
    - source: graphrag/query/indexer_adapters.py::read_indexer_text_units
      type: internal
- file_name: graphrag/query/input/loaders/utils.py
  imports:
  - module: collections.abc
    name: Mapping
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: numpy
    name: null
    alias: np
  functions:
  - name: _get_value
    start_line: 12
    end_line: 34
    code: "def _get_value(\n    data: Mapping[str, Any], column_name: str | None,\
      \ required: bool = True\n) -> Any:\n    \"\"\"\n    Retrieve a column value\
      \ from data.\n\n    If `required` is True, raises a ValueError when:\n     \
      \ - column_name is None, or\n      - column_name is not in data.\n\n    For\
      \ optional columns (required=False), returns None if column_name is None.\n\
      \    \"\"\"\n    if column_name is None:\n        if required:\n           \
      \ msg = \"Column name is None\"\n            raise ValueError(msg)\n       \
      \ return None\n    if column_name in data:\n        return data[column_name]\n\
      \    if required:\n        msg = f\"Column [{column_name}] not found in data\"\
      \n        raise ValueError(msg)\n    return None"
    signature: "def _get_value(\n    data: Mapping[str, Any], column_name: str | None,\
      \ required: bool = True\n) -> Any"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/query/input/loaders/utils.py::_get_value
    called_by:
    - source: graphrag/query/input/loaders/utils.py::to_str
      type: internal
    - source: graphrag/query/input/loaders/utils.py::to_optional_str
      type: internal
    - source: graphrag/query/input/loaders/utils.py::to_list
      type: internal
    - source: graphrag/query/input/loaders/utils.py::to_int
      type: internal
    - source: graphrag/query/input/loaders/utils.py::to_float
      type: internal
    - source: graphrag/query/input/loaders/utils.py::to_dict
      type: internal
  - name: to_str
    start_line: 37
    end_line: 40
    code: "def to_str(data: Mapping[str, Any], column_name: str | None) -> str:\n\
      \    \"\"\"Convert and validate a value to a string.\"\"\"\n    value = _get_value(data,\
      \ column_name, required=True)\n    return str(value)"
    signature: 'def to_str(data: Mapping[str, Any], column_name: str | None) -> str'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_str
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_covariates
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
  - name: to_optional_str
    start_line: 43
    end_line: 46
    code: "def to_optional_str(data: Mapping[str, Any], column_name: str | None) ->\
      \ str | None:\n    \"\"\"Convert and validate a value to an optional string.\"\
      \"\"\n    value = _get_value(data, column_name, required=True)\n    return None\
      \ if value is None else str(value)"
    signature: 'def to_optional_str(data: Mapping[str, Any], column_name: str | None)
      -> str | None'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_optional_str
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_covariates
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
  - name: to_list
    start_line: 49
    end_line: 64
    code: "def to_list(\n    data: Mapping[str, Any], column_name: str | None, item_type:\
      \ type | None = None\n) -> list:\n    \"\"\"Convert and validate a value to\
      \ a list.\"\"\"\n    value = _get_value(data, column_name, required=True)\n\
      \    if isinstance(value, np.ndarray):\n        value = value.tolist()\n   \
      \ if not isinstance(value, list):\n        msg = f\"value is not a list: {value}\
      \ ({type(value)})\"\n        raise TypeError(msg)\n    if item_type is not None:\n\
      \        for v in value:\n            if not isinstance(v, item_type):\n   \
      \             msg = f\"list item is not [{item_type}]: {v} ({type(v)})\"\n \
      \               raise TypeError(msg)\n    return value"
    signature: "def to_list(\n    data: Mapping[str, Any], column_name: str | None,\
      \ item_type: type | None = None\n) -> list"
    decorators: []
    raises:
    - TypeError
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: isinstance
      type: builtin
    - target: value.tolist
      type: unresolved
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_list
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
  - name: to_optional_list
    start_line: 67
    end_line: 88
    code: "def to_optional_list(\n    data: Mapping[str, Any], column_name: str |\
      \ None, item_type: type | None = None\n) -> list | None:\n    \"\"\"Convert\
      \ and validate a value to an optional list.\"\"\"\n    if column_name is None\
      \ or column_name not in data:\n        return None\n    value = data[column_name]\n\
      \    if value is None:\n        return None\n    if isinstance(value, np.ndarray):\n\
      \        value = value.tolist()\n    if isinstance(value, str):\n        value\
      \ = [value]\n    if not isinstance(value, list):\n        msg = f\"value is\
      \ not a list: {value} ({type(value)})\"\n        raise TypeError(msg)\n    if\
      \ item_type is not None:\n        for v in value:\n            if not isinstance(v,\
      \ item_type):\n                msg = f\"list item is not [{item_type}]: {v}\
      \ ({type(v)})\"\n                raise TypeError(msg)\n    return value"
    signature: "def to_optional_list(\n    data: Mapping[str, Any], column_name: str\
      \ | None, item_type: type | None = None\n) -> list | None"
    decorators: []
    raises:
    - TypeError
    calls:
    - target: isinstance
      type: builtin
    - target: value.tolist
      type: unresolved
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_optional_list
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_covariates
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
  - name: to_int
    start_line: 91
    end_line: 99
    code: "def to_int(data: Mapping[str, Any], column_name: str | None) -> int:\n\
      \    \"\"\"Convert and validate a value to an int.\"\"\"\n    value = _get_value(data,\
      \ column_name, required=True)\n    if isinstance(value, float):\n        value\
      \ = int(value)\n    if not isinstance(value, int):\n        msg = f\"value is\
      \ not an int: {value} ({type(value)})\"\n        raise TypeError(msg)\n    return\
      \ int(value)"
    signature: 'def to_int(data: Mapping[str, Any], column_name: str | None) -> int'
    decorators: []
    raises:
    - TypeError
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: isinstance
      type: builtin
    - target: int
      type: builtin
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_int
    called_by: []
  - name: to_optional_int
    start_line: 102
    end_line: 114
    code: "def to_optional_int(data: Mapping[str, Any], column_name: str | None) ->\
      \ int | None:\n    \"\"\"Convert and validate a value to an optional int.\"\"\
      \"\n    if column_name is None or column_name not in data:\n        return None\n\
      \    value = data[column_name]\n    if value is None:\n        return None\n\
      \    if isinstance(value, float):\n        value = int(value)\n    if not isinstance(value,\
      \ int):\n        msg = f\"value is not an int: {value} ({type(value)})\"\n \
      \       raise TypeError(msg)\n    return int(value)"
    signature: 'def to_optional_int(data: Mapping[str, Any], column_name: str | None)
      -> int | None'
    decorators: []
    raises:
    - TypeError
    calls:
    - target: isinstance
      type: builtin
    - target: int
      type: builtin
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_optional_int
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_entities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
  - name: to_float
    start_line: 117
    end_line: 123
    code: "def to_float(data: Mapping[str, Any], column_name: str | None) -> float:\n\
      \    \"\"\"Convert and validate a value to a float.\"\"\"\n    value = _get_value(data,\
      \ column_name, required=True)\n    if not isinstance(value, float):\n      \
      \  msg = f\"value is not a float: {value} ({type(value)})\"\n        raise TypeError(msg)\n\
      \    return float(value)"
    signature: 'def to_float(data: Mapping[str, Any], column_name: str | None) ->
      float'
    decorators: []
    raises:
    - TypeError
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: isinstance
      type: builtin
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    - target: float
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_float
    called_by: []
  - name: to_optional_float
    start_line: 126
    end_line: 135
    code: "def to_optional_float(data: Mapping[str, Any], column_name: str | None)\
      \ -> float | None:\n    \"\"\"Convert and validate a value to an optional float.\"\
      \"\"\n    if column_name is None or column_name not in data:\n        return\
      \ None\n    value = data[column_name]\n    if value is None:\n        return\
      \ None\n    if not isinstance(value, float):\n        return float(value)\n\
      \    return float(value)"
    signature: 'def to_optional_float(data: Mapping[str, Any], column_name: str |
      None) -> float | None'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: float
      type: builtin
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_optional_float
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_relationships
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_community_reports
      type: internal
  - name: to_dict
    start_line: 138
    end_line: 159
    code: "def to_dict(\n    data: Mapping[str, Any],\n    column_name: str | None,\n\
      \    key_type: type | None = None,\n    value_type: type | None = None,\n) ->\
      \ dict:\n    \"\"\"Convert and validate a value to a dict.\"\"\"\n    value\
      \ = _get_value(data, column_name, required=True)\n    if not isinstance(value,\
      \ dict):\n        msg = f\"value is not a dict: {value} ({type(value)})\"\n\
      \        raise TypeError(msg)\n    if key_type is not None:\n        for k in\
      \ value:\n            if not isinstance(k, key_type):\n                msg =\
      \ f\"dict key is not [{key_type}]: {k} ({type(k)})\"\n                raise\
      \ TypeError(msg)\n    if value_type is not None:\n        for v in value.values():\n\
      \            if not isinstance(v, value_type):\n                msg = f\"dict\
      \ value is not [{value_type}]: {v} ({type(v)})\"\n                raise TypeError(msg)\n\
      \    return value"
    signature: "def to_dict(\n    data: Mapping[str, Any],\n    column_name: str |\
      \ None,\n    key_type: type | None = None,\n    value_type: type | None = None,\n\
      ) -> dict"
    decorators: []
    raises:
    - TypeError
    calls:
    - target: graphrag/query/input/loaders/utils.py::_get_value
      type: internal
    - target: isinstance
      type: builtin
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    - target: value.values
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_dict
    called_by: []
  - name: to_optional_dict
    start_line: 162
    end_line: 187
    code: "def to_optional_dict(\n    data: Mapping[str, Any],\n    column_name: str\
      \ | None,\n    key_type: type | None = None,\n    value_type: type | None =\
      \ None,\n) -> dict | None:\n    \"\"\"Convert and validate a value to an optional\
      \ dict.\"\"\"\n    if column_name is None or column_name not in data:\n    \
      \    return None\n    value = data[column_name]\n    if value is None:\n   \
      \     return None\n    if not isinstance(value, dict):\n        msg = f\"value\
      \ is not a dict: {value} ({type(value)})\"\n        raise TypeError(msg)\n \
      \   if key_type is not None:\n        for k in value:\n            if not isinstance(k,\
      \ key_type):\n                msg = f\"dict key is not [{key_type}]: {k} ({type(k)})\"\
      \n                raise TypeError(msg)\n    if value_type is not None:\n   \
      \     for v in value.values():\n            if not isinstance(v, value_type):\n\
      \                msg = f\"dict value is not [{value_type}]: {v} ({type(v)})\"\
      \n                raise TypeError(msg)\n    return value"
    signature: "def to_optional_dict(\n    data: Mapping[str, Any],\n    column_name:\
      \ str | None,\n    key_type: type | None = None,\n    value_type: type | None\
      \ = None,\n) -> dict | None"
    decorators: []
    raises:
    - TypeError
    calls:
    - target: isinstance
      type: builtin
    - target: type
      type: builtin
    - target: TypeError
      type: builtin
    - target: value.values
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/loaders/utils.py::to_optional_dict
    called_by:
    - source: graphrag/query/input/loaders/dfs.py::read_communities
      type: internal
    - source: graphrag/query/input/loaders/dfs.py::read_text_units
      type: internal
- file_name: graphrag/query/input/retrieval/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/input/retrieval/community_reports.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  functions:
  - name: get_candidate_communities
    start_line: 14
    end_line: 36
    code: "def get_candidate_communities(\n    selected_entities: list[Entity],\n\
      \    community_reports: list[CommunityReport],\n    include_community_rank:\
      \ bool = False,\n    use_community_summary: bool = False,\n) -> pd.DataFrame:\n\
      \    \"\"\"Get all communities that are related to selected entities.\"\"\"\n\
      \    selected_community_ids = [\n        entity.community_ids for entity in\
      \ selected_entities if entity.community_ids\n    ]\n    selected_community_ids\
      \ = [\n        item for sublist in selected_community_ids for item in sublist\n\
      \    ]\n    selected_reports = [\n        community\n        for community in\
      \ community_reports\n        if community.id in selected_community_ids\n   \
      \ ]\n    return to_community_report_dataframe(\n        reports=selected_reports,\n\
      \        include_community_rank=include_community_rank,\n        use_community_summary=use_community_summary,\n\
      \    )"
    signature: "def get_candidate_communities(\n    selected_entities: list[Entity],\n\
      \    community_reports: list[CommunityReport],\n    include_community_rank:\
      \ bool = False,\n    use_community_summary: bool = False,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/community_reports.py::to_community_report_dataframe
      type: internal
    visibility: public
    node_id: graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
      type: internal
  - name: to_community_report_dataframe
    start_line: 39
    end_line: 75
    code: "def to_community_report_dataframe(\n    reports: list[CommunityReport],\n\
      \    include_community_rank: bool = False,\n    use_community_summary: bool\
      \ = False,\n) -> pd.DataFrame:\n    \"\"\"Convert a list of communities to a\
      \ pandas dataframe.\"\"\"\n    if len(reports) == 0:\n        return pd.DataFrame()\n\
      \n    # add header\n    header = [\"id\", \"title\"]\n    attribute_cols = list(reports[0].attributes.keys())\
      \ if reports[0].attributes else []\n    attribute_cols = [col for col in attribute_cols\
      \ if col not in header]\n    header.extend(attribute_cols)\n    header.append(\"\
      summary\" if use_community_summary else \"content\")\n    if include_community_rank:\n\
      \        header.append(\"rank\")\n\n    records = []\n    for report in reports:\n\
      \        new_record = [\n            report.short_id if report.short_id else\
      \ \"\",\n            report.title,\n            *[\n                str(report.attributes.get(field,\
      \ \"\"))\n                if report.attributes and report.attributes.get(field)\n\
      \                else \"\"\n                for field in attribute_cols\n  \
      \          ],\n        ]\n        new_record.append(\n            report.summary\
      \ if use_community_summary else report.full_content\n        )\n        if include_community_rank:\n\
      \            new_record.append(str(report.rank))\n        records.append(new_record)\n\
      \    return pd.DataFrame(records, columns=cast(\"Any\", header))"
    signature: "def to_community_report_dataframe(\n    reports: list[CommunityReport],\n\
      \    include_community_rank: bool = False,\n    use_community_summary: bool\
      \ = False,\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: list
      type: builtin
    - target: reports[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: header.append
      type: unresolved
    - target: str
      type: builtin
    - target: report.attributes.get
      type: unresolved
    - target: new_record.append
      type: unresolved
    - target: records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/input/retrieval/community_reports.py::to_community_report_dataframe
    called_by:
    - source: graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
      type: internal
- file_name: graphrag/query/input/retrieval/covariates.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  functions:
  - name: get_candidate_covariates
    start_line: 14
    end_line: 24
    code: "def get_candidate_covariates(\n    selected_entities: list[Entity],\n \
      \   covariates: list[Covariate],\n) -> list[Covariate]:\n    \"\"\"Get all covariates\
      \ that are related to selected entities.\"\"\"\n    selected_entity_names =\
      \ [entity.title for entity in selected_entities]\n    return [\n        covariate\n\
      \        for covariate in covariates\n        if covariate.subject_id in selected_entity_names\n\
      \    ]"
    signature: "def get_candidate_covariates(\n    selected_entities: list[Entity],\n\
      \    covariates: list[Covariate],\n) -> list[Covariate]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/input/retrieval/covariates.py::get_candidate_covariates
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
  - name: to_covariate_dataframe
    start_line: 27
    end_line: 53
    code: "def to_covariate_dataframe(covariates: list[Covariate]) -> pd.DataFrame:\n\
      \    \"\"\"Convert a list of covariates to a pandas dataframe.\"\"\"\n    if\
      \ len(covariates) == 0:\n        return pd.DataFrame()\n\n    # add header\n\
      \    header = [\"id\", \"entity\"]\n    attributes = covariates[0].attributes\
      \ or {} if len(covariates) > 0 else {}\n    attribute_cols = list(attributes.keys())\
      \ if len(covariates) > 0 else []\n    attribute_cols = [col for col in attribute_cols\
      \ if col not in header]\n    header.extend(attribute_cols)\n\n    records =\
      \ []\n    for covariate in covariates:\n        new_record = [\n           \
      \ covariate.short_id if covariate.short_id else \"\",\n            covariate.subject_id,\n\
      \        ]\n        for field in attribute_cols:\n            field_value =\
      \ (\n                str(covariate.attributes.get(field))\n                if\
      \ covariate.attributes and covariate.attributes.get(field)\n               \
      \ else \"\"\n            )\n            new_record.append(field_value)\n   \
      \     records.append(new_record)\n    return pd.DataFrame(records, columns=cast(\"\
      Any\", header))"
    signature: 'def to_covariate_dataframe(covariates: list[Covariate]) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: list
      type: builtin
    - target: attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: str
      type: builtin
    - target: covariate.attributes.get
      type: unresolved
    - target: new_record.append
      type: unresolved
    - target: records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/input/retrieval/covariates.py::to_covariate_dataframe
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
- file_name: graphrag/query/input/retrieval/entities.py
  imports:
  - module: uuid
    name: null
    alias: null
  - module: collections.abc
    name: Iterable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  functions:
  - name: get_entity_by_id
    start_line: 15
    end_line: 20
    code: "def get_entity_by_id(entities: dict[str, Entity], value: str) -> Entity\
      \ | None:\n    \"\"\"Get entity by id.\"\"\"\n    entity = entities.get(value)\n\
      \    if entity is None and is_valid_uuid(value):\n        entity = entities.get(value.replace(\"\
      -\", \"\"))\n    return entity"
    signature: 'def get_entity_by_id(entities: dict[str, Entity], value: str) -> Entity
      | None'
    decorators: []
    raises: []
    calls:
    - target: entities.get
      type: unresolved
    - target: graphrag/query/input/retrieval/entities.py::is_valid_uuid
      type: internal
    - target: value.replace
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_id
    called_by:
    - source: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
      type: internal
    - source: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_id
      type: internal
  - name: get_entity_by_key
    start_line: 23
    end_line: 37
    code: "def get_entity_by_key(\n    entities: Iterable[Entity], key: str, value:\
      \ str | int\n) -> Entity | None:\n    \"\"\"Get entity by key.\"\"\"\n    if\
      \ isinstance(value, str) and is_valid_uuid(value):\n        value_no_dashes\
      \ = value.replace(\"-\", \"\")\n        for entity in entities:\n          \
      \  entity_value = getattr(entity, key)\n            if entity_value in (value,\
      \ value_no_dashes):\n                return entity\n    else:\n        for entity\
      \ in entities:\n            if getattr(entity, key) == value:\n            \
      \    return entity\n    return None"
    signature: "def get_entity_by_key(\n    entities: Iterable[Entity], key: str,\
      \ value: str | int\n) -> Entity | None"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: graphrag/query/input/retrieval/entities.py::is_valid_uuid
      type: internal
    - target: value.replace
      type: unresolved
    - target: getattr
      type: builtin
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_key
    called_by:
    - source: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
      type: internal
    - source: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_key
      type: internal
  - name: get_entity_by_name
    start_line: 40
    end_line: 42
    code: "def get_entity_by_name(entities: Iterable[Entity], entity_name: str) ->\
      \ list[Entity]:\n    \"\"\"Get entities by name.\"\"\"\n    return [entity for\
      \ entity in entities if entity.title == entity_name]"
    signature: 'def get_entity_by_name(entities: Iterable[Entity], entity_name: str)
      -> list[Entity]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_name
    called_by:
    - source: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
      type: internal
  - name: get_entity_by_attribute
    start_line: 45
    end_line: 54
    code: "def get_entity_by_attribute(\n    entities: Iterable[Entity], attribute_name:\
      \ str, attribute_value: Any\n) -> list[Entity]:\n    \"\"\"Get entities by attribute.\"\
      \"\"\n    return [\n        entity\n        for entity in entities\n       \
      \ if entity.attributes\n        and entity.attributes.get(attribute_name) ==\
      \ attribute_value\n    ]"
    signature: "def get_entity_by_attribute(\n    entities: Iterable[Entity], attribute_name:\
      \ str, attribute_value: Any\n) -> list[Entity]"
    decorators: []
    raises: []
    calls:
    - target: entity.attributes.get
      type: unresolved
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_attribute
    called_by: []
  - name: to_entity_dataframe
    start_line: 57
    end_line: 92
    code: "def to_entity_dataframe(\n    entities: list[Entity],\n    include_entity_rank:\
      \ bool = True,\n    rank_description: str = \"number of relationships\",\n)\
      \ -> pd.DataFrame:\n    \"\"\"Convert a list of entities to a pandas dataframe.\"\
      \"\"\n    if len(entities) == 0:\n        return pd.DataFrame()\n    header\
      \ = [\"id\", \"entity\", \"description\"]\n    if include_entity_rank:\n   \
      \     header.append(rank_description)\n    attribute_cols = (\n        list(entities[0].attributes.keys())\
      \ if entities[0].attributes else []\n    )\n    attribute_cols = [col for col\
      \ in attribute_cols if col not in header]\n    header.extend(attribute_cols)\n\
      \n    records = []\n    for entity in entities:\n        new_record = [\n  \
      \          entity.short_id if entity.short_id else \"\",\n            entity.title,\n\
      \            entity.description if entity.description else \"\",\n        ]\n\
      \        if include_entity_rank:\n            new_record.append(str(entity.rank))\n\
      \n        for field in attribute_cols:\n            field_value = (\n      \
      \          str(entity.attributes.get(field))\n                if entity.attributes\
      \ and entity.attributes.get(field)\n                else \"\"\n            )\n\
      \            new_record.append(field_value)\n        records.append(new_record)\n\
      \    return pd.DataFrame(records, columns=cast(\"Any\", header))"
    signature: "def to_entity_dataframe(\n    entities: list[Entity],\n    include_entity_rank:\
      \ bool = True,\n    rank_description: str = \"number of relationships\",\n)\
      \ -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: header.append
      type: unresolved
    - target: list
      type: builtin
    - target: entities[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: new_record.append
      type: unresolved
    - target: str
      type: builtin
    - target: entity.attributes.get
      type: unresolved
    - target: records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::to_entity_dataframe
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
  - name: is_valid_uuid
    start_line: 95
    end_line: 102
    code: "def is_valid_uuid(value: str) -> bool:\n    \"\"\"Determine if a string\
      \ is a valid UUID.\"\"\"\n    try:\n        uuid.UUID(str(value))\n    except\
      \ ValueError:\n        return False\n    else:\n        return True"
    signature: 'def is_valid_uuid(value: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: uuid::UUID
      type: stdlib
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/query/input/retrieval/entities.py::is_valid_uuid
    called_by:
    - source: graphrag/query/input/retrieval/entities.py::get_entity_by_id
      type: internal
    - source: graphrag/query/input/retrieval/entities.py::get_entity_by_key
      type: internal
- file_name: graphrag/query/input/retrieval/relationships.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  functions:
  - name: get_in_network_relationships
    start_line: 14
    end_line: 31
    code: "def get_in_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]:\n    \"\"\"Get all directed relationships between\
      \ selected entities, sorted by ranking_attribute.\"\"\"\n    selected_entity_names\
      \ = [entity.title for entity in selected_entities]\n    selected_relationships\
      \ = [\n        relationship\n        for relationship in relationships\n   \
      \     if relationship.source in selected_entity_names\n        and relationship.target\
      \ in selected_entity_names\n    ]\n    if len(selected_relationships) <= 1:\n\
      \        return selected_relationships\n\n    # sort by ranking attribute\n\
      \    return sort_relationships_by_rank(selected_relationships, ranking_attribute)"
    signature: "def get_in_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
      type: internal
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
    called_by:
    - source: graphrag/query/context_builder/local_context.py::_filter_relationships
      type: internal
  - name: get_out_network_relationships
    start_line: 34
    end_line: 54
    code: "def get_out_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]:\n    \"\"\"Get relationships from selected entities\
      \ to other entities that are not within the selected entities, sorted by ranking_attribute.\"\
      \"\"\n    selected_entity_names = [entity.title for entity in selected_entities]\n\
      \    source_relationships = [\n        relationship\n        for relationship\
      \ in relationships\n        if relationship.source in selected_entity_names\n\
      \        and relationship.target not in selected_entity_names\n    ]\n    target_relationships\
      \ = [\n        relationship\n        for relationship in relationships\n   \
      \     if relationship.target in selected_entity_names\n        and relationship.source\
      \ not in selected_entity_names\n    ]\n    selected_relationships = source_relationships\
      \ + target_relationships\n    return sort_relationships_by_rank(selected_relationships,\
      \ ranking_attribute)"
    signature: "def get_out_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
      type: internal
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
    called_by:
    - source: graphrag/query/context_builder/local_context.py::_filter_relationships
      type: internal
  - name: get_candidate_relationships
    start_line: 57
    end_line: 68
    code: "def get_candidate_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n) -> list[Relationship]:\n    \"\"\"\
      Get all relationships that are associated with the selected entities.\"\"\"\n\
      \    selected_entity_names = [entity.title for entity in selected_entities]\n\
      \    return [\n        relationship\n        for relationship in relationships\n\
      \        if relationship.source in selected_entity_names\n        or relationship.target\
      \ in selected_entity_names\n    ]"
    signature: "def get_candidate_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n) -> list[Relationship]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::get_candidate_relationships
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
  - name: get_entities_from_relationships
    start_line: 71
    end_line: 78
    code: "def get_entities_from_relationships(\n    relationships: list[Relationship],\
      \ entities: list[Entity]\n) -> list[Entity]:\n    \"\"\"Get all entities that\
      \ are associated with the selected relationships.\"\"\"\n    selected_entity_names\
      \ = [relationship.source for relationship in relationships] + [\n        relationship.target\
      \ for relationship in relationships\n    ]\n    return [entity for entity in\
      \ entities if entity.title in selected_entity_names]"
    signature: "def get_entities_from_relationships(\n    relationships: list[Relationship],\
      \ entities: list[Entity]\n) -> list[Entity]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::get_entities_from_relationships
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
  - name: sort_relationships_by_rank
    start_line: 81
    end_line: 102
    code: "def sort_relationships_by_rank(\n    relationships: list[Relationship],\n\
      \    ranking_attribute: str = \"rank\",\n) -> list[Relationship]:\n    \"\"\"\
      Sort relationships by a ranking_attribute.\"\"\"\n    if len(relationships)\
      \ == 0:\n        return relationships\n\n    # sort by ranking attribute\n \
      \   attribute_names = (\n        list(relationships[0].attributes.keys()) if\
      \ relationships[0].attributes else []\n    )\n    if ranking_attribute in attribute_names:\n\
      \        relationships.sort(\n            key=lambda x: int(x.attributes[ranking_attribute])\
      \ if x.attributes else 0,\n            reverse=True,\n        )\n    elif ranking_attribute\
      \ == \"rank\":\n        relationships.sort(key=lambda x: x.rank if x.rank else\
      \ 0.0, reverse=True)\n    elif ranking_attribute == \"weight\":\n        relationships.sort(key=lambda\
      \ x: x.weight if x.weight else 0.0, reverse=True)\n    return relationships"
    signature: "def sort_relationships_by_rank(\n    relationships: list[Relationship],\n\
      \    ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: list
      type: builtin
    - target: relationships[0].attributes.keys
      type: unresolved
    - target: relationships.sort
      type: unresolved
    - target: int
      type: builtin
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
    called_by:
    - source: graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
      type: internal
    - source: graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
      type: internal
  - name: to_relationship_dataframe
    start_line: 105
    end_line: 139
    code: "def to_relationship_dataframe(\n    relationships: list[Relationship],\
      \ include_relationship_weight: bool = True\n) -> pd.DataFrame:\n    \"\"\"Convert\
      \ a list of relationships to a pandas dataframe.\"\"\"\n    if len(relationships)\
      \ == 0:\n        return pd.DataFrame()\n\n    header = [\"id\", \"source\",\
      \ \"target\", \"description\"]\n    if include_relationship_weight:\n      \
      \  header.append(\"weight\")\n    attribute_cols = (\n        list(relationships[0].attributes.keys())\
      \ if relationships[0].attributes else []\n    )\n    attribute_cols = [col for\
      \ col in attribute_cols if col not in header]\n    header.extend(attribute_cols)\n\
      \n    records = []\n    for rel in relationships:\n        new_record = [\n\
      \            rel.short_id if rel.short_id else \"\",\n            rel.source,\n\
      \            rel.target,\n            rel.description if rel.description else\
      \ \"\",\n        ]\n        if include_relationship_weight:\n            new_record.append(str(rel.weight\
      \ if rel.weight else \"\"))\n        for field in attribute_cols:\n        \
      \    field_value = (\n                str(rel.attributes.get(field))\n     \
      \           if rel.attributes and rel.attributes.get(field)\n              \
      \  else \"\"\n            )\n            new_record.append(field_value)\n  \
      \      records.append(new_record)\n    return pd.DataFrame(records, columns=cast(\"\
      Any\", header))"
    signature: "def to_relationship_dataframe(\n    relationships: list[Relationship],\
      \ include_relationship_weight: bool = True\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: header.append
      type: unresolved
    - target: list
      type: builtin
    - target: relationships[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: new_record.append
      type: unresolved
    - target: str
      type: builtin
    - target: rel.attributes.get
      type: unresolved
    - target: records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/input/retrieval/relationships.py::to_relationship_dataframe
    called_by:
    - source: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
- file_name: graphrag/query/input/retrieval/text_units.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  functions:
  - name: get_candidate_text_units
    start_line: 14
    end_line: 24
    code: "def get_candidate_text_units(\n    selected_entities: list[Entity],\n \
      \   text_units: list[TextUnit],\n) -> pd.DataFrame:\n    \"\"\"Get all text\
      \ units that are associated to selected entities.\"\"\"\n    selected_text_ids\
      \ = [\n        entity.text_unit_ids for entity in selected_entities if entity.text_unit_ids\n\
      \    ]\n    selected_text_ids = [item for sublist in selected_text_ids for item\
      \ in sublist]\n    selected_text_units = [unit for unit in text_units if unit.id\
      \ in selected_text_ids]\n    return to_text_unit_dataframe(selected_text_units)"
    signature: "def get_candidate_text_units(\n    selected_entities: list[Entity],\n\
      \    text_units: list[TextUnit],\n) -> pd.DataFrame"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/text_units.py::to_text_unit_dataframe
      type: internal
    visibility: public
    node_id: graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
    called_by:
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
      type: internal
  - name: to_text_unit_dataframe
    start_line: 27
    end_line: 53
    code: "def to_text_unit_dataframe(text_units: list[TextUnit]) -> pd.DataFrame:\n\
      \    \"\"\"Convert a list of text units to a pandas dataframe.\"\"\"\n    if\
      \ len(text_units) == 0:\n        return pd.DataFrame()\n\n    # add header\n\
      \    header = [\"id\", \"text\"]\n    attribute_cols = (\n        list(text_units[0].attributes.keys())\
      \ if text_units[0].attributes else []\n    )\n    attribute_cols = [col for\
      \ col in attribute_cols if col not in header]\n    header.extend(attribute_cols)\n\
      \n    records = []\n    for unit in text_units:\n        new_record = [\n  \
      \          unit.short_id,\n            unit.text,\n            *[\n        \
      \        str(unit.attributes.get(field, \"\"))\n                if unit.attributes\
      \ and unit.attributes.get(field)\n                else \"\"\n              \
      \  for field in attribute_cols\n            ],\n        ]\n        records.append(new_record)\n\
      \    return pd.DataFrame(records, columns=cast(\"Any\", header))"
    signature: 'def to_text_unit_dataframe(text_units: list[TextUnit]) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: list
      type: builtin
    - target: text_units[0].attributes.keys
      type: unresolved
    - target: header.extend
      type: unresolved
    - target: str
      type: builtin
    - target: unit.attributes.get
      type: unresolved
    - target: records.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    visibility: public
    node_id: graphrag/query/input/retrieval/text_units.py::to_text_unit_dataframe
    called_by:
    - source: graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
      type: internal
- file_name: graphrag/query/llm/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/llm/text_utils.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: itertools
    name: islice
    alias: null
  - module: json_repair
    name: repair_json
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: batched
    start_line: 21
    end_line: 33
    code: "def batched(iterable: Iterator, n: int):\n    \"\"\"\n    Batch data into\
      \ tuples of length n. The last batch may be shorter.\n\n    Taken from Python's\
      \ cookbook: https://docs.python.org/3/library/itertools.html#itertools.batched\n\
      \    \"\"\"\n    # batched('ABCDEFG', 3) --> ABC DEF G\n    if n < 1:\n    \
      \    value_error = \"n must be at least one\"\n        raise ValueError(value_error)\n\
      \    it = iter(iterable)\n    while batch := tuple(islice(it, n)):\n       \
      \ yield batch"
    signature: 'def batched(iterable: Iterator, n: int)'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: iter
      type: builtin
    - target: tuple
      type: builtin
    - target: itertools::islice
      type: stdlib
    visibility: public
    node_id: graphrag/query/llm/text_utils.py::batched
    called_by:
    - source: graphrag/query/llm/text_utils.py::chunk_text
      type: internal
  - name: chunk_text
    start_line: 36
    end_line: 42
    code: "def chunk_text(text: str, max_tokens: int, tokenizer: Tokenizer | None\
      \ = None):\n    \"\"\"Chunk text by token length.\"\"\"\n    if tokenizer is\
      \ None:\n        tokenizer = get_tokenizer(encoding_model=defs.ENCODING_MODEL)\n\
      \    tokens = tokenizer.encode(text)  # type: ignore\n    chunk_iterator = batched(iter(tokens),\
      \ max_tokens)\n    yield from (tokenizer.decode(list(chunk)) for chunk in chunk_iterator)"
    signature: 'def chunk_text(text: str, max_tokens: int, tokenizer: Tokenizer |
      None = None)'
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: tokenizer.encode
      type: unresolved
    - target: graphrag/query/llm/text_utils.py::batched
      type: internal
    - target: iter
      type: builtin
    - target: tokenizer.decode
      type: unresolved
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/query/llm/text_utils.py::chunk_text
    called_by: []
  - name: try_parse_json_object
    start_line: 45
    end_line: 103
    code: "def try_parse_json_object(input: str, verbose: bool = True) -> tuple[str,\
      \ dict]:\n    \"\"\"JSON cleaning and formatting utilities.\"\"\"\n    # Sometimes,\
      \ the LLM returns a json string with some extra description, this function will\
      \ clean it up.\n\n    result = None\n    try:\n        # Try parse first\n \
      \       result = json.loads(input)\n    except json.JSONDecodeError:\n     \
      \   if verbose:\n            logger.warning(\"Error decoding faulty json, attempting\
      \ repair\")\n\n    if result:\n        return input, result\n\n    pattern =\
      \ r\"\\{(.*)\\}\"\n    match = re.search(pattern, input, re.DOTALL)\n    input\
      \ = \"{\" + match.group(1) + \"}\" if match else input\n\n    # Clean up json\
      \ string.\n    input = (\n        input.replace(\"{{\", \"{\")\n        .replace(\"\
      }}\", \"}\")\n        .replace('\"[{', \"[{\")\n        .replace('}]\"', \"\
      }]\")\n        .replace(\"\\\\\", \" \")\n        .replace(\"\\\\n\", \" \"\
      )\n        .replace(\"\\n\", \" \")\n        .replace(\"\\r\", \"\")\n     \
      \   .strip()\n    )\n\n    # Remove JSON Markdown Frame\n    if input.startswith(\"\
      ```json\"):\n        input = input[len(\"```json\") :]\n    if input.endswith(\"\
      ```\"):\n        input = input[: len(input) - len(\"```\")]\n\n    try:\n  \
      \      result = json.loads(input)\n    except json.JSONDecodeError:\n      \
      \  # Fixup potentially malformed json string using json_repair.\n        input\
      \ = str(repair_json(json_str=input, return_objects=False))\n\n        # Generate\
      \ JSON-string output using best-attempt prompting & parsing techniques.\n  \
      \      try:\n            result = json.loads(input)\n        except json.JSONDecodeError:\n\
      \            if verbose:\n                logger.exception(\"error loading json,\
      \ json=%s\", input)\n            return input, {}\n        else:\n         \
      \   if not isinstance(result, dict):\n                if verbose:\n        \
      \            logger.exception(\"not expected dict type. type=%s:\", type(result))\n\
      \                return input, {}\n            return input, result\n    else:\n\
      \        return input, result"
    signature: 'def try_parse_json_object(input: str, verbose: bool = True) -> tuple[str,
      dict]'
    decorators: []
    raises: []
    calls:
    - target: json::loads
      type: stdlib
    - target: logger.warning
      type: unresolved
    - target: re::search
      type: stdlib
    - target: match.group
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace('}]\"', \"}]\")\n       \
        \ .replace(\"\\\\\", \" \")\n        .replace(\"\\\\n\", \" \")\n        .replace(\"\
        \\n\", \" \")\n        .replace(\"\\r\", \"\")\n        .strip"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace('}]\"', \"}]\")\n       \
        \ .replace(\"\\\\\", \" \")\n        .replace(\"\\\\n\", \" \")\n        .replace(\"\
        \\n\", \" \")\n        .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace('}]\"', \"}]\")\n       \
        \ .replace(\"\\\\\", \" \")\n        .replace(\"\\\\n\", \" \")\n        .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace('}]\"', \"}]\")\n       \
        \ .replace(\"\\\\\", \" \")\n        .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace('}]\"', \"}]\")\n       \
        \ .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace('\"[{', \"[{\")\n        .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace(\"}}\", \"}\")\n   \
        \     .replace"
      type: unresolved
    - target: "input.replace(\"{{\", \"{\")\n        .replace"
      type: unresolved
    - target: input.replace
      type: unresolved
    - target: input.startswith
      type: unresolved
    - target: len
      type: builtin
    - target: input.endswith
      type: unresolved
    - target: str
      type: builtin
    - target: json_repair::repair_json
      type: external
    - target: logger.exception
      type: unresolved
    - target: isinstance
      type: builtin
    - target: type
      type: builtin
    visibility: public
    node_id: graphrag/query/llm/text_utils.py::try_parse_json_object
    called_by:
    - source: graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
      type: internal
    - source: graphrag/query/structured_search/drift_search/action.py::DriftAction.search
      type: internal
    - source: graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
      type: internal
- file_name: graphrag/query/question_gen/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/question_gen/base.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.query.context_builder.builders
    name: GlobalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.builders
    name: LocalContextBuilder
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 33
    end_line: 45
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    ):\n       \
      \ self.model = model\n        self.context_builder = context_builder\n     \
      \   self.tokenizer = tokenizer or get_tokenizer(model.config)\n        self.model_params\
      \ = model_params or {}\n        self.context_builder_params = context_builder_params\
      \ or {}"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
    called_by: []
  - name: generate
    start_line: 48
    end_line: 55
    code: "async def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult:\n        \"\"\"Generate questions.\"\"\""
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.generate
    called_by: []
  - name: agenerate
    start_line: 58
    end_line: 65
    code: "async def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult:\n        \"\"\"Generate questions asynchronously.\"\
      \"\""
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.agenerate
    called_by: []
- file_name: graphrag/query/question_gen/local_gen.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: graphrag.callbacks.llm_callbacks
    name: BaseLLMCallback
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.query.question_gen_system_prompt
    name: QUESTION_SYSTEM_PROMPT
    alias: null
  - module: graphrag.query.context_builder.builders
    name: ContextBuilderResult
    alias: null
  - module: graphrag.query.context_builder.builders
    name: LocalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.question_gen.base
    name: BaseQuestionGen
    alias: null
  - module: graphrag.query.question_gen.base
    name: QuestionResult
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 29
    end_line: 47
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback]\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    ):\n       \
      \ super().__init__(\n            model=model,\n            context_builder=context_builder,\n\
      \            tokenizer=tokenizer,\n            model_params=model_params,\n\
      \            context_builder_params=context_builder_params,\n        )\n   \
      \     self.system_prompt = system_prompt\n        self.callbacks = callbacks\
      \ or []"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback]\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.__init__
    called_by: []
  - name: agenerate
    start_line: 49
    end_line: 130
    code: "async def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult:\n        \"\"\"\n        Generate a question based\
      \ on the question history and context data.\n\n        If context data is not\
      \ provided, it will be generated by the local context builder\n        \"\"\"\
      \n        start_time = time.time()\n\n        if len(question_history) == 0:\n\
      \            question_text = \"\"\n            conversation_history = None\n\
      \        else:\n            # construct current query and conversation history\n\
      \            question_text = question_history[-1]\n            history = [\n\
      \                {\"role\": \"user\", \"content\": query} for query in question_history[:-1]\n\
      \            ]\n            conversation_history = ConversationHistory.from_list(history)\n\
      \n        if context_data is None:\n            # generate context data based\
      \ on the question history\n            result = cast(\n                \"ContextBuilderResult\"\
      ,\n                self.context_builder.build_context(\n                   \
      \ query=question_text,\n                    conversation_history=conversation_history,\n\
      \                    **kwargs,\n                    **self.context_builder_params,\n\
      \                ),\n            )\n            context_data = cast(\"str\"\
      , result.context_chunks)\n            context_records = result.context_records\n\
      \        else:\n            context_records = {\"context_data\": context_data}\n\
      \        logger.debug(\n            \"GENERATE QUESTION: %s. LAST QUESTION:\
      \ %s\", start_time, question_text\n        )\n        system_prompt = \"\"\n\
      \        try:\n            system_prompt = self.system_prompt.format(\n    \
      \            context_data=context_data, question_count=question_count\n    \
      \        )\n            question_messages = [\n                {\"role\": \"\
      system\", \"content\": system_prompt},\n            ]\n\n            response\
      \ = \"\"\n            async for chunk in self.model.achat_stream(\n        \
      \        prompt=question_text,\n                history=question_messages,\n\
      \                model_parameters=self.model_params,\n            ):\n     \
      \           response += chunk\n                for callback in self.callbacks:\n\
      \                    callback.on_llm_new_token(chunk)\n\n            return\
      \ QuestionResult(\n                response=response.split(\"\\n\"),\n     \
      \           context_data={\n                    \"question_context\": question_text,\n\
      \                    **context_records,\n                },\n              \
      \  completion_time=time.time() - start_time,\n                llm_calls=1,\n\
      \                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n \
      \           )\n\n        except Exception:\n            logger.exception(\"\
      Exception in generating question\")\n            return QuestionResult(\n  \
      \              response=[],\n                context_data=context_records,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n\
      \            )"
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: len
      type: builtin
    - target: graphrag/query/context_builder/conversation_history.py::ConversationHistory::from_list
      type: external
    - target: typing::cast
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    - target: graphrag/query/question_gen/base.py::QuestionResult
      type: internal
    - target: response.split
      type: unresolved
    - target: self.tokenizer.num_tokens
      type: instance
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.agenerate
    called_by: []
  - name: generate
    start_line: 132
    end_line: 213
    code: "async def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult:\n        \"\"\"\n        Generate a question based\
      \ on the question history and context data.\n\n        If context data is not\
      \ provided, it will be generated by the local context builder\n        \"\"\"\
      \n        start_time = time.time()\n        if len(question_history) == 0:\n\
      \            question_text = \"\"\n            conversation_history = None\n\
      \        else:\n            # construct current query and conversation history\n\
      \            question_text = question_history[-1]\n            history = [\n\
      \                {\"role\": \"user\", \"content\": query} for query in question_history[:-1]\n\
      \            ]\n            conversation_history = ConversationHistory.from_list(history)\n\
      \n        if context_data is None:\n            # generate context data based\
      \ on the question history\n            result = cast(\n                \"ContextBuilderResult\"\
      ,\n                self.context_builder.build_context(\n                   \
      \ query=question_text,\n                    conversation_history=conversation_history,\n\
      \                    **kwargs,\n                    **self.context_builder_params,\n\
      \                ),\n            )\n            context_data = cast(\"str\"\
      , result.context_chunks)\n            context_records = result.context_records\n\
      \        else:\n            context_records = {\"context_data\": context_data}\n\
      \        logger.debug(\n            \"GENERATE QUESTION: %s. QUESTION HISTORY:\
      \ %s\", start_time, question_text\n        )\n        system_prompt = \"\"\n\
      \        try:\n            system_prompt = self.system_prompt.format(\n    \
      \            context_data=context_data, question_count=question_count\n    \
      \        )\n            question_messages = [\n                {\"role\": \"\
      system\", \"content\": system_prompt},\n                {\"role\": \"user\"\
      , \"content\": question_text},\n            ]\n\n            response = \"\"\
      \n            async for chunk in self.model.achat_stream(\n                prompt=question_text,\n\
      \                history=question_messages,\n                model_parameters=self.model_params,\n\
      \            ):\n                response += chunk\n                for callback\
      \ in self.callbacks:\n                    callback.on_llm_new_token(chunk)\n\
      \n            return QuestionResult(\n                response=response.split(\"\
      \\n\"),\n                context_data={\n                    \"question_context\"\
      : question_text,\n                    **context_records,\n                },\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n\
      \            )\n\n        except Exception:\n            logger.exception(\"\
      Exception in generating questions\")\n            return QuestionResult(\n \
      \               response=[],\n                context_data=context_records,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=self.tokenizer.num_tokens(system_prompt),\n\
      \            )"
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: len
      type: builtin
    - target: graphrag/query/context_builder/conversation_history.py::ConversationHistory::from_list
      type: external
    - target: typing::cast
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    - target: graphrag/query/question_gen/base.py::QuestionResult
      type: internal
    - target: response.split
      type: unresolved
    - target: self.tokenizer.num_tokens
      type: instance
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.generate
    called_by: []
- file_name: graphrag/query/structured_search/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/structured_search/base.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: Generic
    alias: null
  - module: typing
    name: TypeVar
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.query.context_builder.builders
    name: BasicContextBuilder
    alias: null
  - module: graphrag.query.context_builder.builders
    name: DRIFTContextBuilder
    alias: null
  - module: graphrag.query.context_builder.builders
    name: GlobalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.builders
    name: LocalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 58
    end_line: 70
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
      \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None\
      \ = None,\n    ):\n        self.model = model\n        self.context_builder\
      \ = context_builder\n        self.tokenizer = tokenizer or get_tokenizer()\n\
      \        self.model_params = model_params or {}\n        self.context_builder_params\
      \ = context_builder_params or {}"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
      \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None\
      \ = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/base.py::BaseSearch.__init__
    called_by: []
  - name: search
    start_line: 73
    end_line: 81
    code: "async def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult:\n\
      \        \"\"\"Search for the given query asynchronously.\"\"\"\n        msg\
      \ = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)"
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/base.py::BaseSearch.search
    called_by: []
  - name: stream_search
    start_line: 84
    end_line: 92
    code: "async def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]:\n\
      \        \"\"\"Stream search for the given query.\"\"\"\n        yield \"\"\
      \  # This makes it an async generator.\n        msg = \"Subclasses must implement\
      \ this method\"\n        raise NotImplementedError(msg)"
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/base.py::BaseSearch.stream_search
    called_by: []
- file_name: graphrag/query/structured_search/basic_search/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/structured_search/basic_search/basic_context.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.query.context_builder.builders
    name: BasicContextBuilder
    alias: null
  - module: graphrag.query.context_builder.builders
    name: ContextBuilderResult
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: __init__
    start_line: 28
    end_line: 41
    code: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
      \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
      \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = \"id\",\n    ):\n        self.text_embedder = text_embedder\n      \
      \  self.tokenizer = tokenizer or get_tokenizer()\n        self.text_units =\
      \ text_units\n        self.text_unit_embeddings = text_unit_embeddings\n   \
      \     self.embedding_vectorstore_key = embedding_vectorstore_key\n        self.text_id_map\
      \ = self._map_ids()"
    signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
      \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
      \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = \"id\",\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/structured_search/basic_search/basic_context.py::_map_ids
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
    called_by: []
  - name: build_context
    start_line: 43
    end_line: 105
    code: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
      \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
      \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col:\
      \ str = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult:\n     \
      \   \"\"\"Build the context for the basic search mode.\"\"\"\n        if query\
      \ != \"\":\n            related_texts = self.text_unit_embeddings.similarity_search_by_text(\n\
      \                text=query,\n                text_embedder=lambda t: self.text_embedder.embed(t),\n\
      \                k=k,\n            )\n            related_text_list = [\n  \
      \              {\n                    text_id_col: self.text_id_map[f\"{chunk.document.id}\"\
      ],\n                    text_col: chunk.document.text,\n                }\n\
      \                for chunk in related_texts\n            ]\n            related_text_df\
      \ = pd.DataFrame(related_text_list)\n        else:\n            related_text_df\
      \ = pd.DataFrame({\n                text_id_col: [],\n                text_col:\
      \ [],\n            })\n\n        # add these related text chunks into context\
      \ until we fill up the context window\n        current_tokens = 0\n        text_ids\
      \ = []\n        current_tokens = len(\n            self.tokenizer.encode(text_id_col\
      \ + column_delimiter + text_col + \"\\n\")\n        )\n        for i, row in\
      \ related_text_df.iterrows():\n            text = row[text_id_col] + column_delimiter\
      \ + row[text_col] + \"\\n\"\n            tokens = len(self.tokenizer.encode(text))\n\
      \            if current_tokens + tokens > max_context_tokens:\n            \
      \    msg = f\"Reached token limit: {current_tokens + tokens}. Reverting to previous\
      \ context state\"\n                logger.warning(msg)\n                break\n\
      \n            current_tokens += tokens\n            text_ids.append(i)\n   \
      \     final_text_df = cast(\n            \"pd.DataFrame\",\n            related_text_df[related_text_df.index.isin(text_ids)].reset_index(\n\
      \                drop=True\n            ),\n        )\n        final_text =\
      \ final_text_df.to_csv(\n            index=False, escapechar=\"\\\\\", sep=column_delimiter\n\
      \        )\n\n        return ContextBuilderResult(\n            context_chunks=final_text,\n\
      \            context_records={context_name: final_text_df},\n        )"
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
      \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
      \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col:\
      \ str = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult"
    decorators: []
    raises: []
    calls:
    - target: self.text_unit_embeddings.similarity_search_by_text
      type: instance
    - target: self.text_embedder.embed
      type: instance
    - target: pandas::DataFrame
      type: external
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: related_text_df.iterrows
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: text_ids.append
      type: unresolved
    - target: typing::cast
      type: stdlib
    - target: related_text_df[related_text_df.index.isin(text_ids)].reset_index
      type: unresolved
    - target: related_text_df.index.isin
      type: unresolved
    - target: final_text_df.to_csv
      type: unresolved
    - target: graphrag/query/context_builder/builders.py::ContextBuilderResult
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.build_context
    called_by: []
  - name: _map_ids
    start_line: 107
    end_line: 113
    code: "def _map_ids(self) -> dict[str, str]:\n        \"\"\"Map id to short id\
      \ in the text units.\"\"\"\n        id_map = {}\n        text_units = self.text_units\
      \ or []\n        for unit in text_units:\n            id_map[unit.id] = unit.short_id\n\
      \        return id_map"
    signature: def _map_ids(self) -> dict[str, str]
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext._map_ids
    called_by: []
- file_name: graphrag/query/structured_search/basic_search/search.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.query.basic_search_system_prompt
    name: BASIC_SEARCH_SYSTEM_PROMPT
    alias: null
  - module: graphrag.query.context_builder.builders
    name: BasicContextBuilder
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.structured_search.base
    name: BaseSearch
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 30
    end_line: 50
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    ):\n        super().__init__(\n            model=model,\n\
      \            context_builder=context_builder,\n            tokenizer=tokenizer,\n\
      \            model_params=model_params,\n            context_builder_params=context_builder_params\
      \ or {},\n        )\n        self.system_prompt = system_prompt or BASIC_SEARCH_SYSTEM_PROMPT\n\
      \        self.callbacks = callbacks or []\n        self.response_type = response_type"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.__init__
    called_by: []
  - name: search
    start_line: 52
    end_line: 127
    code: "async def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult:\n\
      \        \"\"\"Build rag search context that fits a single context window and\
      \ generate answer for the user query.\"\"\"\n        start_time = time.time()\n\
      \        search_prompt = \"\"\n        llm_calls, prompt_tokens, output_tokens\
      \ = {}, {}, {}\n\n        context_result = self.context_builder.build_context(\n\
      \            query=query,\n            conversation_history=conversation_history,\n\
      \            **kwargs,\n            **self.context_builder_params,\n       \
      \ )\n\n        llm_calls[\"build_context\"] = context_result.llm_calls\n   \
      \     prompt_tokens[\"build_context\"] = context_result.prompt_tokens\n    \
      \    output_tokens[\"build_context\"] = context_result.output_tokens\n\n   \
      \     logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n\
      \        try:\n            search_prompt = self.system_prompt.format(\n    \
      \            context_data=context_result.context_chunks,\n                response_type=self.response_type,\n\
      \            )\n            search_messages = [\n                {\"role\":\
      \ \"system\", \"content\": search_prompt},\n            ]\n\n            response\
      \ = \"\"\n            async for chunk in self.model.achat_stream(\n        \
      \        prompt=query,\n                history=search_messages,\n         \
      \       model_parameters=self.model_params,\n            ):\n              \
      \  for callback in self.callbacks:\n                    callback.on_llm_new_token(chunk)\n\
      \                response += chunk\n\n            llm_calls[\"response\"] =\
      \ 1\n            prompt_tokens[\"response\"] = len(self.tokenizer.encode(search_prompt))\n\
      \            output_tokens[\"response\"] = len(self.tokenizer.encode(response))\n\
      \n            for callback in self.callbacks:\n                callback.on_context(context_result.context_records)\n\
      \n            return SearchResult(\n                response=response,\n   \
      \             context_data=context_result.context_records,\n               \
      \ context_text=context_result.context_chunks,\n                completion_time=time.time()\
      \ - start_time,\n                llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=sum(output_tokens.values()),\n              \
      \  llm_calls_categories=llm_calls,\n                prompt_tokens_categories=prompt_tokens,\n\
      \                output_tokens_categories=output_tokens,\n            )\n\n\
      \        except Exception:\n            logger.exception(\"Exception in _asearch\"\
      )\n            return SearchResult(\n                response=\"\",\n      \
      \          context_data=context_result.context_records,\n                context_text=context_result.context_chunks,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=0,\n                llm_calls_categories=llm_calls,\n\
      \                prompt_tokens_categories=prompt_tokens,\n                output_tokens_categories=output_tokens,\n\
      \            )"
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: callback.on_context
      type: unresolved
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: sum
      type: builtin
    - target: output_tokens.values
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.search
    called_by: []
  - name: stream_search
    start_line: 129
    end_line: 160
    code: "async def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]:\n\
      \        \"\"\"Build basic search context that fits a single context window\
      \ and generate answer for the user query.\"\"\"\n        start_time = time.time()\n\
      \n        context_result = self.context_builder.build_context(\n           \
      \ query=query,\n            conversation_history=conversation_history,\n   \
      \         **self.context_builder_params,\n        )\n        logger.debug(\"\
      GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n        search_prompt\
      \ = self.system_prompt.format(\n            context_data=context_result.context_chunks,\
      \ response_type=self.response_type\n        )\n        search_messages = [\n\
      \            {\"role\": \"system\", \"content\": search_prompt},\n        ]\n\
      \n        for callback in self.callbacks:\n            callback.on_context(context_result.context_records)\n\
      \n        async for chunk_response in self.model.achat_stream(\n           \
      \ prompt=query,\n            history=search_messages,\n            model_parameters=self.model_params,\n\
      \        ):\n            for callback in self.callbacks:\n                callback.on_llm_new_token(chunk_response)\n\
      \            yield chunk_response"
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: callback.on_context
      type: unresolved
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.stream_search
    called_by: []
- file_name: graphrag/query/structured_search/drift_search/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/structured_search/drift_search/action.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.query.llm.text_utils
    name: try_parse_json_object
    alias: null
  functions:
  - name: __init__
    start_line: 22
    end_line: 46
    code: "def __init__(\n        self,\n        query: str,\n        answer: str\
      \ | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
      \    ):\n        \"\"\"\n        Initialize the DriftAction with a query, optional\
      \ answer, and follow-up actions.\n\n        Args:\n            query (str):\
      \ The query for the action.\n            answer (Optional[str]): The answer\
      \ to the query, if available.\n            follow_ups (Optional[list[DriftAction]]):\
      \ A list of follow-up actions.\n        \"\"\"\n        self.query = query\n\
      \        self.answer: str | None = answer  # Corresponds to an 'intermediate_answer'\n\
      \        self.score: float | None = None\n        self.follow_ups: list[DriftAction]\
      \ = (\n            follow_ups if follow_ups is not None else []\n        )\n\
      \        self.metadata: dict[str, Any] = {\n            \"llm_calls\": 0,\n\
      \            \"prompt_tokens\": 0,\n            \"output_tokens\": 0,\n    \
      \    }"
    signature: "def __init__(\n        self,\n        query: str,\n        answer:\
      \ str | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
      \    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__init__
    called_by: []
  - name: is_complete
    start_line: 49
    end_line: 51
    code: "def is_complete(self) -> bool:\n        \"\"\"Check if the action is complete\
      \ (i.e., an answer is available).\"\"\"\n        return self.answer is not None"
    signature: def is_complete(self) -> bool
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.is_complete
    called_by: []
  - name: search
    start_line: 53
    end_line: 99
    code: "async def search(self, search_engine: Any, global_query: str, scorer: Any\
      \ = None):\n        \"\"\"\n        Execute an asynchronous search using the\
      \ search engine, and update the action with the results.\n\n        If a scorer\
      \ is provided, compute the score for the action.\n\n        Args:\n        \
      \    search_engine (Any): The search engine to execute the query.\n        \
      \    global_query (str): The global query string.\n            scorer (Any,\
      \ optional): Scorer to compute scores for the action.\n\n        Returns\n \
      \       -------\n        self : DriftAction\n            Updated action with\
      \ search results.\n        \"\"\"\n        if self.is_complete:\n          \
      \  logger.warning(\"Action already complete. Skipping search.\")\n         \
      \   return self\n\n        search_result = await search_engine.search(\n   \
      \         drift_query=global_query, query=self.query\n        )\n\n        #\
      \ Do not launch exception as it will roll up with other steps\n        # Instead\
      \ return an empty response and let score -inf handle it\n        _, response\
      \ = try_parse_json_object(search_result.response, verbose=False)\n\n       \
      \ self.answer = response.pop(\"response\", None)\n        self.score = float(response.pop(\"\
      score\", \"-inf\"))\n        self.metadata.update({\"context_data\": search_result.context_data})\n\
      \n        if self.answer is None:\n            logger.warning(\"No answer found\
      \ for query: %s\", self.query)\n\n        self.metadata[\"llm_calls\"] += 1\n\
      \        self.metadata[\"prompt_tokens\"] += search_result.prompt_tokens\n \
      \       self.metadata[\"output_tokens\"] += search_result.output_tokens\n\n\
      \        self.follow_ups = response.pop(\"follow_up_queries\", [])\n       \
      \ if not self.follow_ups:\n            logger.warning(\"No follow-up actions\
      \ found for response: %s\", response)\n\n        if scorer:\n            self.compute_score(scorer)\n\
      \n        return self"
    signature: 'def search(self, search_engine: Any, global_query: str, scorer: Any
      = None)'
    decorators: []
    raises: []
    calls:
    - target: logger.warning
      type: unresolved
    - target: search_engine.search
      type: unresolved
    - target: graphrag/query/llm/text_utils.py::try_parse_json_object
      type: internal
    - target: response.pop
      type: unresolved
    - target: float
      type: builtin
    - target: self.metadata.update
      type: instance
    - target: graphrag/query/structured_search/drift_search/action.py::compute_score
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.search
    called_by: []
  - name: compute_score
    start_line: 101
    end_line: 111
    code: "def compute_score(self, scorer: Any):\n        \"\"\"\n        Compute\
      \ the score for the action using the provided scorer.\n\n        Args:\n   \
      \         scorer (Any): The scorer to compute the score.\n        \"\"\"\n \
      \       score = scorer.compute_score(self.query, self.answer)\n        self.score\
      \ = (\n            score if score is not None else float(\"-inf\")\n       \
      \ )  # Default to -inf for sorting"
    signature: 'def compute_score(self, scorer: Any)'
    decorators: []
    raises: []
    calls:
    - target: scorer.compute_score
      type: unresolved
    - target: float
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.compute_score
    called_by: []
  - name: serialize
    start_line: 113
    end_line: 133
    code: "def serialize(self, include_follow_ups: bool = True) -> dict[str, Any]:\n\
      \        \"\"\"\n        Serialize the action to a dictionary.\n\n        Args:\n\
      \            include_follow_ups (bool): Whether to include follow-up actions\
      \ in the serialization.\n\n        Returns\n        -------\n        dict[str,\
      \ Any]\n            Serialized action as a dictionary.\n        \"\"\"\n   \
      \     data = {\n            \"query\": self.query,\n            \"answer\":\
      \ self.answer,\n            \"score\": self.score,\n            \"metadata\"\
      : self.metadata,\n        }\n        if include_follow_ups:\n            data[\"\
      follow_ups\"] = [action.serialize() for action in self.follow_ups]\n       \
      \ return data"
    signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str,
      Any]'
    decorators: []
    raises: []
    calls:
    - target: action.serialize
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.serialize
    called_by: []
  - name: deserialize
    start_line: 136
    end_line: 163
    code: "def deserialize(cls, data: dict[str, Any]) -> \"DriftAction\":\n      \
      \  \"\"\"\n        Deserialize the action from a dictionary.\n\n        Args:\n\
      \            data (dict[str, Any]): Serialized action data.\n\n        Returns\n\
      \        -------\n        DriftAction\n            A deserialized instance of\
      \ DriftAction.\n        \"\"\"\n        # Ensure 'query' exists in the data,\
      \ raise a ValueError if missing\n        query = data.get(\"query\")\n     \
      \   if query is None:\n            error_message = \"Missing 'query' key in\
      \ serialized data\"\n            raise ValueError(error_message)\n\n       \
      \ # Initialize the DriftAction\n        action = cls(query)\n        action.answer\
      \ = data.get(\"answer\")\n        action.score = data.get(\"score\")\n     \
      \   action.metadata = data.get(\"metadata\", {})\n\n        action.follow_ups\
      \ = [\n            cls.deserialize(fu_data) for fu_data in data.get(\"follow_up_queries\"\
      , [])\n        ]\n        return action"
    signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: data.get
      type: unresolved
    - target: ValueError
      type: builtin
    - target: cls
      type: unresolved
    - target: cls.deserialize
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.deserialize
    called_by: []
  - name: from_primer_response
    start_line: 166
    end_line: 208
    code: "def from_primer_response(\n        cls, query: str, response: str | dict[str,\
      \ Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\":\n        \"\"\"\n \
      \       Create a DriftAction from a DRIFTPrimer response.\n\n        Args:\n\
      \            query (str): The query string.\n            response (str | dict[str,\
      \ Any] | list[dict[str, Any]]): Primer response data.\n\n        Returns\n \
      \       -------\n        DriftAction\n            A new instance of DriftAction\
      \ based on the response.\n\n        Raises\n        ------\n        ValueError\n\
      \            If the response is not a dictionary or expected format.\n     \
      \   \"\"\"\n        if isinstance(response, dict):\n            action = cls(\n\
      \                query,\n                follow_ups=response.get(\"follow_up_queries\"\
      , []),\n                answer=response.get(\"intermediate_answer\"),\n    \
      \        )\n            action.score = response.get(\"score\")\n           \
      \ return action\n\n        # If response is a string, attempt to parse as JSON\n\
      \        if isinstance(response, str):\n            try:\n                parsed_response\
      \ = json.loads(response)\n                if isinstance(parsed_response, dict):\n\
      \                    return cls.from_primer_response(query, parsed_response)\n\
      \                error_message = \"Parsed response must be a dictionary.\"\n\
      \                raise ValueError(error_message)\n            except json.JSONDecodeError\
      \ as e:\n                error_message = f\"Failed to parse response string:\
      \ {e}. Parsed response must be a dictionary.\"\n                raise ValueError(error_message)\
      \ from e\n\n        error_message = f\"Unsupported response type: {type(response).__name__}.\
      \ Expected a dictionary or JSON string.\"\n        raise ValueError(error_message)"
    signature: "def from_primer_response(\n        cls, query: str, response: str\
      \ | dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: isinstance
      type: builtin
    - target: cls
      type: unresolved
    - target: response.get
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: cls.from_primer_response
      type: unresolved
    - target: ValueError
      type: builtin
    - target: type
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.from_primer_response
    called_by: []
  - name: __hash__
    start_line: 210
    end_line: 221
    code: "def __hash__(self) -> int:\n        \"\"\"\n        Allow DriftAction objects\
      \ to be hashable for use in networkx.MultiDiGraph.\n\n        Assumes queries\
      \ are unique.\n\n        Returns\n        -------\n        int\n           \
      \ Hash based on the query.\n        \"\"\"\n        return hash(self.query)"
    signature: def __hash__(self) -> int
    decorators: []
    raises: []
    calls:
    - target: hash
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__hash__
    called_by: []
  - name: __eq__
    start_line: 223
    end_line: 237
    code: "def __eq__(self, other: object) -> bool:\n        \"\"\"\n        Check\
      \ equality based on the query string.\n\n        Args:\n            other (Any):\
      \ Another object to compare with.\n\n        Returns\n        -------\n    \
      \    bool\n            True if the other object is a DriftAction with the same\
      \ query, False otherwise.\n        \"\"\"\n        if not isinstance(other,\
      \ DriftAction):\n            return False\n        return self.query == other.query"
    signature: 'def __eq__(self, other: object) -> bool'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__eq__
    called_by: []
- file_name: graphrag/query/structured_search/drift_search/drift_context.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: dataclasses
    name: asdict
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.drift_search_config
    name: DRIFTSearchConfig
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.prompts.query.drift_search_system_prompt
    name: DRIFT_LOCAL_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.drift_search_system_prompt
    name: DRIFT_REDUCE_PROMPT
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: EntityVectorStoreKey
    alias: null
  - module: graphrag.query.structured_search.base
    name: DRIFTContextBuilder
    alias: null
  - module: graphrag.query.structured_search.drift_search.primer
    name: PrimerQueryProcessor
    alias: null
  - module: graphrag.query.structured_search.local_search.mixed_context
    name: LocalSearchMixedContext
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: __init__
    start_line: 40
    end_line: 78
    code: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
      \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n    \
      \    reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None =\
      \ None,\n        local_system_prompt: str | None = None,\n        local_mixed_context:\
      \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str | None = None,\n    ):\n      \
      \  \"\"\"Initialize the DRIFT search context builder with necessary components.\"\
      \"\"\n        self.config = config or DRIFTSearchConfig()\n        self.model\
      \ = model\n        self.text_embedder = text_embedder\n        self.tokenizer\
      \ = tokenizer or get_tokenizer()\n        self.local_system_prompt = local_system_prompt\
      \ or DRIFT_LOCAL_SYSTEM_PROMPT\n        self.reduce_system_prompt = reduce_system_prompt\
      \ or DRIFT_REDUCE_PROMPT\n\n        self.entities = entities\n        self.entity_text_embeddings\
      \ = entity_text_embeddings\n        self.reports = reports\n        self.text_units\
      \ = text_units\n        self.relationships = relationships\n        self.covariates\
      \ = covariates\n        self.embedding_vectorstore_key = embedding_vectorstore_key\n\
      \n        self.response_type = response_type\n\n        self.local_mixed_context\
      \ = (\n            local_mixed_context or self.init_local_context_builder()\n\
      \        )"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
      \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n    \
      \    reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None =\
      \ None,\n        local_system_prompt: str | None = None,\n        local_mixed_context:\
      \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/drift_search_config.py::DRIFTSearchConfig
      type: internal
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/structured_search/drift_search/drift_context.py::init_local_context_builder
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
    called_by: []
  - name: init_local_context_builder
    start_line: 80
    end_line: 98
    code: "def init_local_context_builder(self) -> LocalSearchMixedContext:\n    \
      \    \"\"\"\n        Initialize the local search mixed context builder.\n\n\
      \        Returns\n        -------\n        LocalSearchMixedContext: Initialized\
      \ local context.\n        \"\"\"\n        return LocalSearchMixedContext(\n\
      \            community_reports=self.reports,\n            text_units=self.text_units,\n\
      \            entities=self.entities,\n            relationships=self.relationships,\n\
      \            covariates=self.covariates,\n            entity_text_embeddings=self.entity_text_embeddings,\n\
      \            embedding_vectorstore_key=self.embedding_vectorstore_key,\n   \
      \         text_embedder=self.text_embedder,\n            tokenizer=self.tokenizer,\n\
      \        )"
    signature: def init_local_context_builder(self) -> LocalSearchMixedContext
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.init_local_context_builder
    called_by: []
  - name: convert_reports_to_df
    start_line: 101
    end_line: 140
    code: "def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame:\n\
      \        \"\"\"\n        Convert a list of CommunityReport objects to a pandas\
      \ DataFrame.\n\n        Args\n        ----\n        reports : list[CommunityReport]\n\
      \            List of CommunityReport objects.\n\n        Returns\n        -------\n\
      \        pd.DataFrame: DataFrame with report data.\n\n        Raises\n     \
      \   ------\n        ValueError: If some reports are missing full content or\
      \ full content embeddings.\n        \"\"\"\n        report_df = pd.DataFrame([asdict(report)\
      \ for report in reports])\n        missing_content_error = \"Some reports are\
      \ missing full content.\"\n        missing_embedding_error = (\n           \
      \ \"Some reports are missing full content embeddings. {missing} out of {total}\"\
      \n        )\n\n        if (\n            \"full_content\" not in report_df.columns\n\
      \            or report_df[\"full_content\"].isna().sum() > 0\n        ):\n \
      \           raise ValueError(missing_content_error)\n\n        if (\n      \
      \      \"full_content_embedding\" not in report_df.columns\n            or report_df[\"\
      full_content_embedding\"].isna().sum() > 0\n        ):\n            raise ValueError(\n\
      \                missing_embedding_error.format(\n                    missing=report_df[\"\
      full_content_embedding\"].isna().sum(),\n                    total=len(report_df),\n\
      \                )\n            )\n        return report_df"
    signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
    decorators:
    - '@staticmethod'
    raises:
    - ValueError
    calls:
    - target: pandas::DataFrame
      type: external
    - target: dataclasses::asdict
      type: stdlib
    - target: report_df["full_content"].isna().sum
      type: unresolved
    - target: report_df["full_content"].isna
      type: unresolved
    - target: ValueError
      type: builtin
    - target: report_df["full_content_embedding"].isna().sum
      type: unresolved
    - target: report_df["full_content_embedding"].isna
      type: unresolved
    - target: missing_embedding_error.format
      type: unresolved
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.convert_reports_to_df
    called_by: []
  - name: check_query_doc_encodings
    start_line: 143
    end_line: 164
    code: "def check_query_doc_encodings(query_embedding: Any, embedding: Any) ->\
      \ bool:\n        \"\"\"\n        Check if the embeddings are compatible.\n\n\
      \        Args\n        ----\n        query_embedding : Any\n            Embedding\
      \ of the query.\n        embedding : Any\n            Embedding to compare against.\n\
      \n        Returns\n        -------\n        bool: True if embeddings match,\
      \ otherwise False.\n        \"\"\"\n        return (\n            query_embedding\
      \ is not None\n            and embedding is not None\n            and isinstance(query_embedding,\
      \ type(embedding))\n            and len(query_embedding) == len(embedding)\n\
      \            and isinstance(query_embedding[0], type(embedding[0]))\n      \
      \  )"
    signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
      -> bool'
    decorators:
    - '@staticmethod'
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: type
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.check_query_doc_encodings
    called_by: []
  - name: build_context
    start_line: 166
    end_line: 227
    code: "async def build_context(\n        self, query: str, **kwargs\n    ) ->\
      \ tuple[pd.DataFrame, dict[str, int]]:\n        \"\"\"\n        Build DRIFT\
      \ search context.\n\n        Args\n        ----\n        query : str\n     \
      \       Search query string.\n\n        Returns\n        -------\n        pd.DataFrame:\
      \ Top-k most similar documents.\n        dict[str, int]: Number of LLM calls,\
      \ and prompts and output tokens.\n\n        Raises\n        ------\n       \
      \ ValueError: If no community reports are available, or embeddings\n       \
      \ are incompatible.\n        \"\"\"\n        if self.reports is None:\n    \
      \        missing_reports_error = (\n                \"No community reports available.\
      \ Please provide a list of reports.\"\n            )\n            raise ValueError(missing_reports_error)\n\
      \n        query_processor = PrimerQueryProcessor(\n            chat_model=self.model,\n\
      \            text_embedder=self.text_embedder,\n            tokenizer=self.tokenizer,\n\
      \            reports=self.reports,\n        )\n\n        query_embedding, token_ct\
      \ = await query_processor(query)\n\n        report_df = self.convert_reports_to_df(self.reports)\n\
      \n        # Check compatibility between query embedding and document embeddings\n\
      \        if not self.check_query_doc_encodings(\n            query_embedding,\
      \ report_df[\"full_content_embedding\"].iloc[0]\n        ):\n            error_message\
      \ = (\n                \"Query and document embeddings are not compatible. \"\
      \n                \"Please ensure that the embeddings are of the same type and\
      \ length.\"\n            )\n            raise ValueError(error_message)\n\n\
      \        # Vectorized cosine similarity computation\n        query_norm = np.linalg.norm(query_embedding)\n\
      \        document_norms = np.linalg.norm(\n            report_df[\"full_content_embedding\"\
      ].to_list(), axis=1\n        )\n        dot_products = np.dot(\n           \
      \ np.vstack(report_df[\"full_content_embedding\"].to_list()), query_embedding\n\
      \        )\n        report_df[\"similarity\"] = dot_products / (document_norms\
      \ * query_norm)\n\n        # Sort by similarity and select top-k\n        top_k\
      \ = report_df.nlargest(self.config.drift_k_followups, \"similarity\")\n\n  \
      \      return top_k.loc[:, [\"short_id\", \"community_id\", \"full_content\"\
      ]], token_ct"
    signature: "def build_context(\n        self, query: str, **kwargs\n    ) -> tuple[pd.DataFrame,\
      \ dict[str, int]]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor
      type: internal
    - target: query_processor
      type: unresolved
    - target: graphrag/query/structured_search/drift_search/drift_context.py::convert_reports_to_df
      type: internal
    - target: graphrag/query/structured_search/drift_search/drift_context.py::check_query_doc_encodings
      type: internal
    - target: numpy::linalg.norm
      type: external
    - target: report_df["full_content_embedding"].to_list
      type: unresolved
    - target: numpy::dot
      type: external
    - target: numpy::vstack
      type: external
    - target: report_df.nlargest
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.build_context
    called_by: []
- file_name: graphrag/query/structured_search/drift_search/primer.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: secrets
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pandas
    name: null
    alias: pd
  - module: tqdm.asyncio
    name: tqdm_asyncio
    alias: null
  - module: graphrag.config.models.drift_search_config
    name: DRIFTSearchConfig
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.prompts.query.drift_search_system_prompt
    name: DRIFT_PRIMER_PROMPT
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 31
    end_line: 50
    code: "def __init__(\n        self,\n        chat_model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        reports: list[CommunityReport],\n        tokenizer:\
      \ Tokenizer | None = None,\n    ):\n        \"\"\"\n        Initialize the PrimerQueryProcessor.\n\
      \n        Args:\n            chat_llm (ChatOpenAI): The language model used\
      \ to process the query.\n            text_embedder (BaseTextEmbedding): The\
      \ text embedding model.\n            reports (list[CommunityReport]): List of\
      \ community reports.\n            tokenizer (Tokenizer, optional): Token encoder\
      \ for token counting.\n        \"\"\"\n        self.chat_model = chat_model\n\
      \        self.text_embedder = text_embedder\n        self.tokenizer = tokenizer\
      \ or get_tokenizer()\n        self.reports = reports"
    signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n   \
      \     text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
      \        tokenizer: Tokenizer | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
    called_by: []
  - name: expand_query
    start_line: 52
    end_line: 83
    code: "async def expand_query(self, query: str) -> tuple[str, dict[str, int]]:\n\
      \        \"\"\"\n        Expand the query using a random community report template.\n\
      \n        Args:\n            query (str): The original search query.\n\n   \
      \     Returns\n        -------\n        tuple[str, dict[str, int]]: Expanded\
      \ query text and the number of tokens used.\n        \"\"\"\n        template\
      \ = secrets.choice(self.reports).full_content  # nosec S311\n\n        prompt\
      \ = f\"\"\"Create a hypothetical answer to the following query: {query}\\n\\\
      n\n                  Format it to follow the structure of the template below:\\\
      n\\n\n                  {template}\\n\"\n                  Ensure that the hypothetical\
      \ answer does not reference new named entities that are not present in the original\
      \ query.\"\"\"\n\n        model_response = await self.chat_model.achat(prompt)\n\
      \        text = model_response.output.content\n\n        prompt_tokens = len(self.tokenizer.encode(prompt))\n\
      \        output_tokens = len(self.tokenizer.encode(text))\n        token_ct\
      \ = {\n            \"llm_calls\": 1,\n            \"prompt_tokens\": prompt_tokens,\n\
      \            \"output_tokens\": output_tokens,\n        }\n        if text ==\
      \ \"\":\n            logger.warning(\"Failed to generate expansion for query:\
      \ %s\", query)\n            return query, token_ct\n        return text, token_ct"
    signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
    decorators: []
    raises: []
    calls:
    - target: secrets::choice
      type: stdlib
    - target: self.chat_model.achat
      type: instance
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.expand_query
    called_by: []
  - name: __call__
    start_line: 85
    end_line: 98
    code: "async def __call__(self, query: str) -> tuple[list[float], dict[str, int]]:\n\
      \        \"\"\"\n        Call method to process the query, expand it, and embed\
      \ the result.\n\n        Args:\n            query (str): The search query.\n\
      \n        Returns\n        -------\n        tuple[list[float], int]: List of\
      \ embeddings for the expanded query and the token count.\n        \"\"\"\n \
      \       hyde_query, token_ct = await self.expand_query(query)\n        logger.debug(\"\
      Expanded query: %s\", hyde_query)\n        return self.text_embedder.embed(hyde_query),\
      \ token_ct"
    signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/structured_search/drift_search/primer.py::expand_query
      type: internal
    - target: logger.debug
      type: unresolved
    - target: self.text_embedder.embed
      type: instance
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__call__
    called_by: []
  - name: __init__
    start_line: 104
    end_line: 120
    code: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n    \
      \    chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n \
      \   ):\n        \"\"\"\n        Initialize the DRIFTPrimer.\n\n        Args:\n\
      \            config (DRIFTSearchConfig): Configuration settings for DRIFT search.\n\
      \            chat_llm (ChatOpenAI): The language model used for searching.\n\
      \            tokenizer (Tokenizer, optional): Tokenizer for managing tokens.\n\
      \        \"\"\"\n        self.chat_model = chat_model\n        self.config =\
      \ config\n        self.tokenizer = tokenizer or get_tokenizer()"
    signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n\
      \        chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
      \    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
    called_by: []
  - name: decompose_query
    start_line: 122
    end_line: 151
    code: "async def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
      \    ) -> tuple[dict, dict[str, int]]:\n        \"\"\"\n        Decompose the\
      \ query into subqueries based on the fetched global structures.\n\n        Args:\n\
      \            query (str): The original search query.\n            reports (pd.DataFrame):\
      \ DataFrame containing community reports.\n\n        Returns\n        -------\n\
      \        tuple[dict, int, int]: Parsed response and the number of prompt and\
      \ output tokens used.\n        \"\"\"\n        community_reports = \"\\n\\n\"\
      .join(reports[\"full_content\"].tolist())\n        prompt = DRIFT_PRIMER_PROMPT.format(\n\
      \            query=query, community_reports=community_reports\n        )\n \
      \       model_response = await self.chat_model.achat(prompt, json=True)\n  \
      \      response = model_response.output.content\n\n        parsed_response =\
      \ json.loads(response)\n\n        token_ct = {\n            \"llm_calls\": 1,\n\
      \            \"prompt_tokens\": len(self.tokenizer.encode(prompt)),\n      \
      \      \"output_tokens\": len(self.tokenizer.encode(response)),\n        }\n\
      \n        return parsed_response, token_ct"
    signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
      \    ) -> tuple[dict, dict[str, int]]"
    decorators: []
    raises: []
    calls:
    - target: '"\n\n".join'
      type: unresolved
    - target: reports["full_content"].tolist
      type: unresolved
    - target: graphrag/prompts/query/drift_search_system_prompt.py::DRIFT_PRIMER_PROMPT::format
      type: external
    - target: self.chat_model.achat
      type: instance
    - target: json::loads
      type: stdlib
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.decompose_query
    called_by: []
  - name: search
    start_line: 153
    end_line: 185
    code: "async def search(\n        self,\n        query: str,\n        top_k_reports:\
      \ pd.DataFrame,\n    ) -> SearchResult:\n        \"\"\"\n        Asynchronous\
      \ search method that processes the query and returns a SearchResult.\n\n   \
      \     Args:\n            query (str): The search query.\n            top_k_reports\
      \ (pd.DataFrame): DataFrame containing the top-k reports.\n\n        Returns\n\
      \        -------\n        SearchResult: The search result containing the response\
      \ and context data.\n        \"\"\"\n        start_time = time.perf_counter()\n\
      \        report_folds = self.split_reports(top_k_reports)\n        tasks = [self.decompose_query(query,\
      \ fold) for fold in report_folds]\n\n        results_with_tokens = await tqdm_asyncio.gather(*tasks,\
      \ leave=False)\n\n        completion_time = time.perf_counter() - start_time\n\
      \n        return SearchResult(\n            response=[response for response,\
      \ _ in results_with_tokens],\n            context_data={\"top_k_reports\": top_k_reports},\n\
      \            context_text=top_k_reports.to_json() or \"\",\n            completion_time=completion_time,\n\
      \            llm_calls=len(results_with_tokens),\n            prompt_tokens=sum(ct[\"\
      prompt_tokens\"] for _, ct in results_with_tokens),\n            output_tokens=sum(ct[\"\
      output_tokens\"] for _, ct in results_with_tokens),\n        )"
    signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
      \ pd.DataFrame,\n    ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::perf_counter
      type: stdlib
    - target: graphrag/query/structured_search/drift_search/primer.py::split_reports
      type: internal
    - target: graphrag/query/structured_search/drift_search/primer.py::decompose_query
      type: internal
    - target: tqdm.asyncio::tqdm_asyncio::gather
      type: external
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: top_k_reports.to_json
      type: unresolved
    - target: len
      type: builtin
    - target: sum
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.search
    called_by: []
  - name: split_reports
    start_line: 187
    end_line: 201
    code: "def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]:\n\
      \        \"\"\"\n        Split the reports into folds, allowing for parallel\
      \ processing.\n\n        Args:\n            reports (pd.DataFrame): DataFrame\
      \ of community reports.\n\n        Returns\n        -------\n        list[pd.DataFrame]:\
      \ List of report folds.\n        \"\"\"\n        primer_folds = self.config.primer_folds\
      \ or 1  # Ensure at least one fold\n        if primer_folds == 1:\n        \
      \    return [reports]\n        return [pd.DataFrame(fold) for fold in np.array_split(reports,\
      \ primer_folds)]"
    signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: numpy::array_split
      type: external
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.split_reports
    called_by: []
- file_name: graphrag/query/structured_search/drift_search/search.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: tqdm.asyncio
    name: tqdm_asyncio
    alias: null
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.language_model.providers.fnllm.utils
    name: get_openai_model_parameters_from_dict
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: EntityVectorStoreKey
    alias: null
  - module: graphrag.query.structured_search.base
    name: BaseSearch
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  - module: graphrag.query.structured_search.drift_search.action
    name: DriftAction
    alias: null
  - module: graphrag.query.structured_search.drift_search.drift_context
    name: DRIFTSearchContextBuilder
    alias: null
  - module: graphrag.query.structured_search.drift_search.primer
    name: DRIFTPrimer
    alias: null
  - module: graphrag.query.structured_search.drift_search.state
    name: QueryState
    alias: null
  - module: graphrag.query.structured_search.local_search.search
    name: LocalSearch
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 37
    end_line: 66
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n\
      \        query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n    ):\n        \"\"\"\n        Initialize the DRIFTSearch\
      \ class.\n\n        Args:\n            llm (ChatOpenAI): The language model\
      \ used for searching.\n            context_builder (DRIFTSearchContextBuilder):\
      \ Builder for search context.\n            config (DRIFTSearchConfig, optional):\
      \ Configuration settings for DRIFTSearch.\n            tokenizer (Tokenizer,\
      \ optional): Token encoder for managing tokens.\n            query_state (QueryState,\
      \ optional): State of the current search query.\n        \"\"\"\n        super().__init__(model,\
      \ context_builder, tokenizer)\n\n        self.context_builder = context_builder\n\
      \        self.tokenizer = tokenizer or get_tokenizer()\n        self.query_state\
      \ = query_state or QueryState()\n        self.primer = DRIFTPrimer(\n      \
      \      config=self.context_builder.config,\n            chat_model=model,\n\
      \            tokenizer=self.tokenizer,\n        )\n        self.callbacks =\
      \ callbacks or []\n        self.local_search = self.init_local_search()"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n\
      \        query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/query/structured_search/drift_search/state.py::QueryState
      type: internal
    - target: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer
      type: internal
    - target: graphrag/query/structured_search/drift_search/search.py::init_local_search
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
    called_by: []
  - name: init_local_search
    start_line: 68
    end_line: 108
    code: "def init_local_search(self) -> LocalSearch:\n        \"\"\"\n        Initialize\
      \ the LocalSearch object with parameters based on the DRIFT search configuration.\n\
      \n        Returns\n        -------\n        LocalSearch: An instance of the\
      \ LocalSearch class with the configured parameters.\n        \"\"\"\n      \
      \  local_context_params = {\n            \"text_unit_prop\": self.context_builder.config.local_search_text_unit_prop,\n\
      \            \"community_prop\": self.context_builder.config.local_search_community_prop,\n\
      \            \"top_k_mapped_entities\": self.context_builder.config.local_search_top_k_mapped_entities,\n\
      \            \"top_k_relationships\": self.context_builder.config.local_search_top_k_relationships,\n\
      \            \"include_entity_rank\": True,\n            \"include_relationship_weight\"\
      : True,\n            \"include_community_rank\": False,\n            \"return_candidate_context\"\
      : False,\n            \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,\n\
      \            \"max_context_tokens\": self.context_builder.config.local_search_max_data_tokens,\n\
      \        }\n\n        model_params = get_openai_model_parameters_from_dict({\n\
      \            \"model\": self.model.config.model,\n            \"max_tokens\"\
      : self.context_builder.config.local_search_llm_max_gen_tokens,\n           \
      \ \"temperature\": self.context_builder.config.local_search_temperature,\n \
      \           \"n\": self.context_builder.config.local_search_n,\n           \
      \ \"top_p\": self.context_builder.config.local_search_top_p,\n            \"\
      max_completion_tokens\": self.context_builder.config.local_search_llm_max_gen_completion_tokens,\n\
      \            \"response_format\": {\"type\": \"json_object\"},\n        })\n\
      \n        return LocalSearch(\n            model=self.model,\n            system_prompt=self.context_builder.local_system_prompt,\n\
      \            context_builder=self.context_builder.local_mixed_context,\n   \
      \         tokenizer=self.tokenizer,\n            model_params=model_params,\n\
      \            context_builder_params=local_context_params,\n            response_type=\"\
      multiple paragraphs\",\n            callbacks=self.callbacks,\n        )"
    signature: def init_local_search(self) -> LocalSearch
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
      type: internal
    - target: graphrag/query/structured_search/local_search/search.py::LocalSearch
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
    called_by: []
  - name: _process_primer_results
    start_line: 110
    end_line: 156
    code: "def _process_primer_results(\n        self, query: str, search_results:\
      \ SearchResult\n    ) -> DriftAction:\n        \"\"\"\n        Process the results\
      \ from the primer search to extract intermediate answers and follow-up queries.\n\
      \n        Args:\n            query (str): The original search query.\n     \
      \       search_results (SearchResult): The results from the primer search.\n\
      \n        Returns\n        -------\n        DriftAction: Action generated from\
      \ the primer response.\n\n        Raises\n        ------\n        RuntimeError:\
      \ If no intermediate answers or follow-up queries are found in the primer response.\n\
      \        \"\"\"\n        response = search_results.response\n        if isinstance(response,\
      \ list) and isinstance(response[0], dict):\n            intermediate_answers\
      \ = [\n                i[\"intermediate_answer\"] for i in response if \"intermediate_answer\"\
      \ in i\n            ]\n\n            if not intermediate_answers:\n        \
      \        error_msg = \"No intermediate answers found in primer response. Ensure\
      \ that the primer response includes intermediate answers.\"\n              \
      \  raise RuntimeError(error_msg)\n\n            intermediate_answer = \"\\n\\\
      n\".join([\n                i[\"intermediate_answer\"] for i in response if\
      \ \"intermediate_answer\" in i\n            ])\n\n            follow_ups = [fu\
      \ for i in response for fu in i.get(\"follow_up_queries\", [])]\n\n        \
      \    if not follow_ups:\n                error_msg = \"No follow-up queries\
      \ found in primer response. Ensure that the primer response includes follow-up\
      \ queries.\"\n                raise RuntimeError(error_msg)\n\n            score\
      \ = sum(i.get(\"score\", float(\"-inf\")) for i in response) / len(response)\n\
      \            response_data = {\n                \"intermediate_answer\": intermediate_answer,\n\
      \                \"follow_up_queries\": follow_ups,\n                \"score\"\
      : score,\n            }\n            return DriftAction.from_primer_response(query,\
      \ response_data)\n        error_msg = \"Response must be a list of dictionaries.\"\
      \n        raise ValueError(error_msg)"
    signature: "def _process_primer_results(\n        self, query: str, search_results:\
      \ SearchResult\n    ) -> DriftAction"
    decorators: []
    raises:
    - RuntimeError
    - ValueError
    calls:
    - target: isinstance
      type: builtin
    - target: RuntimeError
      type: builtin
    - target: '"\n\n".join'
      type: unresolved
    - target: i.get
      type: unresolved
    - target: sum
      type: builtin
    - target: float
      type: builtin
    - target: len
      type: builtin
    - target: graphrag/query/structured_search/drift_search/action.py::DriftAction::from_primer_response
      type: external
    - target: ValueError
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._process_primer_results
    called_by: []
  - name: _search_step
    start_line: 158
    end_line: 177
    code: "async def _search_step(\n        self, global_query: str, search_engine:\
      \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]:\n   \
      \     \"\"\"\n        Perform an asynchronous search step by executing each\
      \ DriftAction asynchronously.\n\n        Args:\n            global_query (str):\
      \ The global query for the search.\n            search_engine (LocalSearch):\
      \ The local search engine instance.\n            actions (list[DriftAction]):\
      \ A list of actions to perform.\n\n        Returns\n        -------\n      \
      \  list[DriftAction]: The results from executing the search actions asynchronously.\n\
      \        \"\"\"\n        tasks = [\n            action.search(search_engine=search_engine,\
      \ global_query=global_query)\n            for action in actions\n        ]\n\
      \        return await tqdm_asyncio.gather(*tasks, leave=False)"
    signature: "def _search_step(\n        self, global_query: str, search_engine:\
      \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]"
    decorators: []
    raises: []
    calls:
    - target: action.search
      type: unresolved
    - target: tqdm.asyncio::tqdm_asyncio::gather
      type: external
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._search_step
    called_by: []
  - name: search
    start_line: 179
    end_line: 301
    code: "async def search(\n        self,\n        query: str,\n        conversation_history:\
      \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult:\n\
      \        \"\"\"\n        Perform an asynchronous DRIFT search.\n\n        Args:\n\
      \            query (str): The query to search for.\n            conversation_history\
      \ (Any, optional): The conversation history, if any.\n            reduce (bool,\
      \ optional): Whether to reduce the response to a single comprehensive response.\n\
      \n        Returns\n        -------\n        SearchResult: The search result\
      \ containing the response and context data.\n\n        Raises\n        ------\n\
      \        ValueError: If the query is empty.\n        \"\"\"\n        if query\
      \ == \"\":\n            error_msg = \"DRIFT Search query cannot be empty.\"\n\
      \            raise ValueError(error_msg)\n\n        llm_calls, prompt_tokens,\
      \ output_tokens = {}, {}, {}\n\n        start_time = time.perf_counter()\n\n\
      \        # Check if query state is empty\n        if not self.query_state.graph:\n\
      \            # Prime the search with the primer\n            primer_context,\
      \ token_ct = await self.context_builder.build_context(query)\n            llm_calls[\"\
      build_context\"] = token_ct[\"llm_calls\"]\n            prompt_tokens[\"build_context\"\
      ] = token_ct[\"prompt_tokens\"]\n            output_tokens[\"build_context\"\
      ] = token_ct[\"output_tokens\"]\n\n            primer_response = await self.primer.search(\n\
      \                query=query, top_k_reports=primer_context\n            )\n\
      \            llm_calls[\"primer\"] = primer_response.llm_calls\n           \
      \ prompt_tokens[\"primer\"] = primer_response.prompt_tokens\n            output_tokens[\"\
      primer\"] = primer_response.output_tokens\n\n            # Package response\
      \ into DriftAction\n            init_action = self._process_primer_results(query,\
      \ primer_response)\n            self.query_state.add_action(init_action)\n \
      \           self.query_state.add_all_follow_ups(init_action, init_action.follow_ups)\n\
      \n        # Main loop\n        epochs = 0\n        llm_call_offset = 0\n   \
      \     while epochs < self.context_builder.config.n_depth:\n            actions\
      \ = self.query_state.rank_incomplete_actions()\n            if len(actions)\
      \ == 0:\n                logger.debug(\"No more actions to take. Exiting DRIFT\
      \ loop.\")\n                break\n            actions = actions[: self.context_builder.config.drift_k_followups]\n\
      \            llm_call_offset += (\n                len(actions) - self.context_builder.config.drift_k_followups\n\
      \            )\n            # Process actions\n            results = await self._search_step(\n\
      \                global_query=query, search_engine=self.local_search, actions=actions\n\
      \            )\n\n            # Update query state\n            for action in\
      \ results:\n                self.query_state.add_action(action)\n          \
      \      self.query_state.add_all_follow_ups(action, action.follow_ups)\n    \
      \        epochs += 1\n\n        t_elapsed = time.perf_counter() - start_time\n\
      \n        # Calculate token usage\n        token_ct = self.query_state.action_token_ct()\n\
      \        llm_calls[\"action\"] = token_ct[\"llm_calls\"]\n        prompt_tokens[\"\
      action\"] = token_ct[\"prompt_tokens\"]\n        output_tokens[\"action\"] =\
      \ token_ct[\"output_tokens\"]\n\n        # Package up context data\n       \
      \ response_state, context_data, context_text = self.query_state.serialize(\n\
      \            include_context=True\n        )\n\n        reduced_response = response_state\n\
      \        if reduce:\n            # Reduce response_state to a single comprehensive\
      \ response\n            for callback in self.callbacks:\n                callback.on_reduce_response_start(response_state)\n\
      \n            model_params = get_openai_model_parameters_from_dict({\n     \
      \           \"model\": self.model.config.model,\n                \"max_tokens\"\
      : self.context_builder.config.reduce_max_tokens,\n                \"temperature\"\
      : self.context_builder.config.reduce_temperature,\n                \"max_completion_tokens\"\
      : self.context_builder.config.reduce_max_completion_tokens,\n            })\n\
      \n            reduced_response = await self._reduce_response(\n            \
      \    responses=response_state,\n                query=query,\n             \
      \   llm_calls=llm_calls,\n                prompt_tokens=prompt_tokens,\n   \
      \             output_tokens=output_tokens,\n                model_params=model_params,\n\
      \            )\n\n            for callback in self.callbacks:\n            \
      \    callback.on_reduce_response_end(reduced_response)\n        return SearchResult(\n\
      \            response=reduced_response,\n            context_data=context_data,\n\
      \            context_text=context_text,\n            completion_time=t_elapsed,\n\
      \            llm_calls=sum(llm_calls.values()),\n            prompt_tokens=sum(prompt_tokens.values()),\n\
      \            output_tokens=sum(output_tokens.values()),\n            llm_calls_categories=llm_calls,\n\
      \            prompt_tokens_categories=prompt_tokens,\n            output_tokens_categories=output_tokens,\n\
      \        )"
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: time::perf_counter
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: self.primer.search
      type: instance
    - target: graphrag/query/structured_search/drift_search/search.py::_process_primer_results
      type: internal
    - target: self.query_state.add_action
      type: instance
    - target: self.query_state.add_all_follow_ups
      type: instance
    - target: self.query_state.rank_incomplete_actions
      type: instance
    - target: len
      type: builtin
    - target: logger.debug
      type: unresolved
    - target: graphrag/query/structured_search/drift_search/search.py::_search_step
      type: internal
    - target: self.query_state.action_token_ct
      type: instance
    - target: self.query_state.serialize
      type: instance
    - target: callback.on_reduce_response_start
      type: unresolved
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
      type: internal
    - target: graphrag/query/structured_search/drift_search/search.py::_reduce_response
      type: internal
    - target: callback.on_reduce_response_end
      type: unresolved
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: sum
      type: builtin
    - target: llm_calls.values
      type: unresolved
    - target: prompt_tokens.values
      type: unresolved
    - target: output_tokens.values
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
    called_by: []
  - name: stream_search
    start_line: 303
    end_line: 340
    code: "async def stream_search(\n        self, query: str, conversation_history:\
      \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]:\n \
      \       \"\"\"\n        Perform a streaming DRIFT search asynchronously.\n\n\
      \        Args:\n            query (str): The query to search for.\n        \
      \    conversation_history (ConversationHistory, optional): The conversation\
      \ history.\n        \"\"\"\n        result = await self.search(\n          \
      \  query=query, conversation_history=conversation_history, reduce=False\n  \
      \      )\n\n        if isinstance(result.response, list):\n            result.response\
      \ = result.response[0]\n\n        for callback in self.callbacks:\n        \
      \    callback.on_reduce_response_start(result.response)\n\n        model_params\
      \ = get_openai_model_parameters_from_dict({\n            \"model\": self.model.config.model,\n\
      \            \"max_tokens\": self.context_builder.config.reduce_max_tokens,\n\
      \            \"temperature\": self.context_builder.config.reduce_temperature,\n\
      \            \"max_completion_tokens\": self.context_builder.config.reduce_max_completion_tokens,\n\
      \        })\n\n        full_response = \"\"\n        async for resp in self._reduce_response_streaming(\n\
      \            responses=result.response,\n            query=query,\n        \
      \    model_params=model_params,\n        ):\n            full_response += resp\n\
      \            yield resp\n\n        for callback in self.callbacks:\n       \
      \     callback.on_reduce_response_end(full_response)"
    signature: "def stream_search(\n        self, query: str, conversation_history:\
      \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/structured_search/drift_search/search.py::search
      type: internal
    - target: isinstance
      type: builtin
    - target: callback.on_reduce_response_start
      type: unresolved
    - target: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
      type: internal
    - target: graphrag/query/structured_search/drift_search/search.py::_reduce_response_streaming
      type: internal
    - target: callback.on_reduce_response_end
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
    called_by: []
  - name: _reduce_response
    start_line: 342
    end_line: 400
    code: "async def _reduce_response(\n        self,\n        responses: str | dict[str,\
      \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
      \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
      \    ) -> str:\n        \"\"\"Reduce the response to a single comprehensive\
      \ response.\n\n        Parameters\n        ----------\n        responses : str|dict[str,\
      \ Any]\n            The responses to reduce.\n        query : str\n        \
      \    The original query.\n        llm_kwargs : dict[str, Any]\n            Additional\
      \ keyword arguments to pass to the LLM.\n\n        Returns\n        -------\n\
      \        str\n            The reduced response.\n        \"\"\"\n        reduce_responses\
      \ = []\n\n        if isinstance(responses, str):\n            reduce_responses\
      \ = [responses]\n        else:\n            reduce_responses = [\n         \
      \       response[\"answer\"]\n                for response in responses.get(\"\
      nodes\", [])\n                if response.get(\"answer\")\n            ]\n\n\
      \        search_prompt = self.context_builder.reduce_system_prompt.format(\n\
      \            context_data=reduce_responses,\n            response_type=self.context_builder.response_type,\n\
      \        )\n        search_messages = [\n            {\"role\": \"system\",\
      \ \"content\": search_prompt},\n        ]\n\n        model_response = await\
      \ self.model.achat(\n            prompt=query,\n            history=search_messages,\n\
      \            model_parameters=llm_kwargs.get(\"model_params\", {}),\n      \
      \  )\n\n        reduced_response = model_response.output.content\n\n       \
      \ llm_calls[\"reduce\"] = 1\n        prompt_tokens[\"reduce\"] = len(self.tokenizer.encode(search_prompt))\
      \ + len(\n            self.tokenizer.encode(query)\n        )\n        output_tokens[\"\
      reduce\"] = len(self.tokenizer.encode(reduced_response))\n\n        return reduced_response"
    signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
      \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
      \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
      \    ) -> str"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: responses.get
      type: unresolved
    - target: response.get
      type: unresolved
    - target: self.context_builder.reduce_system_prompt.format
      type: instance
    - target: self.model.achat
      type: instance
    - target: llm_kwargs.get
      type: unresolved
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response
    called_by: []
  - name: _reduce_response_streaming
    start_line: 402
    end_line: 448
    code: "async def _reduce_response_streaming(\n        self,\n        responses:\
      \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
      \ Any],\n    ) -> AsyncGenerator[str, None]:\n        \"\"\"Reduce the response\
      \ to a single comprehensive response.\n\n        Parameters\n        ----------\n\
      \        responses : str|dict[str, Any]\n            The responses to reduce.\n\
      \        query : str\n            The original query.\n\n        Returns\n \
      \       -------\n        str\n            The reduced response.\n        \"\"\
      \"\n        reduce_responses = []\n\n        if isinstance(responses, str):\n\
      \            reduce_responses = [responses]\n        else:\n            reduce_responses\
      \ = [\n                response[\"answer\"]\n                for response in\
      \ responses.get(\"nodes\", [])\n                if response.get(\"answer\")\n\
      \            ]\n\n        search_prompt = self.context_builder.reduce_system_prompt.format(\n\
      \            context_data=reduce_responses,\n            response_type=self.context_builder.response_type,\n\
      \        )\n        search_messages = [\n            {\"role\": \"system\",\
      \ \"content\": search_prompt},\n        ]\n\n        async for response in self.model.achat_stream(\n\
      \            prompt=query,\n            history=search_messages,\n         \
      \   model_parameters=model_params,\n        ):\n            for callback in\
      \ self.callbacks:\n                callback.on_llm_new_token(response)\n   \
      \         yield response"
    signature: "def _reduce_response_streaming(\n        self,\n        responses:\
      \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
      \ Any],\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: responses.get
      type: unresolved
    - target: response.get
      type: unresolved
    - target: self.context_builder.reduce_system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response_streaming
    called_by: []
- file_name: graphrag/query/structured_search/drift_search/state.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: random
    name: null
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.query.structured_search.drift_search.action
    name: DriftAction
    alias: null
  functions:
  - name: __init__
    start_line: 21
    end_line: 22
    code: "def __init__(self):\n        self.graph = nx.MultiDiGraph()"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls:
    - target: networkx::MultiDiGraph
      type: external
    visibility: protected
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.__init__
    called_by: []
  - name: add_action
    start_line: 24
    end_line: 26
    code: "def add_action(self, action: DriftAction, metadata: dict[str, Any] | None\
      \ = None):\n        \"\"\"Add an action to the graph with optional metadata.\"\
      \"\"\n        self.graph.add_node(action, **(metadata or {}))"
    signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any]
      | None = None)'
    decorators: []
    raises: []
    calls:
    - target: self.graph.add_node
      type: instance
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_action
    called_by: []
  - name: relate_actions
    start_line: 28
    end_line: 32
    code: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
      \ weight: float = 1.0\n    ):\n        \"\"\"Relate two actions in the graph.\"\
      \"\"\n        self.graph.add_edge(parent, child, weight=weight)"
    signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
      \ weight: float = 1.0\n    )"
    decorators: []
    raises: []
    calls:
    - target: self.graph.add_edge
      type: instance
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.relate_actions
    called_by: []
  - name: add_all_follow_ups
    start_line: 34
    end_line: 53
    code: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
      \        follow_ups: list[DriftAction] | list[str],\n        weight: float =\
      \ 1.0,\n    ):\n        \"\"\"Add all follow-up actions and links them to the\
      \ given action.\"\"\"\n        if len(follow_ups) == 0:\n            logger.warning(\"\
      No follow-up actions for action: %s\", action.query)\n\n        for follow_up\
      \ in follow_ups:\n            if isinstance(follow_up, str):\n             \
      \   follow_up = DriftAction(query=follow_up)\n            elif not isinstance(follow_up,\
      \ DriftAction):\n                logger.warning(\n                    \"Follow-up\
      \ action is not a string, found type: %s\", type(follow_up)\n              \
      \  )\n\n            self.add_action(follow_up)\n            self.relate_actions(action,\
      \ follow_up, weight)"
    signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
      \        follow_ups: list[DriftAction] | list[str],\n        weight: float =\
      \ 1.0,\n    )"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: isinstance
      type: builtin
    - target: graphrag/query/structured_search/drift_search/action.py::DriftAction
      type: internal
    - target: type
      type: builtin
    - target: graphrag/query/structured_search/drift_search/state.py::add_action
      type: internal
    - target: graphrag/query/structured_search/drift_search/state.py::relate_actions
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_all_follow_ups
    called_by: []
  - name: find_incomplete_actions
    start_line: 55
    end_line: 57
    code: "def find_incomplete_actions(self) -> list[DriftAction]:\n        \"\"\"\
      Find all unanswered actions in the graph.\"\"\"\n        return [node for node\
      \ in self.graph.nodes if not node.is_complete]"
    signature: def find_incomplete_actions(self) -> list[DriftAction]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.find_incomplete_actions
    called_by: []
  - name: rank_incomplete_actions
    start_line: 59
    end_line: 77
    code: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
      \ float] | None = None\n    ) -> list[DriftAction]:\n        \"\"\"Rank all\
      \ unanswered actions in the graph if scorer available.\"\"\"\n        unanswered\
      \ = self.find_incomplete_actions()\n        if scorer:\n            for node\
      \ in unanswered:\n                node.compute_score(scorer)\n            return\
      \ sorted(\n                unanswered,\n                key=lambda node: (\n\
      \                    node.score if node.score is not None else float(\"-inf\"\
      )\n                ),\n                reverse=True,\n            )\n\n    \
      \    # shuffle the list if no scorer\n        random.shuffle(unanswered)\n \
      \       return list(unanswered)"
    signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
      \ float] | None = None\n    ) -> list[DriftAction]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/structured_search/drift_search/state.py::find_incomplete_actions
      type: internal
    - target: node.compute_score
      type: unresolved
    - target: sorted
      type: builtin
    - target: float
      type: builtin
    - target: random::shuffle
      type: stdlib
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.rank_incomplete_actions
    called_by: []
  - name: serialize
    start_line: 79
    end_line: 117
    code: "def serialize(\n        self, include_context: bool = True\n    ) -> dict[str,\
      \ Any] | tuple[dict[str, Any], dict[str, Any], str]:\n        \"\"\"Serialize\
      \ the graph to a dictionary, including nodes and edges.\"\"\"\n        # Create\
      \ a mapping from nodes to unique IDs\n        node_to_id = {node: idx for idx,\
      \ node in enumerate(self.graph.nodes())}\n\n        # Serialize nodes\n    \
      \    nodes: list[dict[str, Any]] = [\n            {\n                **node.serialize(include_follow_ups=False),\n\
      \                \"id\": node_to_id[node],\n                **self.graph.nodes[node],\n\
      \            }\n            for node in self.graph.nodes()\n        ]\n\n  \
      \      # Serialize edges\n        edges: list[dict[str, Any]] = [\n        \
      \    {\n                \"source\": node_to_id[u],\n                \"target\"\
      : node_to_id[v],\n                \"weight\": edge_data.get(\"weight\", 1.0),\n\
      \            }\n            for u, v, edge_data in self.graph.edges(data=True)\n\
      \        ]\n\n        if include_context:\n            context_data = {\n  \
      \              node[\"query\"]: node[\"metadata\"][\"context_data\"]\n     \
      \           for node in nodes\n                if node[\"metadata\"].get(\"\
      context_data\") and node.get(\"query\")\n            }\n\n            context_text\
      \ = str(context_data)\n\n            return {\"nodes\": nodes, \"edges\": edges},\
      \ context_data, context_text\n\n        return {\"nodes\": nodes, \"edges\"\
      : edges}"
    signature: "def serialize(\n        self, include_context: bool = True\n    )\
      \ -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
    decorators: []
    raises: []
    calls:
    - target: enumerate
      type: builtin
    - target: self.graph.nodes
      type: instance
    - target: node.serialize
      type: unresolved
    - target: edge_data.get
      type: unresolved
    - target: self.graph.edges
      type: instance
    - target: node["metadata"].get
      type: unresolved
    - target: node.get
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.serialize
    called_by: []
  - name: deserialize
    start_line: 119
    end_line: 137
    code: "def deserialize(self, data: dict[str, Any]):\n        \"\"\"Deserialize\
      \ the dictionary back to a graph.\"\"\"\n        self.graph.clear()\n      \
      \  id_to_action = {}\n\n        for node_data in data.get(\"nodes\", []):\n\
      \            node_id = node_data.pop(\"id\")\n            action = DriftAction.deserialize(node_data)\n\
      \            self.add_action(action)\n            id_to_action[node_id] = action\n\
      \n        for edge_data in data.get(\"edges\", []):\n            source_id =\
      \ edge_data[\"source\"]\n            target_id = edge_data[\"target\"]\n   \
      \         weight = edge_data.get(\"weight\", 1.0)\n            source_action\
      \ = id_to_action.get(source_id)\n            target_action = id_to_action.get(target_id)\n\
      \            if source_action and target_action:\n                self.relate_actions(source_action,\
      \ target_action, weight)"
    signature: 'def deserialize(self, data: dict[str, Any])'
    decorators: []
    raises: []
    calls:
    - target: self.graph.clear
      type: instance
    - target: data.get
      type: unresolved
    - target: node_data.pop
      type: unresolved
    - target: graphrag/query/structured_search/drift_search/action.py::DriftAction::deserialize
      type: external
    - target: graphrag/query/structured_search/drift_search/state.py::add_action
      type: internal
    - target: edge_data.get
      type: unresolved
    - target: id_to_action.get
      type: unresolved
    - target: graphrag/query/structured_search/drift_search/state.py::relate_actions
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.deserialize
    called_by: []
  - name: action_token_ct
    start_line: 139
    end_line: 150
    code: "def action_token_ct(self) -> dict[str, int]:\n        \"\"\"Return the\
      \ token count of the action.\"\"\"\n        llm_calls, prompt_tokens, output_tokens\
      \ = 0, 0, 0\n        for action in self.graph.nodes:\n            llm_calls\
      \ += action.metadata[\"llm_calls\"]\n            prompt_tokens += action.metadata[\"\
      prompt_tokens\"]\n            output_tokens += action.metadata[\"output_tokens\"\
      ]\n        return {\n            \"llm_calls\": llm_calls,\n            \"prompt_tokens\"\
      : prompt_tokens,\n            \"output_tokens\": output_tokens,\n        }"
    signature: def action_token_ct(self) -> dict[str, int]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.action_token_ct
    called_by: []
- file_name: graphrag/query/structured_search/global_search/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/structured_search/global_search/community_context.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: graphrag.data_model.community
    name: Community
    alias: null
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.query.context_builder.builders
    name: ContextBuilderResult
    alias: null
  - module: graphrag.query.context_builder.community_context
    name: build_community_context
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.context_builder.dynamic_community_selection
    name: DynamicCommunitySelection
    alias: null
  - module: graphrag.query.structured_search.base
    name: GlobalContextBuilder
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 29
    end_line: 53
    code: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        entities: list[Entity] | None\
      \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
      \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
      \ | None = None,\n        random_state: int = 86,\n    ):\n        self.community_reports\
      \ = community_reports\n        self.entities = entities\n        self.tokenizer\
      \ = tokenizer or get_tokenizer()\n        self.dynamic_community_selection =\
      \ None\n        if dynamic_community_selection and isinstance(\n           \
      \ dynamic_community_selection_kwargs, dict\n        ):\n            self.dynamic_community_selection\
      \ = DynamicCommunitySelection(\n                community_reports=community_reports,\n\
      \                communities=communities,\n                model=dynamic_community_selection_kwargs.pop(\"\
      model\"),\n                tokenizer=dynamic_community_selection_kwargs.pop(\"\
      tokenizer\"),\n                **dynamic_community_selection_kwargs,\n     \
      \       )\n        self.random_state = random_state"
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        entities: list[Entity] | None\
      \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
      \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
      \ | None = None,\n        random_state: int = 86,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: isinstance
      type: builtin
    - target: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection
      type: internal
    - target: dynamic_community_selection_kwargs.pop
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
    called_by: []
  - name: build_context
    start_line: 55
    end_line: 144
    code: "async def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        use_community_summary: bool =\
      \ True,\n        column_delimiter: str = \"|\",\n        shuffle_data: bool\
      \ = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
      \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
      \ bool = True,\n        community_weight_name: str = \"occurrence\",\n     \
      \   normalize_community_weight: bool = True,\n        max_context_tokens: int\
      \ = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
      \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n  \
      \      **kwargs: Any,\n    ) -> ContextBuilderResult:\n        \"\"\"Prepare\
      \ batches of community report data table as context data for global search.\"\
      \"\"\n        conversation_history_context = \"\"\n        final_context_data\
      \ = {}\n        llm_calls, prompt_tokens, output_tokens = 0, 0, 0\n        if\
      \ conversation_history:\n            # build conversation history context\n\
      \            (\n                conversation_history_context,\n            \
      \    conversation_history_context_data,\n            ) = conversation_history.build_context(\n\
      \                include_user_turns_only=conversation_history_user_turns_only,\n\
      \                max_qa_turns=conversation_history_max_turns,\n            \
      \    column_delimiter=column_delimiter,\n                max_context_tokens=max_context_tokens,\n\
      \                recency_bias=False,\n            )\n            if conversation_history_context\
      \ != \"\":\n                final_context_data = conversation_history_context_data\n\
      \n        community_reports = self.community_reports\n        if self.dynamic_community_selection\
      \ is not None:\n            (\n                community_reports,\n        \
      \        dynamic_info,\n            ) = await self.dynamic_community_selection.select(query)\n\
      \            llm_calls += dynamic_info[\"llm_calls\"]\n            prompt_tokens\
      \ += dynamic_info[\"prompt_tokens\"]\n            output_tokens += dynamic_info[\"\
      output_tokens\"]\n\n        community_context, community_context_data = build_community_context(\n\
      \            community_reports=community_reports,\n            entities=self.entities,\n\
      \            tokenizer=self.tokenizer,\n            use_community_summary=use_community_summary,\n\
      \            column_delimiter=column_delimiter,\n            shuffle_data=shuffle_data,\n\
      \            include_community_rank=include_community_rank,\n            min_community_rank=min_community_rank,\n\
      \            community_rank_name=community_rank_name,\n            include_community_weight=include_community_weight,\n\
      \            community_weight_name=community_weight_name,\n            normalize_community_weight=normalize_community_weight,\n\
      \            max_context_tokens=max_context_tokens,\n            single_batch=False,\n\
      \            context_name=context_name,\n            random_state=self.random_state,\n\
      \        )\n\n        # Prepare context_prefix based on whether conversation_history_context\
      \ exists\n        context_prefix = (\n            f\"{conversation_history_context}\\\
      n\\n\"\n            if conversation_history_context\n            else \"\"\n\
      \        )\n\n        final_context = (\n            [f\"{context_prefix}{context}\"\
      \ for context in community_context]\n            if isinstance(community_context,\
      \ list)\n            else f\"{context_prefix}{community_context}\"\n       \
      \ )\n\n        # Update the final context data with the provided community_context_data\n\
      \        final_context_data.update(community_context_data)\n\n        return\
      \ ContextBuilderResult(\n            context_chunks=final_context,\n       \
      \     context_records=final_context_data,\n            llm_calls=llm_calls,\n\
      \            prompt_tokens=prompt_tokens,\n            output_tokens=output_tokens,\n\
      \        )"
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        use_community_summary: bool =\
      \ True,\n        column_delimiter: str = \"|\",\n        shuffle_data: bool\
      \ = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
      \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
      \ bool = True,\n        community_weight_name: str = \"occurrence\",\n     \
      \   normalize_community_weight: bool = True,\n        max_context_tokens: int\
      \ = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
      \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n  \
      \      **kwargs: Any,\n    ) -> ContextBuilderResult"
    decorators: []
    raises: []
    calls:
    - target: conversation_history.build_context
      type: unresolved
    - target: self.dynamic_community_selection.select
      type: instance
    - target: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
    - target: isinstance
      type: builtin
    - target: final_context_data.update
      type: unresolved
    - target: graphrag/query/context_builder/builders.py::ContextBuilderResult
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
    called_by: []
- file_name: graphrag/query/structured_search/global_search/search.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.query.global_search_knowledge_system_prompt
    name: GENERAL_KNOWLEDGE_INSTRUCTION
    alias: null
  - module: graphrag.prompts.query.global_search_map_system_prompt
    name: MAP_SYSTEM_PROMPT
    alias: null
  - module: graphrag.prompts.query.global_search_reduce_system_prompt
    name: NO_DATA_ANSWER
    alias: null
  - module: graphrag.prompts.query.global_search_reduce_system_prompt
    name: REDUCE_SYSTEM_PROMPT
    alias: null
  - module: graphrag.query.context_builder.builders
    name: GlobalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.llm.text_utils
    name: try_parse_json_object
    alias: null
  - module: graphrag.query.structured_search.base
    name: BaseSearch
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 51
    end_line: 97
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n     \
      \   map_system_prompt: str | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str = \"multiple paragraphs\",\n  \
      \      allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
      \ str | None = None,\n        json_mode: bool = True,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n        max_data_tokens: int = 8000,\n        map_llm_params:\
      \ dict[str, Any] | None = None,\n        reduce_llm_params: dict[str, Any] |\
      \ None = None,\n        map_max_length: int = 1000,\n        reduce_max_length:\
      \ int = 2000,\n        context_builder_params: dict[str, Any] | None = None,\n\
      \        concurrent_coroutines: int = 32,\n    ):\n        super().__init__(\n\
      \            model=model,\n            context_builder=context_builder,\n  \
      \          tokenizer=tokenizer,\n            context_builder_params=context_builder_params,\n\
      \        )\n        self.map_system_prompt = map_system_prompt or MAP_SYSTEM_PROMPT\n\
      \        self.reduce_system_prompt = reduce_system_prompt or REDUCE_SYSTEM_PROMPT\n\
      \        self.response_type = response_type\n        self.allow_general_knowledge\
      \ = allow_general_knowledge\n        self.general_knowledge_inclusion_prompt\
      \ = (\n            general_knowledge_inclusion_prompt or GENERAL_KNOWLEDGE_INSTRUCTION\n\
      \        )\n        self.callbacks = callbacks or []\n        self.max_data_tokens\
      \ = max_data_tokens\n\n        self.map_llm_params = map_llm_params if map_llm_params\
      \ else {}\n        self.reduce_llm_params = reduce_llm_params if reduce_llm_params\
      \ else {}\n        if json_mode:\n            self.map_llm_params[\"response_format\"\
      ] = {\"type\": \"json_object\"}\n        else:\n            # remove response_format\
      \ key if json_mode is False\n            self.map_llm_params.pop(\"response_format\"\
      , None)\n        self.map_max_length = map_max_length\n        self.reduce_max_length\
      \ = reduce_max_length\n\n        self.semaphore = asyncio.Semaphore(concurrent_coroutines)"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n     \
      \   map_system_prompt: str | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str = \"multiple paragraphs\",\n  \
      \      allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
      \ str | None = None,\n        json_mode: bool = True,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n        max_data_tokens: int = 8000,\n        map_llm_params:\
      \ dict[str, Any] | None = None,\n        reduce_llm_params: dict[str, Any] |\
      \ None = None,\n        map_max_length: int = 1000,\n        reduce_max_length:\
      \ int = 2000,\n        context_builder_params: dict[str, Any] | None = None,\n\
      \        concurrent_coroutines: int = 32,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: self.map_llm_params.pop
      type: instance
    - target: asyncio::Semaphore
      type: stdlib
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.__init__
    called_by: []
  - name: stream_search
    start_line: 99
    end_line: 133
    code: "async def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]:\n\
      \        \"\"\"Stream the global search response.\"\"\"\n        context_result\
      \ = await self.context_builder.build_context(\n            query=query,\n  \
      \          conversation_history=conversation_history,\n            **self.context_builder_params,\n\
      \        )\n        for callback in self.callbacks:\n            callback.on_map_response_start(context_result.context_chunks)\
      \  # type: ignore\n\n        map_responses = await asyncio.gather(*[\n     \
      \       self._map_response_single_batch(\n                context_data=data,\n\
      \                query=query,\n                max_length=self.map_max_length,\n\
      \                **self.map_llm_params,\n            )\n            for data\
      \ in context_result.context_chunks\n        ])\n\n        for callback in self.callbacks:\n\
      \            callback.on_map_response_end(map_responses)  # type: ignore\n \
      \           callback.on_context(context_result.context_records)\n\n        async\
      \ for response in self._stream_reduce_response(\n            map_responses=map_responses,\
      \  # type: ignore\n            query=query,\n            max_length=self.reduce_max_length,\n\
      \            model_parameters=self.reduce_llm_params,\n        ):\n        \
      \    yield response"
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: self.context_builder.build_context
      type: instance
    - target: callback.on_map_response_start
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    - target: graphrag/query/structured_search/global_search/search.py::_map_response_single_batch
      type: internal
    - target: callback.on_map_response_end
      type: unresolved
    - target: callback.on_context
      type: unresolved
    - target: graphrag/query/structured_search/global_search/search.py::_stream_reduce_response
      type: internal
    visibility: public
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.stream_search
    called_by: []
  - name: search
    start_line: 135
    end_line: 207
    code: "async def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult:\n\
      \        \"\"\"\n        Perform a global search.\n\n        Global search mode\
      \ includes two steps:\n\n        - Step 1: Run parallel LLM calls on communities'\
      \ short summaries to generate answer for each batch\n        - Step 2: Combine\
      \ the answers from step 2 to generate the final answer\n        \"\"\"\n   \
      \     # Step 1: Generate answers for each batch of community short summaries\n\
      \        llm_calls, prompt_tokens, output_tokens = {}, {}, {}\n\n        start_time\
      \ = time.time()\n        context_result = await self.context_builder.build_context(\n\
      \            query=query,\n            conversation_history=conversation_history,\n\
      \            **self.context_builder_params,\n        )\n        llm_calls[\"\
      build_context\"] = context_result.llm_calls\n        prompt_tokens[\"build_context\"\
      ] = context_result.prompt_tokens\n        output_tokens[\"build_context\"] =\
      \ context_result.output_tokens\n\n        for callback in self.callbacks:\n\
      \            callback.on_map_response_start(context_result.context_chunks) \
      \ # type: ignore\n\n        map_responses = await asyncio.gather(*[\n      \
      \      self._map_response_single_batch(\n                context_data=data,\n\
      \                query=query,\n                max_length=self.map_max_length,\n\
      \                **self.map_llm_params,\n            )\n            for data\
      \ in context_result.context_chunks\n        ])\n\n        for callback in self.callbacks:\n\
      \            callback.on_map_response_end(map_responses)\n            callback.on_context(context_result.context_records)\n\
      \n        llm_calls[\"map\"] = sum(response.llm_calls for response in map_responses)\n\
      \        prompt_tokens[\"map\"] = sum(response.prompt_tokens for response in\
      \ map_responses)\n        output_tokens[\"map\"] = sum(response.output_tokens\
      \ for response in map_responses)\n\n        # Step 2: Combine the intermediate\
      \ answers from step 2 to generate the final answer\n        reduce_response\
      \ = await self._reduce_response(\n            map_responses=map_responses,\n\
      \            query=query,\n            **self.reduce_llm_params,\n        )\n\
      \        llm_calls[\"reduce\"] = reduce_response.llm_calls\n        prompt_tokens[\"\
      reduce\"] = reduce_response.prompt_tokens\n        output_tokens[\"reduce\"\
      ] = reduce_response.output_tokens\n\n        return GlobalSearchResult(\n  \
      \          response=reduce_response.response,\n            context_data=context_result.context_records,\n\
      \            context_text=context_result.context_chunks,\n            map_responses=map_responses,\n\
      \            reduce_context_data=reduce_response.context_data,\n           \
      \ reduce_context_text=reduce_response.context_text,\n            completion_time=time.time()\
      \ - start_time,\n            llm_calls=sum(llm_calls.values()),\n          \
      \  prompt_tokens=sum(prompt_tokens.values()),\n            output_tokens=sum(output_tokens.values()),\n\
      \            llm_calls_categories=llm_calls,\n            prompt_tokens_categories=prompt_tokens,\n\
      \            output_tokens_categories=output_tokens,\n        )"
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: callback.on_map_response_start
      type: unresolved
    - target: asyncio::gather
      type: stdlib
    - target: graphrag/query/structured_search/global_search/search.py::_map_response_single_batch
      type: internal
    - target: callback.on_map_response_end
      type: unresolved
    - target: callback.on_context
      type: unresolved
    - target: sum
      type: builtin
    - target: graphrag/query/structured_search/global_search/search.py::_reduce_response
      type: internal
    - target: GlobalSearchResult
      type: unresolved
    - target: llm_calls.values
      type: unresolved
    - target: prompt_tokens.values
      type: unresolved
    - target: output_tokens.values
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.search
    called_by: []
  - name: _map_response_single_batch
    start_line: 209
    end_line: 264
    code: "async def _map_response_single_batch(\n        self,\n        context_data:\
      \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
      \    ) -> SearchResult:\n        \"\"\"Generate answer for a single chunk of\
      \ community reports.\"\"\"\n        start_time = time.time()\n        search_prompt\
      \ = \"\"\n        try:\n            search_prompt = self.map_system_prompt.format(\n\
      \                context_data=context_data, max_length=max_length\n        \
      \    )\n            search_messages = [\n                {\"role\": \"system\"\
      , \"content\": search_prompt},\n            ]\n            async with self.semaphore:\n\
      \                model_response = await self.model.achat(\n                \
      \    prompt=query,\n                    history=search_messages,\n         \
      \           model_parameters=llm_kwargs,\n                    json=True,\n \
      \               )\n                search_response = model_response.output.content\n\
      \                logger.debug(\"Map response: %s\", search_response)\n     \
      \       try:\n                # parse search response json\n               \
      \ processed_response = self._parse_search_response(search_response)\n      \
      \      except ValueError:\n                logger.warning(\n               \
      \     \"Warning: Error parsing search response json - skipping this batch\"\n\
      \                )\n                processed_response = []\n\n            return\
      \ SearchResult(\n                response=processed_response,\n            \
      \    context_data=context_data,\n                context_text=context_data,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=len(self.tokenizer.encode(search_response)),\n\
      \            )\n\n        except Exception:\n            logger.exception(\"\
      Exception in _map_response_single_batch\")\n            return SearchResult(\n\
      \                response=[{\"answer\": \"\", \"score\": 0}],\n            \
      \    context_data=context_data,\n                context_text=context_data,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=0,\n            )"
    signature: "def _map_response_single_batch(\n        self,\n        context_data:\
      \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
      \    ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.map_system_prompt.format
      type: instance
    - target: self.model.achat
      type: instance
    - target: logger.debug
      type: unresolved
    - target: graphrag/query/structured_search/global_search/search.py::_parse_search_response
      type: internal
    - target: logger.warning
      type: unresolved
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._map_response_single_batch
    called_by: []
  - name: _parse_search_response
    start_line: 266
    end_line: 294
    code: "def _parse_search_response(self, search_response: str) -> list[dict[str,\
      \ Any]]:\n        \"\"\"Parse the search response json and return a list of\
      \ key points.\n\n        Parameters\n        ----------\n        search_response:\
      \ str\n            The search response json string\n\n        Returns\n    \
      \    -------\n        list[dict[str, Any]]\n            A list of key points,\
      \ each key point is a dictionary with \"answer\" and \"score\" keys\n      \
      \  \"\"\"\n        search_response, j = try_parse_json_object(search_response)\n\
      \        if j == {}:\n            return [{\"answer\": \"\", \"score\": 0}]\n\
      \n        parsed_elements = json.loads(search_response).get(\"points\")\n  \
      \      if not parsed_elements or not isinstance(parsed_elements, list):\n  \
      \          return [{\"answer\": \"\", \"score\": 0}]\n\n        return [\n \
      \           {\n                \"answer\": element[\"description\"],\n     \
      \           \"score\": int(element[\"score\"]),\n            }\n           \
      \ for element in parsed_elements\n            if \"description\" in element\
      \ and \"score\" in element\n        ]"
    signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
      Any]]'
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/llm/text_utils.py::try_parse_json_object
      type: internal
    - target: json::loads(search_response).get
      type: stdlib
    - target: json::loads
      type: stdlib
    - target: isinstance
      type: builtin
    - target: int
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
    called_by: []
  - name: _reduce_response
    start_line: 296
    end_line: 413
    code: "async def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
      \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult:\n      \
      \  \"\"\"Combine all intermediate responses from single batches into a final\
      \ answer to the user query.\"\"\"\n        text_data = \"\"\n        search_prompt\
      \ = \"\"\n        start_time = time.time()\n        try:\n            # collect\
      \ all key points into a single list to prepare for sorting\n            key_points\
      \ = []\n            for index, response in enumerate(map_responses):\n     \
      \           if not isinstance(response.response, list):\n                  \
      \  continue\n                for element in response.response:\n           \
      \         if not isinstance(element, dict):\n                        continue\n\
      \                    if \"answer\" not in element or \"score\" not in element:\n\
      \                        continue\n                    key_points.append({\n\
      \                        \"analyst\": index,\n                        \"answer\"\
      : element[\"answer\"],\n                        \"score\": element[\"score\"\
      ],\n                    })\n\n            # filter response with score = 0 and\
      \ rank responses by descending order of score\n            filtered_key_points\
      \ = [\n                point\n                for point in key_points\n    \
      \            if point[\"score\"] > 0  # type: ignore\n            ]\n\n    \
      \        if len(filtered_key_points) == 0 and not self.allow_general_knowledge:\n\
      \                # return no data answer if no key points are found\n      \
      \          logger.warning(\n                    \"Warning: All map responses\
      \ have score 0 (i.e., no relevant information found from the dataset), returning\
      \ a canned 'I do not know' answer. You can try enabling `allow_general_knowledge`\
      \ to encourage the LLM to incorporate relevant general knowledge, at the risk\
      \ of increasing hallucinations.\"\n                )\n                return\
      \ SearchResult(\n                    response=NO_DATA_ANSWER,\n            \
      \        context_data=\"\",\n                    context_text=\"\",\n      \
      \              completion_time=time.time() - start_time,\n                 \
      \   llm_calls=0,\n                    prompt_tokens=0,\n                   \
      \ output_tokens=0,\n                )\n\n            filtered_key_points = sorted(\n\
      \                filtered_key_points,\n                key=lambda x: x[\"score\"\
      ],  # type: ignore\n                reverse=True,  # type: ignore\n        \
      \    )\n\n            data = []\n            total_tokens = 0\n            for\
      \ point in filtered_key_points:\n                formatted_response_data = []\n\
      \                formatted_response_data.append(\n                    f\"----Analyst\
      \ {point['analyst'] + 1}----\"\n                )\n                formatted_response_data.append(\n\
      \                    f\"Importance Score: {point['score']}\"  # type: ignore\n\
      \                )\n                formatted_response_data.append(point[\"\
      answer\"])  # type: ignore\n                formatted_response_text = \"\\n\"\
      .join(formatted_response_data)\n                if (\n                    total_tokens\
      \ + len(self.tokenizer.encode(formatted_response_text))\n                  \
      \  > self.max_data_tokens\n                ):\n                    break\n \
      \               data.append(formatted_response_text)\n                total_tokens\
      \ += len(self.tokenizer.encode(formatted_response_text))\n            text_data\
      \ = \"\\n\\n\".join(data)\n\n            search_prompt = self.reduce_system_prompt.format(\n\
      \                report_data=text_data,\n                response_type=self.response_type,\n\
      \                max_length=self.reduce_max_length,\n            )\n       \
      \     if self.allow_general_knowledge:\n                search_prompt += \"\\\
      n\" + self.general_knowledge_inclusion_prompt\n            search_messages =\
      \ [\n                {\"role\": \"system\", \"content\": search_prompt},\n \
      \               {\"role\": \"user\", \"content\": query},\n            ]\n\n\
      \            search_response = \"\"\n            async for chunk_response in\
      \ self.model.achat_stream(\n                prompt=query,\n                history=search_messages,\n\
      \                model_parameters=llm_kwargs,\n            ):\n            \
      \    search_response += chunk_response\n                for callback in self.callbacks:\n\
      \                    callback.on_llm_new_token(chunk_response)\n\n         \
      \   return SearchResult(\n                response=search_response,\n      \
      \          context_data=text_data,\n                context_text=text_data,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=len(self.tokenizer.encode(search_response)),\n\
      \            )\n        except Exception:\n            logger.exception(\"Exception\
      \ in reduce_response\")\n            return SearchResult(\n                response=\"\
      \",\n                context_data=text_data,\n                context_text=text_data,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=0,\n            )"
    signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
      \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: enumerate
      type: builtin
    - target: isinstance
      type: builtin
    - target: key_points.append
      type: unresolved
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: sorted
      type: builtin
    - target: formatted_response_data.append
      type: unresolved
    - target: '"\n".join'
      type: unresolved
    - target: self.tokenizer.encode
      type: instance
    - target: data.append
      type: unresolved
    - target: '"\n\n".join'
      type: unresolved
    - target: self.reduce_system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._reduce_response
    called_by: []
  - name: _stream_reduce_response
    start_line: 415
    end_line: 495
    code: "async def _stream_reduce_response(\n        self,\n        map_responses:\
      \ list[SearchResult],\n        query: str,\n        max_length: int,\n     \
      \   **llm_kwargs,\n    ) -> AsyncGenerator[str, None]:\n        # collect all\
      \ key points into a single list to prepare for sorting\n        key_points =\
      \ []\n        for index, response in enumerate(map_responses):\n           \
      \ if not isinstance(response.response, list):\n                continue\n  \
      \          for element in response.response:\n                if not isinstance(element,\
      \ dict):\n                    continue\n                if \"answer\" not in\
      \ element or \"score\" not in element:\n                    continue\n     \
      \           key_points.append({\n                    \"analyst\": index,\n \
      \                   \"answer\": element[\"answer\"],\n                    \"\
      score\": element[\"score\"],\n                })\n\n        # filter response\
      \ with score = 0 and rank responses by descending order of score\n        filtered_key_points\
      \ = [\n            point\n            for point in key_points\n            if\
      \ point[\"score\"] > 0  # type: ignore\n        ]\n\n        if len(filtered_key_points)\
      \ == 0 and not self.allow_general_knowledge:\n            # return no data answer\
      \ if no key points are found\n            logger.warning(\n                \"\
      Warning: All map responses have score 0 (i.e., no relevant information found\
      \ from the dataset), returning a canned 'I do not know' answer. You can try\
      \ enabling `allow_general_knowledge` to encourage the LLM to incorporate relevant\
      \ general knowledge, at the risk of increasing hallucinations.\"\n         \
      \   )\n            yield NO_DATA_ANSWER\n            return\n\n        filtered_key_points\
      \ = sorted(\n            filtered_key_points,\n            key=lambda x: x[\"\
      score\"],  # type: ignore\n            reverse=True,  # type: ignore\n     \
      \   )\n\n        data = []\n        total_tokens = 0\n        for point in filtered_key_points:\n\
      \            formatted_response_data = [\n                f\"----Analyst {point['analyst']\
      \ + 1}----\",\n                f\"Importance Score: {point['score']}\",\n  \
      \              point[\"answer\"],\n            ]\n            formatted_response_text\
      \ = \"\\n\".join(formatted_response_data)\n            if (\n              \
      \  total_tokens + len(self.tokenizer.encode(formatted_response_text))\n    \
      \            > self.max_data_tokens\n            ):\n                break\n\
      \            data.append(formatted_response_text)\n            total_tokens\
      \ += len(self.tokenizer.encode(formatted_response_text))\n        text_data\
      \ = \"\\n\\n\".join(data)\n\n        search_prompt = self.reduce_system_prompt.format(\n\
      \            report_data=text_data,\n            response_type=self.response_type,\n\
      \            max_length=max_length,\n        )\n        if self.allow_general_knowledge:\n\
      \            search_prompt += \"\\n\" + self.general_knowledge_inclusion_prompt\n\
      \        search_messages = [\n            {\"role\": \"system\", \"content\"\
      : search_prompt},\n        ]\n\n        async for chunk_response in self.model.achat_stream(\n\
      \            prompt=query,\n            history=search_messages,\n         \
      \   **llm_kwargs,\n        ):\n            for callback in self.callbacks:\n\
      \                callback.on_llm_new_token(chunk_response)\n            yield\
      \ chunk_response"
    signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
      \ list[SearchResult],\n        query: str,\n        max_length: int,\n     \
      \   **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: enumerate
      type: builtin
    - target: isinstance
      type: builtin
    - target: key_points.append
      type: unresolved
    - target: len
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: sorted
      type: builtin
    - target: '"\n".join'
      type: unresolved
    - target: self.tokenizer.encode
      type: instance
    - target: data.append
      type: unresolved
    - target: '"\n\n".join'
      type: unresolved
    - target: self.reduce_system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._stream_reduce_response
    called_by: []
- file_name: graphrag/query/structured_search/local_search/__init__.py
  imports: []
  functions: []
- file_name: graphrag/query/structured_search/local_search/mixed_context.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: copy
    name: deepcopy
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.data_model.community_report
    name: CommunityReport
    alias: null
  - module: graphrag.data_model.covariate
    name: Covariate
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.relationship
    name: Relationship
    alias: null
  - module: graphrag.data_model.text_unit
    name: TextUnit
    alias: null
  - module: graphrag.language_model.protocol.base
    name: EmbeddingModel
    alias: null
  - module: graphrag.query.context_builder.builders
    name: ContextBuilderResult
    alias: null
  - module: graphrag.query.context_builder.community_context
    name: build_community_context
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: EntityVectorStoreKey
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: map_query_to_entities
    alias: null
  - module: graphrag.query.context_builder.local_context
    name: build_covariates_context
    alias: null
  - module: graphrag.query.context_builder.local_context
    name: build_entity_context
    alias: null
  - module: graphrag.query.context_builder.local_context
    name: build_relationship_context
    alias: null
  - module: graphrag.query.context_builder.local_context
    name: get_candidate_context
    alias: null
  - module: graphrag.query.context_builder.source_context
    name: build_text_unit_context
    alias: null
  - module: graphrag.query.context_builder.source_context
    name: count_relationships
    alias: null
  - module: graphrag.query.input.retrieval.community_reports
    name: get_candidate_communities
    alias: null
  - module: graphrag.query.input.retrieval.text_units
    name: get_candidate_text_units
    alias: null
  - module: graphrag.query.structured_search.base
    name: LocalContextBuilder
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: __init__
    start_line: 53
    end_line: 85
    code: "def __init__(\n        self,\n        entities: list[Entity],\n       \
      \ entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
      \        text_units: list[TextUnit] | None = None,\n        community_reports:\
      \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n    ):\n        if community_reports is None:\n\
      \            community_reports = []\n        if relationships is None:\n   \
      \         relationships = []\n        if covariates is None:\n            covariates\
      \ = {}\n        if text_units is None:\n            text_units = []\n      \
      \  self.entities = {entity.id: entity for entity in entities}\n        self.community_reports\
      \ = {\n            community.community_id: community for community in community_reports\n\
      \        }\n        self.text_units = {unit.id: unit for unit in text_units}\n\
      \        self.relationships = {\n            relationship.id: relationship for\
      \ relationship in relationships\n        }\n        self.covariates = covariates\n\
      \        self.entity_text_embeddings = entity_text_embeddings\n        self.text_embedder\
      \ = text_embedder\n        self.tokenizer = tokenizer or get_tokenizer()\n \
      \       self.embedding_vectorstore_key = embedding_vectorstore_key"
    signature: "def __init__(\n        self,\n        entities: list[Entity],\n  \
      \      entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
      \        text_units: list[TextUnit] | None = None,\n        community_reports:\
      \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    visibility: protected
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
    called_by: []
  - name: filter_by_entity_keys
    start_line: 87
    end_line: 89
    code: "def filter_by_entity_keys(self, entity_keys: list[int] | list[str]):\n\
      \        \"\"\"Filter entity text embeddings by entity keys.\"\"\"\n       \
      \ self.entity_text_embeddings.filter_by_id(entity_keys)"
    signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
    decorators: []
    raises: []
    calls:
    - target: self.entity_text_embeddings.filter_by_id
      type: instance
    visibility: public
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.filter_by_entity_keys
    called_by: []
  - name: build_context
    start_line: 91
    end_line: 222
    code: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
      \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n \
      \       conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
      \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
      \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
      \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
      \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
      \ str = \"number of relationships\",\n        include_relationship_weight: bool\
      \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n      \
      \  return_candidate_context: bool = False,\n        use_community_summary: bool\
      \ = False,\n        min_community_rank: int = 0,\n        community_context_name:\
      \ str = \"Reports\",\n        column_delimiter: str = \"|\",\n        **kwargs:\
      \ dict[str, Any],\n    ) -> ContextBuilderResult:\n        \"\"\"\n        Build\
      \ data context for local search prompt.\n\n        Build a context by combining\
      \ community reports and entity/relationship/covariate tables, and text units\
      \ using a predefined ratio set by summary_prop.\n        \"\"\"\n        if\
      \ include_entity_names is None:\n            include_entity_names = []\n   \
      \     if exclude_entity_names is None:\n            exclude_entity_names = []\n\
      \        if community_prop + text_unit_prop > 1:\n            value_error =\
      \ (\n                \"The sum of community_prop and text_unit_prop should not\
      \ exceed 1.\"\n            )\n            raise ValueError(value_error)\n\n\
      \        # map user query to entities\n        # if there is conversation history,\
      \ attached the previous user questions to the current query\n        if conversation_history:\n\
      \            pre_user_questions = \"\\n\".join(\n                conversation_history.get_user_turns(conversation_history_max_turns)\n\
      \            )\n            query = f\"{query}\\n{pre_user_questions}\"\n\n\
      \        selected_entities = map_query_to_entities(\n            query=query,\n\
      \            text_embedding_vectorstore=self.entity_text_embeddings,\n     \
      \       text_embedder=self.text_embedder,\n            all_entities_dict=self.entities,\n\
      \            embedding_vectorstore_key=self.embedding_vectorstore_key,\n   \
      \         include_entity_names=include_entity_names,\n            exclude_entity_names=exclude_entity_names,\n\
      \            k=top_k_mapped_entities,\n            oversample_scaler=2,\n  \
      \      )\n\n        # build context\n        final_context = list[str]()\n \
      \       final_context_data = dict[str, pd.DataFrame]()\n\n        if conversation_history:\n\
      \            # build conversation history context\n            (\n         \
      \       conversation_history_context,\n                conversation_history_context_data,\n\
      \            ) = conversation_history.build_context(\n                include_user_turns_only=conversation_history_user_turns_only,\n\
      \                max_qa_turns=conversation_history_max_turns,\n            \
      \    column_delimiter=column_delimiter,\n                max_context_tokens=max_context_tokens,\n\
      \                recency_bias=False,\n            )\n            if conversation_history_context.strip()\
      \ != \"\":\n                final_context.append(conversation_history_context)\n\
      \                final_context_data = conversation_history_context_data\n  \
      \              max_context_tokens = max_context_tokens - len(\n            \
      \        self.tokenizer.encode(conversation_history_context)\n             \
      \   )\n\n        # build community context\n        community_tokens = max(int(max_context_tokens\
      \ * community_prop), 0)\n        community_context, community_context_data =\
      \ self._build_community_context(\n            selected_entities=selected_entities,\n\
      \            max_context_tokens=community_tokens,\n            use_community_summary=use_community_summary,\n\
      \            column_delimiter=column_delimiter,\n            include_community_rank=include_community_rank,\n\
      \            min_community_rank=min_community_rank,\n            return_candidate_context=return_candidate_context,\n\
      \            context_name=community_context_name,\n        )\n        if community_context.strip()\
      \ != \"\":\n            final_context.append(community_context)\n          \
      \  final_context_data = {**final_context_data, **community_context_data}\n\n\
      \        # build local (i.e. entity-relationship-covariate) context\n      \
      \  local_prop = 1 - community_prop - text_unit_prop\n        local_tokens =\
      \ max(int(max_context_tokens * local_prop), 0)\n        local_context, local_context_data\
      \ = self._build_local_context(\n            selected_entities=selected_entities,\n\
      \            max_context_tokens=local_tokens,\n            include_entity_rank=include_entity_rank,\n\
      \            rank_description=rank_description,\n            include_relationship_weight=include_relationship_weight,\n\
      \            top_k_relationships=top_k_relationships,\n            relationship_ranking_attribute=relationship_ranking_attribute,\n\
      \            return_candidate_context=return_candidate_context,\n          \
      \  column_delimiter=column_delimiter,\n        )\n        if local_context.strip()\
      \ != \"\":\n            final_context.append(str(local_context))\n         \
      \   final_context_data = {**final_context_data, **local_context_data}\n\n  \
      \      text_unit_tokens = max(int(max_context_tokens * text_unit_prop), 0)\n\
      \        text_unit_context, text_unit_context_data = self._build_text_unit_context(\n\
      \            selected_entities=selected_entities,\n            max_context_tokens=text_unit_tokens,\n\
      \            return_candidate_context=return_candidate_context,\n        )\n\
      \n        if text_unit_context.strip() != \"\":\n            final_context.append(text_unit_context)\n\
      \            final_context_data = {**final_context_data, **text_unit_context_data}\n\
      \n        return ContextBuilderResult(\n            context_chunks=\"\\n\\n\"\
      .join(final_context),\n            context_records=final_context_data,\n   \
      \     )"
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
      \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n \
      \       conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
      \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
      \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
      \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
      \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
      \ str = \"number of relationships\",\n        include_relationship_weight: bool\
      \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n      \
      \  return_candidate_context: bool = False,\n        use_community_summary: bool\
      \ = False,\n        min_community_rank: int = 0,\n        community_context_name:\
      \ str = \"Reports\",\n        column_delimiter: str = \"|\",\n        **kwargs:\
      \ dict[str, Any],\n    ) -> ContextBuilderResult"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: '"\n".join'
      type: unresolved
    - target: conversation_history.get_user_turns
      type: unresolved
    - target: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
      type: internal
    - target: conversation_history.build_context
      type: unresolved
    - target: conversation_history_context.strip
      type: unresolved
    - target: final_context.append
      type: unresolved
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: max
      type: builtin
    - target: int
      type: builtin
    - target: graphrag/query/structured_search/local_search/mixed_context.py::_build_community_context
      type: internal
    - target: community_context.strip
      type: unresolved
    - target: graphrag/query/structured_search/local_search/mixed_context.py::_build_local_context
      type: internal
    - target: local_context.strip
      type: unresolved
    - target: str
      type: builtin
    - target: graphrag/query/structured_search/local_search/mixed_context.py::_build_text_unit_context
      type: internal
    - target: text_unit_context.strip
      type: unresolved
    - target: graphrag/query/context_builder/builders.py::ContextBuilderResult
      type: internal
    - target: '"\n\n".join'
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
    called_by: []
  - name: _build_community_context
    start_line: 224
    end_line: 304
    code: "def _build_community_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
      \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
      \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
      \ dict[str, pd.DataFrame]]:\n        \"\"\"Add community data to the context\
      \ window until it hits the max_context_tokens limit.\"\"\"\n        if len(selected_entities)\
      \ == 0 or len(self.community_reports) == 0:\n            return (\"\", {context_name.lower():\
      \ pd.DataFrame()})\n\n        community_matches = {}\n        for entity in\
      \ selected_entities:\n            # increase count of the community that this\
      \ entity belongs to\n            if entity.community_ids:\n                for\
      \ community_id in entity.community_ids:\n                    community_matches[community_id]\
      \ = (\n                        community_matches.get(community_id, 0) + 1\n\
      \                    )\n\n        # sort communities by number of matched entities\
      \ and rank\n        selected_communities = [\n            self.community_reports[community_id]\n\
      \            for community_id in community_matches\n            if community_id\
      \ in self.community_reports\n        ]\n        for community in selected_communities:\n\
      \            if community.attributes is None:\n                community.attributes\
      \ = {}\n            community.attributes[\"matches\"] = community_matches[community.community_id]\n\
      \        selected_communities.sort(\n            key=lambda x: (x.attributes[\"\
      matches\"], x.rank),  # type: ignore\n            reverse=True,  # type: ignore\n\
      \        )\n        for community in selected_communities:\n            del\
      \ community.attributes[\"matches\"]  # type: ignore\n\n        context_text,\
      \ context_data = build_community_context(\n            community_reports=selected_communities,\n\
      \            tokenizer=self.tokenizer,\n            use_community_summary=use_community_summary,\n\
      \            column_delimiter=column_delimiter,\n            shuffle_data=False,\n\
      \            include_community_rank=include_community_rank,\n            min_community_rank=min_community_rank,\n\
      \            max_context_tokens=max_context_tokens,\n            single_batch=True,\n\
      \            context_name=context_name,\n        )\n        if isinstance(context_text,\
      \ list) and len(context_text) > 0:\n            context_text = \"\\n\\n\".join(context_text)\n\
      \n        if return_candidate_context:\n            candidate_context_data =\
      \ get_candidate_communities(\n                selected_entities=selected_entities,\n\
      \                community_reports=list(self.community_reports.values()),\n\
      \                use_community_summary=use_community_summary,\n            \
      \    include_community_rank=include_community_rank,\n            )\n       \
      \     context_key = context_name.lower()\n            if context_key not in\
      \ context_data:\n                context_data[context_key] = candidate_context_data\n\
      \                context_data[context_key][\"in_context\"] = False\n       \
      \     else:\n                if (\n                    \"id\" in candidate_context_data.columns\n\
      \                    and \"id\" in context_data[context_key].columns\n     \
      \           ):\n                    candidate_context_data[\"in_context\"] =\
      \ candidate_context_data[\n                        \"id\"\n                \
      \    ].isin(  # cspell:disable-line\n                        context_data[context_key][\"\
      id\"]\n                    )\n                    context_data[context_key]\
      \ = candidate_context_data\n                else:\n                    context_data[context_key][\"\
      in_context\"] = True\n        return (str(context_text), context_data)"
    signature: "def _build_community_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
      \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
      \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
      \ dict[str, pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: context_name.lower
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: community_matches.get
      type: unresolved
    - target: selected_communities.sort
      type: unresolved
    - target: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
    - target: isinstance
      type: builtin
    - target: '"\n\n".join'
      type: unresolved
    - target: graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
      type: internal
    - target: list
      type: builtin
    - target: self.community_reports.values
      type: instance
    - target: "candidate_context_data[\n                        \"id\"\n         \
        \           ].isin"
      type: unresolved
    - target: str
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
    called_by: []
  - name: _build_text_unit_context
    start_line: 306
    end_line: 375
    code: "def _build_text_unit_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]:\n      \
      \  \"\"\"Rank matching text units and add them to the context window until it\
      \ hits the max_context_tokens limit.\"\"\"\n        if not selected_entities\
      \ or not self.text_units:\n            return (\"\", {context_name.lower():\
      \ pd.DataFrame()})\n        selected_text_units = []\n        text_unit_ids_set\
      \ = set()\n\n        unit_info_list = []\n        relationship_values = list(self.relationships.values())\n\
      \n        for index, entity in enumerate(selected_entities):\n            #\
      \ get matching relationships\n            entity_relationships = [\n       \
      \         rel\n                for rel in relationship_values\n            \
      \    if rel.source == entity.title or rel.target == entity.title\n         \
      \   ]\n\n            for text_id in entity.text_unit_ids or []:\n          \
      \      if text_id not in text_unit_ids_set and text_id in self.text_units:\n\
      \                    selected_unit = deepcopy(self.text_units[text_id])\n  \
      \                  num_relationships = count_relationships(\n              \
      \          entity_relationships, selected_unit\n                    )\n    \
      \                text_unit_ids_set.add(text_id)\n                    unit_info_list.append((selected_unit,\
      \ index, num_relationships))\n\n        # sort by entity_order and the number\
      \ of relationships desc\n        unit_info_list.sort(key=lambda x: (x[1], -x[2]))\n\
      \n        selected_text_units = [unit[0] for unit in unit_info_list]\n\n   \
      \     context_text, context_data = build_text_unit_context(\n            text_units=selected_text_units,\n\
      \            tokenizer=self.tokenizer,\n            max_context_tokens=max_context_tokens,\n\
      \            shuffle_data=False,\n            context_name=context_name,\n \
      \           column_delimiter=column_delimiter,\n        )\n\n        if return_candidate_context:\n\
      \            candidate_context_data = get_candidate_text_units(\n          \
      \      selected_entities=selected_entities,\n                text_units=list(self.text_units.values()),\n\
      \            )\n            context_key = context_name.lower()\n           \
      \ if context_key not in context_data:\n                candidate_context_data[\"\
      in_context\"] = False\n                context_data[context_key] = candidate_context_data\n\
      \            else:\n                if (\n                    \"id\" in candidate_context_data.columns\n\
      \                    and \"id\" in context_data[context_key].columns\n     \
      \           ):\n                    candidate_context_data[\"in_context\"] =\
      \ candidate_context_data[\n                        \"id\"\n                \
      \    ].isin(context_data[context_key][\"id\"])\n                    context_data[context_key]\
      \ = candidate_context_data\n                else:\n                    context_data[context_key][\"\
      in_context\"] = True\n\n        return (str(context_text), context_data)"
    signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: context_name.lower
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: set
      type: builtin
    - target: list
      type: builtin
    - target: self.relationships.values
      type: instance
    - target: enumerate
      type: builtin
    - target: copy::deepcopy
      type: stdlib
    - target: graphrag/query/context_builder/source_context.py::count_relationships
      type: internal
    - target: text_unit_ids_set.add
      type: unresolved
    - target: unit_info_list.append
      type: unresolved
    - target: unit_info_list.sort
      type: unresolved
    - target: graphrag/query/context_builder/source_context.py::build_text_unit_context
      type: internal
    - target: graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
      type: internal
    - target: self.text_units.values
      type: instance
    - target: "candidate_context_data[\n                        \"id\"\n         \
        \           ].isin"
      type: unresolved
    - target: str
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
    called_by: []
  - name: _build_local_context
    start_line: 377
    end_line: 493
    code: "def _build_local_context(\n        self,\n        selected_entities: list[Entity],\n\
      \        max_context_tokens: int = 8000,\n        include_entity_rank: bool\
      \ = False,\n        rank_description: str = \"relationship count\",\n      \
      \  include_relationship_weight: bool = False,\n        top_k_relationships:\
      \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n     \
      \   return_candidate_context: bool = False,\n        column_delimiter: str =\
      \ \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]:\n        \"\"\"Build\
      \ data context for local search prompt combining entity/relationship/covariate\
      \ tables.\"\"\"\n        # build entity context\n        entity_context, entity_context_data\
      \ = build_entity_context(\n            selected_entities=selected_entities,\n\
      \            tokenizer=self.tokenizer,\n            max_context_tokens=max_context_tokens,\n\
      \            column_delimiter=column_delimiter,\n            include_entity_rank=include_entity_rank,\n\
      \            rank_description=rank_description,\n            context_name=\"\
      Entities\",\n        )\n        entity_tokens = len(self.tokenizer.encode(entity_context))\n\
      \n        # build relationship-covariate context\n        added_entities = []\n\
      \        final_context = []\n        final_context_data = {}\n\n        # gradually\
      \ add entities and associated metadata to the context until we reach limit\n\
      \        for entity in selected_entities:\n            current_context = []\n\
      \            current_context_data = {}\n            added_entities.append(entity)\n\
      \n            # build relationship context\n            (\n                relationship_context,\n\
      \                relationship_context_data,\n            ) = build_relationship_context(\n\
      \                selected_entities=added_entities,\n                relationships=list(self.relationships.values()),\n\
      \                tokenizer=self.tokenizer,\n                max_context_tokens=max_context_tokens,\n\
      \                column_delimiter=column_delimiter,\n                top_k_relationships=top_k_relationships,\n\
      \                include_relationship_weight=include_relationship_weight,\n\
      \                relationship_ranking_attribute=relationship_ranking_attribute,\n\
      \                context_name=\"Relationships\",\n            )\n          \
      \  current_context.append(relationship_context)\n            current_context_data[\"\
      relationships\"] = relationship_context_data\n            total_tokens = entity_tokens\
      \ + len(\n                self.tokenizer.encode(relationship_context)\n    \
      \        )\n\n            # build covariate context\n            for covariate\
      \ in self.covariates:\n                covariate_context, covariate_context_data\
      \ = build_covariates_context(\n                    selected_entities=added_entities,\n\
      \                    covariates=self.covariates[covariate],\n              \
      \      tokenizer=self.tokenizer,\n                    max_context_tokens=max_context_tokens,\n\
      \                    column_delimiter=column_delimiter,\n                  \
      \  context_name=covariate,\n                )\n                total_tokens\
      \ += len(self.tokenizer.encode(covariate_context))\n                current_context.append(covariate_context)\n\
      \                current_context_data[covariate.lower()] = covariate_context_data\n\
      \n            if total_tokens > max_context_tokens:\n                logger.warning(\n\
      \                    \"Reached token limit - reverting to previous context state\"\
      \n                )\n                break\n\n            final_context = current_context\n\
      \            final_context_data = current_context_data\n\n        # attach entity\
      \ context to final context\n        final_context_text = entity_context + \"\
      \\n\\n\" + \"\\n\\n\".join(final_context)\n        final_context_data[\"entities\"\
      ] = entity_context_data\n\n        if return_candidate_context:\n          \
      \  # we return all the candidate entities/relationships/covariates (not only\
      \ those that were fitted into the context window)\n            # and add a tag\
      \ to indicate which records were included in the context window\n          \
      \  candidate_context_data = get_candidate_context(\n                selected_entities=selected_entities,\n\
      \                entities=list(self.entities.values()),\n                relationships=list(self.relationships.values()),\n\
      \                covariates=self.covariates,\n                include_entity_rank=include_entity_rank,\n\
      \                entity_rank_description=rank_description,\n               \
      \ include_relationship_weight=include_relationship_weight,\n            )\n\
      \            for key in candidate_context_data:\n                candidate_df\
      \ = candidate_context_data[key]\n                if key not in final_context_data:\n\
      \                    final_context_data[key] = candidate_df\n              \
      \      final_context_data[key][\"in_context\"] = False\n                else:\n\
      \                    in_context_df = final_context_data[key]\n\n           \
      \         if \"id\" in in_context_df.columns and \"id\" in candidate_df.columns:\n\
      \                        candidate_df[\"in_context\"] = candidate_df[\n    \
      \                        \"id\"\n                        ].isin(  # cspell:disable-line\n\
      \                            in_context_df[\"id\"]\n                       \
      \ )\n                        final_context_data[key] = candidate_df\n      \
      \              else:\n                        final_context_data[key][\"in_context\"\
      ] = True\n        else:\n            for key in final_context_data:\n      \
      \          final_context_data[key][\"in_context\"] = True\n        return (final_context_text,\
      \ final_context_data)"
    signature: "def _build_local_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
      \ bool = False,\n        rank_description: str = \"relationship count\",\n \
      \       include_relationship_weight: bool = False,\n        top_k_relationships:\
      \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n     \
      \   return_candidate_context: bool = False,\n        column_delimiter: str =\
      \ \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/context_builder/local_context.py::build_entity_context
      type: internal
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: added_entities.append
      type: unresolved
    - target: graphrag/query/context_builder/local_context.py::build_relationship_context
      type: internal
    - target: list
      type: builtin
    - target: self.relationships.values
      type: instance
    - target: current_context.append
      type: unresolved
    - target: graphrag/query/context_builder/local_context.py::build_covariates_context
      type: internal
    - target: covariate.lower
      type: unresolved
    - target: logger.warning
      type: unresolved
    - target: '"\n\n".join'
      type: unresolved
    - target: graphrag/query/context_builder/local_context.py::get_candidate_context
      type: internal
    - target: self.entities.values
      type: instance
    - target: "candidate_df[\n                            \"id\"\n               \
        \         ].isin"
      type: unresolved
    visibility: protected
    node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
    called_by: []
- file_name: graphrag/query/structured_search/local_search/search.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.callbacks.query_callbacks
    name: QueryCallbacks
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  - module: graphrag.prompts.query.local_search_system_prompt
    name: LOCAL_SEARCH_SYSTEM_PROMPT
    alias: null
  - module: graphrag.query.context_builder.builders
    name: LocalContextBuilder
    alias: null
  - module: graphrag.query.context_builder.conversation_history
    name: ConversationHistory
    alias: null
  - module: graphrag.query.structured_search.base
    name: BaseSearch
    alias: null
  - module: graphrag.query.structured_search.base
    name: SearchResult
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 29
    end_line: 49
    code: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    ):\n        super().__init__(\n            model=model,\n\
      \            context_builder=context_builder,\n            tokenizer=tokenizer,\n\
      \            model_params=model_params,\n            context_builder_params=context_builder_params\
      \ or {},\n        )\n        self.system_prompt = system_prompt or LOCAL_SEARCH_SYSTEM_PROMPT\n\
      \        self.callbacks = callbacks or []\n        self.response_type = response_type"
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.__init__
    called_by: []
  - name: search
    start_line: 51
    end_line: 130
    code: "async def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult:\n\
      \        \"\"\"Build local search context that fits a single context window\
      \ and generate answer for the user query.\"\"\"\n        start_time = time.time()\n\
      \        search_prompt = \"\"\n        llm_calls, prompt_tokens, output_tokens\
      \ = {}, {}, {}\n        context_result = self.context_builder.build_context(\n\
      \            query=query,\n            conversation_history=conversation_history,\n\
      \            **kwargs,\n            **self.context_builder_params,\n       \
      \ )\n        llm_calls[\"build_context\"] = context_result.llm_calls\n     \
      \   prompt_tokens[\"build_context\"] = context_result.prompt_tokens\n      \
      \  output_tokens[\"build_context\"] = context_result.output_tokens\n\n     \
      \   logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time, query)\n  \
      \      try:\n            if \"drift_query\" in kwargs:\n                drift_query\
      \ = kwargs[\"drift_query\"]\n                search_prompt = self.system_prompt.format(\n\
      \                    context_data=context_result.context_chunks,\n         \
      \           response_type=self.response_type,\n                    global_query=drift_query,\n\
      \                )\n            else:\n                search_prompt = self.system_prompt.format(\n\
      \                    context_data=context_result.context_chunks,\n         \
      \           response_type=self.response_type,\n                )\n         \
      \   history_messages = [\n                {\"role\": \"system\", \"content\"\
      : search_prompt},\n            ]\n\n            full_response = \"\"\n\n   \
      \         async for response in self.model.achat_stream(\n                prompt=query,\n\
      \                history=history_messages,\n                model_parameters=self.model_params,\n\
      \            ):\n                full_response += response\n               \
      \ for callback in self.callbacks:\n                    callback.on_llm_new_token(response)\n\
      \n            llm_calls[\"response\"] = 1\n            prompt_tokens[\"response\"\
      ] = len(self.tokenizer.encode(search_prompt))\n            output_tokens[\"\
      response\"] = len(self.tokenizer.encode(full_response))\n\n            for callback\
      \ in self.callbacks:\n                callback.on_context(context_result.context_records)\n\
      \n            return SearchResult(\n                response=full_response,\n\
      \                context_data=context_result.context_records,\n            \
      \    context_text=context_result.context_chunks,\n                completion_time=time.time()\
      \ - start_time,\n                llm_calls=sum(llm_calls.values()),\n      \
      \          prompt_tokens=sum(prompt_tokens.values()),\n                output_tokens=sum(output_tokens.values()),\n\
      \                llm_calls_categories=llm_calls,\n                prompt_tokens_categories=prompt_tokens,\n\
      \                output_tokens_categories=output_tokens,\n            )\n\n\
      \        except Exception:\n            logger.exception(\"Exception in _asearch\"\
      )\n            return SearchResult(\n                response=\"\",\n      \
      \          context_data=context_result.context_records,\n                context_text=context_result.context_chunks,\n\
      \                completion_time=time.time() - start_time,\n               \
      \ llm_calls=1,\n                prompt_tokens=len(self.tokenizer.encode(search_prompt)),\n\
      \                output_tokens=0,\n            )"
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    - target: len
      type: builtin
    - target: self.tokenizer.encode
      type: instance
    - target: callback.on_context
      type: unresolved
    - target: graphrag/query/structured_search/base.py::SearchResult
      type: internal
    - target: sum
      type: builtin
    - target: llm_calls.values
      type: unresolved
    - target: prompt_tokens.values
      type: unresolved
    - target: output_tokens.values
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.search
    called_by: []
  - name: stream_search
    start_line: 132
    end_line: 163
    code: "async def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator:\n        \"\"\
      \"Build local search context that fits a single context window and generate\
      \ answer for the user query.\"\"\"\n        start_time = time.time()\n\n   \
      \     context_result = self.context_builder.build_context(\n            query=query,\n\
      \            conversation_history=conversation_history,\n            **self.context_builder_params,\n\
      \        )\n        logger.debug(\"GENERATE ANSWER: %s. QUERY: %s\", start_time,\
      \ query)\n        search_prompt = self.system_prompt.format(\n            context_data=context_result.context_chunks,\
      \ response_type=self.response_type\n        )\n        history_messages = [\n\
      \            {\"role\": \"system\", \"content\": search_prompt},\n        ]\n\
      \n        for callback in self.callbacks:\n            callback.on_context(context_result.context_records)\n\
      \n        async for response in self.model.achat_stream(\n            prompt=query,\n\
      \            history=history_messages,\n            model_parameters=self.model_params,\n\
      \        ):\n            for callback in self.callbacks:\n                callback.on_llm_new_token(response)\n\
      \            yield response"
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator"
    decorators: []
    raises: []
    calls:
    - target: time::time
      type: stdlib
    - target: self.context_builder.build_context
      type: instance
    - target: logger.debug
      type: unresolved
    - target: self.system_prompt.format
      type: instance
    - target: callback.on_context
      type: unresolved
    - target: self.model.achat_stream
      type: instance
    - target: callback.on_llm_new_token
      type: unresolved
    visibility: public
    node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.stream_search
    called_by: []
- file_name: graphrag/storage/__init__.py
  imports: []
  functions: []
- file_name: graphrag/storage/blob_pipeline_storage.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.storage.blob
    name: BlobServiceClient
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: get_timestamp_formatted_with_local_tz
    alias: null
  functions:
  - name: __init__
    start_line: 32
    end_line: 74
    code: "def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Create a new\
      \ BlobStorage instance.\"\"\"\n        connection_string = kwargs.get(\"connection_string\"\
      )\n        storage_account_blob_url = kwargs.get(\"storage_account_blob_url\"\
      )\n        path_prefix = kwargs.get(\"base_dir\")\n        container_name =\
      \ kwargs[\"container_name\"]\n        if container_name is None:\n         \
      \   msg = \"No container name provided for blob storage.\"\n            raise\
      \ ValueError(msg)\n        if connection_string is None and storage_account_blob_url\
      \ is None:\n            msg = \"No storage account blob url provided for blob\
      \ storage.\"\n            raise ValueError(msg)\n\n        logger.info(\"Creating\
      \ blob storage at %s\", container_name)\n        if connection_string:\n   \
      \         self._blob_service_client = BlobServiceClient.from_connection_string(\n\
      \                connection_string\n            )\n        else:\n         \
      \   if storage_account_blob_url is None:\n                msg = \"Either connection_string\
      \ or storage_account_blob_url must be provided.\"\n                raise ValueError(msg)\n\
      \n            self._blob_service_client = BlobServiceClient(\n             \
      \   account_url=storage_account_blob_url,\n                credential=DefaultAzureCredential(),\n\
      \            )\n        self._encoding = kwargs.get(\"encoding\", \"utf-8\"\
      )\n        self._container_name = container_name\n        self._connection_string\
      \ = connection_string\n        self._path_prefix = path_prefix or \"\"\n   \
      \     self._storage_account_blob_url = storage_account_blob_url\n        self._storage_account_name\
      \ = (\n            storage_account_blob_url.split(\"//\")[1].split(\".\")[0]\n\
      \            if storage_account_blob_url\n            else None\n        )\n\
      \        logger.debug(\n            \"creating blob storage at container=%s,\
      \ path=%s\",\n            self._container_name,\n            self._path_prefix,\n\
      \        )\n        self._create_container()"
    signature: 'def __init__(self, **kwargs: Any) -> None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: kwargs.get
      type: unresolved
    - target: ValueError
      type: builtin
    - target: logger.info
      type: unresolved
    - target: azure.storage.blob::BlobServiceClient::from_connection_string
      type: external
    - target: azure.storage.blob::BlobServiceClient
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: storage_account_blob_url.split("//")[1].split
      type: unresolved
    - target: storage_account_blob_url.split
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/storage/blob_pipeline_storage.py::_create_container
      type: internal
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.__init__
    called_by: []
  - name: _create_container
    start_line: 76
    end_line: 85
    code: "def _create_container(self) -> None:\n        \"\"\"Create the container\
      \ if it does not exist.\"\"\"\n        if not self._container_exists():\n  \
      \          container_name = self._container_name\n            container_names\
      \ = [\n                container.name\n                for container in self._blob_service_client.list_containers()\n\
      \            ]\n            if container_name not in container_names:\n    \
      \            self._blob_service_client.create_container(container_name)"
    signature: def _create_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_container_exists
      type: internal
    - target: self._blob_service_client.list_containers
      type: instance
    - target: self._blob_service_client.create_container
      type: instance
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._create_container
    called_by: []
  - name: _delete_container
    start_line: 87
    end_line: 90
    code: "def _delete_container(self) -> None:\n        \"\"\"Delete the container.\"\
      \"\"\n        if self._container_exists():\n            self._blob_service_client.delete_container(self._container_name)"
    signature: def _delete_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_container_exists
      type: internal
    - target: self._blob_service_client.delete_container
      type: instance
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._delete_container
    called_by: []
  - name: _container_exists
    start_line: 92
    end_line: 98
    code: "def _container_exists(self) -> bool:\n        \"\"\"Check if the container\
      \ exists.\"\"\"\n        container_name = self._container_name\n        container_names\
      \ = [\n            container.name for container in self._blob_service_client.list_containers()\n\
      \        ]\n        return container_name in container_names"
    signature: def _container_exists(self) -> bool
    decorators: []
    raises: []
    calls:
    - target: self._blob_service_client.list_containers
      type: instance
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._container_exists
    called_by: []
  - name: find
    start_line: 100
    end_line: 176
    code: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n    \
      \    base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
      \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]:\n\
      \        \"\"\"Find blobs in a container using a file pattern, as well as a\
      \ custom filter function.\n\n        Params:\n            base_dir: The name\
      \ of the base container.\n            file_pattern: The file pattern to use.\n\
      \            file_filter: A dictionary of key-value pairs to filter the blobs.\n\
      \            max_count: The maximum number of blobs to return. If -1, all blobs\
      \ are returned.\n\n        Returns\n        -------\n                An iterator\
      \ of blob names and their corresponding regex matches.\n        \"\"\"\n   \
      \     base_dir = base_dir or \"\"\n\n        logger.info(\n            \"search\
      \ container %s for files matching %s\",\n            self._container_name,\n\
      \            file_pattern.pattern,\n        )\n\n        def _blobname(blob_name:\
      \ str) -> str:\n            if blob_name.startswith(self._path_prefix):\n  \
      \              blob_name = blob_name.replace(self._path_prefix, \"\", 1)\n \
      \           if blob_name.startswith(\"/\"):\n                blob_name = blob_name[1:]\n\
      \            return blob_name\n\n        def item_filter(item: dict[str, Any])\
      \ -> bool:\n            if file_filter is None:\n                return True\n\
      \n            return all(\n                re.search(value, item[key]) for key,\
      \ value in file_filter.items()\n            )\n\n        try:\n            container_client\
      \ = self._blob_service_client.get_container_client(\n                self._container_name\n\
      \            )\n            all_blobs = list(container_client.list_blobs())\n\
      \n            num_loaded = 0\n            num_total = len(list(all_blobs))\n\
      \            num_filtered = 0\n            for blob in all_blobs:\n        \
      \        match = file_pattern.search(blob.name)\n                if match and\
      \ blob.name.startswith(base_dir):\n                    group = match.groupdict()\n\
      \                    if item_filter(group):\n                        yield (_blobname(blob.name),\
      \ group)\n                        num_loaded += 1\n                        if\
      \ max_count > 0 and num_loaded >= max_count:\n                            break\n\
      \                    else:\n                        num_filtered += 1\n    \
      \            else:\n                    num_filtered += 1\n                logger.debug(\n\
      \                    \"Blobs loaded: %d, filtered: %d, total: %d\",\n      \
      \              num_loaded,\n                    num_filtered,\n            \
      \        num_total,\n                )\n        except Exception:  # noqa: BLE001\n\
      \            logger.warning(\n                \"Error finding blobs: base_dir=%s,\
      \ file_pattern=%s, file_filter=%s\",\n                base_dir,\n          \
      \      file_pattern,\n                file_filter,\n            )"
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: list
      type: builtin
    - target: container_client.list_blobs
      type: unresolved
    - target: len
      type: builtin
    - target: file_pattern.search
      type: unresolved
    - target: blob.name.startswith
      type: unresolved
    - target: match.groupdict
      type: unresolved
    - target: graphrag/storage/blob_pipeline_storage.py::item_filter
      type: internal
    - target: graphrag/storage/blob_pipeline_storage.py::_blobname
      type: internal
    - target: logger.debug
      type: unresolved
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.find
    called_by: []
  - name: _blobname
    start_line: 127
    end_line: 132
    code: "def _blobname(blob_name: str) -> str:\n            if blob_name.startswith(self._path_prefix):\n\
      \                blob_name = blob_name.replace(self._path_prefix, \"\", 1)\n\
      \            if blob_name.startswith(\"/\"):\n                blob_name = blob_name[1:]\n\
      \            return blob_name"
    signature: 'def _blobname(blob_name: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: blob_name.startswith
      type: unresolved
    - target: blob_name.replace
      type: unresolved
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._blobname
    called_by: []
  - name: item_filter
    start_line: 134
    end_line: 140
    code: "def item_filter(item: dict[str, Any]) -> bool:\n            if file_filter\
      \ is None:\n                return True\n\n            return all(\n       \
      \         re.search(value, item[key]) for key, value in file_filter.items()\n\
      \            )"
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    decorators: []
    raises: []
    calls:
    - target: all
      type: builtin
    - target: re::search
      type: stdlib
    - target: file_filter.items
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.item_filter
    called_by: []
  - name: get
    start_line: 178
    end_line: 196
    code: "async def get(\n        self, key: str, as_bytes: bool | None = False,\
      \ encoding: str | None = None\n    ) -> Any:\n        \"\"\"Get a value from\
      \ the cache.\"\"\"\n        try:\n            key = self._keyname(key)\n   \
      \         container_client = self._blob_service_client.get_container_client(\n\
      \                self._container_name\n            )\n            blob_client\
      \ = container_client.get_blob_client(key)\n            blob_data = blob_client.download_blob().readall()\n\
      \            if not as_bytes:\n                coding = encoding or self._encoding\n\
      \                blob_data = blob_data.decode(coding)\n        except Exception:\
      \  # noqa: BLE001\n            logger.warning(\"Error getting key %s\", key)\n\
      \            return None\n        else:\n            return blob_data"
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_keyname
      type: internal
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: container_client.get_blob_client
      type: unresolved
    - target: blob_client.download_blob().readall
      type: unresolved
    - target: blob_client.download_blob
      type: unresolved
    - target: blob_data.decode
      type: unresolved
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get
    called_by: []
  - name: set
    start_line: 198
    end_line: 212
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n        \"\"\"Set a value in the cache.\"\"\"\n        try:\n  \
      \          key = self._keyname(key)\n            container_client = self._blob_service_client.get_container_client(\n\
      \                self._container_name\n            )\n            blob_client\
      \ = container_client.get_blob_client(key)\n            if isinstance(value,\
      \ bytes):\n                blob_client.upload_blob(value, overwrite=True)\n\
      \            else:\n                coding = encoding or self._encoding\n  \
      \              blob_client.upload_blob(value.encode(coding), overwrite=True)\n\
      \        except Exception:\n            logger.exception(\"Error setting key\
      \ %s: %s\", key)"
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_keyname
      type: internal
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: container_client.get_blob_client
      type: unresolved
    - target: isinstance
      type: builtin
    - target: blob_client.upload_blob
      type: unresolved
    - target: value.encode
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.set
    called_by: []
  - name: _set_df_json
    start_line: 214
    end_line: 234
    code: "def _set_df_json(self, key: str, dataframe: Any) -> None:\n        \"\"\
      \"Set a json dataframe.\"\"\"\n        if self._connection_string is None and\
      \ self._storage_account_name:\n            dataframe.to_json(\n            \
      \    self._abfs_url(key),\n                storage_options={\n             \
      \       \"account_name\": self._storage_account_name,\n                    \"\
      credential\": DefaultAzureCredential(),\n                },\n              \
      \  orient=\"records\",\n                lines=True,\n                force_ascii=False,\n\
      \            )\n        else:\n            dataframe.to_json(\n            \
      \    self._abfs_url(key),\n                storage_options={\"connection_string\"\
      : self._connection_string},\n                orient=\"records\",\n         \
      \       lines=True,\n                force_ascii=False,\n            )"
    signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
    decorators: []
    raises: []
    calls:
    - target: dataframe.to_json
      type: unresolved
    - target: graphrag/storage/blob_pipeline_storage.py::_abfs_url
      type: internal
    - target: azure.identity::DefaultAzureCredential
      type: external
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_json
    called_by: []
  - name: _set_df_parquet
    start_line: 236
    end_line: 250
    code: "def _set_df_parquet(self, key: str, dataframe: Any) -> None:\n        \"\
      \"\"Set a parquet dataframe.\"\"\"\n        if self._connection_string is None\
      \ and self._storage_account_name:\n            dataframe.to_parquet(\n     \
      \           self._abfs_url(key),\n                storage_options={\n      \
      \              \"account_name\": self._storage_account_name,\n             \
      \       \"credential\": DefaultAzureCredential(),\n                },\n    \
      \        )\n        else:\n            dataframe.to_parquet(\n             \
      \   self._abfs_url(key),\n                storage_options={\"connection_string\"\
      : self._connection_string},\n            )"
    signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
    decorators: []
    raises: []
    calls:
    - target: dataframe.to_parquet
      type: unresolved
    - target: graphrag/storage/blob_pipeline_storage.py::_abfs_url
      type: internal
    - target: azure.identity::DefaultAzureCredential
      type: external
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_parquet
    called_by: []
  - name: has
    start_line: 252
    end_line: 259
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Check if a key exists\
      \ in the cache.\"\"\"\n        key = self._keyname(key)\n        container_client\
      \ = self._blob_service_client.get_container_client(\n            self._container_name\n\
      \        )\n        blob_client = container_client.get_blob_client(key)\n  \
      \      return blob_client.exists()"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_keyname
      type: internal
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: container_client.get_blob_client
      type: unresolved
    - target: blob_client.exists
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.has
    called_by: []
  - name: delete
    start_line: 261
    end_line: 268
    code: "async def delete(self, key: str) -> None:\n        \"\"\"Delete a key from\
      \ the cache.\"\"\"\n        key = self._keyname(key)\n        container_client\
      \ = self._blob_service_client.get_container_client(\n            self._container_name\n\
      \        )\n        blob_client = container_client.get_blob_client(key)\n  \
      \      blob_client.delete_blob()"
    signature: 'def delete(self, key: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_keyname
      type: internal
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: container_client.get_blob_client
      type: unresolved
    - target: blob_client.delete_blob
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.delete
    called_by: []
  - name: clear
    start_line: 270
    end_line: 271
    code: "async def clear(self) -> None:\n        \"\"\"Clear the cache.\"\"\""
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.clear
    called_by: []
  - name: child
    start_line: 273
    end_line: 284
    code: "def child(self, name: str | None) -> \"PipelineStorage\":\n        \"\"\
      \"Create a child storage instance.\"\"\"\n        if name is None:\n       \
      \     return self\n        path = str(Path(self._path_prefix) / name)\n    \
      \    return BlobPipelineStorage(\n            connection_string=self._connection_string,\n\
      \            container_name=self._container_name,\n            encoding=self._encoding,\n\
      \            base_dir=path,\n            storage_account_blob_url=self._storage_account_blob_url,\n\
      \        )"
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: BlobPipelineStorage
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.child
    called_by: []
  - name: keys
    start_line: 286
    end_line: 289
    code: "def keys(self) -> list[str]:\n        \"\"\"Return the keys in the storage.\"\
      \"\"\n        msg = \"Blob storage does yet not support listing keys.\"\n  \
      \      raise NotImplementedError(msg)"
    signature: def keys(self) -> list[str]
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.keys
    called_by: []
  - name: _keyname
    start_line: 291
    end_line: 293
    code: "def _keyname(self, key: str) -> str:\n        \"\"\"Get the key name.\"\
      \"\"\n        return str(Path(self._path_prefix) / key)"
    signature: 'def _keyname(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._keyname
    called_by: []
  - name: _abfs_url
    start_line: 295
    end_line: 298
    code: "def _abfs_url(self, key: str) -> str:\n        \"\"\"Get the ABFS URL.\"\
      \"\"\n        path = str(Path(self._container_name) / self._path_prefix / key)\n\
      \        return f\"abfs://{path}\""
    signature: 'def _abfs_url(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._abfs_url
    called_by: []
  - name: get_creation_date
    start_line: 300
    end_line: 312
    code: "async def get_creation_date(self, key: str) -> str:\n        \"\"\"Get\
      \ a value from the cache.\"\"\"\n        try:\n            key = self._keyname(key)\n\
      \            container_client = self._blob_service_client.get_container_client(\n\
      \                self._container_name\n            )\n            blob_client\
      \ = container_client.get_blob_client(key)\n            timestamp = blob_client.download_blob().properties.creation_time\n\
      \            return get_timestamp_formatted_with_local_tz(timestamp)\n     \
      \   except Exception:  # noqa: BLE001\n            logger.warning(\"Error getting\
      \ key %s\", key)\n            return \"\""
    signature: 'def get_creation_date(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::_keyname
      type: internal
    - target: self._blob_service_client.get_container_client
      type: instance
    - target: container_client.get_blob_client
      type: unresolved
    - target: blob_client.download_blob
      type: unresolved
    - target: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
      type: internal
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
    called_by: []
  - name: validate_blob_container_name
    start_line: 315
    end_line: 365
    code: "def validate_blob_container_name(container_name: str):\n    \"\"\"\n  \
      \  Check if the provided blob container name is valid based on Azure rules.\n\
      \n        - A blob container name must be between 3 and 63 characters in length.\n\
      \        - Start with a letter or number\n        - All letters used in blob\
      \ container names must be lowercase.\n        - Contain only letters, numbers,\
      \ or the hyphen.\n        - Consecutive hyphens are not permitted.\n       \
      \ - Cannot end with a hyphen.\n\n    Args:\n    -----\n    container_name (str)\n\
      \        The blob container name to be validated.\n\n    Returns\n    -------\n\
      \        bool: True if valid, False otherwise.\n    \"\"\"\n    # Check the\
      \ length of the name\n    if len(container_name) < 3 or len(container_name)\
      \ > 63:\n        return ValueError(\n            f\"Container name must be between\
      \ 3 and 63 characters in length. Name provided was {len(container_name)} characters\
      \ long.\"\n        )\n\n    # Check if the name starts with a letter or number\n\
      \    if not container_name[0].isalnum():\n        return ValueError(\n     \
      \       f\"Container name must start with a letter or number. Starting character\
      \ was {container_name[0]}.\"\n        )\n\n    # Check for valid characters\
      \ (letters, numbers, hyphen) and lowercase letters\n    if not re.match(r\"\
      ^[a-z0-9-]+$\", container_name):\n        return ValueError(\n            f\"\
      Container name must only contain:\\n- lowercase letters\\n- numbers\\n- or hyphens\\\
      nName provided was {container_name}.\"\n        )\n\n    # Check for consecutive\
      \ hyphens\n    if \"--\" in container_name:\n        return ValueError(\n  \
      \          f\"Container name cannot contain consecutive hyphens. Name provided\
      \ was {container_name}.\"\n        )\n\n    # Check for hyphens at the end of\
      \ the name\n    if container_name[-1] == \"-\":\n        return ValueError(\n\
      \            f\"Container name cannot end with a hyphen. Name provided was {container_name}.\"\
      \n        )\n\n    return True"
    signature: 'def validate_blob_container_name(container_name: str)'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: ValueError
      type: builtin
    - target: container_name[0].isalnum
      type: unresolved
    - target: re::match
      type: stdlib
    visibility: public
    node_id: graphrag/storage/blob_pipeline_storage.py::validate_blob_container_name
    called_by: []
- file_name: graphrag/storage/cosmosdb_pipeline_storage.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: datetime
    name: timezone
    alias: null
  - module: io
    name: BytesIO
    alias: null
  - module: io
    name: StringIO
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: azure.cosmos
    name: ContainerProxy
    alias: null
  - module: azure.cosmos
    name: CosmosClient
    alias: null
  - module: azure.cosmos
    name: DatabaseProxy
    alias: null
  - module: azure.cosmos.exceptions
    name: CosmosResourceNotFoundError
    alias: null
  - module: azure.cosmos.partition_key
    name: PartitionKey
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: graphrag.logger.progress
    name: Progress
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: get_timestamp_formatted_with_local_tz
    alias: null
  functions:
  - name: __init__
    start_line: 42
    end_line: 86
    code: "def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Create a CosmosDB\
      \ storage instance.\"\"\"\n        logger.info(\"Creating cosmosdb storage\"\
      )\n        cosmosdb_account_url = kwargs.get(\"cosmosdb_account_url\")\n   \
      \     connection_string = kwargs.get(\"connection_string\")\n        database_name\
      \ = kwargs[\"base_dir\"]\n        container_name = kwargs[\"container_name\"\
      ]\n        if not database_name:\n            msg = \"No base_dir provided for\
      \ database name\"\n            raise ValueError(msg)\n        if connection_string\
      \ is None and cosmosdb_account_url is None:\n            msg = \"connection_string\
      \ or cosmosdb_account_url is required.\"\n            raise ValueError(msg)\n\
      \n        if connection_string:\n            self._cosmos_client = CosmosClient.from_connection_string(connection_string)\n\
      \        else:\n            if cosmosdb_account_url is None:\n             \
      \   msg = (\n                    \"Either connection_string or cosmosdb_account_url\
      \ must be provided.\"\n                )\n                raise ValueError(msg)\n\
      \            self._cosmos_client = CosmosClient(\n                url=cosmosdb_account_url,\n\
      \                credential=DefaultAzureCredential(),\n            )\n     \
      \   self._encoding = kwargs.get(\"encoding\", \"utf-8\")\n        self._database_name\
      \ = database_name\n        self._connection_string = connection_string\n   \
      \     self._cosmosdb_account_url = cosmosdb_account_url\n        self._container_name\
      \ = container_name\n        self._cosmosdb_account_name = (\n            cosmosdb_account_url.split(\"\
      //\")[1].split(\".\")[0]\n            if cosmosdb_account_url\n            else\
      \ None\n        )\n        self._no_id_prefixes = []\n        logger.debug(\n\
      \            \"creating cosmosdb storage with account: %s and database: %s and\
      \ container: %s\",\n            self._cosmosdb_account_name,\n            self._database_name,\n\
      \            self._container_name,\n        )\n        self._create_database()\n\
      \        self._create_container()"
    signature: 'def __init__(self, **kwargs: Any) -> None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: logger.info
      type: unresolved
    - target: kwargs.get
      type: unresolved
    - target: ValueError
      type: builtin
    - target: azure.cosmos::CosmosClient::from_connection_string
      type: external
    - target: azure.cosmos::CosmosClient
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: cosmosdb_account_url.split("//")[1].split
      type: unresolved
    - target: cosmosdb_account_url.split
      type: unresolved
    - target: logger.debug
      type: unresolved
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_create_database
      type: internal
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_create_container
      type: internal
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.__init__
    called_by: []
  - name: _create_database
    start_line: 88
    end_line: 92
    code: "def _create_database(self) -> None:\n        \"\"\"Create the database\
      \ if it doesn't exist.\"\"\"\n        self._database_client = self._cosmos_client.create_database_if_not_exists(\n\
      \            id=self._database_name\n        )"
    signature: def _create_database(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._cosmos_client.create_database_if_not_exists
      type: instance
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_database
    called_by: []
  - name: _delete_database
    start_line: 94
    end_line: 100
    code: "def _delete_database(self) -> None:\n        \"\"\"Delete the database\
      \ if it exists.\"\"\"\n        if self._database_client:\n            self._database_client\
      \ = self._cosmos_client.delete_database(\n                self._database_client\n\
      \            )\n        self._container_client = None"
    signature: def _delete_database(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._cosmos_client.delete_database
      type: instance
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_database
    called_by: []
  - name: _create_container
    start_line: 102
    end_line: 111
    code: "def _create_container(self) -> None:\n        \"\"\"Create a container\
      \ for the current container name if it doesn't exist.\"\"\"\n        partition_key\
      \ = PartitionKey(path=\"/id\", kind=\"Hash\")\n        if self._database_client:\n\
      \            self._container_client = (\n                self._database_client.create_container_if_not_exists(\n\
      \                    id=self._container_name,\n                    partition_key=partition_key,\n\
      \                )\n            )"
    signature: def _create_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: azure.cosmos.partition_key::PartitionKey
      type: external
    - target: self._database_client.create_container_if_not_exists
      type: instance
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_container
    called_by: []
  - name: _delete_container
    start_line: 113
    end_line: 118
    code: "def _delete_container(self) -> None:\n        \"\"\"Delete the container\
      \ with the current container name if it exists.\"\"\"\n        if self._database_client\
      \ and self._container_client:\n            self._container_client = self._database_client.delete_container(\n\
      \                self._container_client\n            )"
    signature: def _delete_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._database_client.delete_container
      type: instance
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_container
    called_by: []
  - name: find
    start_line: 120
    end_line: 203
    code: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n    \
      \    base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
      \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]:\n\
      \        \"\"\"Find documents in a Cosmos DB container using a file pattern\
      \ regex and custom file filter (optional).\n\n        Params:\n            base_dir:\
      \ The name of the base directory (not used in Cosmos DB context).\n        \
      \    file_pattern: The file pattern to use.\n            file_filter: A dictionary\
      \ of key-value pairs to filter the documents.\n            max_count: The maximum\
      \ number of documents to return. If -1, all documents are returned.\n\n    \
      \    Returns\n        -------\n            An iterator of document IDs and their\
      \ corresponding regex matches.\n        \"\"\"\n        base_dir = base_dir\
      \ or \"\"\n        logger.info(\n            \"search container %s for documents\
      \ matching %s\",\n            self._container_name,\n            file_pattern.pattern,\n\
      \        )\n        if not self._database_client or not self._container_client:\n\
      \            return\n\n        def item_filter(item: dict[str, Any]) -> bool:\n\
      \            if file_filter is None:\n                return True\n        \
      \    return all(\n                re.search(value, item.get(key, \"\"))\n  \
      \              for key, value in file_filter.items()\n            )\n\n    \
      \    try:\n            query = \"SELECT * FROM c WHERE RegexMatch(c.id, @pattern)\"\
      \n            parameters: list[dict[str, Any]] = [\n                {\"name\"\
      : \"@pattern\", \"value\": file_pattern.pattern}\n            ]\n          \
      \  if file_filter:\n                for key, value in file_filter.items():\n\
      \                    query += f\" AND c.{key} = @{key}\"\n                 \
      \   parameters.append({\"name\": f\"@{key}\", \"value\": value})\n         \
      \   items = list(\n                self._container_client.query_items(\n   \
      \                 query=query,\n                    parameters=parameters,\n\
      \                    enable_cross_partition_query=True,\n                )\n\
      \            )\n            num_loaded = 0\n            num_total = len(items)\n\
      \            if num_total == 0:\n                return\n            num_filtered\
      \ = 0\n            for item in items:\n                match = file_pattern.search(item[\"\
      id\"])\n                if match:\n                    group = match.groupdict()\n\
      \                    if item_filter(group):\n                        yield (item[\"\
      id\"], group)\n                        num_loaded += 1\n                   \
      \     if max_count > 0 and num_loaded >= max_count:\n                      \
      \      break\n                    else:\n                        num_filtered\
      \ += 1\n                else:\n                    num_filtered += 1\n\n   \
      \             progress_status = _create_progress_status(\n                 \
      \   num_loaded, num_filtered, num_total\n                )\n               \
      \ logger.debug(\n                    \"Progress: %s (%d/%d completed)\",\n \
      \                   progress_status.description,\n                    progress_status.completed_items,\n\
      \                    progress_status.total_items,\n                )\n     \
      \   except Exception:  # noqa: BLE001\n            logger.warning(\n       \
      \         \"An error occurred while searching for documents in Cosmos DB.\"\n\
      \            )"
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    decorators: []
    raises: []
    calls:
    - target: logger.info
      type: unresolved
    - target: file_filter.items
      type: unresolved
    - target: parameters.append
      type: unresolved
    - target: list
      type: builtin
    - target: self._container_client.query_items
      type: instance
    - target: len
      type: builtin
    - target: file_pattern.search
      type: unresolved
    - target: match.groupdict
      type: unresolved
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::item_filter
      type: internal
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_create_progress_status
      type: internal
    - target: logger.debug
      type: unresolved
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
    called_by: []
  - name: item_filter
    start_line: 148
    end_line: 154
    code: "def item_filter(item: dict[str, Any]) -> bool:\n            if file_filter\
      \ is None:\n                return True\n            return all(\n         \
      \       re.search(value, item.get(key, \"\"))\n                for key, value\
      \ in file_filter.items()\n            )"
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    decorators: []
    raises: []
    calls:
    - target: all
      type: builtin
    - target: re::search
      type: stdlib
    - target: item.get
      type: unresolved
    - target: file_filter.items
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.item_filter
    called_by: []
  - name: get
    start_line: 205
    end_line: 239
    code: "async def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any:\n        \"\"\"Fetch all items in a container\
      \ that match the given key.\"\"\"\n        try:\n            if not self._database_client\
      \ or not self._container_client:\n                return None\n            if\
      \ as_bytes:\n                prefix = self._get_prefix(key)\n              \
      \  query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"  # noqa:\
      \ S608\n                queried_items = self._container_client.query_items(\n\
      \                    query=query, enable_cross_partition_query=True\n      \
      \          )\n                items_list = list(queried_items)\n           \
      \     for item in items_list:\n                    item[\"id\"] = item[\"id\"\
      ].split(\":\")[1]\n\n                items_json_str = json.dumps(items_list)\n\
      \n                items_df = pd.read_json(\n                    StringIO(items_json_str),\
      \ orient=\"records\", lines=False\n                )\n\n                # Drop\
      \ the \"id\" column if the original dataframe does not include it\n        \
      \        # TODO: Figure out optimal way to handle missing id keys in input dataframes\n\
      \                if prefix in self._no_id_prefixes:\n                    items_df.drop(columns=[\"\
      id\"], axis=1, inplace=True)\n\n                return items_df.to_parquet()\n\
      \            item = self._container_client.read_item(item=key, partition_key=key)\n\
      \            item_body = item.get(\"body\")\n            return json.dumps(item_body)\n\
      \        except Exception:  # noqa: BLE001\n            logger.warning(\"Error\
      \ reading item %s\", key)\n            return None"
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
      type: internal
    - target: self._container_client.query_items
      type: instance
    - target: list
      type: builtin
    - target: item["id"].split
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: pandas::read_json
      type: external
    - target: io::StringIO
      type: stdlib
    - target: items_df.drop
      type: unresolved
    - target: items_df.to_parquet
      type: unresolved
    - target: self._container_client.read_item
      type: instance
    - target: item.get
      type: unresolved
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get
    called_by: []
  - name: set
    start_line: 241
    end_line: 279
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n        \"\"\"Insert the contents of a file into a cosmosdb container\
      \ for the given filename key.\n\n        For better optimization, the file is\
      \ destructured such that each row is a unique cosmosdb item.\n        \"\"\"\
      \n        try:\n            if not self._database_client or not self._container_client:\n\
      \                msg = \"Database or container not initialized\"\n         \
      \       raise ValueError(msg)  # noqa: TRY301\n            # value represents\
      \ a parquet file\n            if isinstance(value, bytes):\n               \
      \ prefix = self._get_prefix(key)\n                value_df = pd.read_parquet(BytesIO(value))\n\
      \                value_json = value_df.to_json(\n                    orient=\"\
      records\", lines=False, force_ascii=False\n                )\n             \
      \   if value_json is None:\n                    logger.error(\"Error converting\
      \ output %s to json\", key)\n                else:\n                    cosmosdb_item_list\
      \ = json.loads(value_json)\n                    for index, cosmosdb_item in\
      \ enumerate(cosmosdb_item_list):\n                        # If the id key does\
      \ not exist in the input dataframe json, create a unique id using the prefix\
      \ and item index\n                        # TODO: Figure out optimal way to\
      \ handle missing id keys in input dataframes\n                        if \"\
      id\" not in cosmosdb_item:\n                            prefixed_id = f\"{prefix}:{index}\"\
      \n                            self._no_id_prefixes.append(prefix)\n        \
      \                else:\n                            prefixed_id = f\"{prefix}:{cosmosdb_item['id']}\"\
      \n                        cosmosdb_item[\"id\"] = prefixed_id\n            \
      \            self._container_client.upsert_item(body=cosmosdb_item)\n      \
      \      # value represents a cache output or stats.json\n            else:\n\
      \                cosmosdb_item = {\n                    \"id\": key,\n     \
      \               \"body\": json.loads(value),\n                }\n          \
      \      self._container_client.upsert_item(body=cosmosdb_item)\n        except\
      \ Exception:\n            logger.exception(\"Error writing item %s\", key)"
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: isinstance
      type: builtin
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
      type: internal
    - target: pandas::read_parquet
      type: external
    - target: io::BytesIO
      type: stdlib
    - target: value_df.to_json
      type: unresolved
    - target: logger.error
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: enumerate
      type: builtin
    - target: self._no_id_prefixes.append
      type: instance
    - target: self._container_client.upsert_item
      type: instance
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.set
    called_by: []
  - name: has
    start_line: 281
    end_line: 296
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Check if the contents\
      \ of the given filename key exist in the cosmosdb storage.\"\"\"\n        if\
      \ not self._database_client or not self._container_client:\n            return\
      \ False\n        if \".parquet\" in key:\n            prefix = self._get_prefix(key)\n\
      \            query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"\
      \  # noqa: S608\n            queried_items = self._container_client.query_items(\n\
      \                query=query, enable_cross_partition_query=True\n          \
      \  )\n            return len(list(queried_items)) > 0\n        query = f\"SELECT\
      \ * FROM c WHERE c.id = '{key}'\"  # noqa: S608\n        queried_items = self._container_client.query_items(\n\
      \            query=query, enable_cross_partition_query=True\n        )\n   \
      \     return len(list(queried_items)) == 1"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
      type: internal
    - target: self._container_client.query_items
      type: instance
    - target: len
      type: builtin
    - target: list
      type: builtin
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.has
    called_by: []
  - name: delete
    start_line: 298
    end_line: 318
    code: "async def delete(self, key: str) -> None:\n        \"\"\"Delete all cosmosdb\
      \ items belonging to the given filename key.\"\"\"\n        if not self._database_client\
      \ or not self._container_client:\n            return\n        try:\n       \
      \     if \".parquet\" in key:\n                prefix = self._get_prefix(key)\n\
      \                query = f\"SELECT * FROM c WHERE STARTSWITH(c.id, '{prefix}')\"\
      \  # noqa: S608\n                queried_items = self._container_client.query_items(\n\
      \                    query=query, enable_cross_partition_query=True\n      \
      \          )\n                for item in queried_items:\n                 \
      \   self._container_client.delete_item(\n                        item=item[\"\
      id\"], partition_key=item[\"id\"]\n                    )\n            else:\n\
      \                self._container_client.delete_item(item=key, partition_key=key)\n\
      \        except CosmosResourceNotFoundError:\n            return\n        except\
      \ Exception:\n            logger.exception(\"Error deleting item %s\", key)"
    signature: 'def delete(self, key: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
      type: internal
    - target: self._container_client.query_items
      type: instance
    - target: self._container_client.delete_item
      type: instance
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.delete
    called_by: []
  - name: clear
    start_line: 320
    end_line: 326
    code: "async def clear(self) -> None:\n        \"\"\"Clear all contents from storage.\n\
      \n        # This currently deletes the database, including all containers and\
      \ data within it.\n        # TODO: We should decide what granularity of deletion\
      \ is the ideal behavior (e.g. delete all items within a container, delete the\
      \ current container, delete the current database)\n        \"\"\"\n        self._delete_database()"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::_delete_database
      type: internal
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.clear
    called_by: []
  - name: keys
    start_line: 328
    end_line: 331
    code: "def keys(self) -> list[str]:\n        \"\"\"Return the keys in the storage.\"\
      \"\"\n        msg = \"CosmosDB storage does yet not support listing keys.\"\n\
      \        raise NotImplementedError(msg)"
    signature: def keys(self) -> list[str]
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.keys
    called_by: []
  - name: child
    start_line: 333
    end_line: 335
    code: "def child(self, name: str | None) -> PipelineStorage:\n        \"\"\"Create\
      \ a child storage instance.\"\"\"\n        return self"
    signature: 'def child(self, name: str | None) -> PipelineStorage'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.child
    called_by: []
  - name: _get_prefix
    start_line: 337
    end_line: 339
    code: "def _get_prefix(self, key: str) -> str:\n        \"\"\"Get the prefix of\
      \ the filename key.\"\"\"\n        return key.split(\".\")[0]"
    signature: 'def _get_prefix(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: key.split
      type: unresolved
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._get_prefix
    called_by: []
  - name: get_creation_date
    start_line: 341
    end_line: 353
    code: "async def get_creation_date(self, key: str) -> str:\n        \"\"\"Get\
      \ a value from the cache.\"\"\"\n        try:\n            if not self._database_client\
      \ or not self._container_client:\n                return \"\"\n            item\
      \ = self._container_client.read_item(item=key, partition_key=key)\n        \
      \    return get_timestamp_formatted_with_local_tz(\n                datetime.fromtimestamp(item[\"\
      _ts\"], tz=timezone.utc)\n            )\n\n        except Exception:  # noqa:\
      \ BLE001\n            logger.warning(\"Error getting key %s\", key)\n      \
      \      return \"\""
    signature: 'def get_creation_date(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: self._container_client.read_item
      type: instance
    - target: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
      type: internal
    - target: datetime::datetime::fromtimestamp
      type: external
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
    called_by: []
  - name: _create_progress_status
    start_line: 356
    end_line: 363
    code: "def _create_progress_status(\n    num_loaded: int, num_filtered: int, num_total:\
      \ int\n) -> Progress:\n    return Progress(\n        total_items=num_total,\n\
      \        completed_items=num_loaded + num_filtered,\n        description=f\"\
      {num_loaded} files loaded ({num_filtered} filtered)\",\n    )"
    signature: "def _create_progress_status(\n    num_loaded: int, num_filtered: int,\
      \ num_total: int\n) -> Progress"
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/progress.py::Progress
      type: internal
    visibility: protected
    node_id: graphrag/storage/cosmosdb_pipeline_storage.py::_create_progress_status
    called_by:
    - source: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
      type: internal
- file_name: graphrag/storage/factory.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: StorageType
    alias: null
  - module: graphrag.storage.blob_pipeline_storage
    name: BlobPipelineStorage
    alias: null
  - module: graphrag.storage.cosmosdb_pipeline_storage
    name: CosmosDBPipelineStorage
    alias: null
  - module: graphrag.storage.file_pipeline_storage
    name: FilePipelineStorage
    alias: null
  - module: graphrag.storage.memory_pipeline_storage
    name: MemoryPipelineStorage
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: register
    start_line: 34
    end_line: 44
    code: "def register(\n        cls, storage_type: str, creator: Callable[..., PipelineStorage]\n\
      \    ) -> None:\n        \"\"\"Register a custom storage implementation.\n\n\
      \        Args:\n            storage_type: The type identifier for the storage.\n\
      \            creator: A class or callable that creates an instance of PipelineStorage.\n\
      \n        \"\"\"\n        cls._registry[storage_type] = creator"
    signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
      \ PipelineStorage]\n    ) -> None"
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/factory.py::StorageFactory.register
    called_by: []
  - name: create_storage
    start_line: 47
    end_line: 66
    code: "def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage:\n\
      \        \"\"\"Create a storage object from the provided type.\n\n        Args:\n\
      \            storage_type: The type of storage to create.\n            kwargs:\
      \ Additional keyword arguments for the storage constructor.\n\n        Returns\n\
      \        -------\n            A PipelineStorage instance.\n\n        Raises\n\
      \        ------\n            ValueError: If the storage type is not registered.\n\
      \        \"\"\"\n        if storage_type not in cls._registry:\n           \
      \ msg = f\"Unknown storage type: {storage_type}\"\n            raise ValueError(msg)\n\
      \n        return cls._registry[storage_type](**kwargs)"
    signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/storage/factory.py::StorageFactory.create_storage
    called_by: []
  - name: get_storage_types
    start_line: 69
    end_line: 71
    code: "def get_storage_types(cls) -> list[str]:\n        \"\"\"Get the registered\
      \ storage implementations.\"\"\"\n        return list(cls._registry.keys())"
    signature: def get_storage_types(cls) -> list[str]
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: list
      type: builtin
    - target: cls._registry.keys
      type: unresolved
    visibility: public
    node_id: graphrag/storage/factory.py::StorageFactory.get_storage_types
    called_by: []
  - name: is_supported_type
    start_line: 74
    end_line: 76
    code: "def is_supported_type(cls, storage_type: str) -> bool:\n        \"\"\"\
      Check if the given storage type is supported.\"\"\"\n        return storage_type\
      \ in cls._registry"
    signature: 'def is_supported_type(cls, storage_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/factory.py::StorageFactory.is_supported_type
    called_by: []
- file_name: graphrag/storage/file_pipeline_storage.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: shutil
    name: null
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: datetime
    name: timezone
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: aiofiles
    name: null
    alias: null
  - module: aiofiles.os
    name: remove
    alias: null
  - module: aiofiles.ospath
    name: exists
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: get_timestamp_formatted_with_local_tz
    alias: null
  functions:
  - name: __init__
    start_line: 33
    end_line: 38
    code: "def __init__(self, **kwargs: Any) -> None:\n        \"\"\"Create a file\
      \ based storage.\"\"\"\n        self._root_dir = kwargs.get(\"base_dir\", \"\
      \")\n        self._encoding = kwargs.get(\"encoding\", \"utf-8\")\n        logger.info(\"\
      Creating file storage at %s\", self._root_dir)\n        Path(self._root_dir).mkdir(parents=True,\
      \ exist_ok=True)"
    signature: 'def __init__(self, **kwargs: Any) -> None'
    decorators: []
    raises: []
    calls:
    - target: kwargs.get
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: Path(self._root_dir).mkdir
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    visibility: protected
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.__init__
    called_by: []
  - name: find
    start_line: 40
    end_line: 85
    code: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n    \
      \    base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
      \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]:\n\
      \        \"\"\"Find files in the storage using a file pattern, as well as a\
      \ custom filter function.\"\"\"\n\n        def item_filter(item: dict[str, Any])\
      \ -> bool:\n            if file_filter is None:\n                return True\n\
      \            return all(\n                re.search(value, item[key]) for key,\
      \ value in file_filter.items()\n            )\n\n        search_path = Path(self._root_dir)\
      \ / (base_dir or \"\")\n        logger.info(\n            \"search %s for files\
      \ matching %s\", search_path, file_pattern.pattern\n        )\n        all_files\
      \ = list(search_path.rglob(\"**/*\"))\n        num_loaded = 0\n        num_total\
      \ = len(all_files)\n        num_filtered = 0\n        for file in all_files:\n\
      \            match = file_pattern.search(f\"{file}\")\n            if match:\n\
      \                group = match.groupdict()\n                if item_filter(group):\n\
      \                    filename = f\"{file}\".replace(self._root_dir, \"\")\n\
      \                    if filename.startswith(os.sep):\n                     \
      \   filename = filename[1:]\n                    yield (filename, group)\n \
      \                   num_loaded += 1\n                    if max_count > 0 and\
      \ num_loaded >= max_count:\n                        break\n                else:\n\
      \                    num_filtered += 1\n            else:\n                num_filtered\
      \ += 1\n            logger.debug(\n                \"Files loaded: %d, filtered:\
      \ %d, total: %d\",\n                num_loaded,\n                num_filtered,\n\
      \                num_total,\n            )"
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: logger.info
      type: unresolved
    - target: list
      type: builtin
    - target: search_path.rglob
      type: unresolved
    - target: len
      type: builtin
    - target: file_pattern.search
      type: unresolved
    - target: match.groupdict
      type: unresolved
    - target: graphrag/storage/file_pipeline_storage.py::item_filter
      type: internal
    - target: f"{file}".replace
      type: unresolved
    - target: filename.startswith
      type: unresolved
    - target: logger.debug
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.find
    called_by: []
  - name: item_filter
    start_line: 49
    end_line: 54
    code: "def item_filter(item: dict[str, Any]) -> bool:\n            if file_filter\
      \ is None:\n                return True\n            return all(\n         \
      \       re.search(value, item[key]) for key, value in file_filter.items()\n\
      \            )"
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    decorators: []
    raises: []
    calls:
    - target: all
      type: builtin
    - target: re::search
      type: stdlib
    - target: file_filter.items
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.item_filter
    called_by: []
  - name: get
    start_line: 87
    end_line: 100
    code: "async def get(\n        self, key: str, as_bytes: bool | None = False,\
      \ encoding: str | None = None\n    ) -> Any:\n        \"\"\"Get method definition.\"\
      \"\"\n        file_path = join_path(self._root_dir, key)\n\n        if await\
      \ self.has(key):\n            return await self._read_file(file_path, as_bytes,\
      \ encoding)\n        if await exists(key):\n            # Lookup for key, as\
      \ it is pressumably a new file loaded from inputs\n            # and not yet\
      \ written to storage\n            return await self._read_file(key, as_bytes,\
      \ encoding)\n\n        return None"
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/file_pipeline_storage.py::join_path
      type: internal
    - target: graphrag/storage/file_pipeline_storage.py::has
      type: internal
    - target: graphrag/storage/file_pipeline_storage.py::_read_file
      type: internal
    - target: aiofiles.ospath::exists
      type: external
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
    called_by: []
  - name: _read_file
    start_line: 102
    end_line: 117
    code: "async def _read_file(\n        self,\n        path: str | Path,\n     \
      \   as_bytes: bool | None = False,\n        encoding: str | None = None,\n \
      \   ) -> Any:\n        \"\"\"Read the contents of a file.\"\"\"\n        read_type\
      \ = \"rb\" if as_bytes else \"r\"\n        encoding = None if as_bytes else\
      \ (encoding or self._encoding)\n\n        async with aiofiles.open(\n      \
      \      path,\n            cast(\"Any\", read_type),\n            encoding=encoding,\n\
      \        ) as f:\n            return await f.read()"
    signature: "def _read_file(\n        self,\n        path: str | Path,\n      \
      \  as_bytes: bool | None = False,\n        encoding: str | None = None,\n  \
      \  ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: aiofiles::open
      type: external
    - target: typing::cast
      type: stdlib
    - target: f.read
      type: unresolved
    visibility: protected
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage._read_file
    called_by: []
  - name: set
    start_line: 119
    end_line: 129
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n        \"\"\"Set method definition.\"\"\"\n        is_bytes = isinstance(value,\
      \ bytes)\n        write_type = \"wb\" if is_bytes else \"w\"\n        encoding\
      \ = None if is_bytes else encoding or self._encoding\n        async with aiofiles.open(\n\
      \            join_path(self._root_dir, key),\n            cast(\"Any\", write_type),\n\
      \            encoding=encoding,\n        ) as f:\n            await f.write(value)"
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: aiofiles::open
      type: external
    - target: graphrag/storage/file_pipeline_storage.py::join_path
      type: internal
    - target: typing::cast
      type: stdlib
    - target: f.write
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
    called_by: []
  - name: has
    start_line: 131
    end_line: 133
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Has method definition.\"\
      \"\"\n        return await exists(join_path(self._root_dir, key))"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls:
    - target: aiofiles.ospath::exists
      type: external
    - target: graphrag/storage/file_pipeline_storage.py::join_path
      type: internal
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
    called_by: []
  - name: delete
    start_line: 135
    end_line: 138
    code: "async def delete(self, key: str) -> None:\n        \"\"\"Delete method\
      \ definition.\"\"\"\n        if await self.has(key):\n            await remove(join_path(self._root_dir,\
      \ key))"
    signature: 'def delete(self, key: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/file_pipeline_storage.py::has
      type: internal
    - target: aiofiles.os::remove
      type: external
    - target: graphrag/storage/file_pipeline_storage.py::join_path
      type: internal
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
    called_by: []
  - name: clear
    start_line: 140
    end_line: 146
    code: "async def clear(self) -> None:\n        \"\"\"Clear method definition.\"\
      \"\"\n        for file in Path(self._root_dir).glob(\"*\"):\n            if\
      \ file.is_dir():\n                shutil.rmtree(file)\n            else:\n \
      \               file.unlink()"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls:
    - target: Path(self._root_dir).glob
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: file.is_dir
      type: unresolved
    - target: shutil::rmtree
      type: stdlib
    - target: file.unlink
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.clear
    called_by: []
  - name: child
    start_line: 148
    end_line: 153
    code: "def child(self, name: str | None) -> \"PipelineStorage\":\n        \"\"\
      \"Create a child storage instance.\"\"\"\n        if name is None:\n       \
      \     return self\n        child_path = str(Path(self._root_dir) / Path(name))\n\
      \        return FilePipelineStorage(base_dir=child_path, encoding=self._encoding)"
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: FilePipelineStorage
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.child
    called_by: []
  - name: keys
    start_line: 155
    end_line: 157
    code: "def keys(self) -> list[str]:\n        \"\"\"Return the keys in the storage.\"\
      \"\"\n        return [item.name for item in Path(self._root_dir).iterdir() if\
      \ item.is_file()]"
    signature: def keys(self) -> list[str]
    decorators: []
    raises: []
    calls:
    - target: Path(self._root_dir).iterdir
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: item.is_file
      type: unresolved
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.keys
    called_by: []
  - name: get_creation_date
    start_line: 159
    end_line: 166
    code: "async def get_creation_date(self, key: str) -> str:\n        \"\"\"Get\
      \ the creation date of a file.\"\"\"\n        file_path = Path(join_path(self._root_dir,\
      \ key))\n\n        creation_timestamp = file_path.stat().st_ctime\n        creation_time_utc\
      \ = datetime.fromtimestamp(creation_timestamp, tz=timezone.utc)\n\n        return\
      \ get_timestamp_formatted_with_local_tz(creation_time_utc)"
    signature: 'def get_creation_date(self, key: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/storage/file_pipeline_storage.py::join_path
      type: internal
    - target: file_path.stat
      type: unresolved
    - target: datetime::datetime::fromtimestamp
      type: external
    - target: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
      type: internal
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
    called_by: []
  - name: join_path
    start_line: 169
    end_line: 171
    code: "def join_path(file_path: str, file_name: str) -> Path:\n    \"\"\"Join\
      \ a path and a file. Independent of the OS.\"\"\"\n    return Path(file_path)\
      \ / Path(file_name).parent / Path(file_name).name"
    signature: 'def join_path(file_path: str, file_name: str) -> Path'
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    visibility: public
    node_id: graphrag/storage/file_pipeline_storage.py::join_path
    called_by:
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
      type: internal
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
      type: internal
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
      type: internal
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
      type: internal
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
      type: internal
- file_name: graphrag/storage/memory_pipeline_storage.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.storage.file_pipeline_storage
    name: FilePipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: __init__
    start_line: 19
    end_line: 22
    code: "def __init__(self):\n        \"\"\"Init method definition.\"\"\"\n    \
      \    super().__init__()\n        self._storage = {}"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.__init__
    called_by: []
  - name: get
    start_line: 24
    end_line: 37
    code: "async def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any:\n        \"\"\"Get the value for the given\
      \ key.\n\n        Args:\n            - key - The key to get the value for.\n\
      \            - as_bytes - Whether or not to return the value as bytes.\n\n \
      \       Returns\n        -------\n            - output - The value for the given\
      \ key.\n        \"\"\"\n        return self._storage.get(key)"
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    decorators: []
    raises: []
    calls:
    - target: self._storage.get
      type: instance
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.get
    called_by: []
  - name: set
    start_line: 39
    end_line: 46
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n        \"\"\"Set the value for the given key.\n\n        Args:\n\
      \            - key - The key to set the value for.\n            - value - The\
      \ value to set.\n        \"\"\"\n        self._storage[key] = value"
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.set
    called_by: []
  - name: has
    start_line: 48
    end_line: 58
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Return True if the\
      \ given key exists in the storage.\n\n        Args:\n            - key - The\
      \ key to check for.\n\n        Returns\n        -------\n            - output\
      \ - True if the key exists in the storage, False otherwise.\n        \"\"\"\n\
      \        return key in self._storage"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.has
    called_by: []
  - name: delete
    start_line: 60
    end_line: 66
    code: "async def delete(self, key: str) -> None:\n        \"\"\"Delete the given\
      \ key from the storage.\n\n        Args:\n            - key - The key to delete.\n\
      \        \"\"\"\n        del self._storage[key]"
    signature: 'def delete(self, key: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.delete
    called_by: []
  - name: clear
    start_line: 68
    end_line: 70
    code: "async def clear(self) -> None:\n        \"\"\"Clear the storage.\"\"\"\n\
      \        self._storage.clear()"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._storage.clear
      type: instance
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.clear
    called_by: []
  - name: child
    start_line: 72
    end_line: 74
    code: "def child(self, name: str | None) -> \"PipelineStorage\":\n        \"\"\
      \"Create a child storage instance.\"\"\"\n        return MemoryPipelineStorage()"
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    decorators: []
    raises: []
    calls:
    - target: MemoryPipelineStorage
      type: unresolved
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.child
    called_by: []
  - name: keys
    start_line: 76
    end_line: 78
    code: "def keys(self) -> list[str]:\n        \"\"\"Return the keys in the storage.\"\
      \"\"\n        return list(self._storage.keys())"
    signature: def keys(self) -> list[str]
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: self._storage.keys
      type: instance
    visibility: public
    node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.keys
    called_by: []
- file_name: graphrag/storage/pipeline_storage.py
  imports:
  - module: re
    name: null
    alias: null
  - module: abc
    name: ABCMeta
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: typing
    name: Any
    alias: null
  functions:
  - name: find
    start_line: 17
    end_line: 24
    code: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n    \
      \    base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
      \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]:\n\
      \        \"\"\"Find files in the storage using a file pattern, as well as a\
      \ custom filter function.\"\"\""
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.find
    called_by: []
  - name: get
    start_line: 27
    end_line: 39
    code: "async def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any:\n        \"\"\"Get the value for the given\
      \ key.\n\n        Args:\n            - key - The key to get the value for.\n\
      \            - as_bytes - Whether or not to return the value as bytes.\n\n \
      \       Returns\n        -------\n            - output - The value for the given\
      \ key.\n        \"\"\""
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get
    called_by: []
  - name: set
    start_line: 42
    end_line: 48
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n        \"\"\"Set the value for the given key.\n\n        Args:\n\
      \            - key - The key to set the value for.\n            - value - The\
      \ value to set.\n        \"\"\""
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.set
    called_by: []
  - name: has
    start_line: 51
    end_line: 60
    code: "async def has(self, key: str) -> bool:\n        \"\"\"Return True if the\
      \ given key exists in the storage.\n\n        Args:\n            - key - The\
      \ key to check for.\n\n        Returns\n        -------\n            - output\
      \ - True if the key exists in the storage, False otherwise.\n        \"\"\""
    signature: 'def has(self, key: str) -> bool'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.has
    called_by: []
  - name: delete
    start_line: 63
    end_line: 68
    code: "async def delete(self, key: str) -> None:\n        \"\"\"Delete the given\
      \ key from the storage.\n\n        Args:\n            - key - The key to delete.\n\
      \        \"\"\""
    signature: 'def delete(self, key: str) -> None'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.delete
    called_by: []
  - name: clear
    start_line: 71
    end_line: 72
    code: "async def clear(self) -> None:\n        \"\"\"Clear the storage.\"\"\""
    signature: def clear(self) -> None
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.clear
    called_by: []
  - name: child
    start_line: 75
    end_line: 76
    code: "def child(self, name: str | None) -> \"PipelineStorage\":\n        \"\"\
      \"Create a child storage instance.\"\"\""
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.child
    called_by: []
  - name: keys
    start_line: 79
    end_line: 80
    code: "def keys(self) -> list[str]:\n        \"\"\"List all keys in the storage.\"\
      \"\""
    signature: def keys(self) -> list[str]
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.keys
    called_by: []
  - name: get_creation_date
    start_line: 83
    end_line: 92
    code: "async def get_creation_date(self, key: str) -> str:\n        \"\"\"Get\
      \ the creation date for the given key.\n\n        Args:\n            - key -\
      \ The key to get the creation date for.\n\n        Returns\n        -------\n\
      \            - output - The creation date for the given key.\n        \"\"\""
    signature: 'def get_creation_date(self, key: str) -> str'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get_creation_date
    called_by: []
  - name: get_timestamp_formatted_with_local_tz
    start_line: 95
    end_line: 99
    code: "def get_timestamp_formatted_with_local_tz(timestamp: datetime) -> str:\n\
      \    \"\"\"Get the formatted timestamp with the local time zone.\"\"\"\n   \
      \ creation_time_local = timestamp.astimezone()\n\n    return creation_time_local.strftime(\"\
      %Y-%m-%d %H:%M:%S %z\")"
    signature: 'def get_timestamp_formatted_with_local_tz(timestamp: datetime) ->
      str'
    decorators: []
    raises: []
    calls:
    - target: timestamp.astimezone
      type: unresolved
    - target: creation_time_local.strftime
      type: unresolved
    visibility: public
    node_id: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
    called_by:
    - source: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
      type: internal
    - source: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
      type: internal
    - source: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
      type: internal
- file_name: graphrag/tokenizer/__init__.py
  imports: []
  functions: []
- file_name: graphrag/tokenizer/get_tokenizer.py
  imports:
  - module: graphrag.config.defaults
    name: ENCODING_MODEL
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.tokenizer.litellm_tokenizer
    name: LitellmTokenizer
    alias: null
  - module: graphrag.tokenizer.tiktoken_tokenizer
    name: TiktokenTokenizer
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: get_tokenizer
    start_line: 13
    end_line: 41
    code: "def get_tokenizer(\n    model_config: LanguageModelConfig | None = None,\n\
      \    encoding_model: str = ENCODING_MODEL,\n) -> Tokenizer:\n    \"\"\"\n  \
      \  Get the tokenizer for the given model configuration or fallback to a tiktoken\
      \ based tokenizer.\n\n    Args\n    ----\n        model_config: LanguageModelConfig,\
      \ optional\n            The model configuration. If not provided or model_config.encoding_model\
      \ is manually set,\n            use a tiktoken based tokenizer. Otherwise, use\
      \ a LitellmTokenizer based on the model name.\n            LiteLLM supports\
      \ token encoding/decoding for the range of models it supports.\n        encoding_model:\
      \ str, optional\n            A tiktoken encoding model to use if no model configuration\
      \ is provided. Only used if a\n            model configuration is not provided.\n\
      \n    Returns\n    -------\n        An instance of a Tokenizer.\n    \"\"\"\n\
      \    if model_config is not None:\n        if model_config.encoding_model.strip()\
      \ != \"\":\n            # User has manually specified a tiktoken encoding model\
      \ to use for the provided model configuration.\n            return TiktokenTokenizer(encoding_name=model_config.encoding_model)\n\
      \n        return LitellmTokenizer(model_name=model_config.model)\n\n    return\
      \ TiktokenTokenizer(encoding_name=encoding_model)"
    signature: "def get_tokenizer(\n    model_config: LanguageModelConfig | None =\
      \ None,\n    encoding_model: str = ENCODING_MODEL,\n) -> Tokenizer"
    decorators: []
    raises: []
    calls:
    - target: model_config.encoding_model.strip
      type: unresolved
    - target: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer
      type: internal
    - target: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer
      type: internal
    visibility: public
    node_id: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
    called_by:
    - source: graphrag/api/prompt_tune.py::generate_indexing_prompts
      type: internal
    - source: graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
      type: internal
    - source: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
      type: internal
    - source: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
      type: internal
    - source: graphrag/index/workflows/create_community_reports.py::create_community_reports
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
      type: internal
    - source: graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
      type: internal
    - source: graphrag/query/context_builder/community_context.py::build_community_context
      type: internal
    - source: graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
      type: internal
    - source: graphrag/query/context_builder/local_context.py::build_entity_context
      type: internal
    - source: graphrag/query/context_builder/local_context.py::build_covariates_context
      type: internal
    - source: graphrag/query/context_builder/local_context.py::build_relationship_context
      type: internal
    - source: graphrag/query/context_builder/source_context.py::build_text_unit_context
      type: internal
    - source: graphrag/query/factory.py::get_local_search_engine
      type: internal
    - source: graphrag/query/factory.py::get_global_search_engine
      type: internal
    - source: graphrag/query/factory.py::get_drift_search_engine
      type: internal
    - source: graphrag/query/factory.py::get_basic_search_engine
      type: internal
    - source: graphrag/query/llm/text_utils.py::chunk_text
      type: internal
    - source: graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
      type: internal
    - source: graphrag/query/structured_search/base.py::BaseSearch.__init__
      type: internal
    - source: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
      type: internal
    - source: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
      type: internal
    - source: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
      type: internal
    - source: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
      type: internal
    - source: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
      type: internal
    - source: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
      type: internal
    - source: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
      type: internal
    - source: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
      type: internal
    - source: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
      type: internal
    - source: tests/unit/utils/test_encoding.py::test_encode_basic
      type: internal
    - source: tests/unit/utils/test_encoding.py::test_num_tokens_empty_input
      type: internal
- file_name: graphrag/tokenizer/litellm_tokenizer.py
  imports:
  - module: litellm
    name: decode
    alias: null
  - module: litellm
    name: encode
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 14
    end_line: 21
    code: "def __init__(self, model_name: str) -> None:\n        \"\"\"Initialize\
      \ the LiteLLM Tokenizer.\n\n        Args\n        ----\n            model_name\
      \ (str): The name of the LiteLLM model to use for tokenization.\n        \"\"\
      \"\n        self.model_name = model_name"
    signature: 'def __init__(self, model_name: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.__init__
    called_by: []
  - name: encode
    start_line: 23
    end_line: 34
    code: "def encode(self, text: str) -> list[int]:\n        \"\"\"Encode the given\
      \ text into a list of tokens.\n\n        Args\n        ----\n            text\
      \ (str): The input text to encode.\n\n        Returns\n        -------\n   \
      \         list[int]: A list of tokens representing the encoded text.\n     \
      \   \"\"\"\n        return encode(model=self.model_name, text=text)"
    signature: 'def encode(self, text: str) -> list[int]'
    decorators: []
    raises: []
    calls:
    - target: litellm::encode
      type: external
    visibility: public
    node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.encode
    called_by: []
  - name: decode
    start_line: 36
    end_line: 47
    code: "def decode(self, tokens: list[int]) -> str:\n        \"\"\"Decode a list\
      \ of tokens back into a string.\n\n        Args\n        ----\n            tokens\
      \ (list[int]): A list of tokens to decode.\n\n        Returns\n        -------\n\
      \            str: The decoded string from the list of tokens.\n        \"\"\"\
      \n        return decode(model=self.model_name, tokens=tokens)"
    signature: 'def decode(self, tokens: list[int]) -> str'
    decorators: []
    raises: []
    calls:
    - target: litellm::decode
      type: external
    visibility: public
    node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.decode
    called_by: []
- file_name: graphrag/tokenizer/tiktoken_tokenizer.py
  imports:
  - module: tiktoken
    name: null
    alias: null
  - module: graphrag.tokenizer.tokenizer
    name: Tokenizer
    alias: null
  functions:
  - name: __init__
    start_line: 14
    end_line: 21
    code: "def __init__(self, encoding_name: str) -> None:\n        \"\"\"Initialize\
      \ the Tiktoken Tokenizer.\n\n        Args\n        ----\n            encoding_name\
      \ (str): The name of the Tiktoken encoding to use for tokenization.\n      \
      \  \"\"\"\n        self.encoding = tiktoken.get_encoding(encoding_name)"
    signature: 'def __init__(self, encoding_name: str) -> None'
    decorators: []
    raises: []
    calls:
    - target: tiktoken::get_encoding
      type: external
    visibility: protected
    node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.__init__
    called_by: []
  - name: encode
    start_line: 23
    end_line: 34
    code: "def encode(self, text: str) -> list[int]:\n        \"\"\"Encode the given\
      \ text into a list of tokens.\n\n        Args\n        ----\n            text\
      \ (str): The input text to encode.\n\n        Returns\n        -------\n   \
      \         list[int]: A list of tokens representing the encoded text.\n     \
      \   \"\"\"\n        return self.encoding.encode(text)"
    signature: 'def encode(self, text: str) -> list[int]'
    decorators: []
    raises: []
    calls:
    - target: self.encoding.encode
      type: instance
    visibility: public
    node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.encode
    called_by: []
  - name: decode
    start_line: 36
    end_line: 47
    code: "def decode(self, tokens: list[int]) -> str:\n        \"\"\"Decode a list\
      \ of tokens back into a string.\n\n        Args\n        ----\n            tokens\
      \ (list[int]): A list of tokens to decode.\n\n        Returns\n        -------\n\
      \            str: The decoded string from the list of tokens.\n        \"\"\"\
      \n        return self.encoding.decode(tokens)"
    signature: 'def decode(self, tokens: list[int]) -> str'
    decorators: []
    raises: []
    calls:
    - target: self.encoding.decode
      type: instance
    visibility: public
    node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.decode
    called_by: []
- file_name: graphrag/tokenizer/tokenizer.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  functions:
  - name: encode
    start_line: 13
    end_line: 25
    code: "def encode(self, text: str) -> list[int]:\n        \"\"\"Encode the given\
      \ text into a list of tokens.\n\n        Args\n        ----\n            text\
      \ (str): The input text to encode.\n\n        Returns\n        -------\n   \
      \         list[int]: A list of tokens representing the encoded text.\n     \
      \   \"\"\"\n        msg = \"The encode method must be implemented by subclasses.\"\
      \n        raise NotImplementedError(msg)"
    signature: 'def encode(self, text: str) -> list[int]'
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.encode
    called_by: []
  - name: decode
    start_line: 28
    end_line: 40
    code: "def decode(self, tokens: list[int]) -> str:\n        \"\"\"Decode a list\
      \ of tokens back into a string.\n\n        Args\n        ----\n            tokens\
      \ (list[int]): A list of tokens to decode.\n\n        Returns\n        -------\n\
      \            str: The decoded string from the list of tokens.\n        \"\"\"\
      \n        msg = \"The decode method must be implemented by subclasses.\"\n \
      \       raise NotImplementedError(msg)"
    signature: 'def decode(self, tokens: list[int]) -> str'
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.decode
    called_by: []
  - name: num_tokens
    start_line: 42
    end_line: 53
    code: "def num_tokens(self, text: str) -> int:\n        \"\"\"Return the number\
      \ of tokens in the given text.\n\n        Args\n        ----\n            text\
      \ (str): The input text to analyze.\n\n        Returns\n        -------\n  \
      \          int: The number of tokens in the input text.\n        \"\"\"\n  \
      \      return len(self.encode(text))"
    signature: 'def num_tokens(self, text: str) -> int'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: graphrag/tokenizer/tokenizer.py::encode
      type: internal
    visibility: public
    node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.num_tokens
    called_by: []
- file_name: graphrag/utils/__init__.py
  imports: []
  functions: []
- file_name: graphrag/utils/api.py
  imports:
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.cache.factory
    name: CacheFactory
    alias: null
  - module: graphrag.cache.pipeline_cache
    name: PipelineCache
    alias: null
  - module: graphrag.config.embeddings
    name: create_index_name
    alias: null
  - module: graphrag.config.models.cache_config
    name: CacheConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  - module: graphrag.storage.factory
    name: StorageFactory
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreSearchResult
    alias: null
  - module: graphrag.vector_stores.factory
    name: VectorStoreFactory
    alias: null
  functions:
  - name: __init__
    start_line: 29
    end_line: 35
    code: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
      \        index_names: list[str],\n    ):\n        self.embedding_stores = embedding_stores\n\
      \        self.index_names = index_names"
    signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
      \        index_names: list[str],\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/utils/api.py::MultiVectorStore.__init__
    called_by: []
  - name: load_documents
    start_line: 37
    end_line: 42
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        \"\"\"Load documents into\
      \ the vector store.\"\"\"\n        msg = \"load_documents method not implemented\"\
      \n        raise NotImplementedError(msg)"
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.load_documents
    called_by: []
  - name: connect
    start_line: 44
    end_line: 47
    code: "def connect(self, **kwargs: Any) -> Any:\n        \"\"\"Connect to vector\
      \ storage.\"\"\"\n        msg = \"connect method not implemented\"\n       \
      \ raise NotImplementedError(msg)"
    signature: 'def connect(self, **kwargs: Any) -> Any'
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.connect
    called_by: []
  - name: filter_by_id
    start_line: 49
    end_line: 52
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      \"\"\"Build a query filter to filter documents by id.\"\"\"\n       \
      \ msg = \"filter_by_id method not implemented\"\n        raise NotImplementedError(msg)"
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators: []
    raises:
    - NotImplementedError
    calls:
    - target: NotImplementedError
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.filter_by_id
    called_by: []
  - name: search_by_id
    start_line: 54
    end_line: 65
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        \"\"\"\
      Search for a document by id.\"\"\"\n        search_index_id = id.split(\"-\"\
      )[0]\n        search_index_name = id.split(\"-\")[1]\n        for index_name,\
      \ embedding_store in zip(\n            self.index_names, self.embedding_stores,\
      \ strict=False\n        ):\n            if index_name == search_index_name:\n\
      \                return embedding_store.search_by_id(search_index_id)\n    \
      \    else:\n            message = f\"Index {search_index_name} not found.\"\n\
      \            raise ValueError(message)"
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: id.split
      type: unresolved
    - target: zip
      type: builtin
    - target: embedding_store.search_by_id
      type: unresolved
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.search_by_id
    called_by: []
  - name: similarity_search_by_vector
    start_line: 67
    end_line: 83
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n    \
      \    \"\"\"Perform a vector-based similarity search.\"\"\"\n        all_results\
      \ = []\n        for index_name, embedding_store in zip(\n            self.index_names,\
      \ self.embedding_stores, strict=False\n        ):\n            results = embedding_store.similarity_search_by_vector(\n\
      \                query_embedding=query_embedding, k=k\n            )\n     \
      \       mod_results = []\n            for r in results:\n                r.document.id\
      \ = str(r.document.id) + f\"-{index_name}\"\n                mod_results +=\
      \ [r]\n            all_results += mod_results\n        return sorted(all_results,\
      \ key=lambda x: x.score, reverse=True)[:k]"
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: zip
      type: builtin
    - target: embedding_store.similarity_search_by_vector
      type: unresolved
    - target: str
      type: builtin
    - target: sorted
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 85
    end_line: 94
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform a text-based similarity search.\"\"\"\n        query_embedding\
      \ = text_embedder(text)\n        if query_embedding:\n            return self.similarity_search_by_vector(\n\
      \                query_embedding=query_embedding, k=k\n            )\n     \
      \   return []"
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: text_embedder
      type: unresolved
    - target: graphrag/utils/api.py::similarity_search_by_vector
      type: internal
    visibility: public
    node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_text
    called_by: []
  - name: get_embedding_store
    start_line: 97
    end_line: 141
    code: "def get_embedding_store(\n    config_args: dict[str, dict],\n    embedding_name:\
      \ str,\n) -> BaseVectorStore:\n    \"\"\"Get the embedding description store.\"\
      \"\"\n    num_indexes = len(config_args)\n    embedding_stores = []\n    index_names\
      \ = []\n    for index, store in config_args.items():\n        vector_store_type\
      \ = store[\"type\"]\n        index_name = create_index_name(\n            store.get(\"\
      container_name\", \"default\"), embedding_name\n        )\n\n        embeddings_schema:\
      \ dict[str, VectorStoreSchemaConfig] = store.get(\n            \"embeddings_schema\"\
      , {}\n        )\n        single_embedding_config: VectorStoreSchemaConfig =\
      \ VectorStoreSchemaConfig()\n\n        if (\n            embeddings_schema is\
      \ not None\n            and embedding_name is not None\n            and embedding_name\
      \ in embeddings_schema\n        ):\n            raw_config = embeddings_schema[embedding_name]\n\
      \            if isinstance(raw_config, dict):\n                single_embedding_config\
      \ = VectorStoreSchemaConfig(**raw_config)\n            else:\n             \
      \   single_embedding_config = raw_config\n\n        if single_embedding_config.index_name\
      \ is None:\n            single_embedding_config.index_name = index_name\n\n\
      \        embedding_store = VectorStoreFactory().create_vector_store(\n     \
      \       vector_store_type=vector_store_type,\n            vector_store_schema_config=single_embedding_config,\n\
      \            **store,\n        )\n        embedding_store.connect(**store)\n\
      \        # If there is only a single index, return the embedding store directly\n\
      \        if num_indexes == 1:\n            return embedding_store\n        embedding_stores.append(embedding_store)\n\
      \        index_names.append(index)\n    return MultiVectorStore(embedding_stores,\
      \ index_names)"
    signature: "def get_embedding_store(\n    config_args: dict[str, dict],\n    embedding_name:\
      \ str,\n) -> BaseVectorStore"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: config_args.items
      type: unresolved
    - target: graphrag/config/embeddings.py::create_index_name
      type: internal
    - target: store.get
      type: unresolved
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    - target: VectorStoreFactory().create_vector_store
      type: unresolved
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory
      type: internal
    - target: embedding_store.connect
      type: unresolved
    - target: embedding_stores.append
      type: unresolved
    - target: index_names.append
      type: unresolved
    - target: MultiVectorStore
      type: unresolved
    visibility: public
    node_id: graphrag/utils/api.py::get_embedding_store
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
  - name: reformat_context_data
    start_line: 144
    end_line: 172
    code: "def reformat_context_data(context_data: dict) -> dict:\n    \"\"\"\n  \
      \  Reformats context_data for all query responses.\n\n    Reformats a dictionary\
      \ of dataframes into a dictionary of lists.\n    One list entry for each record.\
      \ Records are grouped by original\n    dictionary keys.\n\n    Note: depending\
      \ on which query algorithm is used, the context_data may not\n          contain\
      \ the same information (keys). In this case, the default behavior will be to\n\
      \          set these keys as empty lists to preserve a standard output format.\n\
      \    \"\"\"\n    final_format = {\n        \"reports\": [],\n        \"entities\"\
      : [],\n        \"relationships\": [],\n        \"claims\": [],\n        \"sources\"\
      : [],\n    }\n    for key in context_data:\n        records = (\n          \
      \  context_data[key].to_dict(orient=\"records\")\n            if context_data[key]\
      \ is not None and not isinstance(context_data[key], dict)\n            else\
      \ context_data[key]\n        )\n        if len(records) < 1:\n            continue\n\
      \        final_format[key] = records\n    return final_format"
    signature: 'def reformat_context_data(context_data: dict) -> dict'
    decorators: []
    raises: []
    calls:
    - target: context_data[key].to_dict
      type: unresolved
    - target: isinstance
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::reformat_context_data
    called_by: []
  - name: update_context_data
    start_line: 175
    end_line: 247
    code: "def update_context_data(\n    context_data: Any,\n    links: dict[str,\
      \ Any],\n) -> Any:\n    \"\"\"\n    Update context data with the links dict\
      \ so that it contains both the index name and community id.\n\n    Parameters\n\
      \    ----------\n    - context_data (str | list[pd.DataFrame] | dict[str, pd.DataFrame]):\
      \ The context data to update.\n    - links (dict[str, Any]): A dictionary of\
      \ links to the original dataframes.\n\n    Returns\n    -------\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame]: The updated context data.\n    \"\"\"\n    updated_context_data\
      \ = {}\n    for key in context_data:\n        entries = context_data[key].to_dict(orient=\"\
      records\")\n        updated_entry = []\n        if key == \"reports\":\n   \
      \         updated_entry = [\n                dict(\n                    entry,\n\
      \                    index_name=links[\"community_reports\"][int(entry[\"id\"\
      ])][\n                        \"index_name\"\n                    ],\n     \
      \               index_id=links[\"community_reports\"][int(entry[\"id\"])][\"\
      id\"],\n                )\n                for entry in entries\n          \
      \  ]\n        if key == \"entities\":\n            updated_entry = [\n     \
      \           dict(\n                    entry,\n                    entity=entry[\"\
      entity\"].split(\"-\")[0],\n                    index_name=links[\"entities\"\
      ][int(entry[\"id\"])][\"index_name\"],\n                    index_id=links[\"\
      entities\"][int(entry[\"id\"])][\"id\"],\n                )\n              \
      \  for entry in entries\n            ]\n        if key == \"relationships\"\
      :\n            updated_entry = [\n                dict(\n                  \
      \  entry,\n                    source=entry[\"source\"].split(\"-\")[0],\n \
      \                   target=entry[\"target\"].split(\"-\")[0],\n            \
      \        index_name=links[\"relationships\"][int(entry[\"id\"])][\"index_name\"\
      ],\n                    index_id=links[\"relationships\"][int(entry[\"id\"])][\"\
      id\"],\n                )\n                for entry in entries\n          \
      \  ]\n        if key == \"claims\":\n            updated_entry = [\n       \
      \         dict(\n                    entry,\n                    entity=entry[\"\
      entity\"].split(\"-\")[0],\n                    index_name=links[\"covariates\"\
      ][int(entry[\"id\"])][\"index_name\"],\n                    index_id=links[\"\
      covariates\"][int(entry[\"id\"])][\"id\"],\n                )\n            \
      \    for entry in entries\n            ]\n        if key == \"sources\":\n \
      \           updated_entry = [\n                dict(\n                    entry,\n\
      \                    index_name=links[\"text_units\"][int(entry[\"id\"])][\"\
      index_name\"],\n                    index_id=links[\"text_units\"][int(entry[\"\
      id\"])][\"id\"],\n                )\n                for entry in entries\n\
      \            ]\n        updated_context_data[key] = updated_entry\n    return\
      \ updated_context_data"
    signature: "def update_context_data(\n    context_data: Any,\n    links: dict[str,\
      \ Any],\n) -> Any"
    decorators: []
    raises: []
    calls:
    - target: context_data[key].to_dict
      type: unresolved
    - target: dict
      type: builtin
    - target: int
      type: builtin
    - target: entry["entity"].split
      type: unresolved
    - target: entry["source"].split
      type: unresolved
    - target: entry["target"].split
      type: unresolved
    visibility: public
    node_id: graphrag/utils/api.py::update_context_data
    called_by:
    - source: graphrag/api/query.py::multi_index_global_search
      type: internal
    - source: graphrag/api/query.py::multi_index_local_search
      type: internal
    - source: graphrag/api/query.py::multi_index_drift_search
      type: internal
  - name: load_search_prompt
    start_line: 250
    end_line: 261
    code: "def load_search_prompt(root_dir: str, prompt_config: str | None) -> str\
      \ | None:\n    \"\"\"\n    Load the search prompt from disk if configured.\n\
      \n    If not, leave it empty - the search functions will load their defaults.\n\
      \n    \"\"\"\n    if prompt_config:\n        prompt_file = Path(root_dir) /\
      \ prompt_config\n        if prompt_file.exists():\n            return prompt_file.read_bytes().decode(encoding=\"\
      utf-8\")\n    return None"
    signature: 'def load_search_prompt(root_dir: str, prompt_config: str | None) ->
      str | None'
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: prompt_file.exists
      type: unresolved
    - target: prompt_file.read_bytes().decode
      type: unresolved
    - target: prompt_file.read_bytes
      type: unresolved
    visibility: public
    node_id: graphrag/utils/api.py::load_search_prompt
    called_by:
    - source: graphrag/api/query.py::global_search_streaming
      type: internal
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
  - name: create_storage_from_config
    start_line: 264
    end_line: 270
    code: "def create_storage_from_config(output: StorageConfig) -> PipelineStorage:\n\
      \    \"\"\"Create a storage object from the config.\"\"\"\n    storage_config\
      \ = output.model_dump()\n    return StorageFactory().create_storage(\n     \
      \   storage_type=storage_config[\"type\"],\n        kwargs=storage_config,\n\
      \    )"
    signature: 'def create_storage_from_config(output: StorageConfig) -> PipelineStorage'
    decorators: []
    raises: []
    calls:
    - target: output.model_dump
      type: unresolved
    - target: StorageFactory().create_storage
      type: unresolved
    - target: graphrag/storage/factory.py::StorageFactory
      type: internal
    visibility: public
    node_id: graphrag/utils/api.py::create_storage_from_config
    called_by:
    - source: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
    - source: graphrag/index/run/utils.py::get_update_storages
      type: internal
    - source: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
      type: internal
    - source: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
      type: internal
  - name: create_cache_from_config
    start_line: 273
    end_line: 280
    code: "def create_cache_from_config(cache: CacheConfig, root_dir: str) -> PipelineCache:\n\
      \    \"\"\"Create a cache object from the config.\"\"\"\n    cache_config =\
      \ cache.model_dump()\n    kwargs = {**cache_config, \"root_dir\": root_dir}\n\
      \    return CacheFactory().create_cache(\n        cache_type=cache_config[\"\
      type\"],\n        kwargs=kwargs,\n    )"
    signature: 'def create_cache_from_config(cache: CacheConfig, root_dir: str) ->
      PipelineCache'
    decorators: []
    raises: []
    calls:
    - target: cache.model_dump
      type: unresolved
    - target: CacheFactory().create_cache
      type: unresolved
    - target: graphrag.cache.factory::CacheFactory
      type: internal
    visibility: public
    node_id: graphrag/utils/api.py::create_cache_from_config
    called_by:
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
  - name: truncate
    start_line: 283
    end_line: 287
    code: "def truncate(text: str, max_length: int) -> str:\n    \"\"\"Truncate a\
      \ string to a maximum length.\"\"\"\n    if len(text) <= max_length:\n     \
      \   return text\n    return text[:max_length] + \"...[truncated]\""
    signature: 'def truncate(text: str, max_length: int) -> str'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    visibility: public
    node_id: graphrag/utils/api.py::truncate
    called_by:
    - source: graphrag/api/query.py::global_search
      type: internal
    - source: graphrag/api/query.py::multi_index_global_search
      type: internal
    - source: graphrag/api/query.py::local_search
      type: internal
    - source: graphrag/api/query.py::multi_index_local_search
      type: internal
    - source: graphrag/api/query.py::drift_search
      type: internal
    - source: graphrag/api/query.py::multi_index_drift_search
      type: internal
    - source: graphrag/api/query.py::basic_search
      type: internal
- file_name: graphrag/utils/cli.py
  imports:
  - module: argparse
    name: null
    alias: null
  - module: json
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  functions:
  - name: file_exist
    start_line: 11
    end_line: 16
    code: "def file_exist(path):\n    \"\"\"Check for file existence.\"\"\"\n    if\
      \ not Path(path).is_file():\n        msg = f\"File not found: {path}\"\n   \
      \     raise argparse.ArgumentTypeError(msg)\n    return path"
    signature: def file_exist(path)
    decorators: []
    raises: []
    calls:
    - target: Path(path).is_file
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: argparse::ArgumentTypeError
      type: stdlib
    visibility: public
    node_id: graphrag/utils/cli.py::file_exist
    called_by: []
  - name: dir_exist
    start_line: 19
    end_line: 24
    code: "def dir_exist(path):\n    \"\"\"Check for directory existence.\"\"\"\n\
      \    if not Path(path).is_dir():\n        msg = f\"Directory not found: {path}\"\
      \n        raise argparse.ArgumentTypeError(msg)\n    return path"
    signature: def dir_exist(path)
    decorators: []
    raises: []
    calls:
    - target: Path(path).is_dir
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: argparse::ArgumentTypeError
      type: stdlib
    visibility: public
    node_id: graphrag/utils/cli.py::dir_exist
    called_by: []
  - name: redact
    start_line: 27
    end_line: 54
    code: "def redact(config: dict) -> str:\n    \"\"\"Sanitize secrets in a config\
      \ object.\"\"\"\n\n    # Redact any sensitive configuration\n    def redact_dict(config:\
      \ dict) -> dict:\n        if not isinstance(config, dict):\n            return\
      \ config\n\n        result = {}\n        for key, value in config.items():\n\
      \            if key in {\n                \"api_key\",\n                \"connection_string\"\
      ,\n                \"container_name\",\n                \"organization\",\n\
      \            }:\n                if value is not None:\n                   \
      \ result[key] = \"==== REDACTED ====\"\n            elif isinstance(value, dict):\n\
      \                result[key] = redact_dict(value)\n            elif isinstance(value,\
      \ list):\n                result[key] = [redact_dict(i) for i in value]\n  \
      \          else:\n                result[key] = value\n        return result\n\
      \n    redacted_dict = redact_dict(config)\n    return json.dumps(redacted_dict,\
      \ indent=4)"
    signature: 'def redact(config: dict) -> str'
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/cli.py::redact_dict
      type: internal
    - target: json::dumps
      type: stdlib
    visibility: public
    node_id: graphrag/utils/cli.py::redact
    called_by:
    - source: graphrag/api/query.py::local_search_streaming
      type: internal
    - source: graphrag/api/query.py::drift_search_streaming
      type: internal
    - source: graphrag/api/query.py::basic_search_streaming
      type: internal
    - source: graphrag/cli/index.py::_run_index
      type: internal
    - source: graphrag/cli/prompt_tune.py::prompt_tune
      type: internal
  - name: redact_dict
    start_line: 31
    end_line: 51
    code: "def redact_dict(config: dict) -> dict:\n        if not isinstance(config,\
      \ dict):\n            return config\n\n        result = {}\n        for key,\
      \ value in config.items():\n            if key in {\n                \"api_key\"\
      ,\n                \"connection_string\",\n                \"container_name\"\
      ,\n                \"organization\",\n            }:\n                if value\
      \ is not None:\n                    result[key] = \"==== REDACTED ====\"\n \
      \           elif isinstance(value, dict):\n                result[key] = redact_dict(value)\n\
      \            elif isinstance(value, list):\n                result[key] = [redact_dict(i)\
      \ for i in value]\n            else:\n                result[key] = value\n\
      \        return result"
    signature: 'def redact_dict(config: dict) -> dict'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: config.items
      type: unresolved
    - target: graphrag/utils/cli.py::redact_dict
      type: internal
    visibility: public
    node_id: graphrag/utils/cli.py::redact_dict
    called_by:
    - source: graphrag/utils/cli.py::redact
      type: internal
    - source: graphrag/utils/cli.py::redact_dict
      type: internal
- file_name: graphrag/utils/storage.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: io
    name: BytesIO
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: load_table_from_storage
    start_line: 16
    end_line: 27
    code: "async def load_table_from_storage(name: str, storage: PipelineStorage)\
      \ -> pd.DataFrame:\n    \"\"\"Load a parquet from the storage instance.\"\"\"\
      \n    filename = f\"{name}.parquet\"\n    if not await storage.has(filename):\n\
      \        msg = f\"Could not find {filename} in storage!\"\n        raise ValueError(msg)\n\
      \    try:\n        logger.info(\"reading table from storage: %s\", filename)\n\
      \        return pd.read_parquet(BytesIO(await storage.get(filename, as_bytes=True)))\n\
      \    except Exception:\n        logger.exception(\"error loading table from\
      \ storage: %s\", filename)\n        raise"
    signature: 'def load_table_from_storage(name: str, storage: PipelineStorage) ->
      pd.DataFrame'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: storage.has
      type: unresolved
    - target: ValueError
      type: builtin
    - target: logger.info
      type: unresolved
    - target: pandas::read_parquet
      type: external
    - target: io::BytesIO
      type: stdlib
    - target: storage.get
      type: unresolved
    - target: logger.exception
      type: unresolved
    visibility: public
    node_id: graphrag/utils/storage.py::load_table_from_storage
    called_by:
    - source: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - source: graphrag/index/run/run_pipeline.py::_copy_previous_output
      type: internal
    - source: graphrag/index/update/incremental_index.py::get_delta_docs
      type: internal
    - source: graphrag/index/update/incremental_index.py::concat_dataframes
      type: internal
    - source: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_communities.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_community_reports.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_final_documents.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_final_text_units.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_covariates.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/prune_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_communities.py::_update_communities
      type: internal
    - source: graphrag/index/workflows/update_community_reports.py::_update_community_reports
      type: internal
    - source: graphrag/index/workflows/update_covariates.py::_update_covariates
      type: internal
    - source: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
    - source: graphrag/index/workflows/update_text_units.py::_update_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
    - source: tests/verbs/test_extract_covariates.py::test_extract_covariates
      type: internal
    - source: tests/verbs/test_extract_graph.py::test_extract_graph
      type: internal
    - source: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
      type: internal
    - source: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
      type: internal
    - source: tests/verbs/test_prune_graph.py::test_prune_graph
      type: internal
    - source: tests/verbs/util.py::update_document_metadata
      type: internal
  - name: write_table_to_storage
    start_line: 30
    end_line: 34
    code: "async def write_table_to_storage(\n    table: pd.DataFrame, name: str,\
      \ storage: PipelineStorage\n) -> None:\n    \"\"\"Write a table to storage.\"\
      \"\"\n    await storage.set(f\"{name}.parquet\", table.to_parquet())"
    signature: "def write_table_to_storage(\n    table: pd.DataFrame, name: str, storage:\
      \ PipelineStorage\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: storage.set
      type: unresolved
    - target: table.to_parquet
      type: unresolved
    visibility: public
    node_id: graphrag/utils/storage.py::write_table_to_storage
    called_by:
    - source: graphrag/index/run/run_pipeline.py::run_pipeline
      type: internal
    - source: graphrag/index/run/run_pipeline.py::_copy_previous_output
      type: internal
    - source: graphrag/index/update/incremental_index.py::concat_dataframes
      type: internal
    - source: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_communities.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_community_reports.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_community_reports_text.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_final_documents.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_final_text_units.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_covariates.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/load_input_documents.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/load_update_documents.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/prune_graph.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_communities.py::_update_communities
      type: internal
    - source: graphrag/index/workflows/update_community_reports.py::_update_community_reports
      type: internal
    - source: graphrag/index/workflows/update_covariates.py::_update_covariates
      type: internal
    - source: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
      type: internal
    - source: graphrag/index/workflows/update_text_embeddings.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_text_units.py::_update_text_units
      type: internal
    - source: tests/verbs/test_finalize_graph.py::_prep_tables
      type: internal
    - source: tests/verbs/util.py::create_test_context
      type: internal
    - source: tests/verbs/util.py::update_document_metadata
      type: internal
  - name: delete_table_from_storage
    start_line: 37
    end_line: 39
    code: "async def delete_table_from_storage(name: str, storage: PipelineStorage)\
      \ -> None:\n    \"\"\"Delete a table to storage.\"\"\"\n    await storage.delete(f\"\
      {name}.parquet\")"
    signature: 'def delete_table_from_storage(name: str, storage: PipelineStorage)
      -> None'
    decorators: []
    raises: []
    calls:
    - target: storage.delete
      type: unresolved
    visibility: public
    node_id: graphrag/utils/storage.py::delete_table_from_storage
    called_by: []
  - name: storage_has_table
    start_line: 42
    end_line: 44
    code: "async def storage_has_table(name: str, storage: PipelineStorage) -> bool:\n\
      \    \"\"\"Check if a table exists in storage.\"\"\"\n    return await storage.has(f\"\
      {name}.parquet\")"
    signature: 'def storage_has_table(name: str, storage: PipelineStorage) -> bool'
    decorators: []
    raises: []
    calls:
    - target: storage.has
      type: unresolved
    visibility: public
    node_id: graphrag/utils/storage.py::storage_has_table
    called_by:
    - source: graphrag/cli/query.py::_resolve_output_files
      type: internal
    - source: graphrag/index/workflows/create_community_reports.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/create_final_text_units.py::run_workflow
      type: internal
    - source: graphrag/index/workflows/update_covariates.py::run_workflow
      type: internal
- file_name: graphrag/vector_stores/__init__.py
  imports: []
  functions: []
- file_name: graphrag/vector_stores/azure_ai_search.py
  imports:
  - module: json
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: azure.core.credentials
    name: AzureKeyCredential
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.search.documents
    name: SearchClient
    alias: null
  - module: azure.search.documents.indexes
    name: SearchIndexClient
    alias: null
  - module: azure.search.documents.indexes.models
    name: HnswAlgorithmConfiguration
    alias: null
  - module: azure.search.documents.indexes.models
    name: HnswParameters
    alias: null
  - module: azure.search.documents.indexes.models
    name: SearchableField
    alias: null
  - module: azure.search.documents.indexes.models
    name: SearchField
    alias: null
  - module: azure.search.documents.indexes.models
    name: SearchFieldDataType
    alias: null
  - module: azure.search.documents.indexes.models
    name: SearchIndex
    alias: null
  - module: azure.search.documents.indexes.models
    name: SimpleField
    alias: null
  - module: azure.search.documents.indexes.models
    name: VectorSearch
    alias: null
  - module: azure.search.documents.indexes.models
    name: VectorSearchAlgorithmMetric
    alias: null
  - module: azure.search.documents.indexes.models
    name: VectorSearchProfile
    alias: null
  - module: azure.search.documents.models
    name: VectorizedQuery
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreSearchResult
    alias: null
  functions:
  - name: __init__
    start_line: 41
    end_line: 46
    code: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None:\n        super().__init__(\n            vector_store_schema_config=vector_store_schema_config,\
      \ **kwargs\n        )"
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 48
    end_line: 77
    code: "def connect(self, **kwargs: Any) -> Any:\n        \"\"\"Connect to AI search\
      \ vector storage.\"\"\"\n        url = kwargs[\"url\"]\n        api_key = kwargs.get(\"\
      api_key\")\n        audience = kwargs.get(\"audience\")\n\n        self.vector_search_profile_name\
      \ = kwargs.get(\n            \"vector_search_profile_name\", \"vectorSearchProfile\"\
      \n        )\n\n        if url:\n            audience_arg = {\"audience\": audience}\
      \ if audience and not api_key else {}\n            self.db_connection = SearchClient(\n\
      \                endpoint=url,\n                index_name=self.index_name if\
      \ self.index_name else \"\",\n                credential=(\n               \
      \     AzureKeyCredential(api_key) if api_key else DefaultAzureCredential()\n\
      \                ),\n                **audience_arg,\n            )\n      \
      \      self.index_client = SearchIndexClient(\n                endpoint=url,\n\
      \                credential=(\n                    AzureKeyCredential(api_key)\
      \ if api_key else DefaultAzureCredential()\n                ),\n           \
      \     **audience_arg,\n            )\n        else:\n            not_supported_error\
      \ = \"Azure AI Search expects `url`.\"\n            raise ValueError(not_supported_error)"
    signature: 'def connect(self, **kwargs: Any) -> Any'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: kwargs.get
      type: unresolved
    - target: azure.search.documents::SearchClient
      type: external
    - target: azure.core.credentials::AzureKeyCredential
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: azure.search.documents.indexes::SearchIndexClient
      type: external
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.connect
    called_by: []
  - name: load_documents
    start_line: 79
    end_line: 150
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        \"\"\"Load documents into\
      \ an Azure AI Search index.\"\"\"\n        if overwrite:\n            if (\n\
      \                self.index_name is not None\n                and self.index_name\
      \ in self.index_client.list_index_names()\n            ):\n                self.index_client.delete_index(self.index_name)\n\
      \n            # Configure vector search profile\n            vector_search =\
      \ VectorSearch(\n                algorithms=[\n                    HnswAlgorithmConfiguration(\n\
      \                        name=\"HnswAlg\",\n                        parameters=HnswParameters(\n\
      \                            metric=VectorSearchAlgorithmMetric.COSINE\n   \
      \                     ),\n                    )\n                ],\n      \
      \          profiles=[\n                    VectorSearchProfile(\n          \
      \              name=self.vector_search_profile_name,\n                     \
      \   algorithm_configuration_name=\"HnswAlg\",\n                    )\n     \
      \           ],\n            )\n            # Configure the index\n         \
      \   index = SearchIndex(\n                name=self.index_name if self.index_name\
      \ else \"\",\n                fields=[\n                    SimpleField(\n \
      \                       name=self.id_field,\n                        type=SearchFieldDataType.String,\n\
      \                        key=True,\n                    ),\n               \
      \     SearchField(\n                        name=self.vector_field,\n      \
      \                  type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n\
      \                        searchable=True,\n                        hidden=False,\
      \  # DRIFT needs to return the vector for client-side similarity\n         \
      \               vector_search_dimensions=self.vector_size,\n               \
      \         vector_search_profile_name=self.vector_search_profile_name,\n    \
      \                ),\n                    SearchableField(\n                \
      \        name=self.text_field, type=SearchFieldDataType.String\n           \
      \         ),\n                    SimpleField(\n                        name=self.attributes_field,\n\
      \                        type=SearchFieldDataType.String,\n                \
      \    ),\n                ],\n                vector_search=vector_search,\n\
      \            )\n            self.index_client.create_or_update_index(\n    \
      \            index,\n            )\n\n        batch = [\n            {\n   \
      \             self.id_field: doc.id,\n                self.vector_field: doc.vector,\n\
      \                self.text_field: doc.text,\n                self.attributes_field:\
      \ json.dumps(doc.attributes),\n            }\n            for doc in documents\n\
      \            if doc.vector is not None\n        ]\n\n        if len(batch) >\
      \ 0:\n            self.db_connection.upload_documents(batch)"
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: self.index_client.list_index_names
      type: instance
    - target: self.index_client.delete_index
      type: instance
    - target: azure.search.documents.indexes.models::VectorSearch
      type: external
    - target: azure.search.documents.indexes.models::HnswAlgorithmConfiguration
      type: external
    - target: azure.search.documents.indexes.models::HnswParameters
      type: external
    - target: azure.search.documents.indexes.models::VectorSearchProfile
      type: external
    - target: azure.search.documents.indexes.models::SearchIndex
      type: external
    - target: azure.search.documents.indexes.models::SimpleField
      type: external
    - target: azure.search.documents.indexes.models::SearchField
      type: external
    - target: azure.search.documents.indexes.models::SearchFieldDataType::Collection
      type: external
    - target: azure.search.documents.indexes.models::SearchableField
      type: external
    - target: self.index_client.create_or_update_index
      type: instance
    - target: json::dumps
      type: stdlib
    - target: len
      type: builtin
    - target: self.db_connection.upload_documents
      type: instance
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.load_documents
    called_by: []
  - name: filter_by_id
    start_line: 152
    end_line: 166
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      \"\"\"Build a query filter to filter documents by a list of ids.\"\"\"\
      \n        if include_ids is None or len(include_ids) == 0:\n            self.query_filter\
      \ = None\n            # Returning to keep consistency with other methods, but\
      \ not needed\n            return self.query_filter\n\n        # More info about\
      \ odata filtering here: https://learn.microsoft.com/en-us/azure/search/search-query-odata-search-in-function\n\
      \        # search.in is faster that joined and/or conditions\n        id_filter\
      \ = \",\".join([f\"{id!s}\" for id in include_ids])\n        self.query_filter\
      \ = f\"search.in({self.id_field}, '{id_filter}', ',')\"\n\n        # Returning\
      \ to keep consistency with other methods, but not needed\n        # TODO: Refactor\
      \ on a future PR\n        return self.query_filter"
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: '",".join'
      type: unresolved
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.filter_by_id
    called_by: []
  - name: similarity_search_by_vector
    start_line: 168
    end_line: 193
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n    \
      \    \"\"\"Perform a vector-based similarity search.\"\"\"\n        vectorized_query\
      \ = VectorizedQuery(\n            vector=query_embedding, k_nearest_neighbors=k,\
      \ fields=self.vector_field\n        )\n\n        response = self.db_connection.search(\n\
      \            vector_queries=[vectorized_query],\n        )\n\n        return\
      \ [\n            VectorStoreSearchResult(\n                document=VectorStoreDocument(\n\
      \                    id=doc.get(self.id_field, \"\"),\n                    text=doc.get(self.text_field,\
      \ \"\"),\n                    vector=doc.get(self.vector_field, []),\n     \
      \               attributes=(json.loads(doc.get(self.attributes_field, \"{}\"\
      ))),\n                ),\n                # Cosine similarity between 0.333\
      \ and 1.000\n                # https://learn.microsoft.com/en-us/azure/search/hybrid-search-ranking#scores-in-a-hybrid-search-results\n\
      \                score=doc[\"@search.score\"],\n            )\n            for\
      \ doc in response\n        ]"
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: azure.search.documents.models::VectorizedQuery
      type: external
    - target: self.db_connection.search
      type: instance
    - target: graphrag/vector_stores/base.py::VectorStoreSearchResult
      type: internal
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: doc.get
      type: unresolved
    - target: json::loads
      type: stdlib
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 195
    end_line: 204
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform a text-based similarity search.\"\"\"\n        query_embedding\
      \ = text_embedder(text)\n        if query_embedding:\n            return self.similarity_search_by_vector(\n\
      \                query_embedding=query_embedding, k=k\n            )\n     \
      \   return []"
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: text_embedder
      type: unresolved
    - target: graphrag/vector_stores/azure_ai_search.py::similarity_search_by_vector
      type: internal
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_text
    called_by: []
  - name: search_by_id
    start_line: 206
    end_line: 214
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        \"\"\"\
      Search for a document by id.\"\"\"\n        response = self.db_connection.get_document(id)\n\
      \        return VectorStoreDocument(\n            id=response.get(self.id_field,\
      \ \"\"),\n            text=response.get(self.text_field, \"\"),\n          \
      \  vector=response.get(self.vector_field, []),\n            attributes=(json.loads(response.get(self.attributes_field,\
      \ \"{}\"))),\n        )"
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators: []
    raises: []
    calls:
    - target: self.db_connection.get_document
      type: instance
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: response.get
      type: unresolved
    - target: json::loads
      type: stdlib
    visibility: public
    node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.search_by_id
    called_by: []
- file_name: graphrag/vector_stores/base.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: dataclasses
    name: field
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  functions:
  - name: __init__
    start_line: 42
    end_line: 60
    code: "def __init__(\n        self,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n\
      \        db_connection: Any | None = None,\n        document_collection: Any\
      \ | None = None,\n        query_filter: Any | None = None,\n        **kwargs:\
      \ Any,\n    ):\n        self.db_connection = db_connection\n        self.document_collection\
      \ = document_collection\n        self.query_filter = query_filter\n        self.kwargs\
      \ = kwargs\n\n        self.index_name = vector_store_schema_config.index_name\n\
      \        self.id_field = vector_store_schema_config.id_field\n        self.text_field\
      \ = vector_store_schema_config.text_field\n        self.vector_field = vector_store_schema_config.vector_field\n\
      \        self.attributes_field = vector_store_schema_config.attributes_field\n\
      \        self.vector_size = vector_store_schema_config.vector_size"
    signature: "def __init__(\n        self,\n        vector_store_schema_config:\
      \ VectorStoreSchemaConfig,\n        db_connection: Any | None = None,\n    \
      \    document_collection: Any | None = None,\n        query_filter: Any | None\
      \ = None,\n        **kwargs: Any,\n    )"
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 63
    end_line: 64
    code: "def connect(self, **kwargs: Any) -> None:\n        \"\"\"Connect to vector\
      \ storage.\"\"\""
    signature: 'def connect(self, **kwargs: Any) -> None'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.connect
    called_by: []
  - name: load_documents
    start_line: 67
    end_line: 70
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        \"\"\"Load documents into\
      \ the vector-store.\"\"\""
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.load_documents
    called_by: []
  - name: similarity_search_by_vector
    start_line: 73
    end_line: 76
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n    \
      \    \"\"\"Perform ANN search by vector.\"\"\""
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 79
    end_line: 82
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform ANN search by text.\"\"\""
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_text
    called_by: []
  - name: filter_by_id
    start_line: 85
    end_line: 86
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      \"\"\"Build a query filter to filter documents by id.\"\"\""
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.filter_by_id
    called_by: []
  - name: search_by_id
    start_line: 89
    end_line: 90
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        \"\"\"\
      Search for a document by id.\"\"\""
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators:
    - '@abstractmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/base.py::BaseVectorStore.search_by_id
    called_by: []
- file_name: graphrag/vector_stores/cosmosdb.py
  imports:
  - module: json
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: azure.cosmos
    name: ContainerProxy
    alias: null
  - module: azure.cosmos
    name: CosmosClient
    alias: null
  - module: azure.cosmos
    name: DatabaseProxy
    alias: null
  - module: azure.cosmos.exceptions
    name: CosmosHttpResponseError
    alias: null
  - module: azure.cosmos.partition_key
    name: PartitionKey
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreSearchResult
    alias: null
  - module: numpy
    name: dot
    alias: null
  - module: numpy.linalg
    name: norm
    alias: null
  functions:
  - name: __init__
    start_line: 30
    end_line: 35
    code: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None:\n        super().__init__(\n            vector_store_schema_config=vector_store_schema_config,\
      \ **kwargs\n        )"
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 37
    end_line: 63
    code: "def connect(self, **kwargs: Any) -> Any:\n        \"\"\"Connect to CosmosDB\
      \ vector storage.\"\"\"\n        connection_string = kwargs.get(\"connection_string\"\
      )\n        if connection_string:\n            self._cosmos_client = CosmosClient.from_connection_string(connection_string)\n\
      \        else:\n            url = kwargs.get(\"url\")\n            if not url:\n\
      \                msg = \"Either connection_string or url must be provided.\"\
      \n                raise ValueError(msg)\n            self._cosmos_client = CosmosClient(\n\
      \                url=url, credential=DefaultAzureCredential()\n            )\n\
      \n        database_name = kwargs.get(\"database_name\")\n        if database_name\
      \ is None:\n            msg = \"Database name must be provided.\"\n        \
      \    raise ValueError(msg)\n        self._database_name = database_name\n  \
      \      if self.index_name is None:\n            msg = \"Index name is empty\
      \ or not provided.\"\n            raise ValueError(msg)\n        self._container_name\
      \ = self.index_name\n\n        self.vector_size = self.vector_size\n       \
      \ self._create_database()\n        self._create_container()"
    signature: 'def connect(self, **kwargs: Any) -> Any'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: kwargs.get
      type: unresolved
    - target: azure.cosmos::CosmosClient::from_connection_string
      type: external
    - target: ValueError
      type: builtin
    - target: azure.cosmos::CosmosClient
      type: external
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: graphrag/vector_stores/cosmosdb.py::_create_database
      type: internal
    - target: graphrag/vector_stores/cosmosdb.py::_create_container
      type: internal
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.connect
    called_by: []
  - name: _create_database
    start_line: 65
    end_line: 70
    code: "def _create_database(self) -> None:\n        \"\"\"Create the database\
      \ if it doesn't exist.\"\"\"\n        self._cosmos_client.create_database_if_not_exists(id=self._database_name)\n\
      \        self._database_client = self._cosmos_client.get_database_client(\n\
      \            self._database_name\n        )"
    signature: def _create_database(self) -> None
    decorators: []
    raises: []
    calls:
    - target: self._cosmos_client.create_database_if_not_exists
      type: instance
    - target: self._cosmos_client.get_database_client
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_database
    called_by: []
  - name: _delete_database
    start_line: 72
    end_line: 75
    code: "def _delete_database(self) -> None:\n        \"\"\"Delete the database\
      \ if it exists.\"\"\"\n        if self._database_exists():\n            self._cosmos_client.delete_database(self._database_name)"
    signature: def _delete_database(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::_database_exists
      type: internal
    - target: self._cosmos_client.delete_database
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_database
    called_by: []
  - name: _database_exists
    start_line: 77
    end_line: 82
    code: "def _database_exists(self) -> bool:\n        \"\"\"Check if the database\
      \ exists.\"\"\"\n        existing_database_names = [\n            database[\"\
      id\"] for database in self._cosmos_client.list_databases()\n        ]\n    \
      \    return self._database_name in existing_database_names"
    signature: def _database_exists(self) -> bool
    decorators: []
    raises: []
    calls:
    - target: self._cosmos_client.list_databases
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._database_exists
    called_by: []
  - name: _create_container
    start_line: 84
    end_line: 139
    code: "def _create_container(self) -> None:\n        \"\"\"Create the container\
      \ if it doesn't exist.\"\"\"\n        partition_key = PartitionKey(path=f\"\
      /{self.id_field}\", kind=\"Hash\")\n\n        # Define the container vector\
      \ policy\n        vector_embedding_policy = {\n            \"vectorEmbeddings\"\
      : [\n                {\n                    \"path\": f\"/{self.vector_field}\"\
      ,\n                    \"dataType\": \"float32\",\n                    \"distanceFunction\"\
      : \"cosine\",\n                    \"dimensions\": self.vector_size,\n     \
      \           }\n            ]\n        }\n\n        # Define the vector indexing\
      \ policy\n        indexing_policy = {\n            \"indexingMode\": \"consistent\"\
      ,\n            \"automatic\": True,\n            \"includedPaths\": [{\"path\"\
      : \"/*\"}],\n            \"excludedPaths\": [\n                {\"path\": \"\
      /_etag/?\"},\n                {\"path\": f\"/{self.vector_field}/*\"},\n   \
      \         ],\n        }\n\n        # Currently, the CosmosDB emulator does not\
      \ support the diskANN policy.\n        try:\n            # First try with the\
      \ standard diskANN policy\n            indexing_policy[\"vectorIndexes\"] =\
      \ [\n                {\"path\": f\"/{self.vector_field}\", \"type\": \"diskANN\"\
      }\n            ]\n\n            # Create the container and container client\n\
      \            self._database_client.create_container_if_not_exists(\n       \
      \         id=self._container_name,\n                partition_key=partition_key,\n\
      \                indexing_policy=indexing_policy,\n                vector_embedding_policy=vector_embedding_policy,\n\
      \            )\n        except CosmosHttpResponseError:\n            # If diskANN\
      \ fails (likely in emulator), retry without vector indexes\n            indexing_policy.pop(\"\
      vectorIndexes\", None)\n\n            # Create the container with compatible\
      \ indexing policy\n            self._database_client.create_container_if_not_exists(\n\
      \                id=self._container_name,\n                partition_key=partition_key,\n\
      \                indexing_policy=indexing_policy,\n                vector_embedding_policy=vector_embedding_policy,\n\
      \            )\n\n        self._container_client = self._database_client.get_container_client(\n\
      \            self._container_name\n        )"
    signature: def _create_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: azure.cosmos.partition_key::PartitionKey
      type: external
    - target: self._database_client.create_container_if_not_exists
      type: instance
    - target: indexing_policy.pop
      type: unresolved
    - target: self._database_client.get_container_client
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_container
    called_by: []
  - name: _delete_container
    start_line: 141
    end_line: 144
    code: "def _delete_container(self) -> None:\n        \"\"\"Delete the vector store\
      \ container in the database if it exists.\"\"\"\n        if self._container_exists():\n\
      \            self._database_client.delete_container(self._container_name)"
    signature: def _delete_container(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::_container_exists
      type: internal
    - target: self._database_client.delete_container
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_container
    called_by: []
  - name: _container_exists
    start_line: 146
    end_line: 151
    code: "def _container_exists(self) -> bool:\n        \"\"\"Check if the container\
      \ name exists in the database.\"\"\"\n        existing_container_names = [\n\
      \            container[\"id\"] for container in self._database_client.list_containers()\n\
      \        ]\n        return self._container_name in existing_container_names"
    signature: def _container_exists(self) -> bool
    decorators: []
    raises: []
    calls:
    - target: self._database_client.list_containers
      type: instance
    visibility: protected
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._container_exists
    called_by: []
  - name: load_documents
    start_line: 153
    end_line: 179
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        \"\"\"Load documents into\
      \ CosmosDB.\"\"\"\n        # Create a CosmosDB container on overwrite\n    \
      \    if overwrite:\n            self._delete_container()\n            self._create_container()\n\
      \n        if self._container_client is None:\n            msg = \"Container\
      \ client is not initialized.\"\n            raise ValueError(msg)\n\n      \
      \  # Upload documents to CosmosDB\n        for doc in documents:\n         \
      \   if doc.vector is not None:\n                print(\"Document to store:\"\
      )  # noqa: T201\n                print(doc)  # noqa: T201\n                doc_json\
      \ = {\n                    self.id_field: doc.id,\n                    self.vector_field:\
      \ doc.vector,\n                    self.text_field: doc.text,\n            \
      \        self.attributes_field: json.dumps(doc.attributes),\n              \
      \  }\n                print(\"Storing document in CosmosDB:\")  # noqa: T201\n\
      \                print(doc_json)  # noqa: T201\n                self._container_client.upsert_item(doc_json)"
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::_delete_container
      type: internal
    - target: graphrag/vector_stores/cosmosdb.py::_create_container
      type: internal
    - target: ValueError
      type: builtin
    - target: print
      type: builtin
    - target: json::dumps
      type: stdlib
    - target: self._container_client.upsert_item
      type: instance
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.load_documents
    called_by: []
  - name: similarity_search_by_vector
    start_line: 181
    end_line: 241
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n    \
      \    \"\"\"Perform a vector-based similarity search.\"\"\"\n        if self._container_client\
      \ is None:\n            msg = \"Container client is not initialized.\"\n   \
      \         raise ValueError(msg)\n\n        try:\n            query = f\"SELECT\
      \ TOP {k} c.{self.id_field}, c.{self.text_field}, c.{self.vector_field}, c.{self.attributes_field},\
      \ VectorDistance(c.{self.vector_field}, @embedding) AS SimilarityScore FROM\
      \ c ORDER BY VectorDistance(c.{self.vector_field}, @embedding)\"  # noqa: S608\n\
      \            query_params = [{\"name\": \"@embedding\", \"value\": query_embedding}]\n\
      \            items = list(\n                self._container_client.query_items(\n\
      \                    query=query,\n                    parameters=query_params,\n\
      \                    enable_cross_partition_query=True,\n                )\n\
      \            )\n        except (CosmosHttpResponseError, ValueError):\n    \
      \        # Currently, the CosmosDB emulator does not support the VectorDistance\
      \ function.\n            # For emulator or test environments - fetch all items\
      \ and calculate distance locally\n            query = f\"SELECT c.{self.id_field},\
      \ c.{self.text_field}, c.{self.vector_field}, c.{self.attributes_field} FROM\
      \ c\"  # noqa: S608\n            items = list(\n                self._container_client.query_items(\n\
      \                    query=query,\n                    enable_cross_partition_query=True,\n\
      \                )\n            )\n\n            # Calculate cosine similarity\
      \ locally (1 - cosine distance)\n            from numpy import dot\n       \
      \     from numpy.linalg import norm\n\n            def cosine_similarity(a,\
      \ b):\n                if norm(a) * norm(b) == 0:\n                    return\
      \ 0.0\n                return dot(a, b) / (norm(a) * norm(b))\n\n          \
      \  # Calculate scores for all items\n            for item in items:\n      \
      \          item_vector = item.get(self.vector_field, [])\n                similarity\
      \ = cosine_similarity(query_embedding, item_vector)\n                item[\"\
      SimilarityScore\"] = similarity\n\n            # Sort by similarity score (higher\
      \ is better) and take top k\n            items = sorted(\n                items,\
      \ key=lambda x: x.get(\"SimilarityScore\", 0.0), reverse=True\n            )[:k]\n\
      \n        return [\n            VectorStoreSearchResult(\n                document=VectorStoreDocument(\n\
      \                    id=item.get(self.id_field, \"\"),\n                   \
      \ text=item.get(self.text_field, \"\"),\n                    vector=item.get(self.vector_field,\
      \ []),\n                    attributes=(json.loads(item.get(self.attributes_field,\
      \ \"{}\"))),\n                ),\n                score=item.get(\"SimilarityScore\"\
      , 0.0),\n            )\n            for item in items\n        ]"
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: list
      type: builtin
    - target: self._container_client.query_items
      type: instance
    - target: item.get
      type: unresolved
    - target: graphrag/vector_stores/cosmosdb.py::cosine_similarity
      type: internal
    - target: sorted
      type: builtin
    - target: x.get
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreSearchResult
      type: internal
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: json::loads
      type: stdlib
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_vector
    called_by: []
  - name: cosine_similarity
    start_line: 214
    end_line: 217
    code: "def cosine_similarity(a, b):\n                if norm(a) * norm(b) == 0:\n\
      \                    return 0.0\n                return dot(a, b) / (norm(a)\
      \ * norm(b))"
    signature: def cosine_similarity(a, b)
    decorators: []
    raises: []
    calls:
    - target: numpy.linalg::norm
      type: external
    - target: numpy::dot
      type: external
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.cosine_similarity
    called_by: []
  - name: similarity_search_by_text
    start_line: 243
    end_line: 252
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform a text-based similarity search.\"\"\"\n        query_embedding\
      \ = text_embedder(text)\n        if query_embedding:\n            return self.similarity_search_by_vector(\n\
      \                query_embedding=query_embedding, k=k\n            )\n     \
      \   return []"
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: text_embedder
      type: unresolved
    - target: graphrag/vector_stores/cosmosdb.py::similarity_search_by_vector
      type: internal
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_text
    called_by: []
  - name: filter_by_id
    start_line: 254
    end_line: 266
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      \"\"\"Build a query filter to filter documents by a list of ids.\"\"\"\
      \n        if include_ids is None or len(include_ids) == 0:\n            self.query_filter\
      \ = None\n        else:\n            if isinstance(include_ids[0], str):\n \
      \               id_filter = \", \".join([f\"'{id}'\" for id in include_ids])\n\
      \            else:\n                id_filter = \", \".join([str(id) for id\
      \ in include_ids])\n            self.query_filter = (\n                f\"SELECT\
      \ * FROM c WHERE c.{self.id_field} IN ({id_filter})\"  # noqa: S608\n      \
      \      )\n        return self.query_filter"
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: isinstance
      type: builtin
    - target: '", ".join'
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.filter_by_id
    called_by: []
  - name: search_by_id
    start_line: 268
    end_line: 280
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        \"\"\"\
      Search for a document by id.\"\"\"\n        if self._container_client is None:\n\
      \            msg = \"Container client is not initialized.\"\n            raise\
      \ ValueError(msg)\n\n        item = self._container_client.read_item(item=id,\
      \ partition_key=id)\n        return VectorStoreDocument(\n            id=item.get(self.id_field,\
      \ \"\"),\n            vector=item.get(self.vector_field, []),\n            text=item.get(self.text_field,\
      \ \"\"),\n            attributes=(json.loads(item.get(self.attributes_field,\
      \ \"{}\"))),\n        )"
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    - target: self._container_client.read_item
      type: instance
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: item.get
      type: unresolved
    - target: json::loads
      type: stdlib
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.search_by_id
    called_by: []
  - name: clear
    start_line: 282
    end_line: 285
    code: "def clear(self) -> None:\n        \"\"\"Clear the vector store.\"\"\"\n\
      \        self._delete_container()\n        self._delete_database()"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::_delete_container
      type: internal
    - target: graphrag/vector_stores/cosmosdb.py::_delete_database
      type: internal
    visibility: public
    node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.clear
    called_by: []
- file_name: graphrag/vector_stores/factory.py
  imports:
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: graphrag.config.enums
    name: VectorStoreType
    alias: null
  - module: graphrag.vector_stores.azure_ai_search
    name: AzureAISearchVectorStore
    alias: null
  - module: graphrag.vector_stores.cosmosdb
    name: CosmosDBVectorStore
    alias: null
  - module: graphrag.vector_stores.lancedb
    name: LanceDBVectorStore
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  functions:
  - name: register
    start_line: 36
    end_line: 49
    code: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]\n    ) -> None:\n        \"\"\"Register a custom vector store\
      \ implementation.\n\n        Args:\n            vector_store_type: The type\
      \ identifier for the vector store.\n            creator: A class or callable\
      \ that creates an instance of BaseVectorStore.\n\n        Raises\n        ------\n\
      \            TypeError: If creator is a class type instead of a factory function.\n\
      \        \"\"\"\n        cls._registry[vector_store_type] = creator"
    signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]\n    ) -> None"
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.register
    called_by: []
  - name: create_vector_store
    start_line: 52
    end_line: 78
    code: "def create_vector_store(\n        cls,\n        vector_store_type: str,\n\
      \        vector_store_schema_config: VectorStoreSchemaConfig,\n        **kwargs:\
      \ dict,\n    ) -> BaseVectorStore:\n        \"\"\"Create a vector store object\
      \ from the provided type.\n\n        Args:\n            vector_store_type: The\
      \ type of vector store to create.\n            kwargs: Additional keyword arguments\
      \ for the vector store constructor.\n\n        Returns\n        -------\n  \
      \          A BaseVectorStore instance.\n\n        Raises\n        ------\n \
      \           ValueError: If the vector store type is not registered.\n      \
      \  \"\"\"\n        if vector_store_type not in cls._registry:\n            msg\
      \ = f\"Unknown vector store type: {vector_store_type}\"\n            raise ValueError(msg)\n\
      \n        return cls._registry[vector_store_type](\n            vector_store_schema_config=vector_store_schema_config,\
      \ **kwargs\n        )"
    signature: "def create_vector_store(\n        cls,\n        vector_store_type:\
      \ str,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n     \
      \   **kwargs: dict,\n    ) -> BaseVectorStore"
    decorators:
    - '@classmethod'
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.create_vector_store
    called_by: []
  - name: get_vector_store_types
    start_line: 81
    end_line: 83
    code: "def get_vector_store_types(cls) -> list[str]:\n        \"\"\"Get the registered\
      \ vector store implementations.\"\"\"\n        return list(cls._registry.keys())"
    signature: def get_vector_store_types(cls) -> list[str]
    decorators:
    - '@classmethod'
    raises: []
    calls:
    - target: list
      type: builtin
    - target: cls._registry.keys
      type: unresolved
    visibility: public
    node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.get_vector_store_types
    called_by: []
  - name: is_supported_type
    start_line: 86
    end_line: 88
    code: "def is_supported_type(cls, vector_store_type: str) -> bool:\n        \"\
      \"\"Check if the given vector store type is supported.\"\"\"\n        return\
      \ vector_store_type in cls._registry"
    signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
    decorators:
    - '@classmethod'
    raises: []
    calls: []
    visibility: public
    node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.is_supported_type
    called_by: []
- file_name: graphrag/vector_stores/lancedb.py
  imports:
  - module: json
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pyarrow
    name: null
    alias: pa
  - module: numpy
    name: null
    alias: np
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreSearchResult
    alias: null
  - module: lancedb
    name: null
    alias: null
  functions:
  - name: __init__
    start_line: 24
    end_line: 29
    code: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None:\n        super().__init__(\n            vector_store_schema_config=vector_store_schema_config,\
      \ **kwargs\n        )"
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 31
    end_line: 36
    code: "def connect(self, **kwargs: Any) -> Any:\n        \"\"\"Connect to the\
      \ vector storage.\"\"\"\n        self.db_connection = lancedb.connect(kwargs[\"\
      db_uri\"])\n\n        if self.index_name and self.index_name in self.db_connection.table_names():\n\
      \            self.document_collection = self.db_connection.open_table(self.index_name)"
    signature: 'def connect(self, **kwargs: Any) -> Any'
    decorators: []
    raises: []
    calls:
    - target: lancedb::connect
      type: external
    - target: self.db_connection.table_names
      type: instance
    - target: self.db_connection.open_table
      type: instance
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.connect
    called_by: []
  - name: load_documents
    start_line: 38
    end_line: 101
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        \"\"\"Load documents into\
      \ vector storage.\"\"\"\n        # Step 1: Prepare data columns manually\n \
      \       ids = []\n        texts = []\n        vectors = []\n        attributes\
      \ = []\n\n        for document in documents:\n            self.vector_size =\
      \ (\n                len(document.vector) if document.vector else self.vector_size\n\
      \            )\n            if document.vector is not None and len(document.vector)\
      \ == self.vector_size:\n                ids.append(document.id)\n          \
      \      texts.append(document.text)\n                vectors.append(np.array(document.vector,\
      \ dtype=np.float32))\n                attributes.append(json.dumps(document.attributes))\n\
      \n        # Step 2: Handle empty case\n        if len(ids) == 0:\n         \
      \   data = None\n        else:\n            # Step 3: Flatten the vectors and\
      \ build FixedSizeListArray manually\n            flat_vector = np.concatenate(vectors).astype(np.float32)\n\
      \            flat_array = pa.array(flat_vector, type=pa.float32())\n       \
      \     vector_column = pa.FixedSizeListArray.from_arrays(\n                flat_array,\
      \ self.vector_size\n            )\n\n            # Step 4: Create PyArrow table\
      \ (let schema be inferred)\n            data = pa.table({\n                self.id_field:\
      \ pa.array(ids, type=pa.string()),\n                self.text_field: pa.array(texts,\
      \ type=pa.string()),\n                self.vector_field: vector_column,\n  \
      \              self.attributes_field: pa.array(attributes, type=pa.string()),\n\
      \            })\n\n        # NOTE: If modifying the next section of code, ensure\
      \ that the schema remains the same.\n        #       The pyarrow format of the\
      \ 'vector' field may change if the order of operations is changed\n        #\
      \       and will break vector search.\n        if overwrite:\n            if\
      \ data:\n                self.document_collection = self.db_connection.create_table(\n\
      \                    self.index_name if self.index_name else \"\",\n       \
      \             data=data,\n                    mode=\"overwrite\",\n        \
      \            schema=data.schema,\n                )\n            else:\n   \
      \             self.document_collection = self.db_connection.create_table(\n\
      \                    self.index_name if self.index_name else \"\", mode=\"overwrite\"\
      \n                )\n            self.document_collection.create_index(\n  \
      \              vector_column_name=self.vector_field, index_type=\"IVF_FLAT\"\
      \n            )\n        else:\n            # add data to existing table\n \
      \           self.document_collection = self.db_connection.open_table(\n    \
      \            self.index_name if self.index_name else \"\"\n            )\n \
      \           if data:\n                self.document_collection.add(data)"
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: ids.append
      type: unresolved
    - target: texts.append
      type: unresolved
    - target: vectors.append
      type: unresolved
    - target: numpy::array
      type: external
    - target: attributes.append
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: numpy::concatenate(vectors).astype
      type: external
    - target: numpy::concatenate
      type: external
    - target: pyarrow::array
      type: external
    - target: pyarrow::float32
      type: external
    - target: pyarrow::FixedSizeListArray.from_arrays
      type: external
    - target: pyarrow::table
      type: external
    - target: pyarrow::string
      type: external
    - target: self.db_connection.create_table
      type: instance
    - target: self.document_collection.create_index
      type: instance
    - target: self.db_connection.open_table
      type: instance
    - target: self.document_collection.add
      type: instance
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.load_documents
    called_by: []
  - name: filter_by_id
    start_line: 103
    end_line: 115
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      \"\"\"Build a query filter to filter documents by id.\"\"\"\n       \
      \ if len(include_ids) == 0:\n            self.query_filter = None\n        else:\n\
      \            if isinstance(include_ids[0], str):\n                id_filter\
      \ = \", \".join([f\"'{id}'\" for id in include_ids])\n                self.query_filter\
      \ = f\"{self.id_field} in ({id_filter})\"\n            else:\n             \
      \   self.query_filter = (\n                    f\"{self.id_field} in ({', '.join([str(id)\
      \ for id in include_ids])})\"\n                )\n        return self.query_filter"
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: isinstance
      type: builtin
    - target: '", ".join'
      type: unresolved
    - target: ''', ''.join'
      type: unresolved
    - target: str
      type: builtin
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.filter_by_id
    called_by: []
  - name: similarity_search_by_vector
    start_line: 117
    end_line: 151
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
      \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform a vector-based similarity search.\"\"\"\n        if self.query_filter:\n\
      \            docs = (\n                self.document_collection.search(\n  \
      \                  query=query_embedding, vector_column_name=self.vector_field\n\
      \                )\n                .where(self.query_filter, prefilter=True)\n\
      \                .limit(k)\n                .to_list()\n            )\n    \
      \    else:\n            query_embedding = np.array(query_embedding, dtype=np.float32)\n\
      \n            docs = (\n                self.document_collection.search(\n \
      \                   query=query_embedding, vector_column_name=self.vector_field\n\
      \                )\n                .limit(k)\n                .to_list()\n\
      \            )\n        return [\n            VectorStoreSearchResult(\n   \
      \             document=VectorStoreDocument(\n                    id=doc[self.id_field],\n\
      \                    text=doc[self.text_field],\n                    vector=doc[self.vector_field],\n\
      \                    attributes=json.loads(doc[self.attributes_field]),\n  \
      \              ),\n                score=1 - abs(float(doc[\"_distance\"])),\n\
      \            )\n            for doc in docs\n        ]"
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
      \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: "self.document_collection.search(\n                    query=query_embedding,\
        \ vector_column_name=self.vector_field\n                )\n              \
        \  .where(self.query_filter, prefilter=True)\n                .limit(k)\n\
        \                .to_list"
      type: instance
    - target: "self.document_collection.search(\n                    query=query_embedding,\
        \ vector_column_name=self.vector_field\n                )\n              \
        \  .where(self.query_filter, prefilter=True)\n                .limit"
      type: instance
    - target: "self.document_collection.search(\n                    query=query_embedding,\
        \ vector_column_name=self.vector_field\n                )\n              \
        \  .where"
      type: instance
    - target: self.document_collection.search
      type: instance
    - target: numpy::array
      type: external
    - target: "self.document_collection.search(\n                    query=query_embedding,\
        \ vector_column_name=self.vector_field\n                )\n              \
        \  .limit(k)\n                .to_list"
      type: instance
    - target: "self.document_collection.search(\n                    query=query_embedding,\
        \ vector_column_name=self.vector_field\n                )\n              \
        \  .limit"
      type: instance
    - target: graphrag/vector_stores/base.py::VectorStoreSearchResult
      type: internal
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: json::loads
      type: stdlib
    - target: abs
      type: builtin
    - target: float
      type: builtin
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 153
    end_line: 160
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        \"\"\"Perform a similarity search using a given input text.\"\"\"\n\
      \        query_embedding = text_embedder(text)\n        if query_embedding:\n\
      \            return self.similarity_search_by_vector(query_embedding, k)\n \
      \       return []"
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: text_embedder
      type: unresolved
    - target: graphrag/vector_stores/lancedb.py::similarity_search_by_vector
      type: internal
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_text
    called_by: []
  - name: search_by_id
    start_line: 162
    end_line: 176
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        \"\"\"\
      Search for a document by id.\"\"\"\n        doc = (\n            self.document_collection.search()\n\
      \            .where(f\"{self.id_field} == '{id}'\", prefilter=True)\n      \
      \      .to_list()\n        )\n        if doc:\n            return VectorStoreDocument(\n\
      \                id=doc[0][self.id_field],\n                text=doc[0][self.text_field],\n\
      \                vector=doc[0][self.vector_field],\n                attributes=json.loads(doc[0][self.attributes_field]),\n\
      \            )\n        return VectorStoreDocument(id=id, text=None, vector=None)"
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators: []
    raises: []
    calls:
    - target: "self.document_collection.search()\n            .where(f\"{self.id_field}\
        \ == '{id}'\", prefilter=True)\n            .to_list"
      type: instance
    - target: "self.document_collection.search()\n            .where"
      type: instance
    - target: self.document_collection.search
      type: instance
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: json::loads
      type: stdlib
    visibility: public
    node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.search_by_id
    called_by: []
- file_name: tests/__init__.py
  imports:
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.language_model.factory
    name: ModelFactory
    alias: null
  - module: tests.mock_provider
    name: MockChatLLM
    alias: null
  - module: tests.mock_provider
    name: MockEmbeddingLLM
    alias: null
  functions: []
- file_name: tests/conftest.py
  imports: []
  functions:
  - name: pytest_addoption
    start_line: 5
    end_line: 8
    code: "def pytest_addoption(parser):\n    parser.addoption(\n        \"--run_slow\"\
      , action=\"store_true\", default=False, help=\"run slow tests\"\n    )"
    signature: def pytest_addoption(parser)
    decorators: []
    raises: []
    calls:
    - target: parser.addoption
      type: unresolved
    visibility: public
    node_id: tests/conftest.py::pytest_addoption
    called_by: []
- file_name: tests/integration/__init__.py
  imports: []
  functions: []
- file_name: tests/integration/language_model/__init__.py
  imports: []
  functions: []
- file_name: tests/integration/language_model/test_factory.py
  imports:
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: collections.abc
    name: Generator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.language_model.factory
    name: ModelFactory
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelOutput
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelResponse
    alias: null
  - module: graphrag.language_model.response.base
    name: ModelResponse
    alias: null
  functions:
  - name: test_create_custom_chat_model
    start_line: 21
    end_line: 60
    code: "async def test_create_custom_chat_model():\n    class CustomChatModel:\n\
      \        config: Any\n\n        def __init__(self, **kwargs):\n            pass\n\
      \n        async def achat(\n            self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n        ) -> ModelResponse:\n            return BaseModelResponse(output=BaseModelOutput(content=\"\
      content\"))\n\n        def chat(\n            self, prompt: str, history: list\
      \ | None = None, **kwargs: Any\n        ) -> ModelResponse:\n            return\
      \ BaseModelResponse(\n                output=BaseModelOutput(\n            \
      \        content=\"content\", full_response={\"key\": \"value\"}\n         \
      \       )\n            )\n\n        async def achat_stream(\n            self,\
      \ prompt: str, history: list | None = None, **kwargs: Any\n        ) -> AsyncGenerator[str,\
      \ None]:\n            yield \"\"\n\n        def chat_stream(\n            self,\
      \ prompt: str, history: list | None = None, **kwargs: Any\n        ) -> Generator[str,\
      \ None]: ...\n\n    ModelFactory.register_chat(\"custom_chat\", CustomChatModel)\n\
      \    model = ModelManager().get_or_create_chat_model(\"custom\", \"custom_chat\"\
      )\n    assert isinstance(model, CustomChatModel)\n    response = await model.achat(\"\
      prompt\")\n    assert response.output.content == \"content\"\n    assert response.output.full_response\
      \ is None\n\n    response = model.chat(\"prompt\")\n    assert response.output.content\
      \ == \"content\"\n    assert response.output.full_response == {\"key\": \"value\"\
      }"
    signature: def test_create_custom_chat_model()
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/factory.py::ModelFactory::register_chat
      type: external
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: isinstance
      type: builtin
    - target: model.achat
      type: unresolved
    - target: model.chat
      type: unresolved
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::test_create_custom_chat_model
    called_by: []
  - name: __init__
    start_line: 25
    end_line: 26
    code: "def __init__(self, **kwargs):\n            pass"
    signature: def __init__(self, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: tests/integration/language_model/test_factory.py::CustomChatModel.__init__
    called_by: []
  - name: achat
    start_line: 28
    end_line: 31
    code: "async def achat(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> ModelResponse:\n            return BaseModelResponse(output=BaseModelOutput(content=\"\
      content\"))"
    signature: "def achat(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/response/base.py::BaseModelResponse
      type: internal
    - target: graphrag/language_model/response/base.py::BaseModelOutput
      type: internal
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat
    called_by: []
  - name: chat
    start_line: 33
    end_line: 40
    code: "def chat(\n            self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n        ) -> ModelResponse:\n            return BaseModelResponse(\n\
      \                output=BaseModelOutput(\n                    content=\"content\"\
      , full_response={\"key\": \"value\"}\n                )\n            )"
    signature: "def chat(\n            self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n        ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/response/base.py::BaseModelResponse
      type: internal
    - target: graphrag/language_model/response/base.py::BaseModelOutput
      type: internal
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat
    called_by: []
  - name: achat_stream
    start_line: 42
    end_line: 45
    code: "async def achat_stream(\n            self, prompt: str, history: list |\
      \ None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]:\n    \
      \        yield \"\""
    signature: "def achat_stream(\n            self, prompt: str, history: list |\
      \ None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat_stream
    called_by: []
  - name: chat_stream
    start_line: 47
    end_line: 49
    code: "def chat_stream(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> Generator[str, None]: ..."
    signature: "def chat_stream(\n            self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n        ) -> Generator[str, None]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat_stream
    called_by: []
  - name: test_create_custom_embedding_llm
    start_line: 63
    end_line: 97
    code: "async def test_create_custom_embedding_llm():\n    class CustomEmbeddingModel:\n\
      \        config: Any\n\n        def __init__(self, **kwargs):\n            pass\n\
      \n        async def aembed(self, text: str, **kwargs) -> list[float]:\n    \
      \        return [1.0]\n\n        def embed(self, text: str, **kwargs) -> list[float]:\n\
      \            return [1.0]\n\n        async def aembed_batch(\n            self,\
      \ text_list: list[str], **kwargs\n        ) -> list[list[float]]:\n        \
      \    return [[1.0]]\n\n        def embed_batch(self, text_list: list[str], **kwargs)\
      \ -> list[list[float]]:\n            return [[1.0]]\n\n    ModelFactory.register_embedding(\"\
      custom_embedding\", CustomEmbeddingModel)\n    llm = ModelManager().get_or_create_embedding_model(\"\
      custom\", \"custom_embedding\")\n    assert isinstance(llm, CustomEmbeddingModel)\n\
      \    response = await llm.aembed(\"text\")\n    assert response == [1.0]\n\n\
      \    response = llm.embed(\"text\")\n    assert response == [1.0]\n\n    response\
      \ = await llm.aembed_batch([\"text\"])\n    assert response == [[1.0]]\n\n \
      \   response = llm.embed_batch([\"text\"])\n    assert response == [[1.0]]"
    signature: def test_create_custom_embedding_llm()
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/factory.py::ModelFactory::register_embedding
      type: external
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    - target: isinstance
      type: builtin
    - target: llm.aembed
      type: unresolved
    - target: llm.embed
      type: unresolved
    - target: llm.aembed_batch
      type: unresolved
    - target: llm.embed_batch
      type: unresolved
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::test_create_custom_embedding_llm
    called_by: []
  - name: __init__
    start_line: 67
    end_line: 68
    code: "def __init__(self, **kwargs):\n            pass"
    signature: def __init__(self, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.__init__
    called_by: []
  - name: aembed
    start_line: 70
    end_line: 71
    code: "async def aembed(self, text: str, **kwargs) -> list[float]:\n         \
      \   return [1.0]"
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed
    called_by: []
  - name: embed
    start_line: 73
    end_line: 74
    code: "def embed(self, text: str, **kwargs) -> list[float]:\n            return\
      \ [1.0]"
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed
    called_by: []
  - name: aembed_batch
    start_line: 76
    end_line: 79
    code: "async def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
      \        ) -> list[list[float]]:\n            return [[1.0]]"
    signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
      \        ) -> list[list[float]]"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed_batch
    called_by: []
  - name: embed_batch
    start_line: 81
    end_line: 82
    code: "def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]:\n\
      \            return [[1.0]]"
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed_batch
    called_by: []
- file_name: tests/integration/logging/__init__.py
  imports: []
  functions: []
- file_name: tests/integration/logging/test_factory.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.enums
    name: ReportingType
    alias: null
  - module: graphrag.logger.blob_workflow_logger
    name: BlobWorkflowLogger
    alias: null
  - module: graphrag.logger.factory
    name: LoggerFactory
    alias: null
  - module: unittest.mock
    name: MagicMock
    alias: null
  functions:
  - name: test_create_blob_logger
    start_line: 23
    end_line: 31
    code: "def test_create_blob_logger():\n    kwargs = {\n        \"type\": \"blob\"\
      ,\n        \"connection_string\": WELL_KNOWN_BLOB_STORAGE_KEY,\n        \"base_dir\"\
      : \"testbasedir\",\n        \"container_name\": \"testcontainer\",\n    }\n\
      \    logger = LoggerFactory.create_logger(ReportingType.blob.value, kwargs)\n\
      \    assert isinstance(logger, BlobWorkflowLogger)"
    signature: def test_create_blob_logger()
    decorators:
    - '@pytest.mark.skip(reason="Blob storage emulator is not available in this environment")'
    raises: []
    calls:
    - target: graphrag/logger/factory.py::LoggerFactory::create_logger
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/logging/test_factory.py::test_create_blob_logger
    called_by: []
  - name: test_register_and_create_custom_logger
    start_line: 34
    end_line: 53
    code: "def test_register_and_create_custom_logger():\n    \"\"\"Test registering\
      \ and creating a custom logger type.\"\"\"\n    from unittest.mock import MagicMock\n\
      \n    custom_logger_class = MagicMock(spec=logging.Handler)\n    instance =\
      \ MagicMock()\n    instance.initialized = True\n    custom_logger_class.return_value\
      \ = instance\n\n    LoggerFactory.register(\"custom\", lambda **kwargs: custom_logger_class(**kwargs))\n\
      \    logger = LoggerFactory.create_logger(\"custom\", {})\n\n    assert custom_logger_class.called\n\
      \    assert logger is instance\n    # Access the attribute we set on our mock\n\
      \    assert logger.initialized is True  # type: ignore # Attribute only exists\
      \ on our mock\n\n    # Check if it's in the list of registered logger types\n\
      \    assert \"custom\" in LoggerFactory.get_logger_types()\n    assert LoggerFactory.is_supported_type(\"\
      custom\")"
    signature: def test_register_and_create_custom_logger()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: graphrag/logger/factory.py::LoggerFactory::register
      type: external
    - target: custom_logger_class
      type: unresolved
    - target: graphrag/logger/factory.py::LoggerFactory::create_logger
      type: external
    - target: graphrag/logger/factory.py::LoggerFactory::get_logger_types
      type: external
    - target: graphrag/logger/factory.py::LoggerFactory::is_supported_type
      type: external
    visibility: public
    node_id: tests/integration/logging/test_factory.py::test_register_and_create_custom_logger
    called_by: []
  - name: test_get_logger_types
    start_line: 56
    end_line: 60
    code: "def test_get_logger_types():\n    logger_types = LoggerFactory.get_logger_types()\n\
      \    # Check that built-in types are registered\n    assert ReportingType.file.value\
      \ in logger_types\n    assert ReportingType.blob.value in logger_types"
    signature: def test_get_logger_types()
    decorators: []
    raises: []
    calls:
    - target: graphrag/logger/factory.py::LoggerFactory::get_logger_types
      type: external
    visibility: public
    node_id: tests/integration/logging/test_factory.py::test_get_logger_types
    called_by: []
  - name: test_create_unknown_logger
    start_line: 63
    end_line: 65
    code: "def test_create_unknown_logger():\n    with pytest.raises(ValueError, match=\"\
      Unknown reporting type: unknown\"):\n        LoggerFactory.create_logger(\"\
      unknown\", {})"
    signature: def test_create_unknown_logger()
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/logger/factory.py::LoggerFactory::create_logger
      type: external
    visibility: public
    node_id: tests/integration/logging/test_factory.py::test_create_unknown_logger
    called_by: []
- file_name: tests/integration/logging/test_standard_logging.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: tempfile
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.logger.standard_logging
    name: DEFAULT_LOG_FILENAME
    alias: null
  - module: graphrag.logger.standard_logging
    name: init_loggers
    alias: null
  - module: tests.unit.config.utils
    name: get_default_graphrag_config
    alias: null
  functions:
  - name: test_standard_logging
    start_line: 14
    end_line: 17
    code: "def test_standard_logging():\n    \"\"\"Test that standard logging works.\"\
      \"\"\n    logger = logging.getLogger(\"graphrag.test\")\n    assert logger.name\
      \ == \"graphrag.test\""
    signature: def test_standard_logging()
    decorators: []
    raises: []
    calls:
    - target: logging::getLogger
      type: stdlib
    visibility: public
    node_id: tests/integration/logging/test_standard_logging.py::test_standard_logging
    called_by: []
  - name: test_logger_hierarchy
    start_line: 20
    end_line: 34
    code: "def test_logger_hierarchy():\n    \"\"\"Test that logger hierarchy works\
      \ correctly.\"\"\"\n    # reset logging to default state using init_loggers\n\
      \    config = get_default_graphrag_config()\n    init_loggers(config)\n\n  \
      \  root_logger = logging.getLogger(\"graphrag\")\n    child_logger = logging.getLogger(\"\
      graphrag.child\")\n\n    # setting level on root should affect children\n  \
      \  root_logger.setLevel(logging.ERROR)\n    assert child_logger.getEffectiveLevel()\
      \ == logging.ERROR\n\n    # clean up after test\n    root_logger.handlers.clear()"
    signature: def test_logger_hierarchy()
    decorators: []
    raises: []
    calls:
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logging::getLogger
      type: stdlib
    - target: root_logger.setLevel
      type: unresolved
    - target: child_logger.getEffectiveLevel
      type: unresolved
    - target: root_logger.handlers.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
    called_by: []
  - name: test_init_loggers_file_config
    start_line: 37
    end_line: 69
    code: "def test_init_loggers_file_config():\n    \"\"\"Test that init_loggers\
      \ works with file configuration.\"\"\"\n    with tempfile.TemporaryDirectory()\
      \ as temp_dir:\n        config = get_default_graphrag_config(root_dir=temp_dir)\n\
      \n        # call init_loggers with file config\n        init_loggers(config=config)\n\
      \n        logger = logging.getLogger(\"graphrag\")\n\n        # should have\
      \ a file handler\n        file_handlers = [\n            h for h in logger.handlers\
      \ if isinstance(h, logging.FileHandler)\n        ]\n        assert len(file_handlers)\
      \ > 0\n\n        # test that logging works\n        test_message = \"Test init_loggers\
      \ file message\"\n        logger.info(test_message)\n\n        # check that\
      \ the log file was created\n        log_file = Path(temp_dir) / \"logs\" / DEFAULT_LOG_FILENAME\n\
      \        assert log_file.exists()\n\n        with open(log_file) as f:\n   \
      \         content = f.read()\n            assert test_message in content\n\n\
      \        # clean up\n        for handler in logger.handlers[:]:\n          \
      \  if isinstance(handler, logging.FileHandler):\n                handler.close()\n\
      \        logger.handlers.clear()"
    signature: def test_init_loggers_file_config()
    decorators: []
    raises: []
    calls:
    - target: tempfile::TemporaryDirectory
      type: stdlib
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logging::getLogger
      type: stdlib
    - target: isinstance
      type: builtin
    - target: len
      type: builtin
    - target: logger.info
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: log_file.exists
      type: unresolved
    - target: open
      type: builtin
    - target: f.read
      type: unresolved
    - target: handler.close
      type: unresolved
    - target: logger.handlers.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
    called_by: []
  - name: test_init_loggers_file_verbose
    start_line: 72
    end_line: 97
    code: "def test_init_loggers_file_verbose():\n    \"\"\"Test that init_loggers\
      \ works with verbose flag.\"\"\"\n    with tempfile.TemporaryDirectory() as\
      \ temp_dir:\n        config = get_default_graphrag_config(root_dir=temp_dir)\n\
      \n        # call init_loggers with file config\n        init_loggers(config=config,\
      \ verbose=True)\n\n        logger = logging.getLogger(\"graphrag\")\n\n    \
      \    # test that logging works\n        test_message = \"Test init_loggers file\
      \ message\"\n        logger.debug(test_message)\n\n        # check that the\
      \ log file was created\n        log_file = Path(temp_dir) / \"logs\" / DEFAULT_LOG_FILENAME\n\
      \n        with open(log_file) as f:\n            content = f.read()\n      \
      \      assert test_message in content\n\n        # clean up\n        for handler\
      \ in logger.handlers[:]:\n            if isinstance(handler, logging.FileHandler):\n\
      \                handler.close()\n        logger.handlers.clear()"
    signature: def test_init_loggers_file_verbose()
    decorators: []
    raises: []
    calls:
    - target: tempfile::TemporaryDirectory
      type: stdlib
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logging::getLogger
      type: stdlib
    - target: logger.debug
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: open
      type: builtin
    - target: f.read
      type: unresolved
    - target: isinstance
      type: builtin
    - target: handler.close
      type: unresolved
    - target: logger.handlers.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
    called_by: []
  - name: test_init_loggers_custom_filename
    start_line: 100
    end_line: 118
    code: "def test_init_loggers_custom_filename():\n    \"\"\"Test that init_loggers\
      \ works with custom filename.\"\"\"\n    with tempfile.TemporaryDirectory()\
      \ as temp_dir:\n        config = get_default_graphrag_config(root_dir=temp_dir)\n\
      \n        # call init_loggers with file config\n        init_loggers(config=config,\
      \ filename=\"custom-log.log\")\n\n        logger = logging.getLogger(\"graphrag\"\
      )\n\n        # check that the log file was created\n        log_file = Path(temp_dir)\
      \ / \"logs\" / \"custom-log.log\"\n        assert log_file.exists()\n\n    \
      \    # clean up\n        for handler in logger.handlers[:]:\n            if\
      \ isinstance(handler, logging.FileHandler):\n                handler.close()\n\
      \        logger.handlers.clear()"
    signature: def test_init_loggers_custom_filename()
    decorators: []
    raises: []
    calls:
    - target: tempfile::TemporaryDirectory
      type: stdlib
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: graphrag/logger/standard_logging.py::init_loggers
      type: internal
    - target: logging::getLogger
      type: stdlib
    - target: pathlib::Path
      type: stdlib
    - target: log_file.exists
      type: unresolved
    - target: isinstance
      type: builtin
    - target: handler.close
      type: unresolved
    - target: logger.handlers.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
    called_by: []
- file_name: tests/integration/storage/__init__.py
  imports: []
  functions: []
- file_name: tests/integration/storage/test_blob_pipeline_storage.py
  imports:
  - module: re
    name: null
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: graphrag.storage.blob_pipeline_storage
    name: BlobPipelineStorage
    alias: null
  functions:
  - name: test_find
    start_line: 14
    end_line: 48
    code: "async def test_find():\n    storage = BlobPipelineStorage(\n        connection_string=WELL_KNOWN_BLOB_STORAGE_KEY,\n\
      \        container_name=\"testfind\",\n    )\n    try:\n        try:\n     \
      \       items = list(\n                storage.find(base_dir=\"input\", file_pattern=re.compile(r\"\
      .*\\.txt$\"))\n            )\n            items = [item[0] for item in items]\n\
      \            assert items == []\n\n            await storage.set(\n        \
      \        \"input/christmas.txt\", \"Merry Christmas!\", encoding=\"utf-8\"\n\
      \            )\n            items = list(\n                storage.find(base_dir=\"\
      input\", file_pattern=re.compile(r\".*\\.txt$\"))\n            )\n         \
      \   items = [item[0] for item in items]\n            assert items == [\"input/christmas.txt\"\
      ]\n\n            await storage.set(\"test.txt\", \"Hello, World!\", encoding=\"\
      utf-8\")\n            items = list(storage.find(file_pattern=re.compile(r\"\
      .*\\.txt$\")))\n            items = [item[0] for item in items]\n          \
      \  assert items == [\"input/christmas.txt\", \"test.txt\"]\n\n            output\
      \ = await storage.get(\"test.txt\")\n            assert output == \"Hello, World!\"\
      \n        finally:\n            await storage.delete(\"test.txt\")\n       \
      \     output = await storage.get(\"test.txt\")\n            assert output is\
      \ None\n    finally:\n        storage._delete_container()  # noqa: SLF001"
    signature: def test_find()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
      type: internal
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: storage.set
      type: unresolved
    - target: storage.get
      type: unresolved
    - target: storage.delete
      type: unresolved
    - target: storage._delete_container
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_find
    called_by: []
  - name: test_dotprefix
    start_line: 51
    end_line: 63
    code: "async def test_dotprefix():\n    storage = BlobPipelineStorage(\n     \
      \   connection_string=WELL_KNOWN_BLOB_STORAGE_KEY,\n        container_name=\"\
      testfind\",\n        path_prefix=\".\",\n    )\n    try:\n        await storage.set(\"\
      input/christmas.txt\", \"Merry Christmas!\", encoding=\"utf-8\")\n        items\
      \ = list(storage.find(file_pattern=re.compile(r\".*\\.txt$\")))\n        items\
      \ = [item[0] for item in items]\n        assert items == [\"input/christmas.txt\"\
      ]\n    finally:\n        storage._delete_container()  # noqa: SLF001"
    signature: def test_dotprefix()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
      type: internal
    - target: storage.set
      type: unresolved
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: storage._delete_container
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_dotprefix
    called_by: []
  - name: test_get_creation_date
    start_line: 66
    end_line: 81
    code: "async def test_get_creation_date():\n    storage = BlobPipelineStorage(\n\
      \        connection_string=WELL_KNOWN_BLOB_STORAGE_KEY,\n        container_name=\"\
      testfind\",\n        path_prefix=\".\",\n    )\n    try:\n        await storage.set(\"\
      input/christmas.txt\", \"Merry Christmas!\", encoding=\"utf-8\")\n        creation_date\
      \ = await storage.get_creation_date(\"input/christmas.txt\")\n\n        datetime_format\
      \ = \"%Y-%m-%d %H:%M:%S %z\"\n        parsed_datetime = datetime.strptime(creation_date,\
      \ datetime_format).astimezone()\n\n        assert parsed_datetime.strftime(datetime_format)\
      \ == creation_date\n    finally:\n        storage._delete_container()  # noqa:\
      \ SLF001"
    signature: def test_get_creation_date()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
      type: internal
    - target: storage.set
      type: unresolved
    - target: storage.get_creation_date
      type: unresolved
    - target: datetime::datetime::strptime(creation_date, datetime_format).astimezone
      type: external
    - target: datetime::datetime::strptime
      type: external
    - target: parsed_datetime.strftime
      type: unresolved
    - target: storage._delete_container
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_get_creation_date
    called_by: []
  - name: test_child
    start_line: 84
    end_line: 115
    code: "async def test_child():\n    parent = BlobPipelineStorage(\n        connection_string=WELL_KNOWN_BLOB_STORAGE_KEY,\n\
      \        container_name=\"testchild\",\n    )\n    try:\n        try:\n    \
      \        storage = parent.child(\"input\")\n            await storage.set(\"\
      christmas.txt\", \"Merry Christmas!\", encoding=\"utf-8\")\n            items\
      \ = list(storage.find(re.compile(r\".*\\.txt$\")))\n            items = [item[0]\
      \ for item in items]\n            assert items == [\"christmas.txt\"]\n\n  \
      \          await storage.set(\"test.txt\", \"Hello, World!\", encoding=\"utf-8\"\
      )\n            items = list(storage.find(re.compile(r\".*\\.txt$\")))\n    \
      \        items = [item[0] for item in items]\n            print(\"FOUND\", items)\n\
      \            assert items == [\"christmas.txt\", \"test.txt\"]\n\n         \
      \   output = await storage.get(\"test.txt\")\n            assert output == \"\
      Hello, World!\"\n\n            items = list(parent.find(re.compile(r\".*\\.txt$\"\
      )))\n            items = [item[0] for item in items]\n            print(\"FOUND\
      \ ITEMS\", items)\n            assert items == [\"input/christmas.txt\", \"\
      input/test.txt\"]\n        finally:\n            await parent.delete(\"input/test.txt\"\
      )\n            has_test = await parent.has(\"input/test.txt\")\n           \
      \ assert not has_test\n    finally:\n        parent._delete_container()  # noqa:\
      \ SLF001"
    signature: def test_child()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
      type: internal
    - target: parent.child
      type: unresolved
    - target: storage.set
      type: unresolved
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: print
      type: builtin
    - target: storage.get
      type: unresolved
    - target: parent.find
      type: unresolved
    - target: parent.delete
      type: unresolved
    - target: parent.has
      type: unresolved
    - target: parent._delete_container
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_child
    called_by: []
- file_name: tests/integration/storage/test_cosmosdb_storage.py
  imports:
  - module: json
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: sys
    name: null
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.storage.cosmosdb_pipeline_storage
    name: CosmosDBPipelineStorage
    alias: null
  functions:
  - name: test_find
    start_line: 24
    end_line: 67
    code: "async def test_find():\n    storage = CosmosDBPipelineStorage(\n      \
      \  connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n        base_dir=\"\
      testfind\",\n        container_name=\"testfindcontainer\",\n    )\n    try:\n\
      \        try:\n            items = list(storage.find(file_pattern=re.compile(r\"\
      .*\\.json$\")))\n            items = [item[0] for item in items]\n         \
      \   assert items == []\n\n            json_content = {\n                \"content\"\
      : \"Merry Christmas!\",\n            }\n            await storage.set(\n   \
      \             \"christmas.json\", json.dumps(json_content), encoding=\"utf-8\"\
      \n            )\n            items = list(storage.find(file_pattern=re.compile(r\"\
      .*\\.json$\")))\n            items = [item[0] for item in items]\n         \
      \   assert items == [\"christmas.json\"]\n\n            json_content = {\n \
      \               \"content\": \"Hello, World!\",\n            }\n           \
      \ await storage.set(\"test.json\", json.dumps(json_content), encoding=\"utf-8\"\
      )\n            items = list(storage.find(file_pattern=re.compile(r\".*\\.json$\"\
      )))\n            items = [item[0] for item in items]\n            assert items\
      \ == [\"christmas.json\", \"test.json\"]\n\n            output = await storage.get(\"\
      test.json\")\n            output_json = json.loads(output)\n            assert\
      \ output_json[\"content\"] == \"Hello, World!\"\n\n            json_exists =\
      \ await storage.has(\"christmas.json\")\n            assert json_exists is True\n\
      \            json_exists = await storage.has(\"easter.json\")\n            assert\
      \ json_exists is False\n        finally:\n            await storage.delete(\"\
      test.json\")\n            output = await storage.get(\"test.json\")\n      \
      \      assert output is None\n    finally:\n        await storage.clear()"
    signature: def test_find()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
      type: internal
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: storage.set
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: storage.get
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: storage.has
      type: unresolved
    - target: storage.delete
      type: unresolved
    - target: storage.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_cosmosdb_storage.py::test_find
    called_by: []
  - name: test_child
    start_line: 70
    end_line: 80
    code: "async def test_child():\n    storage = CosmosDBPipelineStorage(\n     \
      \   connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n        base_dir=\"\
      testchild\",\n        container_name=\"testchildcontainer\",\n    )\n    try:\n\
      \        child_storage = storage.child(\"child\")\n        assert type(child_storage)\
      \ is CosmosDBPipelineStorage\n    finally:\n        await storage.clear()"
    signature: def test_child()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
      type: internal
    - target: storage.child
      type: unresolved
    - target: type
      type: builtin
    - target: storage.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_cosmosdb_storage.py::test_child
    called_by: []
  - name: test_clear
    start_line: 83
    end_line: 110
    code: "async def test_clear():\n    storage = CosmosDBPipelineStorage(\n     \
      \   connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n        base_dir=\"\
      testclear\",\n        container_name=\"testclearcontainer\",\n    )\n    try:\n\
      \        json_exists = {\n            \"content\": \"Merry Christmas!\",\n \
      \       }\n        await storage.set(\"christmas.json\", json.dumps(json_exists),\
      \ encoding=\"utf-8\")\n        json_exists = {\n            \"content\": \"\
      Happy Easter!\",\n        }\n        await storage.set(\"easter.json\", json.dumps(json_exists),\
      \ encoding=\"utf-8\")\n        await storage.clear()\n\n        items = list(storage.find(file_pattern=re.compile(r\"\
      .*\\.json$\")))\n        items = [item[0] for item in items]\n        assert\
      \ items == []\n\n        output = await storage.get(\"easter.json\")\n     \
      \   assert output is None\n\n        assert storage._container_client is None\
      \  # noqa: SLF001\n        assert storage._database_client is None  # noqa:\
      \ SLF001\n    finally:\n        await storage.clear()"
    signature: def test_clear()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
      type: internal
    - target: storage.set
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: storage.clear
      type: unresolved
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: storage.get
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_cosmosdb_storage.py::test_clear
    called_by: []
  - name: test_get_creation_date
    start_line: 113
    end_line: 133
    code: "async def test_get_creation_date():\n    storage = CosmosDBPipelineStorage(\n\
      \        connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n        base_dir=\"\
      testclear\",\n        container_name=\"testclearcontainer\",\n    )\n    try:\n\
      \        json_content = {\n            \"content\": \"Happy Easter!\",\n   \
      \     }\n        await storage.set(\"easter.json\", json.dumps(json_content),\
      \ encoding=\"utf-8\")\n\n        creation_date = await storage.get_creation_date(\"\
      easter.json\")\n\n        datetime_format = \"%Y-%m-%d %H:%M:%S %z\"\n     \
      \   parsed_datetime = datetime.strptime(creation_date, datetime_format).astimezone()\n\
      \n        assert parsed_datetime.strftime(datetime_format) == creation_date\n\
      \n    finally:\n        await storage.clear()"
    signature: def test_get_creation_date()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
      type: internal
    - target: storage.set
      type: unresolved
    - target: json::dumps
      type: stdlib
    - target: storage.get_creation_date
      type: unresolved
    - target: datetime::datetime::strptime(creation_date, datetime_format).astimezone
      type: external
    - target: datetime::datetime::strptime
      type: external
    - target: parsed_datetime.strftime
      type: unresolved
    - target: storage.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_cosmosdb_storage.py::test_get_creation_date
    called_by: []
- file_name: tests/integration/storage/test_factory.py
  imports:
  - module: sys
    name: null
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.enums
    name: StorageType
    alias: null
  - module: graphrag.storage.blob_pipeline_storage
    name: BlobPipelineStorage
    alias: null
  - module: graphrag.storage.cosmosdb_pipeline_storage
    name: CosmosDBPipelineStorage
    alias: null
  - module: graphrag.storage.factory
    name: StorageFactory
    alias: null
  - module: graphrag.storage.file_pipeline_storage
    name: FilePipelineStorage
    alias: null
  - module: graphrag.storage.memory_pipeline_storage
    name: MemoryPipelineStorage
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  - module: unittest.mock
    name: MagicMock
    alias: null
  - module: re
    name: null
    alias: null
  - module: collections.abc
    name: Iterator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: graphrag.storage.pipeline_storage
    name: PipelineStorage
    alias: null
  functions:
  - name: test_create_blob_storage
    start_line: 27
    end_line: 35
    code: "def test_create_blob_storage():\n    kwargs = {\n        \"type\": \"blob\"\
      ,\n        \"connection_string\": WELL_KNOWN_BLOB_STORAGE_KEY,\n        \"base_dir\"\
      : \"testbasedir\",\n        \"container_name\": \"testcontainer\",\n    }\n\
      \    storage = StorageFactory.create_storage(StorageType.blob.value, kwargs)\n\
      \    assert isinstance(storage, BlobPipelineStorage)"
    signature: def test_create_blob_storage()
    decorators:
    - '@pytest.mark.skip(reason="Blob storage emulator is not available in this environment")'
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_create_blob_storage
    called_by: []
  - name: test_create_cosmosdb_storage
    start_line: 42
    end_line: 50
    code: "def test_create_cosmosdb_storage():\n    kwargs = {\n        \"type\":\
      \ \"cosmosdb\",\n        \"connection_string\": WELL_KNOWN_COSMOS_CONNECTION_STRING,\n\
      \        \"base_dir\": \"testdatabase\",\n        \"container_name\": \"testcontainer\"\
      ,\n    }\n    storage = StorageFactory.create_storage(StorageType.cosmosdb.value,\
      \ kwargs)\n    assert isinstance(storage, CosmosDBPipelineStorage)"
    signature: def test_create_cosmosdb_storage()
    decorators:
    - "@pytest.mark.skipif(\n    not sys.platform.startswith(\"win\"),\n    reason=\"\
      cosmosdb emulator is only available on windows runners at this time\",\n)"
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_create_cosmosdb_storage
    called_by: []
  - name: test_create_file_storage
    start_line: 53
    end_line: 56
    code: "def test_create_file_storage():\n    kwargs = {\"type\": \"file\", \"base_dir\"\
      : \"/tmp/teststorage\"}\n    storage = StorageFactory.create_storage(StorageType.file.value,\
      \ kwargs)\n    assert isinstance(storage, FilePipelineStorage)"
    signature: def test_create_file_storage()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_create_file_storage
    called_by: []
  - name: test_create_memory_storage
    start_line: 59
    end_line: 62
    code: "def test_create_memory_storage():\n    kwargs = {}  # MemoryPipelineStorage\
      \ doesn't accept any constructor parameters\n    storage = StorageFactory.create_storage(StorageType.memory.value,\
      \ kwargs)\n    assert isinstance(storage, MemoryPipelineStorage)"
    signature: def test_create_memory_storage()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_create_memory_storage
    called_by: []
  - name: test_register_and_create_custom_storage
    start_line: 65
    end_line: 87
    code: "def test_register_and_create_custom_storage():\n    \"\"\"Test registering\
      \ and creating a custom storage type.\"\"\"\n    from unittest.mock import MagicMock\n\
      \n    # Create a mock that satisfies the PipelineStorage interface\n    custom_storage_class\
      \ = MagicMock(spec=PipelineStorage)\n    # Make the mock return a mock instance\
      \ when instantiated\n    instance = MagicMock()\n    # We can set attributes\
      \ on the mock instance, even if they don't exist on PipelineStorage\n    instance.initialized\
      \ = True\n    custom_storage_class.return_value = instance\n\n    StorageFactory.register(\"\
      custom\", lambda **kwargs: custom_storage_class(**kwargs))\n    storage = StorageFactory.create_storage(\"\
      custom\", {})\n\n    assert custom_storage_class.called\n    assert storage\
      \ is instance\n    # Access the attribute we set on our mock\n    assert storage.initialized\
      \ is True  # type: ignore # Attribute only exists on our mock\n\n    # Check\
      \ if it's in the list of registered storage types\n    assert \"custom\" in\
      \ StorageFactory.get_storage_types()\n    assert StorageFactory.is_supported_type(\"\
      custom\")"
    signature: def test_register_and_create_custom_storage()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: graphrag/storage/factory.py::StorageFactory::register
      type: external
    - target: custom_storage_class
      type: unresolved
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::get_storage_types
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::is_supported_type
      type: external
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_register_and_create_custom_storage
    called_by: []
  - name: test_get_storage_types
    start_line: 90
    end_line: 96
    code: "def test_get_storage_types():\n    storage_types = StorageFactory.get_storage_types()\n\
      \    # Check that built-in types are registered\n    assert StorageType.file.value\
      \ in storage_types\n    assert StorageType.memory.value in storage_types\n \
      \   assert StorageType.blob.value in storage_types\n    assert StorageType.cosmosdb.value\
      \ in storage_types"
    signature: def test_get_storage_types()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::get_storage_types
      type: external
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_get_storage_types
    called_by: []
  - name: test_create_unknown_storage
    start_line: 99
    end_line: 101
    code: "def test_create_unknown_storage():\n    with pytest.raises(ValueError,\
      \ match=\"Unknown storage type: unknown\"):\n        StorageFactory.create_storage(\"\
      unknown\", {})"
    signature: def test_create_unknown_storage()
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_create_unknown_storage
    called_by: []
  - name: test_register_class_directly_works
    start_line: 104
    end_line: 160
    code: "def test_register_class_directly_works():\n    \"\"\"Test that registering\
      \ a class directly works (StorageFactory allows this).\"\"\"\n    import re\n\
      \    from collections.abc import Iterator\n    from typing import Any\n\n  \
      \  from graphrag.storage.pipeline_storage import PipelineStorage\n\n    class\
      \ CustomStorage(PipelineStorage):\n        def __init__(self, **kwargs):\n \
      \           pass\n\n        def find(\n            self,\n            file_pattern:\
      \ re.Pattern[str],\n            base_dir: str | None = None,\n            file_filter:\
      \ dict[str, Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]:\n            return iter([])\n\n        async def get(\n\
      \            self, key: str, as_bytes: bool | None = None, encoding: str | None\
      \ = None\n        ) -> Any:\n            return None\n\n        async def set(self,\
      \ key: str, value: Any, encoding: str | None = None) -> None:\n            pass\n\
      \n        async def delete(self, key: str) -> None:\n            pass\n\n  \
      \      async def has(self, key: str) -> bool:\n            return False\n\n\
      \        async def clear(self) -> None:\n            pass\n\n        def child(self,\
      \ name: str | None) -> \"PipelineStorage\":\n            return self\n\n   \
      \     def keys(self) -> list[str]:\n            return []\n\n        async def\
      \ get_creation_date(self, key: str) -> str:\n            return \"2024-01-01\
      \ 00:00:00 +0000\"\n\n    # StorageFactory allows registering classes directly\
      \ (no TypeError)\n    StorageFactory.register(\"custom_class\", CustomStorage)\n\
      \n    # Verify it was registered\n    assert \"custom_class\" in StorageFactory.get_storage_types()\n\
      \    assert StorageFactory.is_supported_type(\"custom_class\")\n\n    # Test\
      \ creating an instance\n    storage = StorageFactory.create_storage(\"custom_class\"\
      , {})\n    assert isinstance(storage, CustomStorage)"
    signature: def test_register_class_directly_works()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/factory.py::StorageFactory::register
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::get_storage_types
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::is_supported_type
      type: external
    - target: graphrag/storage/factory.py::StorageFactory::create_storage
      type: external
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::test_register_class_directly_works
    called_by: []
  - name: __init__
    start_line: 113
    end_line: 114
    code: "def __init__(self, **kwargs):\n            pass"
    signature: def __init__(self, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: tests/integration/storage/test_factory.py::CustomStorage.__init__
    called_by: []
  - name: find
    start_line: 116
    end_line: 123
    code: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
      \            base_dir: str | None = None,\n            file_filter: dict[str,\
      \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]:\n            return iter([])"
    signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
      \            base_dir: str | None = None,\n            file_filter: dict[str,\
      \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]"
    decorators: []
    raises: []
    calls:
    - target: iter
      type: builtin
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.find
    called_by: []
  - name: get
    start_line: 125
    end_line: 128
    code: "async def get(\n            self, key: str, as_bytes: bool | None = None,\
      \ encoding: str | None = None\n        ) -> Any:\n            return None"
    signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
      \ encoding: str | None = None\n        ) -> Any"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.get
    called_by: []
  - name: set
    start_line: 130
    end_line: 131
    code: "async def set(self, key: str, value: Any, encoding: str | None = None)\
      \ -> None:\n            pass"
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.set
    called_by: []
  - name: delete
    start_line: 133
    end_line: 134
    code: "async def delete(self, key: str) -> None:\n            pass"
    signature: 'def delete(self, key: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.delete
    called_by: []
  - name: has
    start_line: 136
    end_line: 137
    code: "async def has(self, key: str) -> bool:\n            return False"
    signature: 'def has(self, key: str) -> bool'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.has
    called_by: []
  - name: clear
    start_line: 139
    end_line: 140
    code: "async def clear(self) -> None:\n            pass"
    signature: def clear(self) -> None
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.clear
    called_by: []
  - name: child
    start_line: 142
    end_line: 143
    code: "def child(self, name: str | None) -> \"PipelineStorage\":\n           \
      \ return self"
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.child
    called_by: []
  - name: keys
    start_line: 145
    end_line: 146
    code: "def keys(self) -> list[str]:\n            return []"
    signature: def keys(self) -> list[str]
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.keys
    called_by: []
  - name: get_creation_date
    start_line: 148
    end_line: 149
    code: "async def get_creation_date(self, key: str) -> str:\n            return\
      \ \"2024-01-01 00:00:00 +0000\""
    signature: 'def get_creation_date(self, key: str) -> str'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/storage/test_factory.py::CustomStorage.get_creation_date
    called_by: []
- file_name: tests/integration/storage/test_file_pipeline_storage.py
  imports:
  - module: os
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: datetime
    name: datetime
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: graphrag.storage.file_pipeline_storage
    name: FilePipelineStorage
    alias: null
  functions:
  - name: test_find
    start_line: 17
    end_line: 35
    code: "async def test_find():\n    storage = FilePipelineStorage()\n    items\
      \ = list(\n        storage.find(\n            base_dir=\"tests/fixtures/text/input\"\
      ,\n            file_pattern=re.compile(r\".*\\.txt$\"),\n            file_filter=None,\n\
      \        )\n    )\n    assert items == [(str(Path(\"tests/fixtures/text/input/dulce.txt\"\
      )), {})]\n    output = await storage.get(\"tests/fixtures/text/input/dulce.txt\"\
      )\n    assert len(output) > 0\n\n    await storage.set(\"test.txt\", \"Hello,\
      \ World!\", encoding=\"utf-8\")\n    output = await storage.get(\"test.txt\"\
      )\n    assert output == \"Hello, World!\"\n    await storage.delete(\"test.txt\"\
      )\n    output = await storage.get(\"test.txt\")\n    assert output is None"
    signature: def test_find()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
      type: internal
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: storage.get
      type: unresolved
    - target: len
      type: builtin
    - target: storage.set
      type: unresolved
    - target: storage.delete
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_file_pipeline_storage.py::test_find
    called_by: []
  - name: test_get_creation_date
    start_line: 38
    end_line: 48
    code: "async def test_get_creation_date():\n    storage = FilePipelineStorage()\n\
      \n    creation_date = await storage.get_creation_date(\n        \"tests/fixtures/text/input/dulce.txt\"\
      \n    )\n\n    datetime_format = \"%Y-%m-%d %H:%M:%S %z\"\n    parsed_datetime\
      \ = datetime.strptime(creation_date, datetime_format).astimezone()\n\n    assert\
      \ parsed_datetime.strftime(datetime_format) == creation_date"
    signature: def test_get_creation_date()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
      type: internal
    - target: storage.get_creation_date
      type: unresolved
    - target: datetime::datetime::strptime(creation_date, datetime_format).astimezone
      type: external
    - target: datetime::datetime::strptime
      type: external
    - target: parsed_datetime.strftime
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_file_pipeline_storage.py::test_get_creation_date
    called_by: []
  - name: test_child
    start_line: 51
    end_line: 65
    code: "async def test_child():\n    storage = FilePipelineStorage()\n    storage\
      \ = storage.child(\"tests/fixtures/text/input\")\n    items = list(storage.find(re.compile(r\"\
      .*\\.txt$\")))\n    assert items == [(str(Path(\"dulce.txt\")), {})]\n\n   \
      \ output = await storage.get(\"dulce.txt\")\n    assert len(output) > 0\n\n\
      \    await storage.set(\"test.txt\", \"Hello, World!\", encoding=\"utf-8\")\n\
      \    output = await storage.get(\"test.txt\")\n    assert output == \"Hello,\
      \ World!\"\n    await storage.delete(\"test.txt\")\n    output = await storage.get(\"\
      test.txt\")\n    assert output is None"
    signature: def test_child()
    decorators: []
    raises: []
    calls:
    - target: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
      type: internal
    - target: storage.child
      type: unresolved
    - target: list
      type: builtin
    - target: storage.find
      type: unresolved
    - target: re::compile
      type: stdlib
    - target: str
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: storage.get
      type: unresolved
    - target: len
      type: builtin
    - target: storage.set
      type: unresolved
    - target: storage.delete
      type: unresolved
    visibility: public
    node_id: tests/integration/storage/test_file_pipeline_storage.py::test_child
    called_by: []
- file_name: tests/integration/vector_stores/__init__.py
  imports: []
  functions: []
- file_name: tests/integration/vector_stores/test_azure_ai_search.py
  imports:
  - module: os
    name: null
    alias: null
  - module: unittest.mock
    name: MagicMock
    alias: null
  - module: unittest.mock
    name: patch
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.vector_stores.azure_ai_search
    name: AzureAISearchVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  functions:
  - name: mock_search_client
    start_line: 25
    end_line: 30
    code: "def mock_search_client(self):\n        \"\"\"Create a mock Azure AI Search\
      \ client.\"\"\"\n        with patch(\n            \"graphrag.vector_stores.azure_ai_search.SearchClient\"\
      \n        ) as mock_client:\n            yield mock_client.return_value"
    signature: def mock_search_client(self)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: unittest.mock::patch
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_search_client
    called_by: []
  - name: mock_index_client
    start_line: 33
    end_line: 38
    code: "def mock_index_client(self):\n        \"\"\"Create a mock Azure AI Search\
      \ index client.\"\"\"\n        with patch(\n            \"graphrag.vector_stores.azure_ai_search.SearchIndexClient\"\
      \n        ) as mock_client:\n            yield mock_client.return_value"
    signature: def mock_index_client(self)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: unittest.mock::patch
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_index_client
    called_by: []
  - name: vector_store
    start_line: 41
    end_line: 57
    code: "def vector_store(self, mock_search_client, mock_index_client):\n      \
      \  \"\"\"Create an Azure AI Search vector store instance.\"\"\"\n        vector_store\
      \ = AzureAISearchVectorStore(\n            vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \                index_name=\"test_vectors\", vector_size=5\n            ),\n\
      \        )\n\n        # Create the necessary mocks first\n        vector_store.db_connection\
      \ = mock_search_client\n        vector_store.index_client = mock_index_client\n\
      \n        vector_store.connect(\n            url=TEST_AZURE_AI_SEARCH_URL,\n\
      \            api_key=TEST_AZURE_AI_SEARCH_KEY,\n        )\n        return vector_store"
    signature: def vector_store(self, mock_search_client, mock_index_client)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store
    called_by: []
  - name: vector_store_custom
    start_line: 60
    end_line: 81
    code: "def vector_store_custom(self, mock_search_client, mock_index_client):\n\
      \        \"\"\"Create an Azure AI Search vector store instance.\"\"\"\n    \
      \    vector_store = AzureAISearchVectorStore(\n            vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \                index_name=\"test_vectors\",\n                id_field=\"id_custom\"\
      ,\n                text_field=\"text_custom\",\n                attributes_field=\"\
      attributes_custom\",\n                vector_field=\"vector_custom\",\n    \
      \            vector_size=5,\n            ),\n        )\n\n        # Create the\
      \ necessary mocks first\n        vector_store.db_connection = mock_search_client\n\
      \        vector_store.index_client = mock_index_client\n\n        vector_store.connect(\n\
      \            url=TEST_AZURE_AI_SEARCH_URL,\n            api_key=TEST_AZURE_AI_SEARCH_KEY,\n\
      \        )\n        return vector_store"
    signature: def vector_store_custom(self, mock_search_client, mock_index_client)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store_custom
    called_by: []
  - name: sample_documents
    start_line: 84
    end_line: 99
    code: "def sample_documents(self):\n        \"\"\"Create sample documents for\
      \ testing.\"\"\"\n        return [\n            VectorStoreDocument(\n     \
      \           id=\"doc1\",\n                text=\"This is document 1\",\n   \
      \             vector=[0.1, 0.2, 0.3, 0.4, 0.5],\n                attributes={\"\
      title\": \"Doc 1\", \"category\": \"test\"},\n            ),\n            VectorStoreDocument(\n\
      \                id=\"doc2\",\n                text=\"This is document 2\",\n\
      \                vector=[0.2, 0.3, 0.4, 0.5, 0.6],\n                attributes={\"\
      title\": \"Doc 2\", \"category\": \"test\"},\n            ),\n        ]"
    signature: def sample_documents(self)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.sample_documents
    called_by: []
  - name: test_vector_store_operations
    start_line: 101
    end_line: 161
    code: "async def test_vector_store_operations(\n        self, vector_store, sample_documents,\
      \ mock_search_client, mock_index_client\n    ):\n        \"\"\"Test basic vector\
      \ store operations with Azure AI Search.\"\"\"\n        # Setup mock responses\n\
      \        mock_index_client.list_index_names.return_value = []\n        mock_index_client.create_or_update_index\
      \ = MagicMock()\n        mock_search_client.upload_documents = MagicMock()\n\
      \n        search_results = [\n            {\n                \"id\": \"doc1\"\
      ,\n                \"text\": \"This is document 1\",\n                \"vector\"\
      : [0.1, 0.2, 0.3, 0.4, 0.5],\n                \"attributes\": '{\"title\": \"\
      Doc 1\", \"category\": \"test\"}',\n                \"@search.score\": 0.9,\n\
      \            },\n            {\n                \"id\": \"doc2\",\n        \
      \        \"text\": \"This is document 2\",\n                \"vector\": [0.2,\
      \ 0.3, 0.4, 0.5, 0.6],\n                \"attributes\": '{\"title\": \"Doc 2\"\
      , \"category\": \"test\"}',\n                \"@search.score\": 0.8,\n     \
      \       },\n        ]\n        mock_search_client.search.return_value = search_results\n\
      \n        mock_search_client.get_document.return_value = {\n            \"id\"\
      : \"doc1\",\n            \"text\": \"This is document 1\",\n            \"vector\"\
      : [0.1, 0.2, 0.3, 0.4, 0.5],\n            \"attributes\": '{\"title\": \"Doc\
      \ 1\", \"category\": \"test\"}',\n        }\n\n        vector_store.load_documents(sample_documents)\n\
      \        assert mock_index_client.create_or_update_index.called\n        assert\
      \ mock_search_client.upload_documents.called\n\n        filter_query = vector_store.filter_by_id([\"\
      doc1\", \"doc2\"])\n        assert filter_query == \"search.in(id, 'doc1,doc2',\
      \ ',')\"\n\n        vector_results = vector_store.similarity_search_by_vector(\n\
      \            [0.1, 0.2, 0.3, 0.4, 0.5], k=2\n        )\n        assert len(vector_results)\
      \ == 2\n        assert vector_results[0].document.id == \"doc1\"\n        assert\
      \ vector_results[0].score == 0.9\n\n        # Define a simple text embedder\
      \ function for testing\n        def mock_embedder(text: str) -> list[float]:\n\
      \            return [0.1, 0.2, 0.3, 0.4, 0.5]\n\n        text_results = vector_store.similarity_search_by_text(\n\
      \            \"test query\", mock_embedder, k=2\n        )\n        assert len(text_results)\
      \ == 2\n\n        doc = vector_store.search_by_id(\"doc1\")\n        assert\
      \ doc.id == \"doc1\"\n        assert doc.text == \"This is document 1\"\n  \
      \      assert doc.attributes[\"title\"] == \"Doc 1\""
    signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
      \ mock_search_client, mock_index_client\n    )"
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_operations
    called_by: []
  - name: mock_embedder
    start_line: 150
    end_line: 151
    code: "def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_embedder
    called_by: []
  - name: test_empty_embedding
    start_line: 163
    end_line: 174
    code: "async def test_empty_embedding(self, vector_store, mock_search_client):\n\
      \        \"\"\"Test similarity search by text with empty embedding.\"\"\"\n\n\
      \        # Create a mock embedder that returns None and verify that no results\
      \ are produced\n        def none_embedder(text: str) -> None:\n            return\
      \ None\n\n        results = vector_store.similarity_search_by_text(\n      \
      \      \"test query\", none_embedder, k=1\n        )\n        assert not mock_search_client.search.called\n\
      \        assert len(results) == 0"
    signature: def test_empty_embedding(self, vector_store, mock_search_client)
    decorators: []
    raises: []
    calls:
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: len
      type: builtin
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_empty_embedding
    called_by: []
  - name: none_embedder
    start_line: 167
    end_line: 168
    code: "def none_embedder(text: str) -> None:\n            return None"
    signature: 'def none_embedder(text: str) -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.none_embedder
    called_by: []
  - name: test_vector_store_customization
    start_line: 176
    end_line: 243
    code: "async def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
      \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
      \    ):\n        \"\"\"Test vector store customization with Azure AI Search.\"\
      \"\"\n        # Setup mock responses\n        mock_index_client.list_index_names.return_value\
      \ = []\n        mock_index_client.create_or_update_index = MagicMock()\n   \
      \     mock_search_client.upload_documents = MagicMock()\n\n        search_results\
      \ = [\n            {\n                vector_store_custom.id_field: \"doc1\"\
      ,\n                vector_store_custom.text_field: \"This is document 1\",\n\
      \                vector_store_custom.vector_field: [0.1, 0.2, 0.3, 0.4, 0.5],\n\
      \                vector_store_custom.attributes_field: '{\"title\": \"Doc 1\"\
      , \"category\": \"test\"}',\n                \"@search.score\": 0.9,\n     \
      \       },\n            {\n                vector_store_custom.id_field: \"\
      doc2\",\n                vector_store_custom.text_field: \"This is document\
      \ 2\",\n                vector_store_custom.vector_field: [0.2, 0.3, 0.4, 0.5,\
      \ 0.6],\n                vector_store_custom.attributes_field: '{\"title\":\
      \ \"Doc 2\", \"category\": \"test\"}',\n                \"@search.score\": 0.8,\n\
      \            },\n        ]\n        mock_search_client.search.return_value =\
      \ search_results\n\n        mock_search_client.get_document.return_value = {\n\
      \            vector_store_custom.id_field: \"doc1\",\n            vector_store_custom.text_field:\
      \ \"This is document 1\",\n            vector_store_custom.vector_field: [0.1,\
      \ 0.2, 0.3, 0.4, 0.5],\n            vector_store_custom.attributes_field: '{\"\
      title\": \"Doc 1\", \"category\": \"test\"}',\n        }\n\n        vector_store_custom.load_documents(sample_documents)\n\
      \        assert mock_index_client.create_or_update_index.called\n        assert\
      \ mock_search_client.upload_documents.called\n\n        filter_query = vector_store_custom.filter_by_id([\"\
      doc1\", \"doc2\"])\n        assert (\n            filter_query\n           \
      \ == f\"search.in({vector_store_custom.id_field}, 'doc1,doc2', ',')\"\n    \
      \    )\n\n        vector_results = vector_store_custom.similarity_search_by_vector(\n\
      \            [0.1, 0.2, 0.3, 0.4, 0.5], k=2\n        )\n        assert len(vector_results)\
      \ == 2\n        assert vector_results[0].document.id == \"doc1\"\n        assert\
      \ vector_results[0].score == 0.9\n\n        # Define a simple text embedder\
      \ function for testing\n        def mock_embedder(text: str) -> list[float]:\n\
      \            return [0.1, 0.2, 0.3, 0.4, 0.5]\n\n        text_results = vector_store_custom.similarity_search_by_text(\n\
      \            \"test query\", mock_embedder, k=2\n        )\n        assert len(text_results)\
      \ == 2\n\n        doc = vector_store_custom.search_by_id(\"doc1\")\n       \
      \ assert doc.id == \"doc1\"\n        assert doc.text == \"This is document 1\"\
      \n        assert doc.attributes[\"title\"] == \"Doc 1\""
    signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
      \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
      \    )"
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: vector_store_custom.load_documents
      type: unresolved
    - target: vector_store_custom.filter_by_id
      type: unresolved
    - target: vector_store_custom.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: vector_store_custom.similarity_search_by_text
      type: unresolved
    - target: vector_store_custom.search_by_id
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_customization
    called_by: []
  - name: mock_embedder
    start_line: 232
    end_line: 233
    code: "def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_embedder
    called_by: []
- file_name: tests/integration/vector_stores/test_cosmosdb.py
  imports:
  - module: sys
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.cosmosdb
    name: CosmosDBVectorStore
    alias: null
  functions:
  - name: test_vector_store_operations
    start_line: 25
    end_line: 76
    code: "def test_vector_store_operations():\n    \"\"\"Test basic vector store\
      \ operations with CosmosDB.\"\"\"\n    vector_store = CosmosDBVectorStore(\n\
      \        vector_store_schema_config=VectorStoreSchemaConfig(index_name=\"testvector\"\
      ),\n    )\n\n    try:\n        vector_store.connect(\n            connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n\
      \            database_name=\"test_db\",\n        )\n\n        docs = [\n   \
      \         VectorStoreDocument(\n                id=\"doc1\",\n             \
      \   text=\"This is document 1\",\n                vector=[0.1, 0.2, 0.3, 0.4,\
      \ 0.5],\n                attributes={\"title\": \"Doc 1\", \"category\": \"\
      test\"},\n            ),\n            VectorStoreDocument(\n               \
      \ id=\"doc2\",\n                text=\"This is document 2\",\n             \
      \   vector=[0.2, 0.3, 0.4, 0.5, 0.6],\n                attributes={\"title\"\
      : \"Doc 2\", \"category\": \"test\"},\n            ),\n        ]\n        vector_store.load_documents(docs)\n\
      \n        vector_store.filter_by_id([\"doc1\"])\n\n        doc = vector_store.search_by_id(\"\
      doc1\")\n        assert doc.id == \"doc1\"\n        assert doc.text == \"This\
      \ is document 1\"\n        assert doc.vector is not None\n        assert np.allclose(doc.vector,\
      \ [0.1, 0.2, 0.3, 0.4, 0.5])\n        assert doc.attributes[\"title\"] == \"\
      Doc 1\"\n\n        # Define a simple text embedder function for testing\n  \
      \      def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]  # Return fixed embedding\n\n        vector_results =\
      \ vector_store.similarity_search_by_vector(\n            [0.1, 0.2, 0.3, 0.4,\
      \ 0.5], k=2\n        )\n        assert len(vector_results) > 0\n\n        text_results\
      \ = vector_store.similarity_search_by_text(\n            \"test query\", mock_embedder,\
      \ k=2\n        )\n        assert len(text_results) > 0\n    finally:\n     \
      \   vector_store.clear()"
    signature: def test_vector_store_operations()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: numpy::allclose
      type: external
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: vector_store.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_operations
    called_by: []
  - name: mock_embedder
    start_line: 63
    end_line: 64
    code: "def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]  # Return fixed embedding"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_cosmosdb.py::mock_embedder
    called_by: []
  - name: test_clear
    start_line: 79
    end_line: 105
    code: "def test_clear():\n    \"\"\"Test clearing the vector store.\"\"\"\n  \
      \  vector_store = CosmosDBVectorStore(\n        vector_store_schema_config=VectorStoreSchemaConfig(index_name=\"\
      testclear\"),\n    )\n    try:\n        vector_store.connect(\n            connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n\
      \            database_name=\"testclear\",\n        )\n\n        doc = VectorStoreDocument(\n\
      \            id=\"test\",\n            text=\"Test document\",\n           \
      \ vector=[0.1, 0.2, 0.3, 0.4, 0.5],\n            attributes={\"title\": \"Test\
      \ Doc\"},\n        )\n\n        vector_store.load_documents([doc])\n       \
      \ result = vector_store.search_by_id(\"test\")\n        assert result.id ==\
      \ \"test\"\n\n        # Clear and verify document is removed\n        vector_store.clear()\n\
      \        assert vector_store._database_exists() is False  # noqa: SLF001\n \
      \   finally:\n        pass"
    signature: def test_clear()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: vector_store.clear
      type: unresolved
    - target: vector_store._database_exists
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_cosmosdb.py::test_clear
    called_by: []
  - name: test_vector_store_customization
    start_line: 108
    end_line: 166
    code: "def test_vector_store_customization():\n    \"\"\"Test vector store customization\
      \ with CosmosDB.\"\"\"\n    vector_store = CosmosDBVectorStore(\n        vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \            index_name=\"text-embeddings\",\n            id_field=\"id\",\n\
      \            text_field=\"text_custom\",\n            vector_field=\"vector_custom\"\
      ,\n            attributes_field=\"attributes_custom\",\n            vector_size=5,\n\
      \        ),\n    )\n\n    try:\n        vector_store.connect(\n            connection_string=WELL_KNOWN_COSMOS_CONNECTION_STRING,\n\
      \            database_name=\"test_db\",\n        )\n\n        docs = [\n   \
      \         VectorStoreDocument(\n                id=\"doc1\",\n             \
      \   text=\"This is document 1\",\n                vector=[0.1, 0.2, 0.3, 0.4,\
      \ 0.5],\n                attributes={\"title\": \"Doc 1\", \"category\": \"\
      test\"},\n            ),\n            VectorStoreDocument(\n               \
      \ id=\"doc2\",\n                text=\"This is document 2\",\n             \
      \   vector=[0.2, 0.3, 0.4, 0.5, 0.6],\n                attributes={\"title\"\
      : \"Doc 2\", \"category\": \"test\"},\n            ),\n        ]\n        vector_store.load_documents(docs)\n\
      \n        vector_store.filter_by_id([\"doc1\"])\n\n        doc = vector_store.search_by_id(\"\
      doc1\")\n        assert doc.id == \"doc1\"\n        assert doc.text == \"This\
      \ is document 1\"\n        assert doc.vector is not None\n        assert np.allclose(doc.vector,\
      \ [0.1, 0.2, 0.3, 0.4, 0.5])\n        assert doc.attributes[\"title\"] == \"\
      Doc 1\"\n\n        # Define a simple text embedder function for testing\n  \
      \      def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]  # Return fixed embedding\n\n        vector_results =\
      \ vector_store.similarity_search_by_vector(\n            [0.1, 0.2, 0.3, 0.4,\
      \ 0.5], k=2\n        )\n        assert len(vector_results) > 0\n\n        text_results\
      \ = vector_store.similarity_search_by_text(\n            \"test query\", mock_embedder,\
      \ k=2\n        )\n        assert len(text_results) > 0\n    finally:\n     \
      \   vector_store.clear()"
    signature: def test_vector_store_customization()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: numpy::allclose
      type: external
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: vector_store.clear
      type: unresolved
    visibility: public
    node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_customization
    called_by: []
  - name: mock_embedder
    start_line: 153
    end_line: 154
    code: "def mock_embedder(text: str) -> list[float]:\n            return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]  # Return fixed embedding"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_cosmosdb.py::mock_embedder
    called_by: []
- file_name: tests/integration/vector_stores/test_factory.py
  imports:
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.enums
    name: VectorStoreType
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.vector_stores.azure_ai_search
    name: AzureAISearchVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.cosmosdb
    name: CosmosDBVectorStore
    alias: null
  - module: graphrag.vector_stores.factory
    name: VectorStoreFactory
    alias: null
  - module: graphrag.vector_stores.lancedb
    name: LanceDBVectorStore
    alias: null
  - module: unittest.mock
    name: MagicMock
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  functions:
  - name: test_create_lancedb_vector_store
    start_line: 19
    end_line: 31
    code: "def test_create_lancedb_vector_store():\n    kwargs = {\n        \"db_uri\"\
      : \"/tmp/lancedb\",\n    }\n    vector_store = VectorStoreFactory.create_vector_store(\n\
      \        vector_store_type=VectorStoreType.LanceDB.value,\n        vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \            index_name=\"test_collection\"\n        ),\n        kwargs=kwargs,\n\
      \    )\n    assert isinstance(vector_store, LanceDBVectorStore)\n    assert\
      \ vector_store.index_name == \"test_collection\""
    signature: def test_create_lancedb_vector_store()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_create_lancedb_vector_store
    called_by: []
  - name: test_create_azure_ai_search_vector_store
    start_line: 35
    end_line: 47
    code: "def test_create_azure_ai_search_vector_store():\n    kwargs = {\n     \
      \   \"url\": \"https://test.search.windows.net\",\n        \"api_key\": \"test_key\"\
      ,\n    }\n    vector_store = VectorStoreFactory.create_vector_store(\n     \
      \   vector_store_type=VectorStoreType.AzureAISearch.value,\n        vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \            index_name=\"test_collection\"\n        ),\n        kwargs=kwargs,\n\
      \    )\n    assert isinstance(vector_store, AzureAISearchVectorStore)"
    signature: def test_create_azure_ai_search_vector_store()
    decorators:
    - '@pytest.mark.skip(reason="Azure AI Search requires credentials and setup")'
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_create_azure_ai_search_vector_store
    called_by: []
  - name: test_create_cosmosdb_vector_store
    start_line: 51
    end_line: 65
    code: "def test_create_cosmosdb_vector_store():\n    kwargs = {\n        \"connection_string\"\
      : \"AccountEndpoint=https://test.documents.azure.com:443/;AccountKey=test_key==\"\
      ,\n        \"database_name\": \"test_db\",\n    }\n\n    vector_store = VectorStoreFactory.create_vector_store(\n\
      \        vector_store_type=VectorStoreType.CosmosDB.value,\n        vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \            index_name=\"test_collection\"\n        ),\n        kwargs=kwargs,\n\
      \    )\n\n    assert isinstance(vector_store, CosmosDBVectorStore)"
    signature: def test_create_cosmosdb_vector_store()
    decorators:
    - '@pytest.mark.skip(reason="CosmosDB requires credentials and setup")'
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_create_cosmosdb_vector_store
    called_by: []
  - name: test_register_and_create_custom_vector_store
    start_line: 68
    end_line: 94
    code: "def test_register_and_create_custom_vector_store():\n    \"\"\"Test registering\
      \ and creating a custom vector store type.\"\"\"\n    from unittest.mock import\
      \ MagicMock\n\n    # Create a mock that satisfies the BaseVectorStore interface\n\
      \    custom_vector_store_class = MagicMock(spec=BaseVectorStore)\n    # Make\
      \ the mock return a mock instance when instantiated\n    instance = MagicMock()\n\
      \    instance.initialized = True\n    custom_vector_store_class.return_value\
      \ = instance\n\n    VectorStoreFactory.register(\n        \"custom\", lambda\
      \ **kwargs: custom_vector_store_class(**kwargs)\n    )\n\n    vector_store =\
      \ VectorStoreFactory.create_vector_store(\n        vector_store_type=\"custom\"\
      , vector_store_schema_config=VectorStoreSchemaConfig()\n    )\n\n    assert\
      \ custom_vector_store_class.called\n    assert vector_store is instance\n  \
      \  # Access the attribute we set on our mock\n    assert vector_store.initialized\
      \ is True  # type: ignore # Attribute only exists on our mock\n\n    # Check\
      \ if it's in the list of registered vector store types\n    assert \"custom\"\
      \ in VectorStoreFactory.get_vector_store_types()\n    assert VectorStoreFactory.is_supported_type(\"\
      custom\")"
    signature: def test_register_and_create_custom_vector_store()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::register
      type: external
    - target: custom_vector_store_class
      type: unresolved
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::get_vector_store_types
      type: external
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::is_supported_type
      type: external
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_register_and_create_custom_vector_store
    called_by: []
  - name: test_get_vector_store_types
    start_line: 97
    end_line: 102
    code: "def test_get_vector_store_types():\n    vector_store_types = VectorStoreFactory.get_vector_store_types()\n\
      \    # Check that built-in types are registered\n    assert VectorStoreType.LanceDB.value\
      \ in vector_store_types\n    assert VectorStoreType.AzureAISearch.value in vector_store_types\n\
      \    assert VectorStoreType.CosmosDB.value in vector_store_types"
    signature: def test_get_vector_store_types()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::get_vector_store_types
      type: external
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_get_vector_store_types
    called_by: []
  - name: test_create_unknown_vector_store
    start_line: 105
    end_line: 110
    code: "def test_create_unknown_vector_store():\n    with pytest.raises(ValueError,\
      \ match=\"Unknown vector store type: unknown\"):\n        VectorStoreFactory.create_vector_store(\n\
      \            vector_store_type=\"unknown\",\n            vector_store_schema_config=VectorStoreSchemaConfig(),\n\
      \        )"
    signature: def test_create_unknown_vector_store()
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_create_unknown_vector_store
    called_by: []
  - name: test_is_supported_type
    start_line: 113
    end_line: 120
    code: "def test_is_supported_type():\n    # Test built-in types\n    assert VectorStoreFactory.is_supported_type(VectorStoreType.LanceDB.value)\n\
      \    assert VectorStoreFactory.is_supported_type(VectorStoreType.AzureAISearch.value)\n\
      \    assert VectorStoreFactory.is_supported_type(VectorStoreType.CosmosDB.value)\n\
      \n    # Test unknown type\n    assert not VectorStoreFactory.is_supported_type(\"\
      unknown\")"
    signature: def test_is_supported_type()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::is_supported_type
      type: external
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_is_supported_type
    called_by: []
  - name: test_register_class_directly_works
    start_line: 123
    end_line: 164
    code: "def test_register_class_directly_works():\n    \"\"\"Test that registering\
      \ a class directly works (VectorStoreFactory allows this).\"\"\"\n    from graphrag.vector_stores.base\
      \ import BaseVectorStore\n\n    class CustomVectorStore(BaseVectorStore):\n\
      \        def __init__(self, **kwargs):\n            super().__init__(**kwargs)\n\
      \n        def connect(self, **kwargs):\n            pass\n\n        def load_documents(self,\
      \ documents, overwrite=True):\n            pass\n\n        def similarity_search_by_vector(self,\
      \ query_embedding, k=10, **kwargs):\n            return []\n\n        def similarity_search_by_text(self,\
      \ text, text_embedder, k=10, **kwargs):\n            return []\n\n        def\
      \ filter_by_id(self, include_ids):\n            return {}\n\n        def search_by_id(self,\
      \ id):\n            from graphrag.vector_stores.base import VectorStoreDocument\n\
      \n            return VectorStoreDocument(id=id, text=\"test\", vector=None)\n\
      \n    # VectorStoreFactory allows registering classes directly (no TypeError)\n\
      \    VectorStoreFactory.register(\"custom_class\", CustomVectorStore)\n\n  \
      \  # Verify it was registered\n    assert \"custom_class\" in VectorStoreFactory.get_vector_store_types()\n\
      \    assert VectorStoreFactory.is_supported_type(\"custom_class\")\n\n    #\
      \ Test creating an instance\n    vector_store = VectorStoreFactory.create_vector_store(\n\
      \        vector_store_type=\"custom_class\",\n        vector_store_schema_config=VectorStoreSchemaConfig(),\n\
      \    )\n\n    assert isinstance(vector_store, CustomVectorStore)"
    signature: def test_register_class_directly_works()
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::register
      type: external
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::get_vector_store_types
      type: external
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::is_supported_type
      type: external
    - target: graphrag/vector_stores/factory.py::VectorStoreFactory::create_vector_store
      type: external
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::test_register_class_directly_works
    called_by: []
  - name: __init__
    start_line: 128
    end_line: 129
    code: "def __init__(self, **kwargs):\n            super().__init__(**kwargs)"
    signature: def __init__(self, **kwargs)
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    visibility: protected
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 131
    end_line: 132
    code: "def connect(self, **kwargs):\n            pass"
    signature: def connect(self, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.connect
    called_by: []
  - name: load_documents
    start_line: 134
    end_line: 135
    code: "def load_documents(self, documents, overwrite=True):\n            pass"
    signature: def load_documents(self, documents, overwrite=True)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.load_documents
    called_by: []
  - name: similarity_search_by_vector
    start_line: 137
    end_line: 138
    code: "def similarity_search_by_vector(self, query_embedding, k=10, **kwargs):\n\
      \            return []"
    signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 140
    end_line: 141
    code: "def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs):\n\
      \            return []"
    signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_text
    called_by: []
  - name: filter_by_id
    start_line: 143
    end_line: 144
    code: "def filter_by_id(self, include_ids):\n            return {}"
    signature: def filter_by_id(self, include_ids)
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.filter_by_id
    called_by: []
  - name: search_by_id
    start_line: 146
    end_line: 149
    code: "def search_by_id(self, id):\n            from graphrag.vector_stores.base\
      \ import VectorStoreDocument\n\n            return VectorStoreDocument(id=id,\
      \ text=\"test\", vector=None)"
    signature: def search_by_id(self, id)
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    visibility: public
    node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.search_by_id
    called_by: []
- file_name: tests/integration/vector_stores/test_lancedb.py
  imports:
  - module: shutil
    name: null
    alias: null
  - module: tempfile
    name: null
    alias: null
  - module: numpy
    name: null
    alias: np
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.lancedb
    name: LanceDBVectorStore
    alias: null
  functions:
  - name: sample_documents
    start_line: 21
    end_line: 42
    code: "def sample_documents(self):\n        \"\"\"Create sample documents for\
      \ testing.\"\"\"\n        return [\n            VectorStoreDocument(\n     \
      \           id=\"1\",\n                text=\"This is document 1\",\n      \
      \          vector=[0.1, 0.2, 0.3, 0.4, 0.5],\n                attributes={\"\
      title\": \"Doc 1\", \"category\": \"test\"},\n            ),\n            VectorStoreDocument(\n\
      \                id=\"2\",\n                text=\"This is document 2\",\n \
      \               vector=[0.2, 0.3, 0.4, 0.5, 0.6],\n                attributes={\"\
      title\": \"Doc 2\", \"category\": \"test\"},\n            ),\n            VectorStoreDocument(\n\
      \                id=\"3\",\n                text=\"This is document 3\",\n \
      \               vector=[0.3, 0.4, 0.5, 0.6, 0.7],\n                attributes={\"\
      title\": \"Doc 3\", \"category\": \"test\"},\n            ),\n        ]"
    signature: def sample_documents(self)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents
    called_by: []
  - name: sample_documents_categories
    start_line: 45
    end_line: 66
    code: "def sample_documents_categories(self):\n        \"\"\"Create sample documents\
      \ with different categories for testing.\"\"\"\n        return [\n         \
      \   VectorStoreDocument(\n                id=\"1\",\n                text=\"\
      Document about cats\",\n                vector=[0.1, 0.2, 0.3, 0.4, 0.5],\n\
      \                attributes={\"category\": \"animals\"},\n            ),\n \
      \           VectorStoreDocument(\n                id=\"2\",\n              \
      \  text=\"Document about dogs\",\n                vector=[0.2, 0.3, 0.4, 0.5,\
      \ 0.6],\n                attributes={\"category\": \"animals\"},\n         \
      \   ),\n            VectorStoreDocument(\n                id=\"3\",\n      \
      \          text=\"Document about cars\",\n                vector=[0.3, 0.4,\
      \ 0.5, 0.6, 0.7],\n                attributes={\"category\": \"vehicles\"},\n\
      \            ),\n        ]"
    signature: def sample_documents_categories(self)
    decorators:
    - '@pytest.fixture'
    raises: []
    calls:
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents_categories
    called_by: []
  - name: test_vector_store_operations
    start_line: 68
    end_line: 125
    code: "def test_vector_store_operations(self, sample_documents):\n        \"\"\
      \"Test basic vector store operations with LanceDB.\"\"\"\n        # Create a\
      \ temporary directory for the test database\n        temp_dir = tempfile.mkdtemp()\n\
      \        try:\n            vector_store = LanceDBVectorStore(\n            \
      \    vector_store_schema_config=VectorStoreSchemaConfig(\n                 \
      \   index_name=\"test_collection\", vector_size=5\n                )\n     \
      \       )\n            vector_store.connect(db_uri=temp_dir)\n            vector_store.load_documents(sample_documents[:2])\n\
      \n            if vector_store.index_name:\n                assert (\n      \
      \              vector_store.index_name in vector_store.db_connection.table_names()\n\
      \                )\n\n            doc = vector_store.search_by_id(\"1\")\n \
      \           assert doc.id == \"1\"\n            assert doc.text == \"This is\
      \ document 1\"\n\n            assert doc.vector is not None\n            assert\
      \ np.allclose(doc.vector, [0.1, 0.2, 0.3, 0.4, 0.5])\n            assert doc.attributes[\"\
      title\"] == \"Doc 1\"\n\n            filter_query = vector_store.filter_by_id([\"\
      1\"])\n            assert filter_query == \"id in ('1')\"\n\n            results\
      \ = vector_store.similarity_search_by_vector(\n                [0.1, 0.2, 0.3,\
      \ 0.4, 0.5], k=2\n            )\n            assert 1 <= len(results) <= 2\n\
      \            assert isinstance(results[0].score, float)\n\n            # Test\
      \ append mode\n            vector_store.load_documents([sample_documents[2]],\
      \ overwrite=False)\n            result = vector_store.search_by_id(\"3\")\n\
      \            assert result.id == \"3\"\n            assert result.text == \"\
      This is document 3\"\n\n            # Define a simple text embedder function\
      \ for testing\n            def mock_embedder(text: str) -> list[float]:\n  \
      \              return [0.1, 0.2, 0.3, 0.4, 0.5]\n\n            text_results\
      \ = vector_store.similarity_search_by_text(\n                \"test query\"\
      , mock_embedder, k=2\n            )\n            assert 1 <= len(text_results)\
      \ <= 2\n            assert isinstance(text_results[0].score, float)\n\n    \
      \        # Test non-existent document\n            non_existent = vector_store.search_by_id(\"\
      nonexistent\")\n            assert non_existent.id == \"nonexistent\"\n    \
      \        assert non_existent.text is None\n            assert non_existent.vector\
      \ is None\n        finally:\n            shutil.rmtree(temp_dir)"
    signature: def test_vector_store_operations(self, sample_documents)
    decorators: []
    raises: []
    calls:
    - target: tempfile::mkdtemp
      type: stdlib
    - target: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.db_connection.table_names
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: numpy::allclose
      type: external
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: isinstance
      type: builtin
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: shutil::rmtree
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_operations
    called_by: []
  - name: mock_embedder
    start_line: 110
    end_line: 111
    code: "def mock_embedder(text: str) -> list[float]:\n                return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.mock_embedder
    called_by: []
  - name: test_empty_collection
    start_line: 127
    end_line: 171
    code: "def test_empty_collection(self):\n        \"\"\"Test creating an empty\
      \ collection.\"\"\"\n        # Create a temporary directory for the test database\n\
      \        temp_dir = tempfile.mkdtemp()\n        try:\n            vector_store\
      \ = LanceDBVectorStore(\n                vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \                    index_name=\"empty_collection\", vector_size=5\n      \
      \          )\n            )\n            vector_store.connect(db_uri=temp_dir)\n\
      \n            # Load the vector store with a document, then delete it\n    \
      \        sample_doc = VectorStoreDocument(\n                id=\"tmp\",\n  \
      \              text=\"Temporary document to create schema\",\n             \
      \   vector=[0.1, 0.2, 0.3, 0.4, 0.5],\n                attributes={\"title\"\
      : \"Tmp\"},\n            )\n            vector_store.load_documents([sample_doc])\n\
      \            vector_store.db_connection.open_table(\n                vector_store.index_name\
      \ if vector_store.index_name else \"\"\n            ).delete(\"id = 'tmp'\"\
      )\n\n            # Should still have the collection\n            if vector_store.index_name:\n\
      \                assert (\n                    vector_store.index_name in vector_store.db_connection.table_names()\n\
      \                )\n\n            # Add a document after creating an empty collection\n\
      \            doc = VectorStoreDocument(\n                id=\"1\",\n       \
      \         text=\"This is document 1\",\n                vector=[0.1, 0.2, 0.3,\
      \ 0.4, 0.5],\n                attributes={\"title\": \"Doc 1\"},\n         \
      \   )\n            vector_store.load_documents([doc], overwrite=False)\n\n \
      \           result = vector_store.search_by_id(\"1\")\n            assert result.id\
      \ == \"1\"\n            assert result.text == \"This is document 1\"\n     \
      \   finally:\n            # Clean up - remove the temporary directory\n    \
      \        shutil.rmtree(temp_dir)"
    signature: def test_empty_collection(self)
    decorators: []
    raises: []
    calls:
    - target: tempfile::mkdtemp
      type: stdlib
    - target: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: vector_store.load_documents
      type: unresolved
    - target: "vector_store.db_connection.open_table(\n                vector_store.index_name\
        \ if vector_store.index_name else \"\"\n            ).delete"
      type: unresolved
    - target: vector_store.db_connection.open_table
      type: unresolved
    - target: vector_store.db_connection.table_names
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: shutil::rmtree
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_empty_collection
    called_by: []
  - name: test_filter_search
    start_line: 173
    end_line: 200
    code: "def test_filter_search(self, sample_documents_categories):\n        \"\"\
      \"Test filtered search with LanceDB.\"\"\"\n        # Create a temporary directory\
      \ for the test database\n        temp_dir = tempfile.mkdtemp()\n        try:\n\
      \            vector_store = LanceDBVectorStore(\n                vector_store_schema_config=VectorStoreSchemaConfig(\n\
      \                    index_name=\"filter_collection\", vector_size=5\n     \
      \           )\n            )\n\n            vector_store.connect(db_uri=temp_dir)\n\
      \n            vector_store.load_documents(sample_documents_categories)\n\n \
      \           # Filter to include only documents about animals\n            vector_store.filter_by_id([\"\
      1\", \"2\"])\n            results = vector_store.similarity_search_by_vector(\n\
      \                [0.1, 0.2, 0.3, 0.4, 0.5], k=3\n            )\n\n         \
      \   # Should return at most 2 documents (the filtered ones)\n            assert\
      \ len(results) <= 2\n            ids = [result.document.id for result in results]\n\
      \            assert \"3\" not in ids\n            assert set(ids).issubset({\"\
      1\", \"2\"})\n        finally:\n            shutil.rmtree(temp_dir)"
    signature: def test_filter_search(self, sample_documents_categories)
    decorators: []
    raises: []
    calls:
    - target: tempfile::mkdtemp
      type: stdlib
    - target: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: set(ids).issubset
      type: unresolved
    - target: set
      type: builtin
    - target: shutil::rmtree
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_filter_search
    called_by: []
  - name: test_vector_store_customization
    start_line: 202
    end_line: 264
    code: "def test_vector_store_customization(self, sample_documents):\n        \"\
      \"\"Test vector store customization with LanceDB.\"\"\"\n        # Create a\
      \ temporary directory for the test database\n        temp_dir = tempfile.mkdtemp()\n\
      \        try:\n            vector_store = LanceDBVectorStore(\n            \
      \    vector_store_schema_config=VectorStoreSchemaConfig(\n                 \
      \   index_name=\"text-embeddings\",\n                    id_field=\"id_custom\"\
      ,\n                    text_field=\"text_custom\",\n                    vector_field=\"\
      vector_custom\",\n                    attributes_field=\"attributes_custom\"\
      ,\n                    vector_size=5,\n                ),\n            )\n \
      \           vector_store.connect(db_uri=temp_dir)\n            vector_store.load_documents(sample_documents[:2])\n\
      \n            if vector_store.index_name:\n                assert (\n      \
      \              vector_store.index_name in vector_store.db_connection.table_names()\n\
      \                )\n\n            doc = vector_store.search_by_id(\"1\")\n \
      \           assert doc.id == \"1\"\n            assert doc.text == \"This is\
      \ document 1\"\n\n            assert doc.vector is not None\n            assert\
      \ np.allclose(doc.vector, [0.1, 0.2, 0.3, 0.4, 0.5])\n            assert doc.attributes[\"\
      title\"] == \"Doc 1\"\n\n            filter_query = vector_store.filter_by_id([\"\
      1\"])\n            assert filter_query == f\"{vector_store.id_field} in ('1')\"\
      \n\n            results = vector_store.similarity_search_by_vector(\n      \
      \          [0.1, 0.2, 0.3, 0.4, 0.5], k=2\n            )\n            assert\
      \ 1 <= len(results) <= 2\n            assert isinstance(results[0].score, float)\n\
      \n            # Test append mode\n            vector_store.load_documents([sample_documents[2]],\
      \ overwrite=False)\n            result = vector_store.search_by_id(\"3\")\n\
      \            assert result.id == \"3\"\n            assert result.text == \"\
      This is document 3\"\n\n            # Define a simple text embedder function\
      \ for testing\n            def mock_embedder(text: str) -> list[float]:\n  \
      \              return [0.1, 0.2, 0.3, 0.4, 0.5]\n\n            text_results\
      \ = vector_store.similarity_search_by_text(\n                \"test query\"\
      , mock_embedder, k=2\n            )\n            assert 1 <= len(text_results)\
      \ <= 2\n            assert isinstance(text_results[0].score, float)\n\n    \
      \        # Test non-existent document\n            non_existent = vector_store.search_by_id(\"\
      nonexistent\")\n            assert non_existent.id == \"nonexistent\"\n    \
      \        assert non_existent.text is None\n            assert non_existent.vector\
      \ is None\n        finally:\n            shutil.rmtree(temp_dir)"
    signature: def test_vector_store_customization(self, sample_documents)
    decorators: []
    raises: []
    calls:
    - target: tempfile::mkdtemp
      type: stdlib
    - target: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
      type: internal
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    - target: vector_store.connect
      type: unresolved
    - target: vector_store.load_documents
      type: unresolved
    - target: vector_store.db_connection.table_names
      type: unresolved
    - target: vector_store.search_by_id
      type: unresolved
    - target: numpy::allclose
      type: external
    - target: vector_store.filter_by_id
      type: unresolved
    - target: vector_store.similarity_search_by_vector
      type: unresolved
    - target: len
      type: builtin
    - target: isinstance
      type: builtin
    - target: vector_store.similarity_search_by_text
      type: unresolved
    - target: shutil::rmtree
      type: stdlib
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_customization
    called_by: []
  - name: mock_embedder
    start_line: 249
    end_line: 250
    code: "def mock_embedder(text: str) -> list[float]:\n                return [0.1,\
      \ 0.2, 0.3, 0.4, 0.5]"
    signature: 'def mock_embedder(text: str) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.mock_embedder
    called_by: []
- file_name: tests/mock_provider.py
  imports:
  - module: collections.abc
    name: AsyncGenerator
    alias: null
  - module: collections.abc
    name: Generator
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelOutput
    alias: null
  - module: graphrag.language_model.response.base
    name: BaseModelResponse
    alias: null
  - module: graphrag.language_model.response.base
    name: ModelResponse
    alias: null
  functions:
  - name: __init__
    start_line: 23
    end_line: 34
    code: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
      \ | None = None,\n        config: LanguageModelConfig | None = None,\n     \
      \   json: bool = False,\n        **kwargs: Any,\n    ):\n        self.responses\
      \ = config.responses if config and config.responses else responses\n       \
      \ self.response_index = 0\n        self.config = config or LanguageModelConfig(\n\
      \            type=ModelType.MockChat, model=\"gpt-4o\", api_key=\"mock\"\n \
      \       )"
    signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
      \ | None = None,\n        config: LanguageModelConfig | None = None,\n     \
      \   json: bool = False,\n        **kwargs: Any,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    visibility: protected
    node_id: tests/mock_provider.py::MockChatLLM.__init__
    called_by: []
  - name: achat
    start_line: 36
    end_line: 43
    code: "async def achat(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse:\n        \"\
      \"\"Return the next response in the list.\"\"\"\n        return self.chat(prompt,\
      \ history, **kwargs)"
    signature: "def achat(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: tests/mock_provider.py::chat
      type: internal
    visibility: public
    node_id: tests/mock_provider.py::MockChatLLM.achat
    called_by: []
  - name: achat_stream
    start_line: 45
    end_line: 62
    code: "async def achat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]:\n\
      \        \"\"\"Return the next response in the list.\"\"\"\n        if not self.responses:\n\
      \            return\n\n        for response in self.responses:\n           \
      \ response = (\n                response.model_dump_json()\n               \
      \ if isinstance(response, BaseModel)\n                else response\n      \
      \      )\n\n            yield response"
    signature: "def achat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]"
    decorators: []
    raises: []
    calls:
    - target: response.model_dump_json
      type: unresolved
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/mock_provider.py::MockChatLLM.achat_stream
    called_by: []
  - name: chat
    start_line: 64
    end_line: 85
    code: "def chat(\n        self,\n        prompt: str,\n        history: list |\
      \ None = None,\n        **kwargs,\n    ) -> ModelResponse:\n        \"\"\"Return\
      \ the next response in the list.\"\"\"\n        if not self.responses:\n   \
      \         return BaseModelResponse(output=BaseModelOutput(content=\"\"))\n\n\
      \        response = self.responses[self.response_index % len(self.responses)]\n\
      \        self.response_index += 1\n\n        parsed_json = response if isinstance(response,\
      \ BaseModel) else None\n        response = (\n            response.model_dump_json()\
      \ if isinstance(response, BaseModel) else response\n        )\n\n        return\
      \ BaseModelResponse(\n            output=BaseModelOutput(content=response),\n\
      \            parsed_response=parsed_json,\n        )"
    signature: "def chat(\n        self,\n        prompt: str,\n        history: list\
      \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    decorators: []
    raises: []
    calls:
    - target: graphrag/language_model/response/base.py::BaseModelResponse
      type: internal
    - target: graphrag/language_model/response/base.py::BaseModelOutput
      type: internal
    - target: len
      type: builtin
    - target: isinstance
      type: builtin
    - target: response.model_dump_json
      type: unresolved
    visibility: public
    node_id: tests/mock_provider.py::MockChatLLM.chat
    called_by: []
  - name: chat_stream
    start_line: 87
    end_line: 94
    code: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]:\n \
      \       \"\"\"Return the next response in the list.\"\"\"\n        raise NotImplementedError"
    signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: tests/mock_provider.py::MockChatLLM.chat_stream
    called_by: []
  - name: __init__
    start_line: 100
    end_line: 103
    code: "def __init__(self, **kwargs: Any):\n        self.config = LanguageModelConfig(\n\
      \            type=ModelType.MockEmbedding, model=\"text-embedding-ada-002\"\
      , api_key=\"mock\"\n        )"
    signature: 'def __init__(self, **kwargs: Any)'
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/language_model_config.py::LanguageModelConfig
      type: internal
    visibility: protected
    node_id: tests/mock_provider.py::MockEmbeddingLLM.__init__
    called_by: []
  - name: embed_batch
    start_line: 105
    end_line: 109
    code: "def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]:\n\
      \        \"\"\"Generate an embedding for the input text.\"\"\"\n        if isinstance(text_list,\
      \ str):\n            return [[1.0, 1.0, 1.0]]\n        return [[1.0, 1.0, 1.0]\
      \ for _ in text_list]"
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/mock_provider.py::MockEmbeddingLLM.embed_batch
    called_by: []
  - name: embed
    start_line: 111
    end_line: 113
    code: "def embed(self, text: str, **kwargs: Any) -> list[float]:\n        \"\"\
      \"Generate an embedding for the input text.\"\"\"\n        return [1.0, 1.0,\
      \ 1.0]"
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/mock_provider.py::MockEmbeddingLLM.embed
    called_by: []
  - name: aembed
    start_line: 115
    end_line: 117
    code: "async def aembed(self, text: str, **kwargs: Any) -> list[float]:\n    \
      \    \"\"\"Generate an embedding for the input text.\"\"\"\n        return [1.0,\
      \ 1.0, 1.0]"
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed
    called_by: []
  - name: aembed_batch
    start_line: 119
    end_line: 125
    code: "async def aembed_batch(\n        self, text_list: list[str], **kwargs:\
      \ Any\n    ) -> list[list[float]]:\n        \"\"\"Generate an embedding for\
      \ the input text.\"\"\"\n        if isinstance(text_list, str):\n          \
      \  return [[1.0, 1.0, 1.0]]\n        return [[1.0, 1.0, 1.0] for _ in text_list]"
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    visibility: public
    node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed_batch
    called_by: []
- file_name: tests/notebook/__init__.py
  imports: []
  functions: []
- file_name: tests/notebook/test_notebooks.py
  imports:
  - module: subprocess
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: nbformat
    name: null
    alias: null
  - module: pytest
    name: null
    alias: null
  functions:
  - name: _notebook_run
    start_line: 19
    end_line: 43
    code: "def _notebook_run(filepath: Path):\n    \"\"\"Execute a notebook via nbconvert\
      \ and collect output.\n    :returns execution errors\n    \"\"\"\n    args =\
      \ [\n        \"jupyter\",\n        \"nbconvert\",\n        \"--to\",\n     \
      \   \"notebook\",\n        \"--execute\",\n        \"-y\",\n        \"--no-prompt\"\
      ,\n        \"--stdout\",\n        str(filepath.absolute().resolve()),\n    ]\n\
      \    notebook = subprocess.check_output(args)\n    nb = nbformat.reads(notebook,\
      \ nbformat.current_nbformat)\n\n    return [\n        output\n        for cell\
      \ in nb.cells\n        if \"outputs\" in cell\n        for output in cell[\"\
      outputs\"]\n        if output.output_type == \"error\"\n    ]"
    signature: 'def _notebook_run(filepath: Path)'
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    - target: filepath.absolute().resolve
      type: unresolved
    - target: filepath.absolute
      type: unresolved
    - target: subprocess::check_output
      type: stdlib
    - target: nbformat::reads
      type: external
    visibility: protected
    node_id: tests/notebook/test_notebooks.py::_notebook_run
    called_by:
    - source: tests/notebook/test_notebooks.py::test_notebook
      type: internal
  - name: test_notebook
    start_line: 47
    end_line: 48
    code: "def test_notebook(notebook_path: Path):\n    assert _notebook_run(notebook_path)\
      \ == []"
    signature: 'def test_notebook(notebook_path: Path)'
    decorators:
    - '@pytest.mark.parametrize("notebook_path", notebooks_list)'
    raises: []
    calls:
    - target: tests/notebook/test_notebooks.py::_notebook_run
      type: internal
    visibility: public
    node_id: tests/notebook/test_notebooks.py::test_notebook
    called_by: []
- file_name: tests/smoke/__init__.py
  imports: []
  functions: []
- file_name: tests/smoke/test_fixtures.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: shutil
    name: null
    alias: null
  - module: subprocess
    name: null
    alias: null
  - module: collections.abc
    name: Callable
    alias: null
  - module: functools
    name: wraps
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: ClassVar
    alias: null
  - module: unittest
    name: mock
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: pytest
    name: null
    alias: null
  - module: graphrag.query.context_builder.community_context
    name: NO_COMMUNITY_RECORDS_WARNING
    alias: null
  - module: graphrag.storage.blob_pipeline_storage
    name: BlobPipelineStorage
    alias: null
  functions:
  - name: _load_fixtures
    start_line: 34
    end_line: 48
    code: "def _load_fixtures():\n    \"\"\"Load all fixtures from the tests/data\
      \ folder.\"\"\"\n    params = []\n    fixtures_path = Path(\"./tests/fixtures/\"\
      )\n    # use the min-csv smoke test to hydrate the docsite parquet artifacts\
      \ (see gh-pages.yml)\n    subfolders = [\"min-csv\"] if gh_pages else sorted(os.listdir(fixtures_path))\n\
      \n    for subfolder in subfolders:\n        if not os.path.isdir(fixtures_path\
      \ / subfolder):\n            continue\n\n        config_file = fixtures_path\
      \ / subfolder / \"config.json\"\n        params.append((subfolder, json.loads(config_file.read_bytes().decode(\"\
      utf-8\"))))\n\n    return params[1:]  # disable azure blob connection test"
    signature: def _load_fixtures()
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: sorted
      type: builtin
    - target: os::listdir
      type: stdlib
    - target: os::path.isdir
      type: stdlib
    - target: params.append
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: config_file.read_bytes().decode
      type: unresolved
    - target: config_file.read_bytes
      type: unresolved
    visibility: protected
    node_id: tests/smoke/test_fixtures.py::_load_fixtures
    called_by: []
  - name: pytest_generate_tests
    start_line: 51
    end_line: 68
    code: "def pytest_generate_tests(metafunc):\n    \"\"\"Generate tests for all\
      \ test functions in this module.\"\"\"\n    run_slow = metafunc.config.getoption(\"\
      run_slow\")\n    configs = metafunc.cls.params[metafunc.function.__name__]\n\
      \n    if not run_slow:\n        # Only run tests that are not marked as slow\n\
      \        configs = [config for config in configs if not config[1].get(\"slow\"\
      , False)]\n\n    funcarglist = [params[1] for params in configs]\n    id_list\
      \ = [params[0] for params in configs]\n\n    argnames = sorted(arg for arg in\
      \ funcarglist[0] if arg != \"slow\")\n    metafunc.parametrize(\n        argnames,\n\
      \        [[funcargs[name] for name in argnames] for funcargs in funcarglist],\n\
      \        ids=id_list,\n    )"
    signature: def pytest_generate_tests(metafunc)
    decorators: []
    raises: []
    calls:
    - target: metafunc.config.getoption
      type: unresolved
    - target: config[1].get
      type: unresolved
    - target: sorted
      type: builtin
    - target: metafunc.parametrize
      type: unresolved
    visibility: public
    node_id: tests/smoke/test_fixtures.py::pytest_generate_tests
    called_by: []
  - name: cleanup
    start_line: 71
    end_line: 89
    code: "def cleanup(skip: bool = False):\n    \"\"\"Decorator to cleanup the output\
      \ and cache folders after each test.\"\"\"\n\n    def decorator(func):\n   \
      \     @wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n\
      \                return func(*args, **kwargs)\n            except AssertionError:\n\
      \                raise\n            finally:\n                if not skip:\n\
      \                    root = Path(kwargs[\"input_path\"])\n                 \
      \   shutil.rmtree(root / \"output\", ignore_errors=True)\n                 \
      \   shutil.rmtree(root / \"cache\", ignore_errors=True)\n\n        return wrapper\n\
      \n    return decorator"
    signature: 'def cleanup(skip: bool = False)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/smoke/test_fixtures.py::cleanup
    called_by: []
  - name: decorator
    start_line: 74
    end_line: 87
    code: "def decorator(func):\n        @wraps(func)\n        def wrapper(*args,\
      \ **kwargs):\n            try:\n                return func(*args, **kwargs)\n\
      \            except AssertionError:\n                raise\n            finally:\n\
      \                if not skip:\n                    root = Path(kwargs[\"input_path\"\
      ])\n                    shutil.rmtree(root / \"output\", ignore_errors=True)\n\
      \                    shutil.rmtree(root / \"cache\", ignore_errors=True)\n\n\
      \        return wrapper"
    signature: def decorator(func)
    decorators: []
    raises: []
    calls:
    - target: functools::wraps
      type: stdlib
    visibility: public
    node_id: tests/smoke/test_fixtures.py::decorator
    called_by: []
  - name: wrapper
    start_line: 76
    end_line: 85
    code: "def wrapper(*args, **kwargs):\n            try:\n                return\
      \ func(*args, **kwargs)\n            except AssertionError:\n              \
      \  raise\n            finally:\n                if not skip:\n             \
      \       root = Path(kwargs[\"input_path\"])\n                    shutil.rmtree(root\
      \ / \"output\", ignore_errors=True)\n                    shutil.rmtree(root\
      \ / \"cache\", ignore_errors=True)"
    signature: def wrapper(*args, **kwargs)
    decorators:
    - '@wraps(func)'
    raises: []
    calls:
    - target: func
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: shutil::rmtree
      type: stdlib
    visibility: public
    node_id: tests/smoke/test_fixtures.py::wrapper
    called_by: []
  - name: prepare_azurite_data
    start_line: 92
    end_line: 119
    code: "async def prepare_azurite_data(input_path: str, azure: dict) -> Callable[[],\
      \ None]:\n    \"\"\"Prepare the data for the Azurite tests.\"\"\"\n    input_container\
      \ = azure[\"input_container\"]\n    input_base_dir = azure.get(\"input_base_dir\"\
      )\n\n    root = Path(input_path)\n    input_storage = BlobPipelineStorage(\n\
      \        connection_string=WELL_KNOWN_AZURITE_CONNECTION_STRING,\n        container_name=input_container,\n\
      \    )\n    # Bounce the container if it exists to clear out old run data\n\
      \    input_storage._delete_container()  # noqa: SLF001\n    input_storage._create_container()\
      \  # noqa: SLF001\n\n    # Upload data files\n    txt_files = list((root / \"\
      input\").glob(\"*.txt\"))\n    csv_files = list((root / \"input\").glob(\"*.csv\"\
      ))\n    data_files = txt_files + csv_files\n    for data_file in data_files:\n\
      \        text = data_file.read_bytes().decode(\"utf-8\")\n        file_path\
      \ = (\n            str(Path(input_base_dir) / data_file.name)\n            if\
      \ input_base_dir\n            else data_file.name\n        )\n        await\
      \ input_storage.set(file_path, text, encoding=\"utf-8\")\n\n    return lambda:\
      \ input_storage._delete_container()  # noqa: SLF001"
    signature: 'def prepare_azurite_data(input_path: str, azure: dict) -> Callable[[],
      None]'
    decorators: []
    raises: []
    calls:
    - target: azure.get
      type: unresolved
    - target: pathlib::Path
      type: stdlib
    - target: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
      type: internal
    - target: input_storage._delete_container
      type: unresolved
    - target: input_storage._create_container
      type: unresolved
    - target: list
      type: builtin
    - target: (root / "input").glob
      type: unresolved
    - target: data_file.read_bytes().decode
      type: unresolved
    - target: data_file.read_bytes
      type: unresolved
    - target: str
      type: builtin
    - target: input_storage.set
      type: unresolved
    visibility: public
    node_id: tests/smoke/test_fixtures.py::prepare_azurite_data
    called_by:
    - source: tests/smoke/test_fixtures.py::TestIndexer.test_fixture
      type: internal
  - name: __run_indexer
    start_line: 127
    end_line: 148
    code: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
      \ str,\n    ):\n        command = [\n            \"uv\",\n            \"run\"\
      ,\n            \"poe\",\n            \"index\",\n            \"--verbose\" if\
      \ debug else None,\n            \"--root\",\n            root.resolve().as_posix(),\n\
      \            \"--method\",\n            \"standard\",\n        ]\n        command\
      \ = [arg for arg in command if arg]\n        logger.info(\"running command \"\
      , \" \".join(command))\n        completion = subprocess.run(command, env=os.environ)\n\
      \        assert completion.returncode == 0, (\n            f\"Indexer failed\
      \ with return code: {completion.returncode}\"\n        )"
    signature: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
      \ str,\n    )"
    decorators: []
    raises: []
    calls:
    - target: root.resolve().as_posix
      type: unresolved
    - target: root.resolve
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: '" ".join'
      type: unresolved
    - target: subprocess::run
      type: stdlib
    visibility: private
    node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_indexer
    called_by: []
  - name: __assert_indexer_outputs
    start_line: 150
    end_line: 199
    code: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
      \ dict[str, dict[str, Any]]\n    ):\n        output_path = root / \"output\"\
      \n\n        assert output_path.exists(), \"output folder does not exist\"\n\n\
      \        # Check stats for all workflow\n        stats = json.loads((output_path\
      \ / \"stats.json\").read_bytes().decode(\"utf-8\"))\n\n        # Check all workflows\
      \ run\n        expected_workflows = set(workflow_config.keys())\n        workflows\
      \ = set(stats[\"workflows\"].keys())\n        assert workflows == expected_workflows,\
      \ (\n            f\"Workflows missing from stats.json: {expected_workflows -\
      \ workflows}. Unexpected workflows in stats.json: {workflows - expected_workflows}\"\
      \n        )\n\n        # [OPTIONAL] Check runtime\n        for workflow, config\
      \ in workflow_config.items():\n            # Check expected artifacts\n    \
      \        workflow_artifacts = config.get(\"expected_artifacts\", [])\n     \
      \       # Check max runtime\n            max_runtime = config.get(\"max_runtime\"\
      , None)\n            if max_runtime:\n                assert stats[\"workflows\"\
      ][workflow][\"overall\"] <= max_runtime, (\n                    f\"Expected\
      \ max runtime of {max_runtime}, found: {stats['workflows'][workflow]['overall']}\
      \ for workflow: {workflow}\"\n                )\n            # Check expected\
      \ artifacts\n            for artifact in workflow_artifacts:\n             \
      \   if artifact.endswith(\".parquet\"):\n                    output_df = pd.read_parquet(output_path\
      \ / artifact)\n\n                    # Check number of rows between range\n\
      \                    assert (\n                        config[\"row_range\"\
      ][0]\n                        <= len(output_df)\n                        <=\
      \ config[\"row_range\"][1]\n                    ), (\n                     \
      \   f\"Expected between {config['row_range'][0]} and {config['row_range'][1]},\
      \ found: {len(output_df)} for file: {artifact}\"\n                    )\n\n\
      \                    # Get non-nan rows\n                    nan_df = output_df.loc[\n\
      \                        :,\n                        ~output_df.columns.isin(config.get(\"\
      nan_allowed_columns\", [])),\n                    ]\n                    nan_df\
      \ = nan_df[nan_df.isna().any(axis=1)]\n                    assert len(nan_df)\
      \ == 0, (\n                        f\"Found {len(nan_df)} rows with NaN values\
      \ for file: {artifact} on columns: {nan_df.columns[nan_df.isna().any()].tolist()}\"\
      \n                    )"
    signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
      \ dict[str, dict[str, Any]]\n    )"
    decorators: []
    raises: []
    calls:
    - target: output_path.exists
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: (output_path / "stats.json").read_bytes().decode
      type: unresolved
    - target: (output_path / "stats.json").read_bytes
      type: unresolved
    - target: set
      type: builtin
    - target: workflow_config.keys
      type: unresolved
    - target: stats["workflows"].keys
      type: unresolved
    - target: workflow_config.items
      type: unresolved
    - target: config.get
      type: unresolved
    - target: artifact.endswith
      type: unresolved
    - target: pandas::read_parquet
      type: external
    - target: len
      type: builtin
    - target: output_df.columns.isin
      type: unresolved
    - target: nan_df.isna().any
      type: unresolved
    - target: nan_df.isna
      type: unresolved
    - target: nan_df.columns[nan_df.isna().any()].tolist
      type: unresolved
    visibility: private
    node_id: tests/smoke/test_fixtures.py::TestIndexer.__assert_indexer_outputs
    called_by: []
  - name: __run_query
    start_line: 201
    end_line: 218
    code: "def __run_query(self, root: Path, query_config: dict[str, str]):\n    \
      \    command = [\n            \"uv\",\n            \"run\",\n            \"\
      poe\",\n            \"query\",\n            \"--root\",\n            root.resolve().as_posix(),\n\
      \            \"--method\",\n            query_config[\"method\"],\n        \
      \    \"--community-level\",\n            str(query_config.get(\"community_level\"\
      , 2)),\n            \"--query\",\n            query_config[\"query\"],\n   \
      \     ]\n\n        logger.info(\"running command \", \" \".join(command))\n\
      \        return subprocess.run(command, capture_output=True, text=True)"
    signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
    decorators: []
    raises: []
    calls:
    - target: root.resolve().as_posix
      type: unresolved
    - target: root.resolve
      type: unresolved
    - target: str
      type: builtin
    - target: query_config.get
      type: unresolved
    - target: logger.info
      type: unresolved
    - target: '" ".join'
      type: unresolved
    - target: subprocess::run
      type: stdlib
    visibility: private
    node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_query
    called_by: []
  - name: test_fixture
    start_line: 233
    end_line: 268
    code: "def test_fixture(\n        self,\n        input_path: str,\n        input_file_type:\
      \ str,\n        workflow_config: dict[str, dict[str, Any]],\n        query_config:\
      \ list[dict[str, str]],\n    ):\n        if workflow_config.get(\"skip\"):\n\
      \            print(f\"skipping smoke test {input_path})\")\n            return\n\
      \n        azure = workflow_config.get(\"azure\")\n        root = Path(input_path)\n\
      \        dispose = None\n        if azure is not None:\n            dispose\
      \ = asyncio.run(prepare_azurite_data(input_path, azure))\n\n        print(\"\
      running indexer\")\n        self.__run_indexer(root, input_file_type)\n    \
      \    print(\"indexer complete\")\n\n        if dispose is not None:\n      \
      \      dispose()\n\n        if not workflow_config.get(\"skip_assert\"):\n \
      \           print(\"performing dataset assertions\")\n            self.__assert_indexer_outputs(root,\
      \ workflow_config)\n\n        print(\"running queries\")\n        for query\
      \ in query_config:\n            result = self.__run_query(root, query)\n   \
      \         print(f\"Query: {query}\\nResponse: {result.stdout}\")\n\n       \
      \     assert result.returncode == 0, \"Query failed\"\n            assert result.stdout\
      \ is not None, \"Query returned no output\"\n            assert len(result.stdout)\
      \ > 0, \"Query returned empty output\""
    signature: "def test_fixture(\n        self,\n        input_path: str,\n     \
      \   input_file_type: str,\n        workflow_config: dict[str, dict[str, Any]],\n\
      \        query_config: list[dict[str, str]],\n    )"
    decorators:
    - '@cleanup(skip=debug)'
    - "@mock.patch.dict(\n        os.environ,\n        {\n            **os.environ,\n\
      \            \"BLOB_STORAGE_CONNECTION_STRING\": WELL_KNOWN_AZURITE_CONNECTION_STRING,\n\
      \            \"LOCAL_BLOB_STORAGE_CONNECTION_STRING\": WELL_KNOWN_AZURITE_CONNECTION_STRING,\n\
      \            \"AZURE_AI_SEARCH_URL_ENDPOINT\": os.getenv(\"AZURE_AI_SEARCH_URL_ENDPOINT\"\
      ),\n            \"AZURE_AI_SEARCH_API_KEY\": os.getenv(\"AZURE_AI_SEARCH_API_KEY\"\
      ),\n        },\n        clear=True,\n    )"
    - '@pytest.mark.timeout(2000)'
    raises: []
    calls:
    - target: workflow_config.get
      type: unresolved
    - target: print
      type: builtin
    - target: pathlib::Path
      type: stdlib
    - target: asyncio::run
      type: stdlib
    - target: tests/smoke/test_fixtures.py::prepare_azurite_data
      type: internal
    - target: tests/smoke/test_fixtures.py::__run_indexer
      type: internal
    - target: dispose
      type: unresolved
    - target: tests/smoke/test_fixtures.py::__assert_indexer_outputs
      type: internal
    - target: tests/smoke/test_fixtures.py::__run_query
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/smoke/test_fixtures.py::TestIndexer.test_fixture
    called_by: []
- file_name: tests/unit/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/config/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/config/test_config.py
  imports:
  - module: os
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: unittest
    name: mock
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: pydantic
    name: ValidationError
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.enums
    name: AuthType
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.config.load_config
    name: load_config
    alias: null
  - module: tests.unit.config.utils
    name: DEFAULT_EMBEDDING_MODEL_CONFIG
    alias: null
  - module: tests.unit.config.utils
    name: DEFAULT_MODEL_CONFIG
    alias: null
  - module: tests.unit.config.utils
    name: FAKE_API_KEY
    alias: null
  - module: tests.unit.config.utils
    name: assert_graphrag_configs
    alias: null
  - module: tests.unit.config.utils
    name: get_default_graphrag_config
    alias: null
  functions:
  - name: test_missing_openai_required_api_key
    start_line: 24
    end_line: 42
    code: "def test_missing_openai_required_api_key() -> None:\n    model_config_missing_api_key\
      \ = {\n        defs.DEFAULT_CHAT_MODEL_ID: {\n            \"type\": ModelType.OpenAIChat,\n\
      \            \"model\": defs.DEFAULT_CHAT_MODEL,\n        },\n        defs.DEFAULT_EMBEDDING_MODEL_ID:\
      \ DEFAULT_EMBEDDING_MODEL_CONFIG,\n    }\n\n    # API Key required for OpenAIChat\n\
      \    with pytest.raises(ValidationError):\n        create_graphrag_config({\"\
      models\": model_config_missing_api_key})\n\n    # API Key required for OpenAIEmbedding\n\
      \    model_config_missing_api_key[defs.DEFAULT_CHAT_MODEL_ID][\"type\"] = (\n\
      \        ModelType.OpenAIEmbedding\n    )\n    with pytest.raises(ValidationError):\n\
      \        create_graphrag_config({\"models\": model_config_missing_api_key})"
    signature: def test_missing_openai_required_api_key() -> None
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_missing_openai_required_api_key
    called_by: []
  - name: test_missing_azure_api_key
    start_line: 45
    end_line: 65
    code: "def test_missing_azure_api_key() -> None:\n    model_config_missing_api_key\
      \ = {\n        defs.DEFAULT_CHAT_MODEL_ID: {\n            \"type\": ModelType.AzureOpenAIChat,\n\
      \            \"auth_type\": AuthType.APIKey,\n            \"model\": defs.DEFAULT_CHAT_MODEL,\n\
      \            \"api_base\": \"some_api_base\",\n            \"api_version\":\
      \ \"some_api_version\",\n            \"deployment_name\": \"some_deployment_name\"\
      ,\n        },\n        defs.DEFAULT_EMBEDDING_MODEL_ID: DEFAULT_EMBEDDING_MODEL_CONFIG,\n\
      \    }\n\n    with pytest.raises(ValidationError):\n        create_graphrag_config({\"\
      models\": model_config_missing_api_key})\n\n    # API Key not required for managed\
      \ identity\n    model_config_missing_api_key[defs.DEFAULT_CHAT_MODEL_ID][\"\
      auth_type\"] = (\n        AuthType.AzureManagedIdentity\n    )\n    create_graphrag_config({\"\
      models\": model_config_missing_api_key})"
    signature: def test_missing_azure_api_key() -> None
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_missing_azure_api_key
    called_by: []
  - name: test_conflicting_auth_type
    start_line: 68
    end_line: 79
    code: "def test_conflicting_auth_type() -> None:\n    model_config_invalid_auth_type\
      \ = {\n        defs.DEFAULT_CHAT_MODEL_ID: {\n            \"auth_type\": AuthType.AzureManagedIdentity,\n\
      \            \"type\": ModelType.OpenAIChat,\n            \"model\": defs.DEFAULT_CHAT_MODEL,\n\
      \        },\n        defs.DEFAULT_EMBEDDING_MODEL_ID: DEFAULT_EMBEDDING_MODEL_CONFIG,\n\
      \    }\n\n    with pytest.raises(ValidationError):\n        create_graphrag_config({\"\
      models\": model_config_invalid_auth_type})"
    signature: def test_conflicting_auth_type() -> None
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_conflicting_auth_type
    called_by: []
  - name: test_conflicting_azure_api_key
    start_line: 82
    end_line: 97
    code: "def test_conflicting_azure_api_key() -> None:\n    model_config_conflicting_api_key\
      \ = {\n        defs.DEFAULT_CHAT_MODEL_ID: {\n            \"type\": ModelType.AzureOpenAIChat,\n\
      \            \"auth_type\": AuthType.AzureManagedIdentity,\n            \"model\"\
      : defs.DEFAULT_CHAT_MODEL,\n            \"api_base\": \"some_api_base\",\n \
      \           \"api_version\": \"some_api_version\",\n            \"deployment_name\"\
      : \"some_deployment_name\",\n            \"api_key\": \"THIS_SHOULD_NOT_BE_SET_WHEN_USING_MANAGED_IDENTITY\"\
      ,\n        },\n        defs.DEFAULT_EMBEDDING_MODEL_ID: DEFAULT_EMBEDDING_MODEL_CONFIG,\n\
      \    }\n\n    with pytest.raises(ValidationError):\n        create_graphrag_config({\"\
      models\": model_config_conflicting_api_key})"
    signature: def test_conflicting_azure_api_key() -> None
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_conflicting_azure_api_key
    called_by: []
  - name: test_missing_azure_api_base
    start_line: 110
    end_line: 120
    code: "def test_missing_azure_api_base() -> None:\n    missing_api_base_config\
      \ = base_azure_model_config.copy()\n    del missing_api_base_config[\"api_base\"\
      ]\n\n    with pytest.raises(ValidationError):\n        create_graphrag_config({\n\
      \            \"models\": {\n                defs.DEFAULT_CHAT_MODEL_ID: missing_api_base_config,\n\
      \                defs.DEFAULT_EMBEDDING_MODEL_ID: DEFAULT_EMBEDDING_MODEL_CONFIG,\n\
      \            }\n        })"
    signature: def test_missing_azure_api_base() -> None
    decorators: []
    raises: []
    calls:
    - target: base_azure_model_config.copy
      type: unresolved
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_missing_azure_api_base
    called_by: []
  - name: test_missing_azure_api_version
    start_line: 123
    end_line: 133
    code: "def test_missing_azure_api_version() -> None:\n    missing_api_version_config\
      \ = base_azure_model_config.copy()\n    del missing_api_version_config[\"api_version\"\
      ]\n\n    with pytest.raises(ValidationError):\n        create_graphrag_config({\n\
      \            \"models\": {\n                defs.DEFAULT_CHAT_MODEL_ID: missing_api_version_config,\n\
      \                defs.DEFAULT_EMBEDDING_MODEL_ID: DEFAULT_EMBEDDING_MODEL_CONFIG,\n\
      \            }\n        })"
    signature: def test_missing_azure_api_version() -> None
    decorators: []
    raises: []
    calls:
    - target: base_azure_model_config.copy
      type: unresolved
    - target: pytest::raises
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_missing_azure_api_version
    called_by: []
  - name: test_default_config
    start_line: 136
    end_line: 139
    code: "def test_default_config() -> None:\n    expected = get_default_graphrag_config()\n\
      \    actual = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n \
      \   assert_graphrag_configs(actual, expected)"
    signature: def test_default_config() -> None
    decorators: []
    raises: []
    calls:
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_default_config
    called_by: []
  - name: test_load_minimal_config
    start_line: 143
    end_line: 148
    code: "def test_load_minimal_config() -> None:\n    cwd = Path(__file__).parent\n\
      \    root_dir = (cwd / \"fixtures\" / \"minimal_config\").resolve()\n    expected\
      \ = get_default_graphrag_config(str(root_dir))\n    actual = load_config(root_dir=root_dir)\n\
      \    assert_graphrag_configs(actual, expected)"
    signature: def test_load_minimal_config() -> None
    decorators:
    - '@mock.patch.dict(os.environ, {"CUSTOM_API_KEY": FAKE_API_KEY}, clear=True)'
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: (cwd / "fixtures" / "minimal_config").resolve
      type: unresolved
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_load_minimal_config
    called_by: []
  - name: test_load_config_with_cli_overrides
    start_line: 152
    end_line: 163
    code: "def test_load_config_with_cli_overrides() -> None:\n    cwd = Path(__file__).parent\n\
      \    root_dir = (cwd / \"fixtures\" / \"minimal_config\").resolve()\n    output_dir\
      \ = \"some_output_dir\"\n    expected_output_base_dir = root_dir / output_dir\n\
      \    expected = get_default_graphrag_config(str(root_dir))\n    expected.output.base_dir\
      \ = str(expected_output_base_dir)\n    actual = load_config(\n        root_dir=root_dir,\n\
      \        cli_overrides={\"output.base_dir\": output_dir},\n    )\n    assert_graphrag_configs(actual,\
      \ expected)"
    signature: def test_load_config_with_cli_overrides() -> None
    decorators:
    - '@mock.patch.dict(os.environ, {"CUSTOM_API_KEY": FAKE_API_KEY}, clear=True)'
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: (cwd / "fixtures" / "minimal_config").resolve
      type: unresolved
    - target: tests/unit/config/utils.py::get_default_graphrag_config
      type: internal
    - target: str
      type: builtin
    - target: graphrag/config/load_config.py::load_config
      type: internal
    - target: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
    called_by: []
  - name: test_load_config_missing_env_vars
    start_line: 166
    end_line: 170
    code: "def test_load_config_missing_env_vars() -> None:\n    cwd = Path(__file__).parent\n\
      \    root_dir = (cwd / \"fixtures\" / \"minimal_config_missing_env_var\").resolve()\n\
      \    with pytest.raises(KeyError):\n        load_config(root_dir=root_dir)"
    signature: def test_load_config_missing_env_vars() -> None
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: (cwd / "fixtures" / "minimal_config_missing_env_var").resolve
      type: unresolved
    - target: pytest::raises
      type: external
    - target: graphrag/config/load_config.py::load_config
      type: internal
    visibility: public
    node_id: tests/unit/config/test_config.py::test_load_config_missing_env_vars
    called_by: []
- file_name: tests/unit/config/utils.py
  imports:
  - module: dataclasses
    name: asdict
    alias: null
  - module: pydantic
    name: BaseModel
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.config.models.basic_search_config
    name: BasicSearchConfig
    alias: null
  - module: graphrag.config.models.cache_config
    name: CacheConfig
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.config.models.cluster_graph_config
    name: ClusterGraphConfig
    alias: null
  - module: graphrag.config.models.community_reports_config
    name: CommunityReportsConfig
    alias: null
  - module: graphrag.config.models.drift_search_config
    name: DRIFTSearchConfig
    alias: null
  - module: graphrag.config.models.embed_graph_config
    name: EmbedGraphConfig
    alias: null
  - module: graphrag.config.models.extract_claims_config
    name: ClaimExtractionConfig
    alias: null
  - module: graphrag.config.models.extract_graph_config
    name: ExtractGraphConfig
    alias: null
  - module: graphrag.config.models.extract_graph_nlp_config
    name: ExtractGraphNLPConfig
    alias: null
  - module: graphrag.config.models.extract_graph_nlp_config
    name: TextAnalyzerConfig
    alias: null
  - module: graphrag.config.models.global_search_config
    name: GlobalSearchConfig
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.config.models.language_model_config
    name: LanguageModelConfig
    alias: null
  - module: graphrag.config.models.local_search_config
    name: LocalSearchConfig
    alias: null
  - module: graphrag.config.models.prune_graph_config
    name: PruneGraphConfig
    alias: null
  - module: graphrag.config.models.reporting_config
    name: ReportingConfig
    alias: null
  - module: graphrag.config.models.snapshots_config
    name: SnapshotsConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.config.models.summarize_descriptions_config
    name: SummarizeDescriptionsConfig
    alias: null
  - module: graphrag.config.models.text_embedding_config
    name: TextEmbeddingConfig
    alias: null
  - module: graphrag.config.models.umap_config
    name: UmapConfig
    alias: null
  - module: graphrag.config.models.vector_store_config
    name: VectorStoreConfig
    alias: null
  functions:
  - name: get_default_graphrag_config
    start_line: 60
    end_line: 65
    code: "def get_default_graphrag_config(root_dir: str | None = None) -> GraphRagConfig:\n\
      \    return GraphRagConfig(**{\n        **asdict(defs.graphrag_config_defaults),\n\
      \        \"models\": DEFAULT_MODEL_CONFIG,\n        **({\"root_dir\": root_dir}\
      \ if root_dir else {}),\n    })"
    signature: 'def get_default_graphrag_config(root_dir: str | None = None) -> GraphRagConfig'
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/graph_rag_config.py::GraphRagConfig
      type: internal
    - target: dataclasses::asdict
      type: stdlib
    visibility: public
    node_id: tests/unit/config/utils.py::get_default_graphrag_config
    called_by:
    - source: tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
      type: internal
    - source: tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
      type: internal
    - source: tests/unit/config/test_config.py::test_default_config
      type: internal
    - source: tests/unit/config/test_config.py::test_load_minimal_config
      type: internal
    - source: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
      type: internal
  - name: assert_language_model_configs
    start_line: 68
    end_line: 106
    code: "def assert_language_model_configs(\n    actual: LanguageModelConfig, expected:\
      \ LanguageModelConfig\n) -> None:\n    assert actual.api_key == expected.api_key\n\
      \    assert actual.auth_type == expected.auth_type\n    assert actual.type ==\
      \ expected.type\n    assert actual.model == expected.model\n    assert actual.encoding_model\
      \ == expected.encoding_model\n    assert actual.max_tokens == expected.max_tokens\n\
      \    assert actual.temperature == expected.temperature\n    assert actual.max_completion_tokens\
      \ == expected.max_completion_tokens\n    assert actual.top_p == expected.top_p\n\
      \    assert actual.n == expected.n\n    assert actual.frequency_penalty == expected.frequency_penalty\n\
      \    assert actual.presence_penalty == expected.presence_penalty\n    assert\
      \ actual.request_timeout == expected.request_timeout\n    assert actual.api_base\
      \ == expected.api_base\n    assert actual.api_version == expected.api_version\n\
      \    assert actual.deployment_name == expected.deployment_name\n    assert actual.organization\
      \ == expected.organization\n    assert actual.proxy == expected.proxy\n    assert\
      \ actual.audience == expected.audience\n    assert actual.model_supports_json\
      \ == expected.model_supports_json\n    assert actual.tokens_per_minute == expected.tokens_per_minute\n\
      \    assert actual.requests_per_minute == expected.requests_per_minute\n   \
      \ assert actual.retry_strategy == expected.retry_strategy\n    assert actual.max_retries\
      \ == expected.max_retries\n    assert actual.max_retry_wait == expected.max_retry_wait\n\
      \    assert actual.concurrent_requests == expected.concurrent_requests\n   \
      \ assert actual.async_mode == expected.async_mode\n    if actual.responses is\
      \ not None:\n        assert expected.responses is not None\n        assert len(actual.responses)\
      \ == len(expected.responses)\n        for e, a in zip(actual.responses, expected.responses,\
      \ strict=True):\n            assert isinstance(e, BaseModel)\n            assert\
      \ isinstance(a, BaseModel)\n            assert e.model_dump() == a.model_dump()\n\
      \    else:\n        assert expected.responses is None"
    signature: "def assert_language_model_configs(\n    actual: LanguageModelConfig,\
      \ expected: LanguageModelConfig\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: zip
      type: builtin
    - target: isinstance
      type: builtin
    - target: e.model_dump
      type: unresolved
    - target: a.model_dump
      type: unresolved
    visibility: public
    node_id: tests/unit/config/utils.py::assert_language_model_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_vector_store_configs
    start_line: 109
    end_line: 126
    code: "def assert_vector_store_configs(\n    actual: dict[str, VectorStoreConfig],\n\
      \    expected: dict[str, VectorStoreConfig],\n):\n    assert type(actual) is\
      \ type(expected)\n    assert len(actual) == len(expected)\n    for (index_a,\
      \ store_a), (index_e, store_e) in zip(\n        actual.items(), expected.items(),\
      \ strict=True\n    ):\n        assert index_a == index_e\n        assert store_a.type\
      \ == store_e.type\n        assert store_a.db_uri == store_e.db_uri\n       \
      \ assert store_a.url == store_e.url\n        assert store_a.api_key == store_e.api_key\n\
      \        assert store_a.audience == store_e.audience\n        assert store_a.container_name\
      \ == store_e.container_name\n        assert store_a.overwrite == store_e.overwrite\n\
      \        assert store_a.database_name == store_e.database_name"
    signature: "def assert_vector_store_configs(\n    actual: dict[str, VectorStoreConfig],\n\
      \    expected: dict[str, VectorStoreConfig],\n)"
    decorators: []
    raises: []
    calls:
    - target: type
      type: builtin
    - target: len
      type: builtin
    - target: zip
      type: builtin
    - target: actual.items
      type: unresolved
    - target: expected.items
      type: unresolved
    visibility: public
    node_id: tests/unit/config/utils.py::assert_vector_store_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_reporting_configs
    start_line: 129
    end_line: 136
    code: "def assert_reporting_configs(\n    actual: ReportingConfig, expected: ReportingConfig\n\
      ) -> None:\n    assert actual.type == expected.type\n    assert actual.base_dir\
      \ == expected.base_dir\n    assert actual.connection_string == expected.connection_string\n\
      \    assert actual.container_name == expected.container_name\n    assert actual.storage_account_blob_url\
      \ == expected.storage_account_blob_url"
    signature: "def assert_reporting_configs(\n    actual: ReportingConfig, expected:\
      \ ReportingConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_reporting_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_output_configs
    start_line: 139
    end_line: 145
    code: "def assert_output_configs(actual: StorageConfig, expected: StorageConfig)\
      \ -> None:\n    assert expected.type == actual.type\n    assert expected.base_dir\
      \ == actual.base_dir\n    assert expected.connection_string == actual.connection_string\n\
      \    assert expected.container_name == actual.container_name\n    assert expected.storage_account_blob_url\
      \ == actual.storage_account_blob_url\n    assert expected.cosmosdb_account_url\
      \ == actual.cosmosdb_account_url"
    signature: 'def assert_output_configs(actual: StorageConfig, expected: StorageConfig)
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_output_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_update_output_configs
    start_line: 148
    end_line: 156
    code: "def assert_update_output_configs(\n    actual: StorageConfig, expected:\
      \ StorageConfig\n) -> None:\n    assert expected.type == actual.type\n    assert\
      \ expected.base_dir == actual.base_dir\n    assert expected.connection_string\
      \ == actual.connection_string\n    assert expected.container_name == actual.container_name\n\
      \    assert expected.storage_account_blob_url == actual.storage_account_blob_url\n\
      \    assert expected.cosmosdb_account_url == actual.cosmosdb_account_url"
    signature: "def assert_update_output_configs(\n    actual: StorageConfig, expected:\
      \ StorageConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_update_output_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_cache_configs
    start_line: 159
    end_line: 165
    code: "def assert_cache_configs(actual: CacheConfig, expected: CacheConfig) ->\
      \ None:\n    assert actual.type == expected.type\n    assert actual.base_dir\
      \ == expected.base_dir\n    assert actual.connection_string == expected.connection_string\n\
      \    assert actual.container_name == expected.container_name\n    assert actual.storage_account_blob_url\
      \ == expected.storage_account_blob_url\n    assert actual.cosmosdb_account_url\
      \ == expected.cosmosdb_account_url"
    signature: 'def assert_cache_configs(actual: CacheConfig, expected: CacheConfig)
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_cache_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_input_configs
    start_line: 168
    end_line: 183
    code: "def assert_input_configs(actual: InputConfig, expected: InputConfig) ->\
      \ None:\n    assert actual.storage.type == expected.storage.type\n    assert\
      \ actual.file_type == expected.file_type\n    assert actual.storage.base_dir\
      \ == expected.storage.base_dir\n    assert actual.storage.connection_string\
      \ == expected.storage.connection_string\n    assert (\n        actual.storage.storage_account_blob_url\n\
      \        == expected.storage.storage_account_blob_url\n    )\n    assert actual.storage.container_name\
      \ == expected.storage.container_name\n    assert actual.encoding == expected.encoding\n\
      \    assert actual.file_pattern == expected.file_pattern\n    assert actual.file_filter\
      \ == expected.file_filter\n    assert actual.text_column == expected.text_column\n\
      \    assert actual.title_column == expected.title_column\n    assert actual.metadata\
      \ == expected.metadata"
    signature: 'def assert_input_configs(actual: InputConfig, expected: InputConfig)
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_input_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_embed_graph_configs
    start_line: 186
    end_line: 196
    code: "def assert_embed_graph_configs(\n    actual: EmbedGraphConfig, expected:\
      \ EmbedGraphConfig\n) -> None:\n    assert actual.enabled == expected.enabled\n\
      \    assert actual.dimensions == expected.dimensions\n    assert actual.num_walks\
      \ == expected.num_walks\n    assert actual.walk_length == expected.walk_length\n\
      \    assert actual.window_size == expected.window_size\n    assert actual.iterations\
      \ == expected.iterations\n    assert actual.random_seed == expected.random_seed\n\
      \    assert actual.use_lcc == expected.use_lcc"
    signature: "def assert_embed_graph_configs(\n    actual: EmbedGraphConfig, expected:\
      \ EmbedGraphConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_embed_graph_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_text_embedding_configs
    start_line: 199
    end_line: 207
    code: "def assert_text_embedding_configs(\n    actual: TextEmbeddingConfig, expected:\
      \ TextEmbeddingConfig\n) -> None:\n    assert actual.batch_size == expected.batch_size\n\
      \    assert actual.batch_max_tokens == expected.batch_max_tokens\n    assert\
      \ actual.names == expected.names\n    assert actual.strategy == expected.strategy\n\
      \    assert actual.model_id == expected.model_id\n    assert actual.vector_store_id\
      \ == expected.vector_store_id"
    signature: "def assert_text_embedding_configs(\n    actual: TextEmbeddingConfig,\
      \ expected: TextEmbeddingConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_text_embedding_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_chunking_configs
    start_line: 210
    end_line: 217
    code: "def assert_chunking_configs(actual: ChunkingConfig, expected: ChunkingConfig)\
      \ -> None:\n    assert actual.size == expected.size\n    assert actual.overlap\
      \ == expected.overlap\n    assert actual.group_by_columns == expected.group_by_columns\n\
      \    assert actual.strategy == expected.strategy\n    assert actual.encoding_model\
      \ == expected.encoding_model\n    assert actual.prepend_metadata == expected.prepend_metadata\n\
      \    assert actual.chunk_size_includes_metadata == expected.chunk_size_includes_metadata"
    signature: 'def assert_chunking_configs(actual: ChunkingConfig, expected: ChunkingConfig)
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_chunking_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_snapshots_configs
    start_line: 220
    end_line: 224
    code: "def assert_snapshots_configs(\n    actual: SnapshotsConfig, expected: SnapshotsConfig\n\
      ) -> None:\n    assert actual.embeddings == expected.embeddings\n    assert\
      \ actual.graphml == expected.graphml"
    signature: "def assert_snapshots_configs(\n    actual: SnapshotsConfig, expected:\
      \ SnapshotsConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_snapshots_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_extract_graph_configs
    start_line: 227
    end_line: 234
    code: "def assert_extract_graph_configs(\n    actual: ExtractGraphConfig, expected:\
      \ ExtractGraphConfig\n) -> None:\n    assert actual.prompt == expected.prompt\n\
      \    assert actual.entity_types == expected.entity_types\n    assert actual.max_gleanings\
      \ == expected.max_gleanings\n    assert actual.strategy == expected.strategy\n\
      \    assert actual.model_id == expected.model_id"
    signature: "def assert_extract_graph_configs(\n    actual: ExtractGraphConfig,\
      \ expected: ExtractGraphConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_extract_graph_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_text_analyzer_configs
    start_line: 237
    end_line: 249
    code: "def assert_text_analyzer_configs(\n    actual: TextAnalyzerConfig, expected:\
      \ TextAnalyzerConfig\n) -> None:\n    assert actual.extractor_type == expected.extractor_type\n\
      \    assert actual.model_name == expected.model_name\n    assert actual.max_word_length\
      \ == expected.max_word_length\n    assert actual.word_delimiter == expected.word_delimiter\n\
      \    assert actual.include_named_entities == expected.include_named_entities\n\
      \    assert actual.exclude_nouns == expected.exclude_nouns\n    assert actual.exclude_entity_tags\
      \ == expected.exclude_entity_tags\n    assert actual.exclude_pos_tags == expected.exclude_pos_tags\n\
      \    assert actual.noun_phrase_tags == expected.noun_phrase_tags\n    assert\
      \ actual.noun_phrase_grammars == expected.noun_phrase_grammars"
    signature: "def assert_text_analyzer_configs(\n    actual: TextAnalyzerConfig,\
      \ expected: TextAnalyzerConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_text_analyzer_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_extract_graph_nlp_configs
      type: internal
  - name: assert_extract_graph_nlp_configs
    start_line: 252
    end_line: 257
    code: "def assert_extract_graph_nlp_configs(\n    actual: ExtractGraphNLPConfig,\
      \ expected: ExtractGraphNLPConfig\n) -> None:\n    assert actual.normalize_edge_weights\
      \ == expected.normalize_edge_weights\n    assert_text_analyzer_configs(actual.text_analyzer,\
      \ expected.text_analyzer)\n    assert actual.concurrent_requests == expected.concurrent_requests"
    signature: "def assert_extract_graph_nlp_configs(\n    actual: ExtractGraphNLPConfig,\
      \ expected: ExtractGraphNLPConfig\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: tests/unit/config/utils.py::assert_text_analyzer_configs
      type: internal
    visibility: public
    node_id: tests/unit/config/utils.py::assert_extract_graph_nlp_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_prune_graph_configs
    start_line: 260
    end_line: 269
    code: "def assert_prune_graph_configs(\n    actual: PruneGraphConfig, expected:\
      \ PruneGraphConfig\n) -> None:\n    assert actual.min_node_freq == expected.min_node_freq\n\
      \    assert actual.max_node_freq_std == expected.max_node_freq_std\n    assert\
      \ actual.min_node_degree == expected.min_node_degree\n    assert actual.max_node_degree_std\
      \ == expected.max_node_degree_std\n    assert actual.min_edge_weight_pct ==\
      \ expected.min_edge_weight_pct\n    assert actual.remove_ego_nodes == expected.remove_ego_nodes\n\
      \    assert actual.lcc_only == expected.lcc_only"
    signature: "def assert_prune_graph_configs(\n    actual: PruneGraphConfig, expected:\
      \ PruneGraphConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_prune_graph_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_summarize_descriptions_configs
    start_line: 272
    end_line: 278
    code: "def assert_summarize_descriptions_configs(\n    actual: SummarizeDescriptionsConfig,\
      \ expected: SummarizeDescriptionsConfig\n) -> None:\n    assert actual.prompt\
      \ == expected.prompt\n    assert actual.max_length == expected.max_length\n\
      \    assert actual.strategy == expected.strategy\n    assert actual.model_id\
      \ == expected.model_id"
    signature: "def assert_summarize_descriptions_configs(\n    actual: SummarizeDescriptionsConfig,\
      \ expected: SummarizeDescriptionsConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_summarize_descriptions_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_community_reports_configs
    start_line: 281
    end_line: 289
    code: "def assert_community_reports_configs(\n    actual: CommunityReportsConfig,\
      \ expected: CommunityReportsConfig\n) -> None:\n    assert actual.graph_prompt\
      \ == expected.graph_prompt\n    assert actual.text_prompt == expected.text_prompt\n\
      \    assert actual.max_length == expected.max_length\n    assert actual.max_input_length\
      \ == expected.max_input_length\n    assert actual.strategy == expected.strategy\n\
      \    assert actual.model_id == expected.model_id"
    signature: "def assert_community_reports_configs(\n    actual: CommunityReportsConfig,\
      \ expected: CommunityReportsConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_community_reports_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_extract_claims_configs
    start_line: 292
    end_line: 300
    code: "def assert_extract_claims_configs(\n    actual: ClaimExtractionConfig,\
      \ expected: ClaimExtractionConfig\n) -> None:\n    assert actual.enabled ==\
      \ expected.enabled\n    assert actual.prompt == expected.prompt\n    assert\
      \ actual.description == expected.description\n    assert actual.max_gleanings\
      \ == expected.max_gleanings\n    assert actual.strategy == expected.strategy\n\
      \    assert actual.model_id == expected.model_id"
    signature: "def assert_extract_claims_configs(\n    actual: ClaimExtractionConfig,\
      \ expected: ClaimExtractionConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_extract_claims_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_cluster_graph_configs
    start_line: 303
    end_line: 308
    code: "def assert_cluster_graph_configs(\n    actual: ClusterGraphConfig, expected:\
      \ ClusterGraphConfig\n) -> None:\n    assert actual.max_cluster_size == expected.max_cluster_size\n\
      \    assert actual.use_lcc == expected.use_lcc\n    assert actual.seed == expected.seed"
    signature: "def assert_cluster_graph_configs(\n    actual: ClusterGraphConfig,\
      \ expected: ClusterGraphConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_cluster_graph_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_umap_configs
    start_line: 311
    end_line: 312
    code: "def assert_umap_configs(actual: UmapConfig, expected: UmapConfig) -> None:\n\
      \    assert actual.enabled == expected.enabled"
    signature: 'def assert_umap_configs(actual: UmapConfig, expected: UmapConfig)
      -> None'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_umap_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_local_search_configs
    start_line: 315
    end_line: 326
    code: "def assert_local_search_configs(\n    actual: LocalSearchConfig, expected:\
      \ LocalSearchConfig\n) -> None:\n    assert actual.prompt == expected.prompt\n\
      \    assert actual.text_unit_prop == expected.text_unit_prop\n    assert actual.community_prop\
      \ == expected.community_prop\n    assert (\n        actual.conversation_history_max_turns\
      \ == expected.conversation_history_max_turns\n    )\n    assert actual.top_k_entities\
      \ == expected.top_k_entities\n    assert actual.top_k_relationships == expected.top_k_relationships\n\
      \    assert actual.max_context_tokens == expected.max_context_tokens"
    signature: "def assert_local_search_configs(\n    actual: LocalSearchConfig, expected:\
      \ LocalSearchConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_local_search_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_global_search_configs
    start_line: 329
    end_line: 343
    code: "def assert_global_search_configs(\n    actual: GlobalSearchConfig, expected:\
      \ GlobalSearchConfig\n) -> None:\n    assert actual.map_prompt == expected.map_prompt\n\
      \    assert actual.reduce_prompt == expected.reduce_prompt\n    assert actual.knowledge_prompt\
      \ == expected.knowledge_prompt\n    assert actual.max_context_tokens == expected.max_context_tokens\n\
      \    assert actual.data_max_tokens == expected.data_max_tokens\n    assert actual.map_max_length\
      \ == expected.map_max_length\n    assert actual.reduce_max_length == expected.reduce_max_length\n\
      \    assert actual.dynamic_search_threshold == expected.dynamic_search_threshold\n\
      \    assert actual.dynamic_search_keep_parent == expected.dynamic_search_keep_parent\n\
      \    assert actual.dynamic_search_num_repeats == expected.dynamic_search_num_repeats\n\
      \    assert actual.dynamic_search_use_summary == expected.dynamic_search_use_summary\n\
      \    assert actual.dynamic_search_max_level == expected.dynamic_search_max_level"
    signature: "def assert_global_search_configs(\n    actual: GlobalSearchConfig,\
      \ expected: GlobalSearchConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_global_search_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_drift_search_configs
    start_line: 346
    end_line: 376
    code: "def assert_drift_search_configs(\n    actual: DRIFTSearchConfig, expected:\
      \ DRIFTSearchConfig\n) -> None:\n    assert actual.prompt == expected.prompt\n\
      \    assert actual.reduce_prompt == expected.reduce_prompt\n    assert actual.data_max_tokens\
      \ == expected.data_max_tokens\n    assert actual.reduce_max_tokens == expected.reduce_max_tokens\n\
      \    assert actual.reduce_temperature == expected.reduce_temperature\n    assert\
      \ actual.concurrency == expected.concurrency\n    assert actual.drift_k_followups\
      \ == expected.drift_k_followups\n    assert actual.primer_folds == expected.primer_folds\n\
      \    assert actual.primer_llm_max_tokens == expected.primer_llm_max_tokens\n\
      \    assert actual.n_depth == expected.n_depth\n    assert actual.local_search_text_unit_prop\
      \ == expected.local_search_text_unit_prop\n    assert actual.local_search_community_prop\
      \ == expected.local_search_community_prop\n    assert (\n        actual.local_search_top_k_mapped_entities\n\
      \        == expected.local_search_top_k_mapped_entities\n    )\n    assert (\n\
      \        actual.local_search_top_k_relationships\n        == expected.local_search_top_k_relationships\n\
      \    )\n    assert actual.local_search_max_data_tokens == expected.local_search_max_data_tokens\n\
      \    assert actual.local_search_temperature == expected.local_search_temperature\n\
      \    assert actual.local_search_top_p == expected.local_search_top_p\n    assert\
      \ actual.local_search_n == expected.local_search_n\n    assert (\n        actual.local_search_llm_max_gen_tokens\n\
      \        == expected.local_search_llm_max_gen_tokens\n    )"
    signature: "def assert_drift_search_configs(\n    actual: DRIFTSearchConfig, expected:\
      \ DRIFTSearchConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_drift_search_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_basic_search_configs
    start_line: 379
    end_line: 383
    code: "def assert_basic_search_configs(\n    actual: BasicSearchConfig, expected:\
      \ BasicSearchConfig\n) -> None:\n    assert actual.prompt == expected.prompt\n\
      \    assert actual.k == expected.k"
    signature: "def assert_basic_search_configs(\n    actual: BasicSearchConfig, expected:\
      \ BasicSearchConfig\n) -> None"
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/config/utils.py::assert_basic_search_configs
    called_by:
    - source: tests/unit/config/utils.py::assert_graphrag_configs
      type: internal
  - name: assert_graphrag_configs
    start_line: 386
    end_line: 435
    code: "def assert_graphrag_configs(actual: GraphRagConfig, expected: GraphRagConfig)\
      \ -> None:\n    assert actual.root_dir == expected.root_dir\n\n    a_keys =\
      \ sorted(actual.models.keys())\n    e_keys = sorted(expected.models.keys())\n\
      \    assert len(a_keys) == len(e_keys)\n    for a, e in zip(a_keys, e_keys,\
      \ strict=False):\n        assert a == e\n        assert_language_model_configs(actual.models[a],\
      \ expected.models[e])\n\n    assert_vector_store_configs(actual.vector_store,\
      \ expected.vector_store)\n    assert_reporting_configs(actual.reporting, expected.reporting)\n\
      \    assert_output_configs(actual.output, expected.output)\n\n    if expected.outputs\
      \ is not None:\n        assert actual.outputs is not None\n        assert len(actual.outputs)\
      \ == len(expected.outputs)\n        for a, e in zip(actual.outputs.keys(), expected.outputs.keys(),\
      \ strict=True):\n            assert_output_configs(actual.outputs[a], expected.outputs[e])\n\
      \    else:\n        assert actual.outputs is None\n\n    assert_update_output_configs(\n\
      \        actual.update_index_output, expected.update_index_output\n    )\n\n\
      \    assert_cache_configs(actual.cache, expected.cache)\n    assert_input_configs(actual.input,\
      \ expected.input)\n    assert_embed_graph_configs(actual.embed_graph, expected.embed_graph)\n\
      \    assert_text_embedding_configs(actual.embed_text, expected.embed_text)\n\
      \    assert_chunking_configs(actual.chunks, expected.chunks)\n    assert_snapshots_configs(actual.snapshots,\
      \ expected.snapshots)\n    assert_extract_graph_configs(actual.extract_graph,\
      \ expected.extract_graph)\n    assert_extract_graph_nlp_configs(\n        actual.extract_graph_nlp,\
      \ expected.extract_graph_nlp\n    )\n    assert_summarize_descriptions_configs(\n\
      \        actual.summarize_descriptions, expected.summarize_descriptions\n  \
      \  )\n    assert_community_reports_configs(\n        actual.community_reports,\
      \ expected.community_reports\n    )\n    assert_extract_claims_configs(actual.extract_claims,\
      \ expected.extract_claims)\n    assert_prune_graph_configs(actual.prune_graph,\
      \ expected.prune_graph)\n    assert_cluster_graph_configs(actual.cluster_graph,\
      \ expected.cluster_graph)\n    assert_umap_configs(actual.umap, expected.umap)\n\
      \    assert_local_search_configs(actual.local_search, expected.local_search)\n\
      \    assert_global_search_configs(actual.global_search, expected.global_search)\n\
      \    assert_drift_search_configs(actual.drift_search, expected.drift_search)\n\
      \    assert_basic_search_configs(actual.basic_search, expected.basic_search)"
    signature: 'def assert_graphrag_configs(actual: GraphRagConfig, expected: GraphRagConfig)
      -> None'
    decorators: []
    raises: []
    calls:
    - target: sorted
      type: builtin
    - target: actual.models.keys
      type: unresolved
    - target: expected.models.keys
      type: unresolved
    - target: len
      type: builtin
    - target: zip
      type: builtin
    - target: tests/unit/config/utils.py::assert_language_model_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_vector_store_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_reporting_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_output_configs
      type: internal
    - target: actual.outputs.keys
      type: unresolved
    - target: expected.outputs.keys
      type: unresolved
    - target: tests/unit/config/utils.py::assert_update_output_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_cache_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_input_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_embed_graph_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_text_embedding_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_chunking_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_snapshots_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_extract_graph_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_extract_graph_nlp_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_summarize_descriptions_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_community_reports_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_extract_claims_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_prune_graph_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_cluster_graph_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_umap_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_local_search_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_global_search_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_drift_search_configs
      type: internal
    - target: tests/unit/config/utils.py::assert_basic_search_configs
      type: internal
    visibility: public
    node_id: tests/unit/config/utils.py::assert_graphrag_configs
    called_by:
    - source: tests/unit/config/test_config.py::test_default_config
      type: internal
    - source: tests/unit/config/test_config.py::test_load_minimal_config
      type: internal
    - source: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
      type: internal
- file_name: tests/unit/indexing/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/graph/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/graph/extractors/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/graph/extractors/community_reports/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py
  imports:
  - module: math
    name: null
    alias: null
  - module: platform
    name: null
    alias: null
  - module: graphrag.index.operations.summarize_communities.graph_context.sort_context
    name: sort_context
    alias: null
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  functions:
  - name: test_sort_context
    start_line: 206
    end_line: 213
    code: "def test_sort_context():\n    tokenizer = get_tokenizer()\n    ctx = sort_context(context,\
      \ tokenizer=tokenizer)\n    assert ctx is not None, \"Context is none\"\n  \
      \  num = tokenizer.num_tokens(ctx)\n    assert num == 828 if platform.system()\
      \ == \"Windows\" else 826, (\n        f\"num_tokens is not matched for platform\
      \ (win = 827, else 826): {num}\"\n    )"
    signature: def test_sort_context()
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    - target: platform::system
      type: stdlib
    visibility: public
    node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
    called_by: []
  - name: test_sort_context_max_tokens
    start_line: 216
    end_line: 221
    code: "def test_sort_context_max_tokens():\n    tokenizer = get_tokenizer()\n\
      \    ctx = sort_context(context, tokenizer=tokenizer, max_context_tokens=800)\n\
      \    assert ctx is not None, \"Context is none\"\n    num = tokenizer.num_tokens(ctx)\n\
      \    assert num <= 800, f\"num_tokens is not less than or equal to 800: {num}\""
    signature: def test_sort_context_max_tokens()
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
      type: internal
    - target: tokenizer.num_tokens
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
    called_by: []
- file_name: tests/unit/indexing/graph/utils/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/graph/utils/test_stable_lcc.py
  imports:
  - module: unittest
    name: null
    alias: null
  - module: networkx
    name: null
    alias: nx
  - module: graphrag.index.utils.stable_lcc
    name: stable_largest_connected_component
    alias: null
  functions:
  - name: test_undirected_graph_run_twice_produces_same_graph
    start_line: 11
    end_line: 21
    code: "def test_undirected_graph_run_twice_produces_same_graph(self):\n      \
      \  graph_in_1 = self._create_strongly_connected_graph()\n        graph_out_1\
      \ = stable_largest_connected_component(graph_in_1)\n\n        graph_in_2 = self._create_strongly_connected_graph_with_edges_flipped()\n\
      \        graph_out_2 = stable_largest_connected_component(graph_in_2)\n\n  \
      \      # Make sure they're the same\n        assert \"\".join(nx.generate_graphml(graph_out_1))\
      \ == \"\".join(\n            nx.generate_graphml(graph_out_2)\n        )"
    signature: def test_undirected_graph_run_twice_produces_same_graph(self)
    decorators: []
    raises: []
    calls:
    - target: tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph
      type: internal
    - target: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
    - target: tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
      type: internal
    - target: '"".join'
      type: unresolved
    - target: networkx::generate_graphml
      type: external
    visibility: public
    node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
    called_by: []
  - name: test_directed_graph_keeps_source_target_intact
    start_line: 23
    end_line: 34
    code: "def test_directed_graph_keeps_source_target_intact(self):\n        # create\
      \ the test graph as a directed graph\n        graph_in = self._create_strongly_connected_graph_with_edges_flipped(\n\
      \            digraph=True\n        )\n        graph_out = stable_largest_connected_component(graph_in.copy())\n\
      \n        # Make sure edges are the same and the direction is preserved\n  \
      \      edges_1 = [f\"{edge[0]} -> {edge[1]}\" for edge in graph_in.edges(data=True)]\n\
      \        edges_2 = [f\"{edge[0]} -> {edge[1]}\" for edge in graph_out.edges(data=True)]\n\
      \n        assert edges_1 == edges_2"
    signature: def test_directed_graph_keeps_source_target_intact(self)
    decorators: []
    raises: []
    calls:
    - target: tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
      type: internal
    - target: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
    - target: graph_in.copy
      type: unresolved
    - target: graph_in.edges
      type: unresolved
    - target: graph_out.edges
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
    called_by: []
  - name: test_directed_graph_run_twice_produces_same_graph
    start_line: 36
    end_line: 47
    code: "def test_directed_graph_run_twice_produces_same_graph(self):\n        #\
      \ create the test graph as a directed graph\n        graph_in = self._create_strongly_connected_graph_with_edges_flipped(\n\
      \            digraph=True\n        )\n        graph_out_1 = stable_largest_connected_component(graph_in.copy())\n\
      \        graph_out_2 = stable_largest_connected_component(graph_in.copy())\n\
      \n        # Make sure the output is identical when run multiple times\n    \
      \    assert \"\".join(nx.generate_graphml(graph_out_1)) == \"\".join(\n    \
      \        nx.generate_graphml(graph_out_2)\n        )"
    signature: def test_directed_graph_run_twice_produces_same_graph(self)
    decorators: []
    raises: []
    calls:
    - target: tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
      type: internal
    - target: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
      type: internal
    - target: graph_in.copy
      type: unresolved
    - target: '"".join'
      type: unresolved
    - target: networkx::generate_graphml
      type: external
    visibility: public
    node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
    called_by: []
  - name: _create_strongly_connected_graph
    start_line: 49
    end_line: 59
    code: "def _create_strongly_connected_graph(self, digraph=False):\n        graph\
      \ = nx.Graph() if not digraph else nx.DiGraph()\n        graph.add_node(\"1\"\
      , node_name=1)\n        graph.add_node(\"2\", node_name=2)\n        graph.add_node(\"\
      3\", node_name=3)\n        graph.add_node(\"4\", node_name=4)\n        graph.add_edge(\"\
      4\", \"5\", degree=4)\n        graph.add_edge(\"3\", \"4\", degree=3)\n    \
      \    graph.add_edge(\"2\", \"3\", degree=2)\n        graph.add_edge(\"1\", \"\
      2\", degree=1)\n        return graph"
    signature: def _create_strongly_connected_graph(self, digraph=False)
    decorators: []
    raises: []
    calls:
    - target: networkx::Graph
      type: external
    - target: networkx::DiGraph
      type: external
    - target: graph.add_node
      type: unresolved
    - target: graph.add_edge
      type: unresolved
    visibility: protected
    node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph
    called_by: []
  - name: _create_strongly_connected_graph_with_edges_flipped
    start_line: 61
    end_line: 71
    code: "def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False):\n\
      \        graph = nx.Graph() if not digraph else nx.DiGraph()\n        graph.add_node(\"\
      1\", node_name=1)\n        graph.add_node(\"2\", node_name=2)\n        graph.add_node(\"\
      3\", node_name=3)\n        graph.add_node(\"4\", node_name=4)\n        graph.add_edge(\"\
      5\", \"4\", degree=4)\n        graph.add_edge(\"4\", \"3\", degree=3)\n    \
      \    graph.add_edge(\"3\", \"2\", degree=2)\n        graph.add_edge(\"2\", \"\
      1\", degree=1)\n        return graph"
    signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
    decorators: []
    raises: []
    calls:
    - target: networkx::Graph
      type: external
    - target: networkx::DiGraph
      type: external
    - target: graph.add_node
      type: unresolved
    - target: graph.add_edge
      type: unresolved
    visibility: protected
    node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph_with_edges_flipped
    called_by: []
- file_name: tests/unit/indexing/input/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/input/test_csv_loader.py
  imports:
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  functions:
  - name: test_csv_loader_one_file
    start_line: 11
    end_line: 22
    code: "async def test_csv_loader_one_file():\n    config = InputConfig(\n    \
      \    storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-csv\"\
      ,\n        ),\n        file_type=InputFileType.csv,\n        file_pattern=\"\
      .*\\\\.csv$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (2, 4)\n    assert documents[\"title\"].iloc[0] == \"input.csv\""
    signature: def test_csv_loader_one_file()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
    called_by: []
  - name: test_csv_loader_one_file_with_title
    start_line: 25
    end_line: 37
    code: "async def test_csv_loader_one_file_with_title():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-csv\"\
      ,\n        ),\n        file_type=InputFileType.csv,\n        file_pattern=\"\
      .*\\\\.csv$\",\n        title_column=\"title\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (2, 4)\n    assert documents[\"title\"].iloc[0] == \"Hello\""
    signature: def test_csv_loader_one_file_with_title()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
    called_by: []
  - name: test_csv_loader_one_file_with_metadata
    start_line: 40
    end_line: 53
    code: "async def test_csv_loader_one_file_with_metadata():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-csv\"\
      ,\n        ),\n        file_type=InputFileType.csv,\n        file_pattern=\"\
      .*\\\\.csv$\",\n        title_column=\"title\",\n        metadata=[\"title\"\
      ],\n    )\n    storage = create_storage_from_config(config.storage)\n    documents\
      \ = await create_input(config=config, storage=storage)\n    assert documents.shape\
      \ == (2, 5)\n    assert documents[\"metadata\"][0] == {\"title\": \"Hello\"}"
    signature: def test_csv_loader_one_file_with_metadata()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
    called_by: []
  - name: test_csv_loader_multiple_files
    start_line: 56
    end_line: 66
    code: "async def test_csv_loader_multiple_files():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/multiple-csvs\"\
      ,\n        ),\n        file_type=InputFileType.csv,\n        file_pattern=\"\
      .*\\\\.csv$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (4, 4)"
    signature: def test_csv_loader_multiple_files()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
    called_by: []
- file_name: tests/unit/indexing/input/test_json_loader.py
  imports:
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  functions:
  - name: test_json_loader_one_file_one_object
    start_line: 11
    end_line: 22
    code: "async def test_json_loader_one_file_one_object():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-json-one-object\"\
      ,\n        ),\n        file_type=InputFileType.json,\n        file_pattern=\"\
      .*\\\\.json$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (1, 4)\n    assert documents[\"title\"].iloc[0] == \"input.json\""
    signature: def test_json_loader_one_file_one_object()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
    called_by: []
  - name: test_json_loader_one_file_multiple_objects
    start_line: 25
    end_line: 37
    code: "async def test_json_loader_one_file_multiple_objects():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-json-multiple-objects\"\
      ,\n        ),\n        file_type=InputFileType.json,\n        file_pattern=\"\
      .*\\\\.json$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    print(documents)\n\
      \    assert documents.shape == (3, 4)\n    assert documents[\"title\"].iloc[0]\
      \ == \"input.json\""
    signature: def test_json_loader_one_file_multiple_objects()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    - target: print
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
    called_by: []
  - name: test_json_loader_one_file_with_title
    start_line: 40
    end_line: 52
    code: "async def test_json_loader_one_file_with_title():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-json-one-object\"\
      ,\n        ),\n        file_type=InputFileType.json,\n        file_pattern=\"\
      .*\\\\.json$\",\n        title_column=\"title\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (1, 4)\n    assert documents[\"title\"].iloc[0] == \"Hello\""
    signature: def test_json_loader_one_file_with_title()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
    called_by: []
  - name: test_json_loader_one_file_with_metadata
    start_line: 55
    end_line: 68
    code: "async def test_json_loader_one_file_with_metadata():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-json-one-object\"\
      ,\n        ),\n        file_type=InputFileType.json,\n        file_pattern=\"\
      .*\\\\.json$\",\n        title_column=\"title\",\n        metadata=[\"title\"\
      ],\n    )\n    storage = create_storage_from_config(config.storage)\n    documents\
      \ = await create_input(config=config, storage=storage)\n    assert documents.shape\
      \ == (1, 5)\n    assert documents[\"metadata\"][0] == {\"title\": \"Hello\"}"
    signature: def test_json_loader_one_file_with_metadata()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
    called_by: []
  - name: test_json_loader_multiple_files
    start_line: 71
    end_line: 81
    code: "async def test_json_loader_multiple_files():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/multiple-jsons\"\
      ,\n        ),\n        file_type=InputFileType.json,\n        file_pattern=\"\
      .*\\\\.json$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (4, 4)"
    signature: def test_json_loader_multiple_files()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
    called_by: []
- file_name: tests/unit/indexing/input/test_txt_loader.py
  imports:
  - module: graphrag.config.enums
    name: InputFileType
    alias: null
  - module: graphrag.config.models.input_config
    name: InputConfig
    alias: null
  - module: graphrag.config.models.storage_config
    name: StorageConfig
    alias: null
  - module: graphrag.index.input.factory
    name: create_input
    alias: null
  - module: graphrag.utils.api
    name: create_storage_from_config
    alias: null
  functions:
  - name: test_txt_loader_one_file
    start_line: 11
    end_line: 22
    code: "async def test_txt_loader_one_file():\n    config = InputConfig(\n    \
      \    storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-txt\"\
      ,\n        ),\n        file_type=InputFileType.text,\n        file_pattern=\"\
      .*\\\\.txt$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (1, 4)\n    assert documents[\"title\"].iloc[0] == \"input.txt\""
    signature: def test_txt_loader_one_file()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
    called_by: []
  - name: test_txt_loader_one_file_with_metadata
    start_line: 25
    end_line: 38
    code: "async def test_txt_loader_one_file_with_metadata():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/one-txt\"\
      ,\n        ),\n        file_type=InputFileType.text,\n        file_pattern=\"\
      .*\\\\.txt$\",\n        metadata=[\"title\"],\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (1, 5)\n    # unlike csv, we cannot set the title to anything\
      \ other than the filename\n    assert documents[\"metadata\"][0] == {\"title\"\
      : \"input.txt\"}"
    signature: def test_txt_loader_one_file_with_metadata()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
    called_by: []
  - name: test_txt_loader_multiple_files
    start_line: 41
    end_line: 51
    code: "async def test_txt_loader_multiple_files():\n    config = InputConfig(\n\
      \        storage=StorageConfig(\n            base_dir=\"tests/unit/indexing/input/data/multiple-txts\"\
      ,\n        ),\n        file_type=InputFileType.text,\n        file_pattern=\"\
      .*\\\\.txt$\",\n    )\n    storage = create_storage_from_config(config.storage)\n\
      \    documents = await create_input(config=config, storage=storage)\n    assert\
      \ documents.shape == (2, 4)"
    signature: def test_txt_loader_multiple_files()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/models/input_config.py::InputConfig
      type: internal
    - target: graphrag/config/models/storage_config.py::StorageConfig
      type: internal
    - target: graphrag/utils/api.py::create_storage_from_config
      type: internal
    - target: graphrag/index/input/factory.py::create_input
      type: internal
    visibility: public
    node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
    called_by: []
- file_name: tests/unit/indexing/operations/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/operations/chunk_text/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  imports:
  - module: unittest
    name: mock
    alias: null
  - module: unittest.mock
    name: ANY
    alias: null
  - module: unittest.mock
    name: Mock
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.enums
    name: ChunkStrategyType
    alias: null
  - module: graphrag.index.operations.chunk_text.chunk_text
    name: _get_num_total
    alias: null
  - module: graphrag.index.operations.chunk_text.chunk_text
    name: chunk_text
    alias: null
  - module: graphrag.index.operations.chunk_text.chunk_text
    name: load_strategy
    alias: null
  - module: graphrag.index.operations.chunk_text.chunk_text
    name: run_strategy
    alias: null
  - module: graphrag.index.operations.chunk_text.typing
    name: TextChunk
    alias: null
  functions:
  - name: test_get_num_total_default
    start_line: 23
    end_line: 27
    code: "def test_get_num_total_default():\n    output = pd.DataFrame({\"column\"\
      : [\"a\", \"b\", \"c\"]})\n\n    total = _get_num_total(output, \"column\")\n\
      \    assert total == 3"
    signature: def test_get_num_total_default()
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_default
    called_by: []
  - name: test_get_num_total_array
    start_line: 30
    end_line: 34
    code: "def test_get_num_total_array():\n    output = pd.DataFrame({\"column\"\
      : [[\"a\", \"b\", \"c\"], [\"x\", \"y\"]]})\n\n    total = _get_num_total(output,\
      \ \"column\")\n    assert total == 5"
    signature: def test_get_num_total_array()
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_array
    called_by: []
  - name: test_load_strategy_tokens
    start_line: 37
    end_line: 42
    code: "def test_load_strategy_tokens():\n    strategy_type = ChunkStrategyType.tokens\n\
      \n    strategy_loaded = load_strategy(strategy_type)\n\n    assert strategy_loaded.__name__\
      \ == \"run_tokens\""
    signature: def test_load_strategy_tokens()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_tokens
    called_by: []
  - name: test_load_strategy_sentence
    start_line: 45
    end_line: 50
    code: "def test_load_strategy_sentence():\n    strategy_type = ChunkStrategyType.sentence\n\
      \n    strategy_loaded = load_strategy(strategy_type)\n\n    assert strategy_loaded.__name__\
      \ == \"run_sentences\""
    signature: def test_load_strategy_sentence()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_sentence
    called_by: []
  - name: test_load_strategy_none
    start_line: 53
    end_line: 59
    code: "def test_load_strategy_none():\n    strategy_type = ChunkStrategyType\n\
      \n    with pytest.raises(\n        ValueError, match=\"Unknown strategy: <enum\
      \ 'ChunkStrategyType'>\"\n    ):\n        load_strategy(strategy_type)  # type:\
      \ ignore"
    signature: def test_load_strategy_none()
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_none
    called_by: []
  - name: test_run_strategy_str
    start_line: 62
    end_line: 76
    code: "def test_run_strategy_str():\n    input = \"text test for run strategy\"\
      \n    config = Mock()\n    tick = Mock()\n    strategy_mocked = Mock()\n\n \
      \   strategy_mocked.return_value = [\n        TextChunk(\n            text_chunk=\"\
      text test for run strategy\",\n            source_doc_indices=[0],\n       \
      \ )\n    ]\n\n    runned = run_strategy(strategy_mocked, input, config, tick)\n\
      \    assert runned == [\"text test for run strategy\"]"
    signature: def test_run_strategy_str()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    - target: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_str
    called_by: []
  - name: test_run_strategy_arr_str
    start_line: 79
    end_line: 98
    code: "def test_run_strategy_arr_str():\n    input = [\"text test for run strategy\"\
      , \"use for strategy\"]\n    config = Mock()\n    tick = Mock()\n    strategy_mocked\
      \ = Mock()\n\n    strategy_mocked.return_value = [\n        TextChunk(\n   \
      \         text_chunk=\"text test for run strategy\", source_doc_indices=[0],\
      \ n_tokens=5\n        ),\n        TextChunk(text_chunk=\"use for strategy\"\
      , source_doc_indices=[1], n_tokens=3),\n    ]\n\n    expected = [\n        \"\
      text test for run strategy\",\n        \"use for strategy\",\n    ]\n\n    runned\
      \ = run_strategy(strategy_mocked, input, config, tick)\n    assert runned ==\
      \ expected"
    signature: def test_run_strategy_arr_str()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    - target: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_str
    called_by: []
  - name: test_run_strategy_arr_tuple
    start_line: 101
    end_line: 128
    code: "def test_run_strategy_arr_tuple():\n    input = [(\"text test for run strategy\"\
      , \"3\"), (\"use for strategy\", \"5\")]\n    config = Mock()\n    tick = Mock()\n\
      \    strategy_mocked = Mock()\n\n    strategy_mocked.return_value = [\n    \
      \    TextChunk(\n            text_chunk=\"text test for run strategy\", source_doc_indices=[0],\
      \ n_tokens=5\n        ),\n        TextChunk(text_chunk=\"use for strategy\"\
      , source_doc_indices=[1], n_tokens=3),\n    ]\n\n    expected = [\n        (\n\
      \            [\"text test for run strategy\"],\n            \"text test for\
      \ run strategy\",\n            5,\n        ),\n        (\n            [\"use\
      \ for strategy\"],\n            \"use for strategy\",\n            3,\n    \
      \    ),\n    ]\n\n    runned = run_strategy(strategy_mocked, input, config,\
      \ tick)\n    assert runned == expected"
    signature: def test_run_strategy_arr_tuple()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    - target: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple
    called_by: []
  - name: test_run_strategy_arr_tuple_same_doc
    start_line: 131
    end_line: 158
    code: "def test_run_strategy_arr_tuple_same_doc():\n    input = [(\"text test\
      \ for run strategy\", \"3\"), (\"use for strategy\", \"5\")]\n    config = Mock()\n\
      \    tick = Mock()\n    strategy_mocked = Mock()\n\n    strategy_mocked.return_value\
      \ = [\n        TextChunk(\n            text_chunk=\"text test for run strategy\"\
      , source_doc_indices=[0], n_tokens=5\n        ),\n        TextChunk(text_chunk=\"\
      use for strategy\", source_doc_indices=[0], n_tokens=3),\n    ]\n\n    expected\
      \ = [\n        (\n            [\"text test for run strategy\"],\n          \
      \  \"text test for run strategy\",\n            5,\n        ),\n        (\n\
      \            [\"text test for run strategy\"],\n            \"use for strategy\"\
      ,\n            3,\n        ),\n    ]\n\n    runned = run_strategy(strategy_mocked,\
      \ input, config, tick)\n    assert runned == expected"
    signature: def test_run_strategy_arr_tuple_same_doc()
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/typing.py::TextChunk
      type: internal
    - target: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple_same_doc
    called_by: []
  - name: test_chunk_text
    start_line: 164
    end_line: 181
    code: "def test_chunk_text(mock_progress_ticker, mock_run_strategy, mock_load_strategy):\n\
      \    input_data = pd.DataFrame({\"name\": [\"The Shining\"]})\n    column =\
      \ \"name\"\n    size = 10\n    overlap = 2\n    encoding_model = \"model\"\n\
      \    strategy = ChunkStrategyType.sentence\n    callbacks = Mock()\n    callbacks.progress\
      \ = Mock()\n\n    mock_load_strategy.return_value = Mock()\n    mock_progress_ticker.return_value\
      \ = Mock()\n\n    chunk_text(input_data, column, size, overlap, encoding_model,\
      \ strategy, callbacks)\n\n    mock_run_strategy.assert_called_with(\n      \
      \  mock_load_strategy(), \"The Shining\", ANY, mock_progress_ticker.return_value\n\
      \    )"
    signature: def test_chunk_text(mock_progress_ticker, mock_run_strategy, mock_load_strategy)
    decorators:
    - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.load_strategy")'
    - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.run_strategy")'
    - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.progress_ticker")'
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
      type: internal
    - target: mock_run_strategy.assert_called_with
      type: unresolved
    - target: mock_load_strategy
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_chunk_text
    called_by: []
- file_name: tests/unit/indexing/operations/chunk_text/test_strategies.py
  imports:
  - module: unittest.mock
    name: Mock
    alias: null
  - module: unittest.mock
    name: patch
    alias: null
  - module: graphrag.config.models.chunking_config
    name: ChunkingConfig
    alias: null
  - module: graphrag.index.operations.chunk_text.bootstrap
    name: bootstrap
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: get_encoding_fn
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: run_sentences
    alias: null
  - module: graphrag.index.operations.chunk_text.strategies
    name: run_tokens
    alias: null
  - module: graphrag.index.operations.chunk_text.typing
    name: TextChunk
    alias: null
  functions:
  - name: setup_method
    start_line: 17
    end_line: 18
    code: "def setup_method(self, method):\n        bootstrap()"
    signature: def setup_method(self, method)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
      type: internal
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
    called_by: []
  - name: test_basic_functionality
    start_line: 20
    end_line: 30
    code: "def test_basic_functionality(self):\n        \"\"\"Test basic sentence\
      \ splitting without metadata\"\"\"\n        input = [\"This is a test. Another\
      \ sentence.\"]\n        tick = Mock()\n        chunks = list(run_sentences(input,\
      \ ChunkingConfig(), tick))\n\n        assert len(chunks) == 2\n        assert\
      \ chunks[0].text_chunk == \"This is a test.\"\n        assert chunks[1].text_chunk\
      \ == \"Another sentence.\"\n        assert all(c.source_doc_indices == [0] for\
      \ c in chunks)\n        tick.assert_called_once_with(1)"
    signature: def test_basic_functionality(self)
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: list
      type: builtin
    - target: graphrag/index/operations/chunk_text/strategies.py::run_sentences
      type: internal
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: len
      type: builtin
    - target: all
      type: builtin
    - target: tick.assert_called_once_with
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
    called_by: []
  - name: test_multiple_documents
    start_line: 32
    end_line: 41
    code: "def test_multiple_documents(self):\n        \"\"\"Test processing multiple\
      \ input documents\"\"\"\n        input = [\"First. Document.\", \"Second. Doc.\"\
      ]\n        tick = Mock()\n        chunks = list(run_sentences(input, ChunkingConfig(),\
      \ tick))\n\n        assert len(chunks) == 4\n        assert chunks[0].source_doc_indices\
      \ == [0]\n        assert chunks[2].source_doc_indices == [1]\n        assert\
      \ tick.call_count == 2"
    signature: def test_multiple_documents(self)
    decorators: []
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: list
      type: builtin
    - target: graphrag/index/operations/chunk_text/strategies.py::run_sentences
      type: internal
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
    called_by: []
  - name: test_mixed_whitespace_handling
    start_line: 43
    end_line: 48
    code: "def test_mixed_whitespace_handling(self):\n        \"\"\"Test input with\
      \ irregular whitespace\"\"\"\n        input = [\"   Sentence with spaces.  Another\
      \ one!   \"]\n        chunks = list(run_sentences(input, ChunkingConfig(), Mock()))\n\
      \        assert chunks[0].text_chunk == \"   Sentence with spaces.\"\n     \
      \   assert chunks[1].text_chunk == \"Another one!\""
    signature: def test_mixed_whitespace_handling(self)
    decorators: []
    raises: []
    calls:
    - target: list
      type: builtin
    - target: graphrag/index/operations/chunk_text/strategies.py::run_sentences
      type: internal
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: unittest.mock::Mock
      type: stdlib
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
    called_by: []
  - name: test_basic_functionality
    start_line: 53
    end_line: 72
    code: "def test_basic_functionality(self, mock_get_encoding):\n        mock_encoder\
      \ = Mock()\n        mock_encoder.encode.side_effect = lambda x: list(x.encode())\n\
      \        mock_encoder.decode.side_effect = lambda x: bytes(x).decode()\n   \
      \     mock_get_encoding.return_value = mock_encoder\n\n        # Input and config\n\
      \        input = [\n            \"Marley was dead: to begin with. There is no\
      \ doubt whatever about that. The register of his burial was signed by the clergyman,\
      \ the clerk, the undertaker, and the chief mourner. Scrooge signed it. And Scrooge's\
      \ name was good upon 'Change, for anything he chose to put his hand to.\"\n\
      \        ]\n        config = ChunkingConfig(size=5, overlap=1, encoding_model=\"\
      fake-encoding\")\n        tick = Mock()\n\n        # Run the function\n    \
      \    chunks = list(run_tokens(input, config, tick))\n\n        # Verify output\n\
      \        assert len(chunks) > 0\n        assert all(isinstance(chunk, TextChunk)\
      \ for chunk in chunks)\n        tick.assert_called_once_with(1)"
    signature: def test_basic_functionality(self, mock_get_encoding)
    decorators:
    - '@patch("tiktoken.get_encoding")'
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: list
      type: builtin
    - target: x.encode
      type: unresolved
    - target: bytes(x).decode
      type: unresolved
    - target: bytes
      type: builtin
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: graphrag/index/operations/chunk_text/strategies.py::run_tokens
      type: internal
    - target: len
      type: builtin
    - target: all
      type: builtin
    - target: isinstance
      type: builtin
    - target: tick.assert_called_once_with
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
    called_by: []
  - name: test_non_string_input
    start_line: 75
    end_line: 90
    code: "def test_non_string_input(self, mock_get_encoding):\n        \"\"\"Test\
      \ handling of non-string input (e.g., numbers).\"\"\"\n        mock_encoder\
      \ = Mock()\n        mock_encoder.encode.side_effect = lambda x: list(str(x).encode())\n\
      \        mock_encoder.decode.side_effect = lambda x: bytes(x).decode()\n   \
      \     mock_get_encoding.return_value = mock_encoder\n\n        input = [123]\
      \  # Non-string input\n        config = ChunkingConfig(size=5, overlap=1, encoding_model=\"\
      fake-encoding\")\n        tick = Mock()\n\n        chunks = list(run_tokens(input,\
      \ config, tick))  # type: ignore\n\n        # Verify non-string input is handled\n\
      \        assert len(chunks) > 0\n        assert \"123\" in chunks[0].text_chunk"
    signature: def test_non_string_input(self, mock_get_encoding)
    decorators:
    - '@patch("tiktoken.get_encoding")'
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: list
      type: builtin
    - target: str(x).encode
      type: unresolved
    - target: str
      type: builtin
    - target: bytes(x).decode
      type: unresolved
    - target: bytes
      type: builtin
    - target: graphrag/config/models/chunking_config.py::ChunkingConfig
      type: internal
    - target: graphrag/index/operations/chunk_text/strategies.py::run_tokens
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
    called_by: []
  - name: test_get_encoding_fn_encode
    start_line: 94
    end_line: 109
    code: "def test_get_encoding_fn_encode(mock_get_encoding):\n    # Create a mock\
      \ encoding object with encode and decode methods\n    mock_encoding = Mock()\n\
      \    mock_encoding.encode = Mock(return_value=[1, 2, 3])\n    mock_encoding.decode\
      \ = Mock(return_value=\"decoded text\")\n\n    # Configure the mock_get_encoding\
      \ to return the mock encoding object\n    mock_get_encoding.return_value = mock_encoding\n\
      \n    # Call the function to get encode and decode functions\n    encode, _\
      \ = get_encoding_fn(\"mock_encoding\")\n\n    # Test the encode function\n \
      \   encoded_text = encode(\"test text\")\n    assert encoded_text == [1, 2,\
      \ 3]\n    mock_encoding.encode.assert_called_once_with(\"test text\")"
    signature: def test_get_encoding_fn_encode(mock_get_encoding)
    decorators:
    - '@patch("tiktoken.get_encoding")'
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
      type: internal
    - target: encode
      type: ambiguous
      candidates: *id002
    - target: mock_encoding.encode.assert_called_once_with
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_encode
    called_by: []
  - name: test_get_encoding_fn_decode
    start_line: 113
    end_line: 127
    code: "def test_get_encoding_fn_decode(mock_get_encoding):\n    # Create a mock\
      \ encoding object with encode and decode methods\n    mock_encoding = Mock()\n\
      \    mock_encoding.encode = Mock(return_value=[1, 2, 3])\n    mock_encoding.decode\
      \ = Mock(return_value=\"decoded text\")\n\n    # Configure the mock_get_encoding\
      \ to return the mock encoding object\n    mock_get_encoding.return_value = mock_encoding\n\
      \n    # Call the function to get encode and decode functions\n    _, decode\
      \ = get_encoding_fn(\"mock_encoding\")\n\n    decoded_text = decode([1, 2, 3])\n\
      \    assert decoded_text == \"decoded text\"\n    mock_encoding.decode.assert_called_once_with([1,\
      \ 2, 3])"
    signature: def test_get_encoding_fn_decode(mock_get_encoding)
    decorators:
    - '@patch("tiktoken.get_encoding")'
    raises: []
    calls:
    - target: unittest.mock::Mock
      type: stdlib
    - target: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
      type: internal
    - target: decode
      type: ambiguous
      candidates:
      - graphrag/index/operations/chunk_text/strategies.py::decode
      - graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.decode
      - graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.decode
      - graphrag/tokenizer/tokenizer.py::Tokenizer.decode
      - tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.decode
      - tests/unit/indexing/text_splitting/test_text_splitting.py::decode
    - target: mock_encoding.decode.assert_called_once_with
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_decode
    called_by: []
- file_name: tests/unit/indexing/test_init_content.py
  imports:
  - module: re
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: typing
    name: cast
    alias: null
  - module: yaml
    name: null
    alias: null
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.init_content
    name: INIT_YAML
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: test_init_yaml
    start_line: 14
    end_line: 17
    code: "def test_init_yaml():\n    data = yaml.load(INIT_YAML, Loader=yaml.FullLoader)\n\
      \    config = create_graphrag_config(data)\n    GraphRagConfig.model_validate(config,\
      \ strict=True)"
    signature: def test_init_yaml()
    decorators: []
    raises: []
    calls:
    - target: yaml::load
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::GraphRagConfig::model_validate
      type: external
    visibility: public
    node_id: tests/unit/indexing/test_init_content.py::test_init_yaml
    called_by: []
  - name: test_init_yaml_uncommented
    start_line: 20
    end_line: 31
    code: "def test_init_yaml_uncommented():\n    lines = INIT_YAML.splitlines()\n\
      \    lines = [line for line in lines if \"##\" not in line]\n\n    def uncomment_line(line:\
      \ str) -> str:\n        leading_whitespace = cast(\"Any\", re.search(r\"^(\\\
      s*)\", line)).group(1)\n        return re.sub(r\"^\\s*# \", leading_whitespace,\
      \ line, count=1)\n\n    content = \"\\n\".join([uncomment_line(line) for line\
      \ in lines])\n    data = yaml.load(content, Loader=yaml.FullLoader)\n    config\
      \ = create_graphrag_config(data)\n    GraphRagConfig.model_validate(config,\
      \ strict=True)"
    signature: def test_init_yaml_uncommented()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/init_content.py::INIT_YAML::splitlines
      type: external
    - target: '"\n".join'
      type: unresolved
    - target: tests/unit/indexing/test_init_content.py::uncomment_line
      type: internal
    - target: yaml::load
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/config/models/graph_rag_config.py::GraphRagConfig::model_validate
      type: external
    visibility: public
    node_id: tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
    called_by: []
  - name: uncomment_line
    start_line: 24
    end_line: 26
    code: "def uncomment_line(line: str) -> str:\n        leading_whitespace = cast(\"\
      Any\", re.search(r\"^(\\s*)\", line)).group(1)\n        return re.sub(r\"^\\\
      s*# \", leading_whitespace, line, count=1)"
    signature: 'def uncomment_line(line: str) -> str'
    decorators: []
    raises: []
    calls:
    - target: cast("Any", re.search(r"^(\s*)", line)).group
      type: unresolved
    - target: typing::cast
      type: stdlib
    - target: re::search
      type: stdlib
    - target: re::sub
      type: stdlib
    visibility: public
    node_id: tests/unit/indexing/test_init_content.py::uncomment_line
    called_by:
    - source: tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
      type: internal
- file_name: tests/unit/indexing/text_splitting/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/text_splitting/test_text_splitting.py
  imports:
  - module: unittest
    name: mock
    alias: null
  - module: unittest.mock
    name: MagicMock
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: tiktoken
    name: null
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: NoopTextSplitter
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: TokenChunkerOptions
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: TokenTextSplitter
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: split_multiple_texts_on_tokens
    alias: null
  - module: graphrag.index.text_splitting.text_splitting
    name: split_single_text_on_tokens
    alias: null
  functions:
  - name: test_noop_text_splitter
    start_line: 19
    end_line: 23
    code: "def test_noop_text_splitter() -> None:\n    splitter = NoopTextSplitter()\n\
      \n    assert list(splitter.split_text(\"some text\")) == [\"some text\"]\n \
      \   assert list(splitter.split_text([\"some\", \"text\"])) == [\"some\", \"\
      text\"]"
    signature: def test_noop_text_splitter() -> None
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter
      type: internal
    - target: list
      type: builtin
    - target: splitter.split_text
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_noop_text_splitter
    called_by: []
  - name: encode
    start_line: 27
    end_line: 28
    code: "def encode(self, text):\n        return [ord(char) for char in text]"
    signature: def encode(self, text)
    decorators: []
    raises: []
    calls:
    - target: ord
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.encode
    called_by: []
  - name: decode
    start_line: 30
    end_line: 31
    code: "def decode(self, token_ids):\n        return \"\".join(chr(id) for id in\
      \ token_ids)"
    signature: def decode(self, token_ids)
    decorators: []
    raises: []
    calls:
    - target: '"".join'
      type: unresolved
    - target: chr
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.decode
    called_by: []
  - name: test_split_text_str_empty
    start_line: 34
    end_line: 38
    code: "def test_split_text_str_empty():\n    splitter = TokenTextSplitter(chunk_size=5,\
      \ chunk_overlap=2)\n    result = splitter.split_text(\"\")\n\n    assert result\
      \ == []"
    signature: def test_split_text_str_empty()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: splitter.split_text
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_empty
    called_by: []
  - name: test_split_text_str_bool
    start_line: 41
    end_line: 45
    code: "def test_split_text_str_bool():\n    splitter = TokenTextSplitter(chunk_size=5,\
      \ chunk_overlap=2)\n    result = splitter.split_text(None)  # type: ignore\n\
      \n    assert result == []"
    signature: def test_split_text_str_bool()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: splitter.split_text
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_bool
    called_by: []
  - name: test_split_text_str_int
    start_line: 48
    end_line: 51
    code: "def test_split_text_str_int():\n    splitter = TokenTextSplitter(chunk_size=5,\
      \ chunk_overlap=2)\n    with pytest.raises(TypeError):\n        splitter.split_text(123)\
      \  # type: ignore"
    signature: def test_split_text_str_int()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: pytest::raises
      type: external
    - target: splitter.split_text
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_int
    called_by: []
  - name: test_split_text_large_input
    start_line: 55
    end_line: 63
    code: "def test_split_text_large_input(mock_split):\n    large_text = \"a\" *\
      \ 10_000\n    mock_split.return_value = [\"chunk\"] * 2_000\n    splitter =\
      \ TokenTextSplitter(chunk_size=5, chunk_overlap=2)\n\n    result = splitter.split_text(large_text)\n\
      \n    assert len(result) == 2_000, \"Large input was not split correctly\"\n\
      \    mock_split.assert_called_once()"
    signature: def test_split_text_large_input(mock_split)
    decorators:
    - '@mock.patch("graphrag.index.text_splitting.text_splitting.split_single_text_on_tokens")'
    raises: []
    calls:
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: splitter.split_text
      type: unresolved
    - target: len
      type: builtin
    - target: mock_split.assert_called_once
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_large_input
    called_by: []
  - name: test_token_text_splitter
    start_line: 68
    end_line: 80
    code: "def test_token_text_splitter(mock_tokenizer, mock_split_text):\n    text\
      \ = \"chunk1 chunk2 chunk3\"\n    expected_chunks = [\"chunk1\", \"chunk2\"\
      , \"chunk3\"]\n\n    mocked_tokenizer = MagicMock()\n    mock_tokenizer.return_value\
      \ = mocked_tokenizer\n    mock_split_text.return_value = expected_chunks\n\n\
      \    splitter = TokenTextSplitter()\n\n    splitter.split_text([\"chunk1\",\
      \ \"chunk2\", \"chunk3\"])\n\n    mock_split_text.assert_called_once_with(text=text,\
      \ tokenizer=mocked_tokenizer)"
    signature: def test_token_text_splitter(mock_tokenizer, mock_split_text)
    decorators:
    - '@mock.patch("graphrag.index.text_splitting.text_splitting.split_single_text_on_tokens")'
    - '@mock.patch("graphrag.index.text_splitting.text_splitting.TokenChunkerOptions")'
    raises: []
    calls:
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
      type: internal
    - target: splitter.split_text
      type: unresolved
    - target: mock_split_text.assert_called_once_with
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_token_text_splitter
    called_by: []
  - name: test_split_single_text_on_tokens
    start_line: 83
    end_line: 110
    code: "def test_split_single_text_on_tokens():\n    text = \"This is a test text,\
      \ meaning to be taken seriously by this test only.\"\n    mocked_tokenizer =\
      \ MockTokenizer()\n    tokenizer = TokenChunkerOptions(\n        chunk_overlap=5,\n\
      \        tokens_per_chunk=10,\n        decode=mocked_tokenizer.decode,\n   \
      \     encode=lambda text: mocked_tokenizer.encode(text),\n    )\n\n    expected_splits\
      \ = [\n        \"This is a \",\n        \"is a test \",\n        \"test text,\"\
      ,\n        \"text, mean\",\n        \" meaning t\",\n        \"ing to be \"\
      ,\n        \"o be taken\",\n        \"taken seri\",  # cspell:disable-line\n\
      \        \" seriously\",\n        \"ously by t\",  # cspell:disable-line\n \
      \       \" by this t\",\n        \"his test o\",\n        \"est only.\",\n \
      \   ]\n\n    result = split_single_text_on_tokens(text=text, tokenizer=tokenizer)\n\
      \    assert result == expected_splits"
    signature: def test_split_single_text_on_tokens()
    decorators: []
    raises: []
    calls:
    - target: MockTokenizer
      type: unresolved
    - target: graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
      type: internal
    - target: mocked_tokenizer.encode
      type: unresolved
    - target: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
      type: internal
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens
    called_by: []
  - name: test_split_multiple_texts_on_tokens
    start_line: 113
    end_line: 129
    code: "def test_split_multiple_texts_on_tokens():\n    texts = [\n        \"This\
      \ is a test text, meaning to be taken seriously by this test only.\",\n    \
      \    \"This is th second text, meaning to be taken seriously by this test only.\"\
      ,\n    ]\n\n    mocked_tokenizer = MockTokenizer()\n    mock_tick = MagicMock()\n\
      \    tokenizer = TokenChunkerOptions(\n        chunk_overlap=5,\n        tokens_per_chunk=10,\n\
      \        decode=mocked_tokenizer.decode,\n        encode=lambda text: mocked_tokenizer.encode(text),\n\
      \    )\n\n    split_multiple_texts_on_tokens(texts, tokenizer, tick=mock_tick)\n\
      \    mock_tick.assert_called()"
    signature: def test_split_multiple_texts_on_tokens()
    decorators: []
    raises: []
    calls:
    - target: MockTokenizer
      type: unresolved
    - target: unittest.mock::MagicMock
      type: stdlib
    - target: graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
      type: internal
    - target: mocked_tokenizer.encode
      type: unresolved
    - target: graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
      type: internal
    - target: mock_tick.assert_called
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_multiple_texts_on_tokens
    called_by: []
  - name: test_split_single_text_on_tokens_no_overlap
    start_line: 132
    end_line: 170
    code: "def test_split_single_text_on_tokens_no_overlap():\n    text = \"This is\
      \ a test text, meaning to be taken seriously by this test only.\"\n    enc =\
      \ tiktoken.get_encoding(\"cl100k_base\")\n\n    def encode(text: str) -> list[int]:\n\
      \        if not isinstance(text, str):\n            text = f\"{text}\"\n   \
      \     return enc.encode(text)\n\n    def decode(tokens: list[int]) -> str:\n\
      \        return enc.decode(tokens)\n\n    tokenizer = TokenChunkerOptions(\n\
      \        chunk_overlap=1,\n        tokens_per_chunk=2,\n        decode=decode,\n\
      \        encode=lambda text: encode(text),\n    )\n\n    expected_splits = [\n\
      \        \"This is\",\n        \" is a\",\n        \" a test\",\n        \"\
      \ test text\",\n        \" text,\",\n        \", meaning\",\n        \" meaning\
      \ to\",\n        \" to be\",\n        \" be taken\",  # cspell:disable-line\n\
      \        \" taken seriously\",  # cspell:disable-line\n        \" seriously\
      \ by\",\n        \" by this\",  # cspell:disable-line\n        \" this test\"\
      ,\n        \" test only\",\n        \" only.\",\n    ]\n\n    result = split_single_text_on_tokens(text=text,\
      \ tokenizer=tokenizer)\n    assert result == expected_splits"
    signature: def test_split_single_text_on_tokens_no_overlap()
    decorators: []
    raises: []
    calls:
    - target: tiktoken::get_encoding
      type: external
    - target: graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
      type: internal
    - target: tests/unit/indexing/text_splitting/test_text_splitting.py::encode
      type: internal
    - target: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
      type: internal
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
    called_by: []
  - name: encode
    start_line: 136
    end_line: 139
    code: "def encode(text: str) -> list[int]:\n        if not isinstance(text, str):\n\
      \            text = f\"{text}\"\n        return enc.encode(text)"
    signature: 'def encode(text: str) -> list[int]'
    decorators: []
    raises: []
    calls:
    - target: isinstance
      type: builtin
    - target: enc.encode
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::encode
    called_by:
    - source: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
      type: internal
  - name: decode
    start_line: 141
    end_line: 142
    code: "def decode(tokens: list[int]) -> str:\n        return enc.decode(tokens)"
    signature: 'def decode(tokens: list[int]) -> str'
    decorators: []
    raises: []
    calls:
    - target: enc.decode
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::decode
    called_by: []
- file_name: tests/unit/indexing/verbs/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/entities/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/entities/extraction/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/entities/extraction/strategies/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  imports:
  - module: unittest
    name: null
    alias: null
  - module: graphrag.index.operations.extract_graph.graph_intelligence_strategy
    name: run_extract_graph
    alias: null
  - module: graphrag.index.operations.extract_graph.typing
    name: Document
    alias: null
  - module: tests.unit.indexing.verbs.helpers.mock_llm
    name: create_mock_llm
    alias: null
  functions:
  - name: test_run_extract_graph_single_document_correct_entities_returned
    start_line: 15
    end_line: 45
    code: "async def test_run_extract_graph_single_document_correct_entities_returned(self):\n\
      \        results = await run_extract_graph(\n            docs=[Document(\"test_text\"\
      , \"1\")],\n            entity_types=[\"person\"],\n            args={\n   \
      \             \"max_gleanings\": 0,\n                \"summarize_descriptions\"\
      : False,\n            },\n            model=create_mock_llm(\n             \
      \   responses=[\n                    \"\"\"\n                    (\"entity\"\
      <|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is a test company)\n            \
      \        ##\n                    (\"entity\"<|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2\
      \ owns TEST_ENTITY_1 and also shares an address with TEST_ENTITY_1)\n      \
      \              ##\n                    (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3\
      \ is director of TEST_ENTITY_1)\n                    ##\n                  \
      \  (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and TEST_ENTITY_2\
      \ are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2 and the two\
      \ companies also share the same address)<|>2)\n                    ##\n    \
      \                (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
      \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
      \                    \"\"\".strip()\n                ],\n                name=\"\
      test_run_extract_graph_single_document_correct_entities_returned\",\n      \
      \      ),\n        )\n\n        # self.assertItemsEqual isn't available yet,\
      \ or I am just silly\n        # so we sort the lists and compare them\n    \
      \    assert sorted([\"TEST_ENTITY_1\", \"TEST_ENTITY_2\", \"TEST_ENTITY_3\"\
      ]) == sorted([\n            entity[\"title\"] for entity in results.entities\n\
      \        ])"
    signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    - target: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
      type: internal
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
        \ an address with TEST_ENTITY_1)\n                    ##\n               \
        \     (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3 is director of\
        \ TEST_ENTITY_1)\n                    ##\n                    (\"relationship\"\
        <|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and TEST_ENTITY_2 are related\
        \ because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2 and the two companies\
        \ also share the same address)<|>2)\n                    ##\n            \
        \        (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
        \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
        \                    \"\"\".strip"
      type: unresolved
    - target: sorted
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
    called_by: []
  - name: test_run_extract_graph_multiple_documents_correct_entities_returned
    start_line: 47
    end_line: 83
    code: "async def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
      \        self,\n    ):\n        results = await run_extract_graph(\n       \
      \     docs=[Document(\"text_1\", \"1\"), Document(\"text_2\", \"2\")],\n   \
      \         entity_types=[\"person\"],\n            args={\n                \"\
      max_gleanings\": 0,\n                \"summarize_descriptions\": False,\n  \
      \          },\n            model=create_mock_llm(\n                responses=[\n\
      \                    \"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
      \ is a test company)\n                    ##\n                    (\"entity\"\
      <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
      \ an address with TEST_ENTITY_1)\n                    ##\n                 \
      \   (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and TEST_ENTITY_2\
      \ are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2 and the two\
      \ companies also share the same address)<|>2)\n                    ##\n    \
      \                \"\"\".strip(),\n                    \"\"\"\n             \
      \       (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is a test company)\n\
      \                    ##\n                    (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3\
      \ is director of TEST_ENTITY_1)\n                    ##\n                  \
      \  (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1 and TEST_ENTITY_3\
      \ are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n   \
      \                 \"\"\".strip(),\n                ],\n                name=\"\
      test_run_extract_graph_multiple_documents_correct_entities_returned\",\n   \
      \         ),\n        )\n\n        # self.assertItemsEqual isn't available yet,\
      \ or I am just silly\n        # so we sort the lists and compare them\n    \
      \    assert sorted([\"TEST_ENTITY_1\", \"TEST_ENTITY_2\", \"TEST_ENTITY_3\"\
      ]) == sorted([\n            entity[\"title\"] for entity in results.entities\n\
      \        ])"
    signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
      \        self,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    - target: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
      type: internal
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
        \ an address with TEST_ENTITY_1)\n                    ##\n               \
        \     (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and\
        \ TEST_ENTITY_2 are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2\
        \ and the two companies also share the same address)<|>2)\n              \
        \      ##\n                    \"\"\".strip"
      type: unresolved
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3 is director of TEST_ENTITY_1)\n\
        \                    ##\n                    (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
        \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
        \                    \"\"\".strip"
      type: unresolved
    - target: sorted
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
    called_by: []
  - name: test_run_extract_graph_multiple_documents_correct_edges_returned
    start_line: 85
    end_line: 125
    code: "async def test_run_extract_graph_multiple_documents_correct_edges_returned(self):\n\
      \        results = await run_extract_graph(\n            docs=[Document(\"text_1\"\
      , \"1\"), Document(\"text_2\", \"2\")],\n            entity_types=[\"person\"\
      ],\n            args={\n                \"max_gleanings\": 0,\n            \
      \    \"summarize_descriptions\": False,\n            },\n            model=create_mock_llm(\n\
      \                responses=[\n                    \"\"\"\n                 \
      \   (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is a test company)\n\
      \                    ##\n                    (\"entity\"<|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2\
      \ owns TEST_ENTITY_1 and also shares an address with TEST_ENTITY_1)\n      \
      \              ##\n                    (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1\
      \ and TEST_ENTITY_2 are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2\
      \ and the two companies also share the same address)<|>2)\n                \
      \    ##\n                    \"\"\".strip(),\n                    \"\"\"\n \
      \                   (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is\
      \ a test company)\n                    ##\n                    (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3\
      \ is director of TEST_ENTITY_1)\n                    ##\n                  \
      \  (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1 and TEST_ENTITY_3\
      \ are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n   \
      \                 \"\"\".strip(),\n                ],\n                name=\"\
      test_run_extract_graph_multiple_documents_correct_edges_returned\",\n      \
      \      ),\n        )\n\n        # self.assertItemsEqual isn't available yet,\
      \ or I am just silly\n        # so we sort the lists and compare them\n    \
      \    graph = results.graph\n        assert graph is not None, \"No graph returned!\"\
      \n\n        # convert to strings for more visual comparison\n        edges_str\
      \ = sorted([f\"{edge[0]} -> {edge[1]}\" for edge in graph.edges])\n        assert\
      \ edges_str == sorted([\n            \"TEST_ENTITY_1 -> TEST_ENTITY_2\",\n \
      \           \"TEST_ENTITY_1 -> TEST_ENTITY_3\",\n        ])"
    signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    - target: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
      type: internal
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
        \ an address with TEST_ENTITY_1)\n                    ##\n               \
        \     (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and\
        \ TEST_ENTITY_2 are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2\
        \ and the two companies also share the same address)<|>2)\n              \
        \      ##\n                    \"\"\".strip"
      type: unresolved
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3 is director of TEST_ENTITY_1)\n\
        \                    ##\n                    (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
        \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
        \                    \"\"\".strip"
      type: unresolved
    - target: sorted
      type: builtin
    visibility: public
    node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
    called_by: []
  - name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    start_line: 127
    end_line: 174
    code: "async def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
      \        self,\n    ):\n        results = await run_extract_graph(\n       \
      \     docs=[Document(\"text_1\", \"1\"), Document(\"text_2\", \"2\")],\n   \
      \         entity_types=[\"person\"],\n            args={\n                \"\
      max_gleanings\": 0,\n                \"summarize_descriptions\": False,\n  \
      \          },\n            model=create_mock_llm(\n                responses=[\n\
      \                    \"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
      \ is a test company)\n                    ##\n                    (\"entity\"\
      <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
      \ an address with TEST_ENTITY_1)\n                    ##\n                 \
      \   (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and TEST_ENTITY_2\
      \ are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2 and the two\
      \ companies also share the same address)<|>2)\n                    ##\n    \
      \                \"\"\".strip(),\n                    \"\"\"\n             \
      \       (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is a test company)\n\
      \                    ##\n                    (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3\
      \ is director of TEST_ENTITY_1)\n                    ##\n                  \
      \  (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1 and TEST_ENTITY_3\
      \ are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n   \
      \                 \"\"\".strip(),\n                ],\n                name=\"\
      test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped\"\
      ,\n            ),\n        )\n\n        graph = results.graph\n        assert\
      \ graph is not None, \"No graph returned!\"\n\n        # TODO: The edges might\
      \ come back in any order, but we're assuming they're coming\n        # back\
      \ in the order that we passed in the docs, that might not be true\n        assert\
      \ (\n            graph.nodes[\"TEST_ENTITY_3\"].get(\"source_id\") == \"2\"\n\
      \        )  # TEST_ENTITY_3 should be in just 2\n        assert (\n        \
      \    graph.nodes[\"TEST_ENTITY_2\"].get(\"source_id\") == \"1\"\n        ) \
      \ # TEST_ENTITY_2 should be in just 1\n        ids_str = graph.nodes[\"TEST_ENTITY_1\"\
      ].get(\"source_id\") or \"\"\n        assert sorted(ids_str.split(\",\")) ==\
      \ sorted([\n            \"1\",\n            \"2\",\n        ])  # TEST_ENTITY_1\
      \ should be 1 and 2"
    signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
      \        self,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    - target: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
      type: internal
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
        \ an address with TEST_ENTITY_1)\n                    ##\n               \
        \     (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and\
        \ TEST_ENTITY_2 are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2\
        \ and the two companies also share the same address)<|>2)\n              \
        \      ##\n                    \"\"\".strip"
      type: unresolved
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3 is director of TEST_ENTITY_1)\n\
        \                    ##\n                    (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
        \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
        \                    \"\"\".strip"
      type: unresolved
    - target: graph.nodes["TEST_ENTITY_3"].get
      type: unresolved
    - target: graph.nodes["TEST_ENTITY_2"].get
      type: unresolved
    - target: graph.nodes["TEST_ENTITY_1"].get
      type: unresolved
    - target: sorted
      type: builtin
    - target: ids_str.split
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    called_by: []
  - name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    start_line: 176
    end_line: 218
    code: "async def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
      \        self,\n    ):\n        results = await run_extract_graph(\n       \
      \     docs=[Document(\"text_1\", \"1\"), Document(\"text_2\", \"2\")],\n   \
      \         entity_types=[\"person\"],\n            args={\n                \"\
      max_gleanings\": 0,\n                \"summarize_descriptions\": False,\n  \
      \          },\n            model=create_mock_llm(\n                responses=[\n\
      \                    \"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
      \ is a test company)\n                    ##\n                    (\"entity\"\
      <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
      \ an address with TEST_ENTITY_1)\n                    ##\n                 \
      \   (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and TEST_ENTITY_2\
      \ are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2 and the two\
      \ companies also share the same address)<|>2)\n                    ##\n    \
      \                \"\"\".strip(),\n                    \"\"\"\n             \
      \       (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1 is a test company)\n\
      \                    ##\n                    (\"entity\"<|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3\
      \ is director of TEST_ENTITY_1)\n                    ##\n                  \
      \  (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1 and TEST_ENTITY_3\
      \ are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n   \
      \                 \"\"\".strip(),\n                ],\n                name=\"\
      test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped\",\n\
      \            ),\n        )\n\n        graph = results.graph\n        assert\
      \ graph is not None, \"No graph returned!\"\n        edges = list(graph.edges(data=True))\n\
      \n        # should only have 2 edges\n        assert len(edges) == 2\n\n   \
      \     # Sort by source_id for consistent ordering\n        edge_source_ids =\
      \ sorted([edge[2].get(\"source_id\", \"\") for edge in edges])\n        assert\
      \ edge_source_ids[0].split(\",\") == [\"1\"]\n        assert edge_source_ids[1].split(\"\
      ,\") == [\"2\"]"
    signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
      \        self,\n    )"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
      type: internal
    - target: graphrag/index/operations/extract_graph/typing.py::Document
      type: internal
    - target: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
      type: internal
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_2<|>COMPANY<|>TEST_ENTITY_2 owns TEST_ENTITY_1 and also shares\
        \ an address with TEST_ENTITY_1)\n                    ##\n               \
        \     (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_2<|>TEST_ENTITY_1 and\
        \ TEST_ENTITY_2 are related because TEST_ENTITY_1 is 100% owned by TEST_ENTITY_2\
        \ and the two companies also share the same address)<|>2)\n              \
        \      ##\n                    \"\"\".strip"
      type: unresolved
    - target: "\"\"\"\n                    (\"entity\"<|>TEST_ENTITY_1<|>COMPANY<|>TEST_ENTITY_1\
        \ is a test company)\n                    ##\n                    (\"entity\"\
        <|>TEST_ENTITY_3<|>PERSON<|>TEST_ENTITY_3 is director of TEST_ENTITY_1)\n\
        \                    ##\n                    (\"relationship\"<|>TEST_ENTITY_1<|>TEST_ENTITY_3<|>TEST_ENTITY_1\
        \ and TEST_ENTITY_3 are related because TEST_ENTITY_3 is director of TEST_ENTITY_1<|>1))\n\
        \                    \"\"\".strip"
      type: unresolved
    - target: list
      type: builtin
    - target: graph.edges
      type: unresolved
    - target: len
      type: builtin
    - target: sorted
      type: builtin
    - target: edge[2].get
      type: unresolved
    - target: edge_source_ids[0].split
      type: unresolved
    - target: edge_source_ids[1].split
      type: unresolved
    visibility: public
    node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    called_by: []
- file_name: tests/unit/indexing/verbs/helpers/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/indexing/verbs/helpers/mock_llm.py
  imports:
  - module: pydantic
    name: BaseModel
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.language_model.protocol.base
    name: ChatModel
    alias: null
  functions:
  - name: create_mock_llm
    start_line: 9
    end_line: 13
    code: "def create_mock_llm(responses: list[str | BaseModel], name: str = \"mock\"\
      ) -> ChatModel:\n    \"\"\"Creates a mock LLM that returns the given responses.\"\
      \"\"\n    return ModelManager().get_or_create_chat_model(\n        name, \"\
      mock_chat\", responses=responses\n    )"
    signature: 'def create_mock_llm(responses: list[str | BaseModel], name: str =
      "mock") -> ChatModel'
    decorators: []
    raises: []
    calls:
    - target: ModelManager().get_or_create_chat_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    visibility: public
    node_id: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
    called_by:
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
      type: internal
    - source: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
      type: internal
- file_name: tests/unit/litellm_services/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/litellm_services/test_rate_limiter.py
  imports:
  - module: threading
    name: null
    alias: null
  - module: time
    name: null
    alias: null
  - module: math
    name: ceil
    alias: null
  - module: queue
    name: Queue
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter
    name: RateLimiter
    alias: null
  - module: graphrag.language_model.providers.litellm.services.rate_limiter.rate_limiter_factory
    name: RateLimiterFactory
    alias: null
  - module: tests.unit.litellm_services.utils
    name: assert_max_num_values_per_period
    alias: null
  - module: tests.unit.litellm_services.utils
    name: assert_stagger
    alias: null
  - module: tests.unit.litellm_services.utils
    name: bin_time_intervals
    alias: null
  functions:
  - name: test_binning
    start_line: 35
    end_line: 46
    code: "def test_binning():\n    \"\"\"Test binning timings into 1-second intervals.\"\
      \"\"\n    values = [0.1, 0.2, 0.3, 0.4, 1.1, 1.2, 1.3, 1.4, 5.1]\n    binned_values\
      \ = bin_time_intervals(values, 1)\n    assert binned_values == [\n        [0.1,\
      \ 0.2, 0.3, 0.4],\n        [1.1, 1.2, 1.3, 1.4],\n        [],\n        [],\n\
      \        [],\n        [5.1],\n    ]"
    signature: def test_binning()
    decorators: []
    raises: []
    calls:
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_binning
    called_by: []
  - name: test_rate_limiter_validation
    start_line: 49
    end_line: 90
    code: "def test_rate_limiter_validation():\n    \"\"\"Test that the rate limiter\
      \ can be created with valid parameters.\"\"\"\n\n    # Valid parameters\n  \
      \  rate_limiter = rate_limiter_factory.create(\n        strategy=\"static\"\
      , rpm=60, tpm=10000, period_in_seconds=60\n    )\n    assert rate_limiter is\
      \ not None\n\n    # Invalid strategy\n    with pytest.raises(\n        ValueError,\n\
      \        match=r\"Strategy 'invalid_strategy' is not registered.\",\n    ):\n\
      \        rate_limiter_factory.create(strategy=\"invalid_strategy\", rpm=60,\
      \ tpm=10000)\n\n    # Both rpm and tpm are None\n    with pytest.raises(\n \
      \       ValueError,\n        match=r\"Both TPM and RPM cannot be None \\(disabled\\\
      ), one or both must be set to a positive integer.\",\n    ):\n        rate_limiter_factory.create(strategy=\"\
      static\")\n\n    # Invalid rpm\n    with pytest.raises(\n        ValueError,\n\
      \        match=r\"RPM and TPM must be either None \\(disabled\\) or positive\
      \ integers.\",\n    ):\n        rate_limiter_factory.create(strategy=\"static\"\
      , rpm=-10)\n\n    # Invalid tpm\n    with pytest.raises(\n        ValueError,\n\
      \        match=r\"RPM and TPM must be either None \\(disabled\\) or positive\
      \ integers.\",\n    ):\n        rate_limiter_factory.create(strategy=\"static\"\
      , tpm=-10)\n\n    # Invalid period_in_seconds\n    with pytest.raises(\n   \
      \     ValueError, match=r\"Period in seconds must be a positive integer.\"\n\
      \    ):\n        rate_limiter_factory.create(strategy=\"static\", rpm=10, period_in_seconds=-10)"
    signature: def test_rate_limiter_validation()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: pytest::raises
      type: external
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rate_limiter_validation
    called_by: []
  - name: test_rpm
    start_line: 93
    end_line: 118
    code: "def test_rpm():\n    \"\"\"Test that the rate limiter enforces RPM limits.\"\
      \"\"\n    rate_limiter = rate_limiter_factory.create(\n        strategy=\"static\"\
      , rpm=_rpm, period_in_seconds=_period_in_seconds\n    )\n\n    time_values:\
      \ list[float] = []\n    start_time = time.time()\n    for _ in range(_num_requests):\n\
      \        with rate_limiter.acquire(token_count=_tokens_per_request):\n     \
      \       time_values.append(time.time() - start_time)\n\n    assert len(time_values)\
      \ == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10 and _rpm =\
      \ 4, we expect the requests to be\n    distributed across ceil(10/4) = 3 bins:\n\
      \    with a stagger of 1/4 = 0.25 seconds between requests.\n    \"\"\"\n\n\
      \    expected_num_bins = ceil(_num_requests / _rpm)\n    assert len(binned_time_values)\
      \ == expected_num_bins\n\n    assert_max_num_values_per_period(binned_time_values,\
      \ _rpm)\n    assert_stagger(time_values, _stagger)"
    signature: def test_rpm()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: range
      type: builtin
    - target: rate_limiter.acquire
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: len
      type: builtin
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_stagger
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
    called_by: []
  - name: test_tpm
    start_line: 121
    end_line: 146
    code: "def test_tpm():\n    \"\"\"Test that the rate limiter enforces TPM limits.\"\
      \"\"\n    rate_limiter = rate_limiter_factory.create(\n        strategy=\"static\"\
      , tpm=_tpm, period_in_seconds=_period_in_seconds\n    )\n\n    time_values:\
      \ list[float] = []\n    start_time = time.time()\n    for _ in range(_num_requests):\n\
      \        with rate_limiter.acquire(token_count=_tokens_per_request):\n     \
      \       time_values.append(time.time() - start_time)\n\n    assert len(time_values)\
      \ == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10, _tpm = 75\
      \ and _tokens_per_request = 25, we expect the requests to be\n    distributed\
      \ across ceil( (10 * 25) / 75) ) = 4 bins\n    and max requests per bin = (75\
      \ / 25) = 3 requests per bin.\n    \"\"\"\n\n    expected_num_bins = ceil((_num_requests\
      \ * _tokens_per_request) / _tpm)\n    assert len(binned_time_values) == expected_num_bins\n\
      \n    max_num_of_requests_per_bin = _tpm // _tokens_per_request\n    assert_max_num_values_per_period(binned_time_values,\
      \ max_num_of_requests_per_bin)"
    signature: def test_tpm()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: range
      type: builtin
    - target: rate_limiter.acquire
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: len
      type: builtin
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm
    called_by: []
  - name: test_token_in_request_exceeds_tpm
    start_line: 149
    end_line: 175
    code: "def test_token_in_request_exceeds_tpm():\n    \"\"\"Test that the rate\
      \ limiter allows for requests that use more tokens than the TPM.\n\n    A rate\
      \ limiter could be configured with a tpm of 1000 but a request may use 2000\
      \ tokens,\n    greater than the tpm limit but still below the context window\
      \ limit of the underlying model.\n    In this case, the request should still\
      \ be allowed to proceed but may take up its own rate limit bin.\n    \"\"\"\n\
      \    rate_limiter = rate_limiter_factory.create(\n        strategy=\"static\"\
      , tpm=_tpm, period_in_seconds=_period_in_seconds\n    )\n\n    time_values:\
      \ list[float] = []\n    start_time = time.time()\n    for _ in range(2):\n \
      \       with rate_limiter.acquire(token_count=_tpm * 2):\n            time_values.append(time.time()\
      \ - start_time)\n\n    assert len(time_values) == 2\n    binned_time_values\
      \ = bin_time_intervals(time_values, _period_in_seconds)\n\n    \"\"\"\n    Since\
      \ each request exceeds the tpm, we expect each request to still be fired off\
      \ but to be in its own bin\n    \"\"\"\n\n    assert len(binned_time_values)\
      \ == 2\n\n    assert_max_num_values_per_period(binned_time_values, 1)"
    signature: def test_token_in_request_exceeds_tpm()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: range
      type: builtin
    - target: rate_limiter.acquire
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: len
      type: builtin
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
    called_by: []
  - name: test_rpm_and_tpm_with_rpm_as_limiting_factor
    start_line: 178
    end_line: 204
    code: "def test_rpm_and_tpm_with_rpm_as_limiting_factor():\n    \"\"\"Test that\
      \ the rate limiter enforces RPM and TPM limits.\"\"\"\n    rate_limiter = rate_limiter_factory.create(\n\
      \        strategy=\"static\", rpm=_rpm, tpm=_tpm, period_in_seconds=_period_in_seconds\n\
      \    )\n\n    time_values: list[float] = []\n    start_time = time.time()\n\
      \    for _ in range(_num_requests):\n        # Use 0 tokens per request to simulate\
      \ RPM as the limiting factor\n        with rate_limiter.acquire(token_count=0):\n\
      \            time_values.append(time.time() - start_time)\n\n    assert len(time_values)\
      \ == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10 and _rpm =\
      \ 4, we expect the requests to be\n    distributed across ceil(10/4) = 3 bins:\n\
      \    with a stagger of 1/4 = 0.25 seconds between requests.\n    \"\"\"\n\n\
      \    expected_num_bins = ceil(_num_requests / _rpm)\n    assert len(binned_time_values)\
      \ == expected_num_bins\n\n    assert_max_num_values_per_period(binned_time_values,\
      \ _rpm)\n    assert_stagger(time_values, _stagger)"
    signature: def test_rpm_and_tpm_with_rpm_as_limiting_factor()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: range
      type: builtin
    - target: rate_limiter.acquire
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: len
      type: builtin
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_stagger
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
    called_by: []
  - name: test_rpm_and_tpm_with_tpm_as_limiting_factor
    start_line: 207
    end_line: 233
    code: "def test_rpm_and_tpm_with_tpm_as_limiting_factor():\n    \"\"\"Test that\
      \ the rate limiter enforces TPM limits.\"\"\"\n    rate_limiter = rate_limiter_factory.create(\n\
      \        strategy=\"static\", rpm=_rpm, tpm=_tpm, period_in_seconds=_period_in_seconds\n\
      \    )\n\n    time_values: list[float] = []\n    start_time = time.time()\n\
      \    for _ in range(_num_requests):\n        with rate_limiter.acquire(token_count=_tokens_per_request):\n\
      \            time_values.append(time.time() - start_time)\n\n    assert len(time_values)\
      \ == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10, _tpm = 75\
      \ and _tokens_per_request = 25, we expect the requests to be\n    distributed\
      \ across ceil( (10 * 25) / 75) ) = 4 bins\n    and max requests per bin = (75\
      \ / 25) = 3 requests per bin.\n    \"\"\"\n\n    expected_num_bins = ceil((_num_requests\
      \ * _tokens_per_request) / _tpm)\n    assert len(binned_time_values) == expected_num_bins\n\
      \n    max_num_of_requests_per_bin = _tpm // _tokens_per_request\n    assert_max_num_values_per_period(binned_time_values,\
      \ max_num_of_requests_per_bin)\n    assert_stagger(time_values, _stagger)"
    signature: def test_rpm_and_tpm_with_tpm_as_limiting_factor()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: range
      type: builtin
    - target: rate_limiter.acquire
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: len
      type: builtin
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_stagger
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
    called_by: []
  - name: _run_rate_limiter
    start_line: 236
    end_line: 248
    code: "def _run_rate_limiter(\n    rate_limiter: RateLimiter,\n    # Acquire cost\n\
      \    input_queue: Queue[int | None],\n    # time value\n    output_queue: Queue[float\
      \ | None],\n):\n    while True:\n        token_count = input_queue.get()\n \
      \       if token_count is None:\n            break\n        with rate_limiter.acquire(token_count=token_count):\n\
      \            output_queue.put(time.time())"
    signature: "def _run_rate_limiter(\n    rate_limiter: RateLimiter,\n    # Acquire\
      \ cost\n    input_queue: Queue[int | None],\n    # time value\n    output_queue:\
      \ Queue[float | None],\n)"
    decorators: []
    raises: []
    calls:
    - target: input_queue.get
      type: unresolved
    - target: rate_limiter.acquire
      type: unresolved
    - target: output_queue.put
      type: unresolved
    - target: time::time
      type: stdlib
    visibility: protected
    node_id: tests/unit/litellm_services/test_rate_limiter.py::_run_rate_limiter
    called_by: []
  - name: test_rpm_threaded
    start_line: 251
    end_line: 308
    code: "def test_rpm_threaded():\n    \"\"\"Test that the rate limiter enforces\
      \ RPM limits in a threaded environment.\"\"\"\n    rate_limiter = rate_limiter_factory.create(\n\
      \        strategy=\"static\", rpm=_rpm, tpm=_tpm, period_in_seconds=_period_in_seconds\n\
      \    )\n\n    input_queue: Queue[int | None] = Queue()\n    output_queue: Queue[float\
      \ | None] = Queue()\n\n    # Spin up threads for half the number of requests\n\
      \    threads = [\n        threading.Thread(\n            target=_run_rate_limiter,\n\
      \            args=(rate_limiter, input_queue, output_queue),\n        )\n  \
      \      for _ in range(_num_requests // 2)  # Create 5 threads\n    ]\n\n   \
      \ for thread in threads:\n        thread.start()\n\n    start_time = time.time()\n\
      \    for _ in range(_num_requests):\n        # Use 0 tokens per request to simulate\
      \ RPM as the limiting factor\n        input_queue.put(0)\n\n    # Signal threads\
      \ to stop\n    for _ in range(len(threads)):\n        input_queue.put(None)\n\
      \n    for thread in threads:\n        thread.join()\n\n    output_queue.put(None)\
      \  # Signal end of output\n\n    time_values = []\n    while True:\n       \
      \ time_value = output_queue.get()\n        if time_value is None:\n        \
      \    break\n        time_values.append(time_value - start_time)\n\n    time_values.sort()\n\
      \n    assert len(time_values) == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10 and _rpm =\
      \ 4, we expect the requests to be\n    distributed across ceil(10/4) = 3 bins:\n\
      \    with a stagger of 1/4 = 0.25 seconds between requests.\n    \"\"\"\n\n\
      \    expected_num_bins = ceil(_num_requests / _rpm)\n    assert len(binned_time_values)\
      \ == expected_num_bins\n\n    assert_max_num_values_per_period(binned_time_values,\
      \ _rpm)\n    assert_stagger(time_values, _stagger)"
    signature: def test_rpm_threaded()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: queue::Queue
      type: stdlib
    - target: threading::Thread
      type: stdlib
    - target: range
      type: builtin
    - target: thread.start
      type: unresolved
    - target: time::time
      type: stdlib
    - target: input_queue.put
      type: unresolved
    - target: len
      type: builtin
    - target: thread.join
      type: unresolved
    - target: output_queue.put
      type: unresolved
    - target: output_queue.get
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: time_values.sort
      type: unresolved
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_stagger
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
    called_by: []
  - name: test_tpm_threaded
    start_line: 311
    end_line: 368
    code: "def test_tpm_threaded():\n    \"\"\"Test that the rate limiter enforces\
      \ TPM limits in a threaded environment.\"\"\"\n    rate_limiter = rate_limiter_factory.create(\n\
      \        strategy=\"static\", rpm=_rpm, tpm=_tpm, period_in_seconds=_period_in_seconds\n\
      \    )\n\n    input_queue: Queue[int | None] = Queue()\n    output_queue: Queue[float\
      \ | None] = Queue()\n\n    # Spin up threads for half the number of requests\n\
      \    threads = [\n        threading.Thread(\n            target=_run_rate_limiter,\n\
      \            args=(rate_limiter, input_queue, output_queue),\n        )\n  \
      \      for _ in range(_num_requests // 2)  # Create 5 threads\n    ]\n\n   \
      \ for thread in threads:\n        thread.start()\n\n    start_time = time.time()\n\
      \    for _ in range(_num_requests):\n        input_queue.put(_tokens_per_request)\n\
      \n    # Signal threads to stop\n    for _ in range(len(threads)):\n        input_queue.put(None)\n\
      \n    for thread in threads:\n        thread.join()\n\n    output_queue.put(None)\
      \  # Signal end of output\n\n    time_values = []\n    while True:\n       \
      \ time_value = output_queue.get()\n        if time_value is None:\n        \
      \    break\n        time_values.append(time_value - start_time)\n\n    time_values.sort()\n\
      \n    assert len(time_values) == _num_requests\n    binned_time_values = bin_time_intervals(time_values,\
      \ _period_in_seconds)\n\n    \"\"\"\n    With _num_requests = 10, _tpm = 75\
      \ and _tokens_per_request = 25, we expect the requests to be\n    distributed\
      \ across ceil( (10 * 25) / 75) ) = 4 bins\n    and max requests per bin = (75\
      \ / 25) = 3 requests per bin.\n    \"\"\"\n\n    expected_num_bins = ceil((_num_requests\
      \ * _tokens_per_request) / _tpm)\n    assert len(binned_time_values) == expected_num_bins\n\
      \n    max_num_of_requests_per_bin = _tpm // _tokens_per_request\n    assert_max_num_values_per_period(binned_time_values,\
      \ max_num_of_requests_per_bin)\n    assert_stagger(time_values, _stagger)"
    signature: def test_tpm_threaded()
    decorators: []
    raises: []
    calls:
    - target: rate_limiter_factory.create
      type: unresolved
    - target: queue::Queue
      type: stdlib
    - target: threading::Thread
      type: stdlib
    - target: range
      type: builtin
    - target: thread.start
      type: unresolved
    - target: time::time
      type: stdlib
    - target: input_queue.put
      type: unresolved
    - target: len
      type: builtin
    - target: thread.join
      type: unresolved
    - target: output_queue.put
      type: unresolved
    - target: output_queue.get
      type: unresolved
    - target: time_values.append
      type: unresolved
    - target: time_values.sort
      type: unresolved
    - target: tests/unit/litellm_services/utils.py::bin_time_intervals
      type: internal
    - target: math::ceil
      type: stdlib
    - target: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
      type: internal
    - target: tests/unit/litellm_services/utils.py::assert_stagger
      type: internal
    visibility: public
    node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
    called_by: []
- file_name: tests/unit/litellm_services/test_retries.py
  imports:
  - module: time
    name: null
    alias: null
  - module: pytest
    name: null
    alias: null
  - module: graphrag.language_model.providers.litellm.services.retry.retry_factory
    name: RetryFactory
    alias: null
  functions:
  - name: test_retries
    start_line: 46
    end_line: 81
    code: "def test_retries(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None:\n    \"\"\"\n    Test various retry\
      \ strategies with various configurations.\n\n    Args\n    ----\n        strategy:\
      \ The retry strategy to use.\n        max_retries: The maximum number of retry\
      \ attempts.\n        max_retry_wait: The maximum wait time between retries.\n\
      \    \"\"\"\n    retry_service = retry_factory.create(\n        strategy=strategy,\n\
      \        max_retries=max_retries,\n        max_retry_wait=max_retry_wait,\n\
      \    )\n\n    retries = 0\n\n    def mock_func():\n        nonlocal retries\n\
      \        retries += 1\n        msg = \"Mock error for testing retries\"\n  \
      \      raise ValueError(msg)\n\n    start_time = time.time()\n    with pytest.raises(ValueError,\
      \ match=\"Mock error for testing retries\"):\n        retry_service.retry(func=mock_func)\n\
      \    elapsed_time = time.time() - start_time\n\n    # subtract 1 from retries\
      \ because the first call is not a retry\n    assert retries - 1 == max_retries,\
      \ f\"Expected {max_retries} retries, got {retries}\"\n    assert elapsed_time\
      \ >= expected_time, (\n        f\"Expected elapsed time >= {expected_time},\
      \ got {elapsed_time}\"\n    )"
    signature: "def test_retries(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None"
    decorators:
    - "@pytest.mark.parametrize(\n    (\"strategy\", \"max_retries\", \"max_retry_wait\"\
      , \"expected_time\"),\n    [\n        (\n            \"native\",\n         \
      \   3,  # 3 retries\n            0,  # native retry does not adhere to max_retry_wait\n\
      \            0,  # immediate retry, expect 0 seconds elapsed time\n        ),\n\
      \        (\n            \"exponential_backoff\",\n            3,  # 3 retries\n\
      \            0,  # exponential retry does not adhere to max_retry_wait\n   \
      \         14,  # (2^1 + jitter) + (2^2 + jitter) + (2^3 + jitter) = 2 + 4 +\
      \ 8 + 3*jitter = 14 seconds min total runtime\n        ),\n        (\n     \
      \       \"random_wait\",\n            3,  # 3 retries\n            2,  # random\
      \ wait [0, 2] seconds\n            0,  # unpredictable, don't know what the\
      \ total runtime will be\n        ),\n        (\n            \"incremental_wait\"\
      ,\n            3,  # 3 retries\n            3,  # wait for a max of 3 seconds\
      \ on a single retry.\n            6,  # Wait 3/3 * 1 on first retry, 3/3 * 2\
      \ on second, 3/3 * 3 on third, 1 + 2 + 3 = 6 seconds total runtime.\n      \
      \  ),\n    ],\n)"
    raises:
    - ValueError
    calls:
    - target: retry_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: pytest::raises
      type: external
    - target: retry_service.retry
      type: unresolved
    visibility: public
    node_id: tests/unit/litellm_services/test_retries.py::test_retries
    called_by: []
  - name: mock_func
    start_line: 66
    end_line: 70
    code: "def mock_func():\n        nonlocal retries\n        retries += 1\n    \
      \    msg = \"Mock error for testing retries\"\n        raise ValueError(msg)"
    signature: def mock_func()
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: tests/unit/litellm_services/test_retries.py::mock_func
    called_by: []
  - name: test_retries_async
    start_line: 113
    end_line: 148
    code: "async def test_retries_async(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None:\n    \"\"\"\n    Test various retry\
      \ strategies with various configurations.\n\n    Args\n    ----\n        strategy:\
      \ The retry strategy to use.\n        max_retries: The maximum number of retry\
      \ attempts.\n        max_retry_wait: The maximum wait time between retries.\n\
      \    \"\"\"\n    retry_service = retry_factory.create(\n        strategy=strategy,\n\
      \        max_retries=max_retries,\n        max_retry_wait=max_retry_wait,\n\
      \    )\n\n    retries = 0\n\n    async def mock_func():  # noqa: RUF029\n  \
      \      nonlocal retries\n        retries += 1\n        msg = \"Mock error for\
      \ testing retries\"\n        raise ValueError(msg)\n\n    start_time = time.time()\n\
      \    with pytest.raises(ValueError, match=\"Mock error for testing retries\"\
      ):\n        await retry_service.aretry(func=mock_func)\n    elapsed_time = time.time()\
      \ - start_time\n\n    # subtract 1 from retries because the first call is not\
      \ a retry\n    assert retries - 1 == max_retries, f\"Expected {max_retries}\
      \ retries, got {retries}\"\n    assert elapsed_time >= expected_time, (\n  \
      \      f\"Expected elapsed time >= {expected_time}, got {elapsed_time}\"\n \
      \   )"
    signature: "def test_retries_async(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None"
    decorators:
    - "@pytest.mark.parametrize(\n    (\"strategy\", \"max_retries\", \"max_retry_wait\"\
      , \"expected_time\"),\n    [\n        (\n            \"native\",\n         \
      \   3,  # 3 retries\n            0,  # native retry does not adhere to max_retry_wait\n\
      \            0,  # immediate retry, expect 0 seconds elapsed time\n        ),\n\
      \        (\n            \"exponential_backoff\",\n            3,  # 3 retries\n\
      \            0,  # exponential retry does not adhere to max_retry_wait\n   \
      \         14,  # (2^1 + jitter) + (2^2 + jitter) + (2^3 + jitter) = 2 + 4 +\
      \ 8 + 3*jitter = 14 seconds min total runtime\n        ),\n        (\n     \
      \       \"random_wait\",\n            3,  # 3 retries\n            2,  # random\
      \ wait [0, 2] seconds\n            0,  # unpredictable, don't know what the\
      \ total runtime will be\n        ),\n        (\n            \"incremental_wait\"\
      ,\n            3,  # 3 retries\n            3,  # wait for a max of 3 seconds\
      \ on a single retry.\n            6,  # Wait 3/3 * 1 on first retry, 3/3 * 2\
      \ on second, 3/3 * 3 on third, 1 + 2 + 3 = 6 seconds total runtime.\n      \
      \  ),\n    ],\n)"
    raises:
    - ValueError
    calls:
    - target: retry_factory.create
      type: unresolved
    - target: time::time
      type: stdlib
    - target: pytest::raises
      type: external
    - target: retry_service.aretry
      type: unresolved
    visibility: public
    node_id: tests/unit/litellm_services/test_retries.py::test_retries_async
    called_by: []
  - name: mock_func
    start_line: 133
    end_line: 137
    code: "async def mock_func():  # noqa: RUF029\n        nonlocal retries\n    \
      \    retries += 1\n        msg = \"Mock error for testing retries\"\n      \
      \  raise ValueError(msg)"
    signature: def mock_func()
    decorators: []
    raises:
    - ValueError
    calls:
    - target: ValueError
      type: builtin
    visibility: public
    node_id: tests/unit/litellm_services/test_retries.py::mock_func
    called_by: []
- file_name: tests/unit/litellm_services/utils.py
  imports: []
  functions:
  - name: bin_time_intervals
    start_line: 7
    end_line: 23
    code: "def bin_time_intervals(\n    time_values: list[float], time_interval: int\n\
      ) -> list[list[float]]:\n    \"\"\"Bin values.\"\"\"\n    bins: list[list[float]]\
      \ = []\n\n    bin_number = 0\n    for time_value in time_values:\n        upper_bound\
      \ = (bin_number * time_interval) + time_interval\n        while time_value >=\
      \ upper_bound:\n            bin_number += 1\n            upper_bound = (bin_number\
      \ * time_interval) + time_interval\n        while len(bins) <= bin_number:\n\
      \            bins.append([])\n        bins[bin_number].append(time_value)\n\n\
      \    return bins"
    signature: "def bin_time_intervals(\n    time_values: list[float], time_interval:\
      \ int\n) -> list[list[float]]"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: bins.append
      type: unresolved
    - target: bins[bin_number].append
      type: unresolved
    visibility: public
    node_id: tests/unit/litellm_services/utils.py::bin_time_intervals
    called_by:
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_binning
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_tpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
      type: internal
  - name: assert_max_num_values_per_period
    start_line: 26
    end_line: 31
    code: "def assert_max_num_values_per_period(\n    periods: list[list[float]],\
      \ max_values_per_period: int\n):\n    \"\"\"Assert the number of values per\
      \ period.\"\"\"\n    for period in periods:\n        assert len(period) <= max_values_per_period"
    signature: "def assert_max_num_values_per_period(\n    periods: list[list[float]],\
      \ max_values_per_period: int\n)"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    visibility: public
    node_id: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
    called_by:
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_tpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
      type: internal
  - name: assert_stagger
    start_line: 34
    end_line: 37
    code: "def assert_stagger(time_values: list[float], stagger: float):\n    \"\"\
      \"Assert stagger.\"\"\"\n    for i in range(1, len(time_values)):\n        assert\
      \ time_values[i] - time_values[i - 1] >= stagger"
    signature: 'def assert_stagger(time_values: list[float], stagger: float)'
    decorators: []
    raises: []
    calls:
    - target: range
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: tests/unit/litellm_services/utils.py::assert_stagger
    called_by:
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
      type: internal
    - source: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
      type: internal
- file_name: tests/unit/query/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/query/context_builder/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/query/context_builder/test_entity_extraction.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: graphrag.config.models.vector_store_schema_config
    name: VectorStoreSchemaConfig
    alias: null
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.data_model.types
    name: TextEmbedder
    alias: null
  - module: graphrag.language_model.manager
    name: ModelManager
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: EntityVectorStoreKey
    alias: null
  - module: graphrag.query.context_builder.entity_extraction
    name: map_query_to_entities
    alias: null
  - module: graphrag.vector_stores.base
    name: BaseVectorStore
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreDocument
    alias: null
  - module: graphrag.vector_stores.base
    name: VectorStoreSearchResult
    alias: null
  functions:
  - name: __init__
    start_line: 22
    end_line: 26
    code: "def __init__(self, documents: list[VectorStoreDocument]) -> None:\n   \
      \     super().__init__(\n            vector_store_schema_config=VectorStoreSchemaConfig(index_name=\"\
      mock\")\n        )\n        self.documents = documents"
    signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
    decorators: []
    raises: []
    calls:
    - target: super().__init__
      type: unresolved
    - target: super
      type: builtin
    - target: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
      type: internal
    visibility: protected
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.__init__
    called_by: []
  - name: connect
    start_line: 28
    end_line: 29
    code: "def connect(self, **kwargs: Any) -> None:\n        raise NotImplementedError"
    signature: 'def connect(self, **kwargs: Any) -> None'
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.connect
    called_by: []
  - name: load_documents
    start_line: 31
    end_line: 34
    code: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None:\n        raise NotImplementedError"
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.load_documents
    called_by: []
  - name: similarity_search_by_vector
    start_line: 36
    end_line: 42
    code: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n    \
      \    return [\n            VectorStoreSearchResult(document=document, score=1)\n\
      \            for document in self.documents[:k]\n        ]"
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: graphrag/vector_stores/base.py::VectorStoreSearchResult
      type: internal
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_vector
    called_by: []
  - name: similarity_search_by_text
    start_line: 44
    end_line: 55
    code: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]:\n\
      \        return sorted(\n            [\n                VectorStoreSearchResult(\n\
      \                    document=document, score=abs(len(text) - len(document.text\
      \ or \"\"))\n                )\n                for document in self.documents\n\
      \            ],\n            key=lambda x: x.score,\n        )[:k]"
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    decorators: []
    raises: []
    calls:
    - target: sorted
      type: builtin
    - target: graphrag/vector_stores/base.py::VectorStoreSearchResult
      type: internal
    - target: abs
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_text
    called_by: []
  - name: filter_by_id
    start_line: 57
    end_line: 58
    code: "def filter_by_id(self, include_ids: list[str] | list[int]) -> Any:\n  \
      \      return [document for document in self.documents if document.id in include_ids]"
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.filter_by_id
    called_by: []
  - name: search_by_id
    start_line: 60
    end_line: 63
    code: "def search_by_id(self, id: str) -> VectorStoreDocument:\n        result\
      \ = self.documents[0]\n        result.id = id\n        return result"
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.search_by_id
    called_by: []
  - name: test_map_query_to_entities
    start_line: 66
    end_line: 190
    code: "def test_map_query_to_entities():\n    entities = [\n        Entity(\n\
      \            id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\",\n            short_id=\"\
      sid1\",\n            title=\"t1\",\n            rank=2,\n        ),\n      \
      \  Entity(\n            id=\"c4f93564-4507-4ee4-b102-98add401a965\",\n     \
      \       short_id=\"sid2\",\n            title=\"t22\",\n            rank=4,\n\
      \        ),\n        Entity(\n            id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n            short_id=\"sid3\",\n            title=\"t333\",\n           \
      \ rank=1,\n        ),\n        Entity(\n            id=\"8fd6d72a-8e9d-4183-8a97-c38bcc971c83\"\
      ,\n            short_id=\"sid4\",\n            title=\"t4444\",\n          \
      \  rank=3,\n        ),\n    ]\n\n    assert map_query_to_entities(\n       \
      \ query=\"t22\",\n        text_embedding_vectorstore=MockBaseVectorStore([\n\
      \            VectorStoreDocument(id=entity.id, text=entity.title, vector=None)\n\
      \            for entity in entities\n        ]),\n        text_embedder=ModelManager().get_or_create_embedding_model(\n\
      \            model_type=\"mock_embedding\", name=\"mock\"\n        ),\n    \
      \    all_entities_dict={entity.id: entity for entity in entities},\n       \
      \ embedding_vectorstore_key=EntityVectorStoreKey.ID,\n        k=1,\n       \
      \ oversample_scaler=1,\n    ) == [\n        Entity(\n            id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n            short_id=\"sid2\",\n            title=\"t22\",\n            rank=4,\n\
      \        )\n    ]\n\n    assert map_query_to_entities(\n        query=\"t22\"\
      ,\n        text_embedding_vectorstore=MockBaseVectorStore([\n            VectorStoreDocument(id=entity.title,\
      \ text=entity.title, vector=None)\n            for entity in entities\n    \
      \    ]),\n        text_embedder=ModelManager().get_or_create_embedding_model(\n\
      \            model_type=\"mock_embedding\", name=\"mock\"\n        ),\n    \
      \    all_entities_dict={entity.id: entity for entity in entities},\n       \
      \ embedding_vectorstore_key=EntityVectorStoreKey.TITLE,\n        k=1,\n    \
      \    oversample_scaler=1,\n    ) == [\n        Entity(\n            id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n            short_id=\"sid2\",\n            title=\"t22\",\n            rank=4,\n\
      \        )\n    ]\n\n    assert map_query_to_entities(\n        query=\"\",\n\
      \        text_embedding_vectorstore=MockBaseVectorStore([\n            VectorStoreDocument(id=entity.id,\
      \ text=entity.title, vector=None)\n            for entity in entities\n    \
      \    ]),\n        text_embedder=ModelManager().get_or_create_embedding_model(\n\
      \            model_type=\"mock_embedding\", name=\"mock\"\n        ),\n    \
      \    all_entities_dict={entity.id: entity for entity in entities},\n       \
      \ embedding_vectorstore_key=EntityVectorStoreKey.ID,\n        k=2,\n    ) ==\
      \ [\n        Entity(\n            id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n            short_id=\"sid2\",\n            title=\"t22\",\n            rank=4,\n\
      \        ),\n        Entity(\n            id=\"8fd6d72a-8e9d-4183-8a97-c38bcc971c83\"\
      ,\n            short_id=\"sid4\",\n            title=\"t4444\",\n          \
      \  rank=3,\n        ),\n    ]\n\n    assert map_query_to_entities(\n       \
      \ query=\"\",\n        text_embedding_vectorstore=MockBaseVectorStore([\n  \
      \          VectorStoreDocument(id=entity.id, text=entity.title, vector=None)\n\
      \            for entity in entities\n        ]),\n        text_embedder=ModelManager().get_or_create_embedding_model(\n\
      \            model_type=\"mock_embedding\", name=\"mock\"\n        ),\n    \
      \    all_entities_dict={entity.id: entity for entity in entities},\n       \
      \ embedding_vectorstore_key=EntityVectorStoreKey.TITLE,\n        k=2,\n    )\
      \ == [\n        Entity(\n            id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n            short_id=\"sid2\",\n            title=\"t22\",\n            rank=4,\n\
      \        ),\n        Entity(\n            id=\"8fd6d72a-8e9d-4183-8a97-c38bcc971c83\"\
      ,\n            short_id=\"sid4\",\n            title=\"t4444\",\n          \
      \  rank=3,\n        ),\n    ]"
    signature: def test_map_query_to_entities()
    decorators: []
    raises: []
    calls:
    - target: graphrag/data_model/entity.py::Entity
      type: internal
    - target: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
      type: internal
    - target: MockBaseVectorStore
      type: unresolved
    - target: graphrag/vector_stores/base.py::VectorStoreDocument
      type: internal
    - target: ModelManager().get_or_create_embedding_model
      type: unresolved
    - target: graphrag/language_model/manager.py::ModelManager
      type: internal
    visibility: public
    node_id: tests/unit/query/context_builder/test_entity_extraction.py::test_map_query_to_entities
    called_by: []
- file_name: tests/unit/query/input/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/query/input/retrieval/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/query/input/retrieval/test_entities.py
  imports:
  - module: graphrag.data_model.entity
    name: Entity
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: get_entity_by_id
    alias: null
  - module: graphrag.query.input.retrieval.entities
    name: get_entity_by_key
    alias: null
  functions:
  - name: test_get_entity_by_id
    start_line: 11
    end_line: 89
    code: "def test_get_entity_by_id():\n    assert (\n        get_entity_by_id(\n\
      \            {\n                entity.id: entity\n                for entity\
      \ in [\n                    Entity(\n                        id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\"\
      ,\n                        short_id=\"sid1\",\n                        title=\"\
      title1\",\n                    ),\n                ]\n            },\n     \
      \       \"00000000-0000-0000-0000-000000000000\",\n        )\n        is None\n\
      \    )\n\n    assert get_entity_by_id(\n        {\n            entity.id: entity\n\
      \            for entity in [\n                Entity(\n                    id=\"\
      2da37c7a-50a8-44d4-aa2c-fd401e19976c\",\n                    short_id=\"sid1\"\
      ,\n                    title=\"title1\",\n                ),\n             \
      \   Entity(\n                    id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n                    short_id=\"sid2\",\n                    title=\"title2\"\
      ,\n                ),\n                Entity(\n                    id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n                    short_id=\"sid3\",\n                    title=\"title3\"\
      ,\n                ),\n            ]\n        },\n        \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n    ) == Entity(\n        id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\", short_id=\"\
      sid3\", title=\"title3\"\n    )\n\n    assert get_entity_by_id(\n        {\n\
      \            entity.id: entity\n            for entity in [\n              \
      \  Entity(\n                    id=\"2da37c7a50a844d4aa2cfd401e19976c\",\n \
      \                   short_id=\"sid1\",\n                    title=\"title1\"\
      ,\n                ),\n                Entity(\n                    id=\"c4f9356445074ee4b10298add401a965\"\
      ,\n                    short_id=\"sid2\",\n                    title=\"title2\"\
      ,\n                ),\n                Entity(\n                    id=\"7c6f2bc947c9445393a3d2e174a02cd9\"\
      ,\n                    short_id=\"sid3\",\n                    title=\"title3\"\
      ,\n                ),\n            ]\n        },\n        \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n    ) == Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\"\
      , title=\"title3\")\n\n    assert get_entity_by_id(\n        {\n           \
      \ entity.id: entity\n            for entity in [\n                Entity(id=\"\
      id1\", short_id=\"sid1\", title=\"title1\"),\n                Entity(id=\"id2\"\
      , short_id=\"sid2\", title=\"title2\"),\n                Entity(id=\"id3\",\
      \ short_id=\"sid3\", title=\"title3\"),\n            ]\n        },\n       \
      \ \"id3\",\n    ) == Entity(id=\"id3\", short_id=\"sid3\", title=\"title3\")"
    signature: def test_get_entity_by_id()
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/entities.py::get_entity_by_id
      type: internal
    - target: graphrag/data_model/entity.py::Entity
      type: internal
    visibility: public
    node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_id
    called_by: []
  - name: test_get_entity_by_key
    start_line: 92
    end_line: 167
    code: "def test_get_entity_by_key():\n    assert (\n        get_entity_by_key(\n\
      \            [\n                Entity(\n                    id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\"\
      ,\n                    short_id=\"sid1\",\n                    title=\"title1\"\
      ,\n                ),\n            ],\n            \"id\",\n            \"00000000-0000-0000-0000-000000000000\"\
      ,\n        )\n        is None\n    )\n\n    assert get_entity_by_key(\n    \
      \    [\n            Entity(\n                id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\"\
      ,\n                short_id=\"sid1\",\n                title=\"title1\",\n \
      \           ),\n            Entity(\n                id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      ,\n                short_id=\"sid2\",\n                title=\"title2\",\n \
      \           ),\n            Entity(\n                id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n                short_id=\"sid3\",\n                title=\"title3\",\n \
      \           ),\n        ],\n        \"id\",\n        \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n    ) == Entity(\n        id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\", short_id=\"\
      sid3\", title=\"title3\"\n    )\n\n    assert get_entity_by_key(\n        [\n\
      \            Entity(\n                id=\"2da37c7a50a844d4aa2cfd401e19976c\"\
      , short_id=\"sid1\", title=\"title1\"\n            ),\n            Entity(\n\
      \                id=\"c4f9356445074ee4b10298add401a965\", short_id=\"sid2\"\
      , title=\"title2\"\n            ),\n            Entity(\n                id=\"\
      7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\", title=\"title3\"\n  \
      \          ),\n        ],\n        \"id\",\n        \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n    ) == Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\"\
      , title=\"title3\")\n\n    assert get_entity_by_key(\n        [\n          \
      \  Entity(id=\"id1\", short_id=\"sid1\", title=\"title1\"),\n            Entity(id=\"\
      id2\", short_id=\"sid2\", title=\"title2\"),\n            Entity(id=\"id3\"\
      , short_id=\"sid3\", title=\"title3\"),\n        ],\n        \"id\",\n     \
      \   \"id3\",\n    ) == Entity(id=\"id3\", short_id=\"sid3\", title=\"title3\"\
      )\n\n    assert get_entity_by_key(\n        [\n            Entity(id=\"id1\"\
      , short_id=\"sid1\", title=\"title1\", rank=1),\n            Entity(id=\"id2\"\
      , short_id=\"sid2\", title=\"title2a\", rank=2),\n            Entity(id=\"id3\"\
      , short_id=\"sid3\", title=\"title3\", rank=3),\n            Entity(id=\"id2\"\
      , short_id=\"sid2\", title=\"title2b\", rank=2),\n        ],\n        \"rank\"\
      ,\n        2,\n    ) == Entity(id=\"id2\", short_id=\"sid2\", title=\"title2a\"\
      , rank=2)"
    signature: def test_get_entity_by_key()
    decorators: []
    raises: []
    calls:
    - target: graphrag/query/input/retrieval/entities.py::get_entity_by_key
      type: internal
    - target: graphrag/data_model/entity.py::Entity
      type: internal
    visibility: public
    node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_key
    called_by: []
- file_name: tests/unit/utils/__init__.py
  imports: []
  functions: []
- file_name: tests/unit/utils/test_embeddings.py
  imports:
  - module: pytest
    name: null
    alias: null
  - module: graphrag.config.embeddings
    name: create_index_name
    alias: null
  functions:
  - name: test_create_index_name
    start_line: 9
    end_line: 11
    code: "def test_create_index_name():\n    collection = create_index_name(\"default\"\
      , \"entity.title\")\n    assert collection == \"default-entity-title\""
    signature: def test_create_index_name()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/embeddings.py::create_index_name
      type: internal
    visibility: public
    node_id: tests/unit/utils/test_embeddings.py::test_create_index_name
    called_by: []
  - name: test_create_index_name_invalid_embedding_throws
    start_line: 14
    end_line: 16
    code: "def test_create_index_name_invalid_embedding_throws():\n    with pytest.raises(KeyError):\n\
      \        create_index_name(\"default\", \"invalid.name\")"
    signature: def test_create_index_name_invalid_embedding_throws()
    decorators: []
    raises: []
    calls:
    - target: pytest::raises
      type: external
    - target: graphrag/config/embeddings.py::create_index_name
      type: internal
    visibility: public
    node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_throws
    called_by: []
  - name: test_create_index_name_invalid_embedding_does_not_throw
    start_line: 19
    end_line: 21
    code: "def test_create_index_name_invalid_embedding_does_not_throw():\n    collection\
      \ = create_index_name(\"default\", \"invalid.name\", validate=False)\n    assert\
      \ collection == \"default-invalid-name\""
    signature: def test_create_index_name_invalid_embedding_does_not_throw()
    decorators: []
    raises: []
    calls:
    - target: graphrag/config/embeddings.py::create_index_name
      type: internal
    visibility: public
    node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_does_not_throw
    called_by: []
- file_name: tests/unit/utils/test_encoding.py
  imports:
  - module: graphrag.tokenizer.get_tokenizer
    name: get_tokenizer
    alias: null
  functions:
  - name: test_encode_basic
    start_line: 7
    end_line: 11
    code: "def test_encode_basic():\n    tokenizer = get_tokenizer()\n    result =\
      \ tokenizer.encode(\"abc def\")\n\n    assert result == [13997, 711], \"Encoding\
      \ failed to return expected tokens\""
    signature: def test_encode_basic()
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: tokenizer.encode
      type: unresolved
    visibility: public
    node_id: tests/unit/utils/test_encoding.py::test_encode_basic
    called_by: []
  - name: test_num_tokens_empty_input
    start_line: 14
    end_line: 18
    code: "def test_num_tokens_empty_input():\n    tokenizer = get_tokenizer()\n \
      \   result = len(tokenizer.encode(\"\"))\n\n    assert result == 0, \"Token\
      \ count for empty input should be 0\""
    signature: def test_num_tokens_empty_input()
    decorators: []
    raises: []
    calls:
    - target: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
      type: internal
    - target: len
      type: builtin
    - target: tokenizer.encode
      type: unresolved
    visibility: public
    node_id: tests/unit/utils/test_encoding.py::test_num_tokens_empty_input
    called_by: []
- file_name: tests/verbs/__init__.py
  imports: []
  functions: []
- file_name: tests/verbs/test_create_base_text_units.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.index.workflows.create_base_text_units
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_create_base_text_units
    start_line: 17
    end_line: 28
    code: "async def test_create_base_text_units():\n    expected = load_test_table(\"\
      text_units\")\n\n    context = await create_test_context()\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n\n    await run_workflow(config, context)\n\
      \n    actual = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \n    compare_outputs(actual, expected, columns=[\"text\", \"document_ids\"\
      , \"n_tokens\"])"
    signature: def test_create_base_text_units()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
    called_by: []
  - name: test_create_base_text_units_metadata
    start_line: 31
    end_line: 47
    code: "async def test_create_base_text_units_metadata():\n    expected = load_test_table(\"\
      text_units_metadata\")\n\n    context = await create_test_context()\n\n    config\
      \ = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n    # test data\
      \ was created with 4o, so we need to match the encoding for chunks to be identical\n\
      \    config.chunks.encoding_model = \"o200k_base\"\n    config.input.metadata\
      \ = [\"title\"]\n    config.chunks.prepend_metadata = True\n\n    await update_document_metadata(config.input.metadata,\
      \ context)\n\n    await run_workflow(config, context)\n\n    actual = await\
      \ load_table_from_storage(\"text_units\", context.output_storage)\n    compare_outputs(actual,\
      \ expected)"
    signature: def test_create_base_text_units_metadata()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: tests/verbs/util.py::update_document_metadata
      type: internal
    - target: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
    called_by: []
  - name: test_create_base_text_units_metadata_included_in_chunk
    start_line: 50
    end_line: 68
    code: "async def test_create_base_text_units_metadata_included_in_chunk():\n \
      \   expected = load_test_table(\"text_units_metadata_included_chunk\")\n\n \
      \   context = await create_test_context()\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n    # test data was created with 4o, so we\
      \ need to match the encoding for chunks to be identical\n    config.chunks.encoding_model\
      \ = \"o200k_base\"\n    config.input.metadata = [\"title\"]\n    config.chunks.prepend_metadata\
      \ = True\n    config.chunks.chunk_size_includes_metadata = True\n\n    await\
      \ update_document_metadata(config.input.metadata, context)\n\n    await run_workflow(config,\
      \ context)\n\n    actual = await load_table_from_storage(\"text_units\", context.output_storage)\n\
      \    # only check the columns from the base workflow - our expected table is\
      \ the final and will have more\n    compare_outputs(actual, expected, columns=[\"\
      text\", \"document_ids\", \"n_tokens\"])"
    signature: def test_create_base_text_units_metadata_included_in_chunk()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: tests/verbs/util.py::update_document_metadata
      type: internal
    - target: graphrag/index/workflows/create_base_text_units.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
    called_by: []
- file_name: tests/verbs/test_create_communities.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.data_model.schemas
    name: COMMUNITIES_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.workflows.create_communities
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_create_communities
    start_line: 19
    end_line: 48
    code: "async def test_create_communities():\n    expected = load_test_table(\"\
      communities\")\n\n    context = await create_test_context(\n        storage=[\n\
      \            \"entities\",\n            \"relationships\",\n        ],\n   \
      \ )\n\n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\
      \n    await run_workflow(\n        config,\n        context,\n    )\n\n    actual\
      \ = await load_table_from_storage(\"communities\", context.output_storage)\n\
      \n    columns = list(expected.columns.values)\n    # don't compare period since\
      \ it is created with the current date each time\n    columns.remove(\"period\"\
      )\n    compare_outputs(\n        actual,\n        expected,\n        columns=columns,\n\
      \    )\n\n    for column in COMMUNITIES_FINAL_COLUMNS:\n        assert column\
      \ in actual.columns"
    signature: def test_create_communities()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/create_communities.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: list
      type: builtin
    - target: columns.remove
      type: unresolved
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_communities.py::test_create_communities
    called_by: []
- file_name: tests/verbs/test_create_community_reports.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.data_model.schemas
    name: COMMUNITY_REPORTS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.operations.summarize_communities.community_reports_extractor
    name: CommunityReportResponse
    alias: null
  - module: graphrag.index.operations.summarize_communities.community_reports_extractor
    name: FindingModel
    alias: null
  - module: graphrag.index.workflows.create_community_reports
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_create_community_reports
    start_line: 42
    end_line: 81
    code: "async def test_create_community_reports():\n    expected = load_test_table(\"\
      community_reports\")\n\n    context = await create_test_context(\n        storage=[\n\
      \            \"covariates\",\n            \"relationships\",\n            \"\
      entities\",\n            \"communities\",\n        ]\n    )\n\n    config =\
      \ create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n    llm_settings\
      \ = config.get_language_model_config(\n        config.community_reports.model_id\n\
      \    ).model_dump()\n    llm_settings[\"type\"] = ModelType.MockChat\n    llm_settings[\"\
      responses\"] = MOCK_RESPONSES\n    llm_settings[\"parse_json\"] = True\n   \
      \ config.community_reports.strategy = {\n        \"type\": \"graph_intelligence\"\
      ,\n        \"llm\": llm_settings,\n        \"graph_prompt\": \"\",\n    }\n\n\
      \    await run_workflow(config, context)\n\n    actual = await load_table_from_storage(\"\
      community_reports\", context.output_storage)\n\n    assert len(actual.columns)\
      \ == len(expected.columns)\n\n    # only assert a couple of columns that are\
      \ not mock - most of this table is LLM-generated\n    compare_outputs(actual,\
      \ expected, columns=[\"community\", \"level\"])\n\n    # assert a handful of\
      \ mock data items to confirm they get put in the right spot\n    assert actual[\"\
      rank\"][:1][0] == 2\n    assert actual[\"rating_explanation\"][:1][0] == \"\
      <rating_explanation>\"\n\n    for column in COMMUNITY_REPORTS_FINAL_COLUMNS:\n\
      \        assert column in actual.columns"
    signature: def test_create_community_reports()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: "config.get_language_model_config(\n        config.community_reports.model_id\n\
        \    ).model_dump"
      type: unresolved
    - target: config.get_language_model_config
      type: unresolved
    - target: graphrag/index/workflows/create_community_reports.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_community_reports.py::test_create_community_reports
    called_by: []
- file_name: tests/verbs/test_create_final_documents.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.data_model.schemas
    name: DOCUMENTS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.workflows.create_final_documents
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_create_final_documents
    start_line: 20
    end_line: 36
    code: "async def test_create_final_documents():\n    expected = load_test_table(\"\
      documents\")\n\n    context = await create_test_context(\n        storage=[\"\
      text_units\"],\n    )\n\n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\
      \n    await run_workflow(config, context)\n\n    actual = await load_table_from_storage(\"\
      documents\", context.output_storage)\n\n    compare_outputs(actual, expected)\n\
      \n    for column in DOCUMENTS_FINAL_COLUMNS:\n        assert column in actual.columns"
    signature: def test_create_final_documents()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/create_final_documents.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents
    called_by: []
  - name: test_create_final_documents_with_metadata_column
    start_line: 39
    end_line: 59
    code: "async def test_create_final_documents_with_metadata_column():\n    context\
      \ = await create_test_context(\n        storage=[\"text_units\"],\n    )\n\n\
      \    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n \
      \   config.input.metadata = [\"title\"]\n\n    # simulate the metadata construction\
      \ during initial input loading\n    await update_document_metadata(config.input.metadata,\
      \ context)\n\n    expected = await load_table_from_storage(\"documents\", context.output_storage)\n\
      \n    await run_workflow(config, context)\n\n    actual = await load_table_from_storage(\"\
      documents\", context.output_storage)\n\n    compare_outputs(actual, expected)\n\
      \n    for column in DOCUMENTS_FINAL_COLUMNS:\n        assert column in actual.columns"
    signature: def test_create_final_documents_with_metadata_column()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: tests/verbs/util.py::update_document_metadata
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: graphrag/index/workflows/create_final_documents.py::run_workflow
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
    called_by: []
- file_name: tests/verbs/test_create_final_text_units.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.data_model.schemas
    name: TEXT_UNITS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.workflows.create_final_text_units
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_create_final_text_units
    start_line: 19
    end_line: 41
    code: "async def test_create_final_text_units():\n    expected = load_test_table(\"\
      text_units\")\n\n    context = await create_test_context(\n        storage=[\n\
      \            \"text_units\",\n            \"entities\",\n            \"relationships\"\
      ,\n            \"covariates\",\n        ],\n    )\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n    config.extract_claims.enabled = True\n\
      \n    await run_workflow(config, context)\n\n    actual = await load_table_from_storage(\"\
      text_units\", context.output_storage)\n\n    for column in TEXT_UNITS_FINAL_COLUMNS:\n\
      \        assert column in actual.columns\n\n    compare_outputs(actual, expected)"
    signature: def test_create_final_text_units()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/create_final_text_units.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: tests/verbs/util.py::compare_outputs
      type: internal
    visibility: public
    node_id: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
    called_by: []
- file_name: tests/verbs/test_extract_covariates.py
  imports:
  - module: pandas.testing
    name: assert_series_equal
    alias: null
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.data_model.schemas
    name: COVARIATES_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.workflows.extract_covariates
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_extract_covariates
    start_line: 27
    end_line: 79
    code: "async def test_extract_covariates():\n    input = load_test_table(\"text_units\"\
      )\n\n    context = await create_test_context(\n        storage=[\"text_units\"\
      ],\n    )\n\n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\
      \    llm_settings = config.get_language_model_config(\n        config.extract_claims.model_id\n\
      \    ).model_dump()\n    llm_settings[\"type\"] = ModelType.MockChat\n    llm_settings[\"\
      responses\"] = MOCK_LLM_RESPONSES\n    config.extract_claims.enabled = True\n\
      \    config.extract_claims.strategy = {\n        \"type\": \"graph_intelligence\"\
      ,\n        \"llm\": llm_settings,\n        \"claim_description\": \"description\"\
      ,\n    }\n\n    await run_workflow(config, context)\n\n    actual = await load_table_from_storage(\"\
      covariates\", context.output_storage)\n\n    for column in COVARIATES_FINAL_COLUMNS:\n\
      \        assert column in actual.columns\n\n    # our mock only returns one\
      \ covariate per text unit, so that's a 1:1 mapping versus the LLM-extracted\
      \ content in the test data\n    assert len(actual) == len(input)\n\n    # assert\
      \ all of the columns that covariates copied from the input\n    assert_series_equal(actual[\"\
      text_unit_id\"], input[\"id\"], check_names=False)\n\n    # make sure the human\
      \ ids are incrementing\n    assert actual[\"human_readable_id\"][0] == 0\n \
      \   assert actual[\"human_readable_id\"][1] == 1\n\n    # check that the mock\
      \ data is parsed and inserted into the correct columns\n    assert actual[\"\
      covariate_type\"][0] == \"claim\"\n    assert actual[\"subject_id\"][0] == \"\
      COMPANY A\"\n    assert actual[\"object_id\"][0] == \"GOVERNMENT AGENCY B\"\n\
      \    assert actual[\"type\"][0] == \"ANTI-COMPETITIVE PRACTICES\"\n    assert\
      \ actual[\"status\"][0] == \"TRUE\"\n    assert actual[\"start_date\"][0] ==\
      \ \"2022-01-10T00:00:00\"\n    assert actual[\"end_date\"][0] == \"2022-01-10T00:00:00\"\
      \n    assert (\n        actual[\"description\"][0]\n        == \"Company A was\
      \ found to engage in anti-competitive practices because it was fined for bid\
      \ rigging in multiple public tenders published by Government Agency B according\
      \ to an article published on 2022/01/10\"\n    )\n    assert (\n        actual[\"\
      source_text\"][0]\n        == \"According to an article published on 2022/01/10,\
      \ Company A was fined for bid rigging while participating in multiple public\
      \ tenders published by Government Agency B.\"\n    )"
    signature: def test_extract_covariates()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: "config.get_language_model_config(\n        config.extract_claims.model_id\n\
        \    ).model_dump"
      type: unresolved
    - target: config.get_language_model_config
      type: unresolved
    - target: graphrag/index/workflows/extract_covariates.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    - target: pandas.testing::assert_series_equal
      type: external
    visibility: public
    node_id: tests/verbs/test_extract_covariates.py::test_extract_covariates
    called_by: []
- file_name: tests/verbs/test_extract_graph.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.index.workflows.extract_graph
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_extract_graph
    start_line: 37
    end_line: 78
    code: "async def test_extract_graph():\n    context = await create_test_context(\n\
      \        storage=[\"text_units\"],\n    )\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n    extract_claims_llm_settings = config.get_language_model_config(\n\
      \        config.extract_graph.model_id\n    ).model_dump()\n    extract_claims_llm_settings[\"\
      type\"] = ModelType.MockChat\n    extract_claims_llm_settings[\"responses\"\
      ] = MOCK_LLM_ENTITY_RESPONSES\n    config.extract_graph.strategy = {\n     \
      \   \"type\": \"graph_intelligence\",\n        \"llm\": extract_claims_llm_settings,\n\
      \    }\n    summarize_llm_settings = config.get_language_model_config(\n   \
      \     config.summarize_descriptions.model_id\n    ).model_dump()\n    summarize_llm_settings[\"\
      type\"] = ModelType.MockChat\n    summarize_llm_settings[\"responses\"] = MOCK_LLM_SUMMARIZATION_RESPONSES\n\
      \    config.summarize_descriptions.strategy = {\n        \"type\": \"graph_intelligence\"\
      ,\n        \"llm\": summarize_llm_settings,\n        \"max_input_tokens\": 1000,\n\
      \        \"max_summary_length\": 100,\n    }\n\n    await run_workflow(config,\
      \ context)\n\n    nodes_actual = await load_table_from_storage(\"entities\"\
      , context.output_storage)\n    edges_actual = await load_table_from_storage(\n\
      \        \"relationships\", context.output_storage\n    )\n\n    assert len(nodes_actual.columns)\
      \ == 5\n    assert len(edges_actual.columns) == 5\n\n    # TODO: with the combined\
      \ verb we can't force summarization\n    # this is because the mock responses\
      \ always result in a single description, which is returned verbatim rather than\
      \ summarized\n    # we need to update the mocking to provide somewhat unique\
      \ graphs so a true merge happens\n    # the assertion should grab a node and\
      \ ensure the description matches the mock description, not the original as we\
      \ are doing below\n    assert nodes_actual[\"description\"].to_numpy()[0] ==\
      \ \"Company_A is a test company\""
    signature: def test_extract_graph()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: "config.get_language_model_config(\n        config.extract_graph.model_id\n\
        \    ).model_dump"
      type: unresolved
    - target: config.get_language_model_config
      type: unresolved
    - target: "config.get_language_model_config(\n        config.summarize_descriptions.model_id\n\
        \    ).model_dump"
      type: unresolved
    - target: graphrag/index/workflows/extract_graph.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    - target: nodes_actual["description"].to_numpy
      type: unresolved
    visibility: public
    node_id: tests/verbs/test_extract_graph.py::test_extract_graph
    called_by: []
- file_name: tests/verbs/test_extract_graph_nlp.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.index.workflows.extract_graph_nlp
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_extract_graph_nlp
    start_line: 16
    end_line: 35
    code: "async def test_extract_graph_nlp():\n    context = await create_test_context(\n\
      \        storage=[\"text_units\"],\n    )\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n\n    await run_workflow(config, context)\n\
      \n    nodes_actual = await load_table_from_storage(\"entities\", context.output_storage)\n\
      \    edges_actual = await load_table_from_storage(\n        \"relationships\"\
      , context.output_storage\n    )\n\n    # this will be the raw count of entities\
      \ and edges with no pruning\n    # with NLP it is deterministic, so we can assert\
      \ exact row counts\n    assert len(nodes_actual) == 1148\n    assert len(nodes_actual.columns)\
      \ == 5\n    assert len(edges_actual) == 29445\n    assert len(edges_actual.columns)\
      \ == 5"
    signature: def test_extract_graph_nlp()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
    called_by: []
- file_name: tests/verbs/test_finalize_graph.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.data_model.schemas
    name: ENTITIES_FINAL_COLUMNS
    alias: null
  - module: graphrag.data_model.schemas
    name: RELATIONSHIPS_FINAL_COLUMNS
    alias: null
  - module: graphrag.index.workflows.finalize_graph
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: test_finalize_graph
    start_line: 21
    end_line: 40
    code: "async def test_finalize_graph():\n    context = await _prep_tables()\n\n\
      \    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\n\
      \    await run_workflow(config, context)\n\n    nodes_actual = await load_table_from_storage(\"\
      entities\", context.output_storage)\n    edges_actual = await load_table_from_storage(\n\
      \        \"relationships\", context.output_storage\n    )\n\n    # x and y will\
      \ be zero with the default configuration, because we do not embed/umap\n   \
      \ assert nodes_actual[\"x\"].sum() == 0\n    assert nodes_actual[\"y\"].sum()\
      \ == 0\n\n    for column in ENTITIES_FINAL_COLUMNS:\n        assert column in\
      \ nodes_actual.columns\n    for column in RELATIONSHIPS_FINAL_COLUMNS:\n   \
      \     assert column in edges_actual.columns"
    signature: def test_finalize_graph()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/test_finalize_graph.py::_prep_tables
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: nodes_actual["x"].sum
      type: unresolved
    - target: nodes_actual["y"].sum
      type: unresolved
    visibility: public
    node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph
    called_by: []
  - name: test_finalize_graph_umap
    start_line: 43
    end_line: 65
    code: "async def test_finalize_graph_umap():\n    context = await _prep_tables()\n\
      \n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\n\
      \    config.embed_graph.enabled = True\n    config.umap.enabled = True\n\n \
      \   await run_workflow(config, context)\n\n    nodes_actual = await load_table_from_storage(\"\
      entities\", context.output_storage)\n    edges_actual = await load_table_from_storage(\n\
      \        \"relationships\", context.output_storage\n    )\n\n    # x and y should\
      \ have some value other than zero due to umap\n    assert nodes_actual[\"x\"\
      ].sum() != 0\n    assert nodes_actual[\"y\"].sum() != 0\n\n    for column in\
      \ ENTITIES_FINAL_COLUMNS:\n        assert column in nodes_actual.columns\n \
      \   for column in RELATIONSHIPS_FINAL_COLUMNS:\n        assert column in edges_actual.columns"
    signature: def test_finalize_graph_umap()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/test_finalize_graph.py::_prep_tables
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/workflows/finalize_graph.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: nodes_actual["x"].sum
      type: unresolved
    - target: nodes_actual["y"].sum
      type: unresolved
    visibility: public
    node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
    called_by: []
  - name: _prep_tables
    start_line: 68
    end_line: 80
    code: "async def _prep_tables():\n    context = await create_test_context(\n \
      \       storage=[\"entities\", \"relationships\"],\n    )\n\n    # edit the\
      \ tables to eliminate final fields that wouldn't be on the inputs\n    entities\
      \ = load_test_table(\"entities\")\n    entities.drop(columns=[\"x\", \"y\",\
      \ \"degree\"], inplace=True)\n    await write_table_to_storage(entities, \"\
      entities\", context.output_storage)\n    relationships = load_test_table(\"\
      relationships\")\n    relationships.drop(columns=[\"combined_degree\"], inplace=True)\n\
      \    await write_table_to_storage(relationships, \"relationships\", context.output_storage)\n\
      \    return context"
    signature: def _prep_tables()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: entities.drop
      type: unresolved
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    - target: relationships.drop
      type: unresolved
    visibility: protected
    node_id: tests/verbs/test_finalize_graph.py::_prep_tables
    called_by:
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph
      type: internal
    - source: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
      type: internal
- file_name: tests/verbs/test_generate_text_embeddings.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.embeddings
    name: all_embeddings
    alias: null
  - module: graphrag.config.enums
    name: ModelType
    alias: null
  - module: graphrag.index.operations.embed_text.embed_text
    name: TextEmbedStrategyType
    alias: null
  - module: graphrag.index.workflows.generate_text_embeddings
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_generate_text_embeddings
    start_line: 21
    end_line: 68
    code: "async def test_generate_text_embeddings():\n    context = await create_test_context(\n\
      \        storage=[\n            \"documents\",\n            \"relationships\"\
      ,\n            \"text_units\",\n            \"entities\",\n            \"community_reports\"\
      ,\n        ]\n    )\n\n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\
      \    llm_settings = config.get_language_model_config(\n        config.embed_text.model_id\n\
      \    ).model_dump()\n    llm_settings[\"type\"] = ModelType.MockEmbedding\n\n\
      \    config.embed_text.strategy = {\n        \"type\": TextEmbedStrategyType.openai,\n\
      \        \"llm\": llm_settings,\n    }\n    config.embed_text.names = list(all_embeddings)\n\
      \    config.snapshots.embeddings = True\n\n    await run_workflow(config, context)\n\
      \n    parquet_files = context.output_storage.keys()\n\n    for field in all_embeddings:\n\
      \        assert f\"embeddings.{field}.parquet\" in parquet_files\n\n    # entity\
      \ description should always be here, let's assert its format\n    entity_description_embeddings\
      \ = await load_table_from_storage(\n        \"embeddings.entity.description\"\
      , context.output_storage\n    )\n\n    assert len(entity_description_embeddings.columns)\
      \ == 2\n    assert \"id\" in entity_description_embeddings.columns\n    assert\
      \ \"embedding\" in entity_description_embeddings.columns\n\n    # every other\
      \ embedding is optional but we've turned them all on, so check a random one\n\
      \    document_text_embeddings = await load_table_from_storage(\n        \"embeddings.document.text\"\
      , context.output_storage\n    )\n\n    assert len(document_text_embeddings.columns)\
      \ == 2\n    assert \"id\" in document_text_embeddings.columns\n    assert \"\
      embedding\" in document_text_embeddings.columns"
    signature: def test_generate_text_embeddings()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: "config.get_language_model_config(\n        config.embed_text.model_id\n\
        \    ).model_dump"
      type: unresolved
    - target: config.get_language_model_config
      type: unresolved
    - target: list
      type: builtin
    - target: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
      type: internal
    - target: context.output_storage.keys
      type: unresolved
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
    called_by: []
- file_name: tests/verbs/test_pipeline_state.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  - module: graphrag.index.run.utils
    name: create_run_context
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.index.typing.workflow
    name: WorkflowFunctionOutput
    alias: null
  - module: graphrag.index.workflows.factory
    name: PipelineFactory
    alias: null
  - module: tests.verbs.util
    name: DEFAULT_MODEL_CONFIG
    alias: null
  functions:
  - name: run_workflow_1
    start_line: 15
    end_line: 19
    code: "async def run_workflow_1(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n):\n    context.state[\"count\"] = 1\n    return\
      \ WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow_1(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: tests/verbs/test_pipeline_state.py::run_workflow_1
    called_by: []
  - name: run_workflow_2
    start_line: 22
    end_line: 26
    code: "async def run_workflow_2(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n):\n    context.state[\"count\"] += 1\n    return\
      \ WorkflowFunctionOutput(result=None)"
    signature: "def run_workflow_2(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n)"
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/typing/workflow.py::WorkflowFunctionOutput
      type: internal
    visibility: public
    node_id: tests/verbs/test_pipeline_state.py::run_workflow_2
    called_by: []
  - name: test_pipeline_state
    start_line: 29
    end_line: 41
    code: "async def test_pipeline_state():\n    # checks that we can update the arbitrary\
      \ state block within the pipeline run context\n    PipelineFactory.register(\"\
      workflow_1\", run_workflow_1)\n    PipelineFactory.register(\"workflow_2\",\
      \ run_workflow_2)\n\n    config = create_graphrag_config({\"models\": DEFAULT_MODEL_CONFIG})\n\
      \    config.workflows = [\"workflow_1\", \"workflow_2\"]\n    context = create_run_context()\n\
      \n    for _, fn in PipelineFactory.create_pipeline(config).run():\n        await\
      \ fn(config, context)\n\n    assert context.state[\"count\"] == 2"
    signature: def test_pipeline_state()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/factory.py::PipelineFactory::register
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/run/utils.py::create_run_context
      type: internal
    - target: graphrag/index/workflows/factory.py::PipelineFactory::create_pipeline(config).run
      type: external
    - target: graphrag/index/workflows/factory.py::PipelineFactory::create_pipeline
      type: external
    - target: fn
      type: unresolved
    visibility: public
    node_id: tests/verbs/test_pipeline_state.py::test_pipeline_state
    called_by: []
  - name: test_pipeline_existing_state
    start_line: 44
    end_line: 54
    code: "async def test_pipeline_existing_state():\n    PipelineFactory.register(\"\
      workflow_2\", run_workflow_2)\n\n    config = create_graphrag_config({\"models\"\
      : DEFAULT_MODEL_CONFIG})\n    config.workflows = [\"workflow_2\"]\n    context\
      \ = create_run_context(state={\"count\": 4})\n\n    for _, fn in PipelineFactory.create_pipeline(config).run():\n\
      \        await fn(config, context)\n\n    assert context.state[\"count\"] ==\
      \ 5"
    signature: def test_pipeline_existing_state()
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/workflows/factory.py::PipelineFactory::register
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/index/run/utils.py::create_run_context
      type: internal
    - target: graphrag/index/workflows/factory.py::PipelineFactory::create_pipeline(config).run
      type: external
    - target: graphrag/index/workflows/factory.py::PipelineFactory::create_pipeline
      type: external
    - target: fn
      type: unresolved
    visibility: public
    node_id: tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
    called_by: []
- file_name: tests/verbs/test_prune_graph.py
  imports:
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.models.prune_graph_config
    name: PruneGraphConfig
    alias: null
  - module: graphrag.index.workflows.prune_graph
    name: run_workflow
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  functions:
  - name: test_prune_graph
    start_line: 17
    end_line: 31
    code: "async def test_prune_graph():\n    context = await create_test_context(\n\
      \        storage=[\"entities\", \"relationships\"],\n    )\n\n    config = create_graphrag_config({\"\
      models\": DEFAULT_MODEL_CONFIG})\n    config.prune_graph = PruneGraphConfig(\n\
      \        min_node_freq=4, min_node_degree=0, min_edge_weight_pct=0\n    )\n\n\
      \    await run_workflow(config, context)\n\n    nodes_actual = await load_table_from_storage(\"\
      entities\", context.output_storage)\n\n    assert len(nodes_actual) == 20"
    signature: def test_prune_graph()
    decorators: []
    raises: []
    calls:
    - target: tests/verbs/util.py::create_test_context
      type: internal
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: graphrag/config/models/prune_graph_config.py::PruneGraphConfig
      type: internal
    - target: graphrag/index/workflows/prune_graph.py::run_workflow
      type: internal
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: len
      type: builtin
    visibility: public
    node_id: tests/verbs/test_prune_graph.py::test_prune_graph
    called_by: []
- file_name: tests/verbs/util.py
  imports:
  - module: pandas
    name: null
    alias: pd
  - module: pandas.testing
    name: assert_series_equal
    alias: null
  - module: graphrag.config.defaults
    name: null
    alias: defs
  - module: graphrag.index.run.utils
    name: create_run_context
    alias: null
  - module: graphrag.index.typing.context
    name: PipelineRunContext
    alias: null
  - module: graphrag.utils.storage
    name: load_table_from_storage
    alias: null
  - module: graphrag.utils.storage
    name: write_table_to_storage
    alias: null
  functions:
  - name: create_test_context
    start_line: 36
    end_line: 50
    code: "async def create_test_context(storage: list[str] | None = None) -> PipelineRunContext:\n\
      \    \"\"\"Create a test context with tables loaded into storage storage.\"\"\
      \"\n    context = create_run_context()\n\n    # always set the input docs, but\
      \ since our stored table is final, drop what wouldn't be in the original source\
      \ input\n    input = load_test_table(\"documents\")\n    input.drop(columns=[\"\
      text_unit_ids\"], inplace=True)\n    await write_table_to_storage(input, \"\
      documents\", context.output_storage)\n\n    if storage:\n        for name in\
      \ storage:\n            table = load_test_table(name)\n            await write_table_to_storage(table,\
      \ name, context.output_storage)\n\n    return context"
    signature: 'def create_test_context(storage: list[str] | None = None) -> PipelineRunContext'
    decorators: []
    raises: []
    calls:
    - target: graphrag/index/run/utils.py::create_run_context
      type: internal
    - target: tests/verbs/util.py::load_test_table
      type: internal
    - target: input.drop
      type: unresolved
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: public
    node_id: tests/verbs/util.py::create_test_context
    called_by:
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
    - source: tests/verbs/test_extract_covariates.py::test_extract_covariates
      type: internal
    - source: tests/verbs/test_extract_graph.py::test_extract_graph
      type: internal
    - source: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
      type: internal
    - source: tests/verbs/test_finalize_graph.py::_prep_tables
      type: internal
    - source: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
      type: internal
    - source: tests/verbs/test_prune_graph.py::test_prune_graph
      type: internal
  - name: load_test_table
    start_line: 53
    end_line: 55
    code: "def load_test_table(output: str) -> pd.DataFrame:\n    \"\"\"Pass in the\
      \ workflow output (generally the workflow name)\"\"\"\n    return pd.read_parquet(f\"\
      tests/verbs/data/{output}.parquet\")"
    signature: 'def load_test_table(output: str) -> pd.DataFrame'
    decorators: []
    raises: []
    calls:
    - target: pandas::read_parquet
      type: external
    visibility: public
    node_id: tests/verbs/util.py::load_test_table
    called_by:
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
    - source: tests/verbs/test_extract_covariates.py::test_extract_covariates
      type: internal
    - source: tests/verbs/test_finalize_graph.py::_prep_tables
      type: internal
    - source: tests/verbs/util.py::create_test_context
      type: internal
  - name: compare_outputs
    start_line: 58
    end_line: 86
    code: "def compare_outputs(\n    actual: pd.DataFrame, expected: pd.DataFrame,\
      \ columns: list[str] | None = None\n) -> None:\n    \"\"\"Compare the actual\
      \ and expected dataframes, optionally specifying columns to compare.\n    This\
      \ uses assert_series_equal since we are sometimes intentionally omitting columns\
      \ from the actual output.\n    \"\"\"\n    cols = expected.columns if columns\
      \ is None else columns\n\n    assert len(actual) == len(expected), (\n     \
      \   f\"Expected: {len(expected)} rows, Actual: {len(actual)} rows\"\n    )\n\
      \n    for column in cols:\n        assert column in actual.columns\n       \
      \ try:\n            # dtypes can differ since the test data is read from parquet\
      \ and our workflow runs in memory\n            if column != \"id\":  # don't\
      \ check uuids\n                assert_series_equal(\n                    actual[column],\n\
      \                    expected[column],\n                    check_dtype=False,\n\
      \                    check_index=False,\n                )\n        except AssertionError:\n\
      \            print(\"Expected:\")\n            print(expected[column])\n   \
      \         print(\"Actual:\")\n            print(actual[column])\n          \
      \  raise"
    signature: "def compare_outputs(\n    actual: pd.DataFrame, expected: pd.DataFrame,\
      \ columns: list[str] | None = None\n) -> None"
    decorators: []
    raises: []
    calls:
    - target: len
      type: builtin
    - target: pandas.testing::assert_series_equal
      type: external
    - target: print
      type: builtin
    visibility: public
    node_id: tests/verbs/util.py::compare_outputs
    called_by:
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_communities.py::test_create_communities
      type: internal
    - source: tests/verbs/test_create_community_reports.py::test_create_community_reports
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
    - source: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
      type: internal
  - name: update_document_metadata
    start_line: 89
    end_line: 95
    code: "async def update_document_metadata(metadata: list[str], context: PipelineRunContext):\n\
      \    \"\"\"Takes the default documents and adds the configured metadata columns\
      \ for later parsing by the text units and final documents workflows.\"\"\"\n\
      \    documents = await load_table_from_storage(\"documents\", context.output_storage)\n\
      \    documents[\"metadata\"] = documents[metadata].apply(lambda row: row.to_dict(),\
      \ axis=1)\n    await write_table_to_storage(\n        documents, \"documents\"\
      , context.output_storage\n    )  # write to the runtime context storage only"
    signature: 'def update_document_metadata(metadata: list[str], context: PipelineRunContext)'
    decorators: []
    raises: []
    calls:
    - target: graphrag/utils/storage.py::load_table_from_storage
      type: internal
    - target: documents[metadata].apply
      type: unresolved
    - target: row.to_dict
      type: unresolved
    - target: graphrag/utils/storage.py::write_table_to_storage
      type: internal
    visibility: public
    node_id: tests/verbs/util.py::update_document_metadata
    called_by:
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
      type: internal
    - source: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
      type: internal
    - source: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
      type: internal
- file_name: unified-search-app/app/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/app_logic.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: typing
    name: TYPE_CHECKING
    alias: null
  - module: streamlit
    name: null
    alias: st
  - module: knowledge_loader.data_sources.loader
    name: create_datasource
    alias: null
  - module: knowledge_loader.data_sources.loader
    name: load_dataset_listing
    alias: null
  - module: knowledge_loader.model
    name: load_model
    alias: null
  - module: rag.typing
    name: SearchResult
    alias: null
  - module: rag.typing
    name: SearchType
    alias: null
  - module: state.session_variables
    name: SessionVariables
    alias: null
  - module: ui.search
    name: display_search_result
    alias: null
  - module: graphrag.api
    name: null
    alias: api
  - module: pandas
    name: null
    alias: pd
  functions:
  - name: initialize
    start_line: 30
    end_line: 48
    code: "def initialize() -> SessionVariables:\n    \"\"\"Initialize app logic.\"\
      \"\"\n    if \"session_variables\" not in st.session_state:\n        st.set_page_config(\n\
      \            layout=\"wide\",\n            initial_sidebar_state=\"collapsed\"\
      ,\n            page_title=\"GraphRAG\",\n        )\n        sv = SessionVariables()\n\
      \        datasets = load_dataset_listing()\n        sv.datasets.value = datasets\n\
      \        sv.dataset.value = (\n            st.query_params[\"dataset\"].lower()\n\
      \            if \"dataset\" in st.query_params\n            else datasets[0].key\n\
      \        )\n        load_dataset(sv.dataset.value, sv)\n        st.session_state[\"\
      session_variables\"] = sv\n    return st.session_state[\"session_variables\"\
      ]"
    signature: def initialize() -> SessionVariables
    decorators: []
    raises: []
    calls:
    - target: streamlit::set_page_config
      type: external
    - target: state.session_variables::SessionVariables
      type: external
    - target: knowledge_loader.data_sources.loader::load_dataset_listing
      type: external
    - target: streamlit::query_params["dataset"].lower
      type: external
    - target: unified-search-app/app/app_logic.py::load_dataset
      type: internal
    visibility: public
    node_id: unified-search-app/app/app_logic.py::initialize
    called_by: []
  - name: load_dataset
    start_line: 51
    end_line: 60
    code: "def load_dataset(dataset: str, sv: SessionVariables):\n    \"\"\"Load dataset\
      \ from the dropdown.\"\"\"\n    sv.dataset.value = dataset\n    sv.dataset_config.value\
      \ = next(\n        (d for d in sv.datasets.value if d.key == dataset), None\n\
      \    )\n    if sv.dataset_config.value is not None:\n        sv.datasource.value\
      \ = create_datasource(f\"{sv.dataset_config.value.path}\")  # type: ignore\n\
      \        sv.graphrag_config.value = sv.datasource.value.read_settings(\"settings.yaml\"\
      )\n        load_knowledge_model(sv)"
    signature: 'def load_dataset(dataset: str, sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: next
      type: builtin
    - target: knowledge_loader.data_sources.loader::create_datasource
      type: external
    - target: sv.datasource.value.read_settings
      type: unresolved
    - target: unified-search-app/app/app_logic.py::load_knowledge_model
      type: internal
    visibility: public
    node_id: unified-search-app/app/app_logic.py::load_dataset
    called_by:
    - source: unified-search-app/app/app_logic.py::initialize
      type: internal
  - name: dataset_name
    start_line: 63
    end_line: 65
    code: "def dataset_name(key: str, sv: SessionVariables) -> str:\n    \"\"\"Get\
      \ dataset name.\"\"\"\n    return next((d for d in sv.datasets.value if d.key\
      \ == key), None).name  # type: ignore"
    signature: 'def dataset_name(key: str, sv: SessionVariables) -> str'
    decorators: []
    raises: []
    calls:
    - target: next
      type: builtin
    visibility: public
    node_id: unified-search-app/app/app_logic.py::dataset_name
    called_by: []
  - name: run_all_searches
    start_line: 68
    end_line: 103
    code: "async def run_all_searches(query: str, sv: SessionVariables) -> list[SearchResult]:\n\
      \    \"\"\"Run all search engines and return the results.\"\"\"\n    loop =\
      \ asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    tasks = []\n\
      \    if sv.include_drift_search.value:\n        tasks.append(\n            run_drift_search(\n\
      \                query=query,\n                sv=sv,\n            )\n     \
      \   )\n\n    if sv.include_basic_rag.value:\n        tasks.append(\n       \
      \     run_basic_search(\n                query=query,\n                sv=sv,\n\
      \            )\n        )\n    if sv.include_local_search.value:\n        tasks.append(\n\
      \            run_local_search(\n                query=query,\n             \
      \   sv=sv,\n            )\n        )\n    if sv.include_global_search.value:\n\
      \        tasks.append(\n            run_global_search(\n                query=query,\n\
      \                sv=sv,\n            )\n        )\n\n    return await asyncio.gather(*tasks)"
    signature: 'def run_all_searches(query: str, sv: SessionVariables) -> list[SearchResult]'
    decorators: []
    raises: []
    calls:
    - target: asyncio::new_event_loop
      type: stdlib
    - target: asyncio::set_event_loop
      type: stdlib
    - target: tasks.append
      type: unresolved
    - target: unified-search-app/app/app_logic.py::run_drift_search
      type: internal
    - target: unified-search-app/app/app_logic.py::run_basic_search
      type: internal
    - target: unified-search-app/app/app_logic.py::run_local_search
      type: internal
    - target: unified-search-app/app/app_logic.py::run_global_search
      type: internal
    - target: asyncio::gather
      type: stdlib
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_all_searches
    called_by: []
  - name: run_generate_questions
    start_line: 106
    end_line: 119
    code: "async def run_generate_questions(query: str, sv: SessionVariables):\n \
      \   \"\"\"Run global search to generate questions for the dataset.\"\"\"\n \
      \   loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    tasks\
      \ = []\n\n    tasks.append(\n        run_global_search_question_generation(\n\
      \            query=query,\n            sv=sv,\n        )\n    )\n\n    return\
      \ await asyncio.gather(*tasks)"
    signature: 'def run_generate_questions(query: str, sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: asyncio::new_event_loop
      type: stdlib
    - target: asyncio::set_event_loop
      type: stdlib
    - target: tasks.append
      type: unresolved
    - target: unified-search-app/app/app_logic.py::run_global_search_question_generation
      type: internal
    - target: asyncio::gather
      type: stdlib
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_generate_questions
    called_by: []
  - name: run_global_search_question_generation
    start_line: 122
    end_line: 145
    code: "async def run_global_search_question_generation(\n    query: str,\n   \
      \ sv: SessionVariables,\n) -> SearchResult:\n    \"\"\"Run global search question\
      \ generation process.\"\"\"\n    empty_context_data: dict[str, pd.DataFrame]\
      \ = {}\n\n    response, context_data = await api.global_search(\n        config=sv.graphrag_config.value,\n\
      \        entities=sv.entities.value,\n        communities=sv.communities.value,\n\
      \        community_reports=sv.community_reports.value,\n        dynamic_community_selection=True,\n\
      \        response_type=\"Single paragraph\",\n        community_level=sv.dataset_config.value.community_level,\n\
      \        query=query,\n    )\n\n    # display response and reference context\
      \ to UI\n    return SearchResult(\n        search_type=SearchType.Global,\n\
      \        response=str(response),\n        context=context_data if isinstance(context_data,\
      \ dict) else empty_context_data,\n    )"
    signature: "def run_global_search_question_generation(\n    query: str,\n    sv:\
      \ SessionVariables,\n) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: graphrag.api::global_search
      type: internal
    - target: rag.typing::SearchResult
      type: external
    - target: str
      type: builtin
    - target: isinstance
      type: builtin
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_global_search_question_generation
    called_by:
    - source: unified-search-app/app/app_logic.py::run_generate_questions
      type: internal
  - name: run_local_search
    start_line: 148
    end_line: 199
    code: "async def run_local_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult:\n    \"\"\"Run local search.\"\"\"\n    print(f\"Local search\
      \ query: {query}\")  # noqa T201\n\n    # build local search engine\n    response_placeholder\
      \ = st.session_state[\n        f\"{SearchType.Local.value.lower()}_response_placeholder\"\
      \n    ]\n    response_container = st.session_state[f\"{SearchType.Local.value.lower()}_container\"\
      ]\n\n    with response_placeholder, st.spinner(\"Generating answer using local\
      \ search...\"):\n        empty_context_data: dict[str, pd.DataFrame] = {}\n\n\
      \        response, context_data = await api.local_search(\n            config=sv.graphrag_config.value,\n\
      \            communities=sv.communities.value,\n            entities=sv.entities.value,\n\
      \            community_reports=sv.community_reports.value,\n            text_units=sv.text_units.value,\n\
      \            relationships=sv.relationships.value,\n            covariates=sv.covariates.value,\n\
      \            community_level=sv.dataset_config.value.community_level,\n    \
      \        response_type=\"Multiple Paragraphs\",\n            query=query,\n\
      \        )\n\n        print(f\"Local Response: {response}\")  # noqa T201\n\
      \        print(f\"Context data: {context_data}\")  # noqa T201\n\n    # display\
      \ response and reference context to UI\n    search_result = SearchResult(\n\
      \        search_type=SearchType.Local,\n        response=str(response),\n  \
      \      context=context_data if isinstance(context_data, dict) else empty_context_data,\n\
      \    )\n\n    display_search_result(\n        container=response_container,\
      \ result=search_result, stats=None\n    )\n\n    if \"response_lengths\" not\
      \ in st.session_state:\n        st.session_state.response_lengths = []\n\n \
      \   st.session_state[\"response_lengths\"].append({\n        \"result\": search_result,\n\
      \        \"search\": SearchType.Local.value.lower(),\n    })\n\n    return search_result"
    signature: "def run_local_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: rag.typing::SearchType::Local.value.lower
      type: external
    - target: streamlit::spinner
      type: external
    - target: graphrag.api::local_search
      type: internal
    - target: rag.typing::SearchResult
      type: external
    - target: str
      type: builtin
    - target: isinstance
      type: builtin
    - target: ui.search::display_search_result
      type: external
    - target: streamlit::session_state["response_lengths"].append
      type: external
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_local_search
    called_by:
    - source: unified-search-app/app/app_logic.py::run_all_searches
      type: internal
  - name: run_global_search
    start_line: 202
    end_line: 251
    code: "async def run_global_search(query: str, sv: SessionVariables) -> SearchResult:\n\
      \    \"\"\"Run global search.\"\"\"\n    print(f\"Global search query: {query}\"\
      )  # noqa T201\n\n    # build global search engine\n    response_placeholder\
      \ = st.session_state[\n        f\"{SearchType.Global.value.lower()}_response_placeholder\"\
      \n    ]\n    response_container = st.session_state[\n        f\"{SearchType.Global.value.lower()}_container\"\
      \n    ]\n\n    response_placeholder.empty()\n    with response_placeholder,\
      \ st.spinner(\"Generating answer using global search...\"):\n        empty_context_data:\
      \ dict[str, pd.DataFrame] = {}\n\n        response, context_data = await api.global_search(\n\
      \            config=sv.graphrag_config.value,\n            entities=sv.entities.value,\n\
      \            communities=sv.communities.value,\n            community_reports=sv.community_reports.value,\n\
      \            dynamic_community_selection=False,\n            response_type=\"\
      Multiple Paragraphs\",\n            community_level=sv.dataset_config.value.community_level,\n\
      \            query=query,\n        )\n\n        print(f\"Context data: {context_data}\"\
      )  # noqa T201\n        print(f\"Global Response: {response}\")  # noqa T201\n\
      \n    # display response and reference context to UI\n    search_result = SearchResult(\n\
      \        search_type=SearchType.Global,\n        response=str(response),\n \
      \       context=context_data if isinstance(context_data, dict) else empty_context_data,\n\
      \    )\n\n    display_search_result(\n        container=response_container,\
      \ result=search_result, stats=None\n    )\n\n    if \"response_lengths\" not\
      \ in st.session_state:\n        st.session_state.response_lengths = []\n\n \
      \   st.session_state[\"response_lengths\"].append({\n        \"result\": search_result,\n\
      \        \"search\": SearchType.Global.value.lower(),\n    })\n\n    return\
      \ search_result"
    signature: 'def run_global_search(query: str, sv: SessionVariables) -> SearchResult'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: rag.typing::SearchType::Global.value.lower
      type: external
    - target: response_placeholder.empty
      type: unresolved
    - target: streamlit::spinner
      type: external
    - target: graphrag.api::global_search
      type: internal
    - target: rag.typing::SearchResult
      type: external
    - target: str
      type: builtin
    - target: isinstance
      type: builtin
    - target: ui.search::display_search_result
      type: external
    - target: streamlit::session_state["response_lengths"].append
      type: external
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_global_search
    called_by:
    - source: unified-search-app/app/app_logic.py::run_all_searches
      type: internal
  - name: run_drift_search
    start_line: 254
    end_line: 304
    code: "async def run_drift_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult:\n    \"\"\"Run drift search.\"\"\"\n    print(f\"Drift search\
      \ query: {query}\")  # noqa T201\n\n    # build drift search engine\n    response_placeholder\
      \ = st.session_state[\n        f\"{SearchType.Drift.value.lower()}_response_placeholder\"\
      \n    ]\n    response_container = st.session_state[f\"{SearchType.Drift.value.lower()}_container\"\
      ]\n\n    with response_placeholder, st.spinner(\"Generating answer using drift\
      \ search...\"):\n        empty_context_data: dict[str, pd.DataFrame] = {}\n\n\
      \        response, context_data = await api.drift_search(\n            config=sv.graphrag_config.value,\n\
      \            entities=sv.entities.value,\n            communities=sv.communities.value,\n\
      \            community_reports=sv.community_reports.value,\n            text_units=sv.text_units.value,\n\
      \            relationships=sv.relationships.value,\n            community_level=sv.dataset_config.value.community_level,\n\
      \            response_type=\"Multiple Paragraphs\",\n            query=query,\n\
      \        )\n\n        print(f\"Drift Response: {response}\")  # noqa T201\n\
      \        print(f\"Context data: {context_data}\")  # noqa T201\n\n    # display\
      \ response and reference context to UI\n    search_result = SearchResult(\n\
      \        search_type=SearchType.Drift,\n        response=str(response),\n  \
      \      context=context_data if isinstance(context_data, dict) else empty_context_data,\n\
      \    )\n\n    display_search_result(\n        container=response_container,\
      \ result=search_result, stats=None\n    )\n\n    if \"response_lengths\" not\
      \ in st.session_state:\n        st.session_state.response_lengths = []\n\n \
      \   st.session_state[\"response_lengths\"].append({\n        \"result\": None,\n\
      \        \"search\": SearchType.Drift.value.lower(),\n    })\n\n    return search_result"
    signature: "def run_drift_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: rag.typing::SearchType::Drift.value.lower
      type: external
    - target: streamlit::spinner
      type: external
    - target: graphrag.api::drift_search
      type: internal
    - target: rag.typing::SearchResult
      type: external
    - target: str
      type: builtin
    - target: isinstance
      type: builtin
    - target: ui.search::display_search_result
      type: external
    - target: streamlit::session_state["response_lengths"].append
      type: external
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_drift_search
    called_by:
    - source: unified-search-app/app/app_logic.py::run_all_searches
      type: internal
  - name: run_basic_search
    start_line: 307
    end_line: 351
    code: "async def run_basic_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult:\n    \"\"\"Run basic search.\"\"\"\n    print(f\"Basic search\
      \ query: {query}\")  # noqa T201\n\n    # build local search engine\n    response_placeholder\
      \ = st.session_state[\n        f\"{SearchType.Basic.value.lower()}_response_placeholder\"\
      \n    ]\n    response_container = st.session_state[f\"{SearchType.Basic.value.lower()}_container\"\
      ]\n\n    with response_placeholder, st.spinner(\"Generating answer using basic\
      \ RAG...\"):\n        empty_context_data: dict[str, pd.DataFrame] = {}\n\n \
      \       response, context_data = await api.basic_search(\n            config=sv.graphrag_config.value,\n\
      \            text_units=sv.text_units.value,\n            query=query,\n   \
      \     )\n\n        print(f\"Basic Response: {response}\")  # noqa T201\n   \
      \     print(f\"Context data: {context_data}\")  # noqa T201\n\n    # display\
      \ response and reference context to UI\n    search_result = SearchResult(\n\
      \        search_type=SearchType.Basic,\n        response=str(response),\n  \
      \      context=context_data if isinstance(context_data, dict) else empty_context_data,\n\
      \    )\n\n    display_search_result(\n        container=response_container,\
      \ result=search_result, stats=None\n    )\n\n    if \"response_lengths\" not\
      \ in st.session_state:\n        st.session_state.response_lengths = []\n\n \
      \   st.session_state[\"response_lengths\"].append({\n        \"search\": SearchType.Basic.value.lower(),\n\
      \        \"result\": search_result,\n    })\n\n    return search_result"
    signature: "def run_basic_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: rag.typing::SearchType::Basic.value.lower
      type: external
    - target: streamlit::spinner
      type: external
    - target: graphrag.api::basic_search
      type: internal
    - target: rag.typing::SearchResult
      type: external
    - target: str
      type: builtin
    - target: isinstance
      type: builtin
    - target: ui.search::display_search_result
      type: external
    - target: streamlit::session_state["response_lengths"].append
      type: external
    visibility: public
    node_id: unified-search-app/app/app_logic.py::run_basic_search
    called_by:
    - source: unified-search-app/app/app_logic.py::run_all_searches
      type: internal
  - name: load_knowledge_model
    start_line: 354
    end_line: 368
    code: "def load_knowledge_model(sv: SessionVariables):\n    \"\"\"Load knowledge\
      \ model from the datasource.\"\"\"\n    print(\"Loading knowledge model...\"\
      , sv.dataset.value, sv.dataset_config.value)  # noqa T201\n    model = load_model(sv.dataset.value,\
      \ sv.datasource.value)\n\n    sv.generated_questions.value = []\n    sv.selected_question.value\
      \ = \"\"\n    sv.entities.value = model.entities\n    sv.relationships.value\
      \ = model.relationships\n    sv.covariates.value = model.covariates\n    sv.community_reports.value\
      \ = model.community_reports\n    sv.communities.value = model.communities\n\
      \    sv.text_units.value = model.text_units\n\n    return sv"
    signature: 'def load_knowledge_model(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: print
      type: builtin
    - target: knowledge_loader.model::load_model
      type: external
    visibility: public
    node_id: unified-search-app/app/app_logic.py::load_knowledge_model
    called_by:
    - source: unified-search-app/app/app_logic.py::load_dataset
      type: internal
- file_name: unified-search-app/app/data_config.py
  imports: []
  functions: []
- file_name: unified-search-app/app/home_page.py
  imports:
  - module: asyncio
    name: null
    alias: null
  - module: streamlit
    name: null
    alias: st
  - module: app_logic
    name: dataset_name
    alias: null
  - module: app_logic
    name: initialize
    alias: null
  - module: app_logic
    name: run_all_searches
    alias: null
  - module: app_logic
    name: run_generate_questions
    alias: null
  - module: rag.typing
    name: SearchType
    alias: null
  - module: st_tabs
    name: TabBar
    alias: null
  - module: state.session_variables
    name: SessionVariables
    alias: null
  - module: ui.full_graph
    name: create_full_graph_ui
    alias: null
  - module: ui.questions_list
    name: create_questions_list_ui
    alias: null
  - module: ui.report_details
    name: create_report_details_ui
    alias: null
  - module: ui.report_list
    name: create_report_list_ui
    alias: null
  - module: ui.search
    name: display_citations
    alias: null
  - module: ui.search
    name: format_suggested_questions
    alias: null
  - module: ui.search
    name: init_search_ui
    alias: null
  - module: ui.sidebar
    name: create_side_bar
    alias: null
  functions:
  - name: main
    start_line: 21
    end_line: 256
    code: "async def main():\n    \"\"\"Return main streamlit component to render\
      \ the app.\"\"\"\n    sv = initialize()\n\n    create_side_bar(sv)\n\n    st.markdown(\n\
      \        \"#### GraphRAG: A Novel Knowledge Graph-based Approach to Retrieval\
      \ Augmented Generation (RAG)\"\n    )\n    st.markdown(\"##### Dataset selected:\
      \ \" + dataset_name(sv.dataset.value, sv))\n    st.markdown(sv.dataset_config.value.description)\n\
      \n    def on_click_reset(sv: SessionVariables):\n        sv.generated_questions.value\
      \ = []\n        sv.selected_question.value = \"\"\n        sv.show_text_input.value\
      \ = True\n\n    def on_change(sv: SessionVariables):\n        sv.question.value\
      \ = st.session_state[question_input]\n\n    question_input = \"question_input\"\
      \n\n    generate_questions = st.button(\"Suggest some questions\")\n\n    question\
      \ = \"\"\n\n    if len(sv.question.value.strip()) > 0:\n        question = sv.question.value\n\
      \n    if generate_questions:\n        with st.spinner(\"Generating suggested\
      \ questions...\"):\n            try:\n                result = await run_generate_questions(\n\
      \                    query=f\"Generate numbered list only with the top {sv.suggested_questions.value}\
      \ most important questions of this dataset (numbered list only without titles\
      \ or anything extra)\",\n                    sv=sv,\n                )\n   \
      \             for result_item in result:\n                    questions = format_suggested_questions(result_item.response)\n\
      \                    sv.generated_questions.value = questions\n            \
      \        sv.show_text_input.value = False\n            except Exception as e:\
      \  # noqa: BLE001\n                print(f\"Search exception: {e}\")  # noqa\
      \ T201\n                st.write(e)\n\n    if sv.show_text_input.value is True:\n\
      \        st.text_input(\n            \"Ask a question to compare the results\"\
      ,\n            key=question_input,\n            on_change=on_change,\n     \
      \       value=question,\n            kwargs={\"sv\": sv},\n        )\n\n   \
      \ if len(sv.generated_questions.value) != 0:\n        create_questions_list_ui(sv)\n\
      \n    if sv.show_text_input.value is False:\n        st.button(label=\"Reset\"\
      , on_click=on_click_reset, kwargs={\"sv\": sv})\n\n    tab_id = TabBar(\n  \
      \      tabs=[\"Search\", \"Graph Explorer\"],\n        color=\"#fc9e9e\",\n\
      \        activeColor=\"#ff4b4b\",\n        default=0,\n    )\n\n    if tab_id\
      \ == 0:\n        if len(sv.question.value.strip()) > 0:\n            question\
      \ = sv.question.value\n\n        if sv.selected_question.value != \"\":\n  \
      \          question = sv.selected_question.value\n            sv.question.value\
      \ = question\n\n        if question:\n            st.write(f\"##### Answering\
      \ the question: *{question}*\")\n\n        ss_basic = None\n        ss_local\
      \ = None\n        ss_global = None\n        ss_drift = None\n\n        ss_basic_citations\
      \ = None\n        ss_local_citations = None\n        ss_global_citations = None\n\
      \        ss_drift_citations = None\n\n        count = sum([\n            sv.include_basic_rag.value,\n\
      \            sv.include_local_search.value,\n            sv.include_global_search.value,\n\
      \            sv.include_drift_search.value,\n        ])\n\n        if count\
      \ > 0:\n            columns = st.columns(count)\n            index = 0\n   \
      \         if sv.include_basic_rag.value:\n                ss_basic = columns[index]\n\
      \                index += 1\n            if sv.include_local_search.value:\n\
      \                ss_local = columns[index]\n                index += 1\n   \
      \         if sv.include_global_search.value:\n                ss_global = columns[index]\n\
      \                index += 1\n            if sv.include_drift_search.value:\n\
      \                ss_drift = columns[index]\n\n        else:\n            st.write(\"\
      Please select at least one search option from the sidebar.\")\n\n        with\
      \ st.container():\n            if ss_basic:\n                with ss_basic:\n\
      \                    init_search_ui(\n                        container=ss_basic,\n\
      \                        search_type=SearchType.Basic,\n                   \
      \     title=\"##### GraphRAG: Basic RAG\",\n                        caption=\"\
      ###### Answer context: Fixed number of text chunks of raw documents\",\n   \
      \                 )\n\n            if ss_local:\n                with ss_local:\n\
      \                    init_search_ui(\n                        container=ss_local,\n\
      \                        search_type=SearchType.Local,\n                   \
      \     title=\"##### GraphRAG: Local Search\",\n                        caption=\"\
      ###### Answer context: Graph index query results with relevant document text\
      \ chunks\",\n                    )\n\n            if ss_global:\n          \
      \      with ss_global:\n                    init_search_ui(\n              \
      \          container=ss_global,\n                        search_type=SearchType.Global,\n\
      \                        title=\"##### GraphRAG: Global Search\",\n        \
      \                caption=\"###### Answer context: AI-generated network reports\
      \ covering all input documents\",\n                    )\n\n            if ss_drift:\n\
      \                with ss_drift:\n                    init_search_ui(\n     \
      \                   container=ss_drift,\n                        search_type=SearchType.Drift,\n\
      \                        title=\"##### GraphRAG: Drift Search\",\n         \
      \               caption=\"###### Answer context: Includes community information\"\
      ,\n                    )\n\n        count = sum([\n            sv.include_basic_rag.value,\n\
      \            sv.include_local_search.value,\n            sv.include_global_search.value,\n\
      \            sv.include_drift_search.value,\n        ])\n\n        if count\
      \ > 0:\n            columns = st.columns(count)\n            index = 0\n   \
      \         if sv.include_basic_rag.value:\n                ss_basic_citations\
      \ = columns[index]\n                index += 1\n            if sv.include_local_search.value:\n\
      \                ss_local_citations = columns[index]\n                index\
      \ += 1\n            if sv.include_global_search.value:\n                ss_global_citations\
      \ = columns[index]\n                index += 1\n            if sv.include_drift_search.value:\n\
      \                ss_drift_citations = columns[index]\n\n        with st.container():\n\
      \            if ss_basic_citations:\n                with ss_basic_citations:\n\
      \                    st.empty()\n            if ss_local_citations:\n      \
      \          with ss_local_citations:\n                    st.empty()\n      \
      \      if ss_global_citations:\n                with ss_global_citations:\n\
      \                    st.empty()\n            if ss_drift_citations:\n      \
      \          with ss_drift_citations:\n                    st.empty()\n\n    \
      \    if question != \"\" and question != sv.question_in_progress.value:\n  \
      \          sv.question_in_progress.value = question\n            try:\n    \
      \            await run_all_searches(query=question, sv=sv)\n\n             \
      \   if \"response_lengths\" not in st.session_state:\n                    st.session_state.response_lengths\
      \ = []\n\n                for result in st.session_state.response_lengths:\n\
      \                    if result[\"search\"] == SearchType.Basic.value.lower():\n\
      \                        display_citations(\n                            container=ss_basic_citations,\n\
      \                            result=result[\"result\"],\n                  \
      \      )\n                    if result[\"search\"] == SearchType.Local.value.lower():\n\
      \                        display_citations(\n                            container=ss_local_citations,\n\
      \                            result=result[\"result\"],\n                  \
      \      )\n                    if result[\"search\"] == SearchType.Global.value.lower():\n\
      \                        display_citations(\n                            container=ss_global_citations,\n\
      \                            result=result[\"result\"],\n                  \
      \      )\n                    elif result[\"search\"] == SearchType.Drift.value.lower():\n\
      \                        display_citations(\n                            container=ss_drift_citations,\n\
      \                            result=result[\"result\"],\n                  \
      \      )\n            except Exception as e:  # noqa: BLE001\n             \
      \   print(f\"Search exception: {e}\")  # noqa T201\n                st.write(e)\n\
      \n    if tab_id == 1:\n        report_list, graph, report_content = st.columns([0.20,\
      \ 0.55, 0.25])\n\n        with report_list:\n            st.markdown(\"#####\
      \ Community Reports\")\n            create_report_list_ui(sv)\n\n        with\
      \ graph:\n            title, dropdown = st.columns([0.80, 0.20])\n         \
      \   title.markdown(\"##### Entity Graph (All entities)\")\n            dropdown.selectbox(\n\
      \                \"Community level\", options=[0, 1], key=sv.graph_community_level.key\n\
      \            )\n            create_full_graph_ui(sv)\n\n        with report_content:\n\
      \            st.markdown(\"##### Selected Report\")\n            create_report_details_ui(sv)"
    signature: def main()
    decorators: []
    raises: []
    calls:
    - target: app_logic::initialize
      type: external
    - target: ui.sidebar::create_side_bar
      type: external
    - target: streamlit::markdown
      type: external
    - target: app_logic::dataset_name
      type: external
    - target: streamlit::button
      type: external
    - target: len
      type: builtin
    - target: sv.question.value.strip
      type: unresolved
    - target: streamlit::spinner
      type: external
    - target: app_logic::run_generate_questions
      type: external
    - target: ui.search::format_suggested_questions
      type: external
    - target: print
      type: builtin
    - target: streamlit::write
      type: external
    - target: streamlit::text_input
      type: external
    - target: ui.questions_list::create_questions_list_ui
      type: external
    - target: st_tabs::TabBar
      type: external
    - target: sum
      type: builtin
    - target: streamlit::columns
      type: external
    - target: streamlit::container
      type: external
    - target: ui.search::init_search_ui
      type: external
    - target: streamlit::empty
      type: external
    - target: app_logic::run_all_searches
      type: external
    - target: rag.typing::SearchType::Basic.value.lower
      type: external
    - target: ui.search::display_citations
      type: external
    - target: rag.typing::SearchType::Local.value.lower
      type: external
    - target: rag.typing::SearchType::Global.value.lower
      type: external
    - target: rag.typing::SearchType::Drift.value.lower
      type: external
    - target: ui.report_list::create_report_list_ui
      type: external
    - target: title.markdown
      type: unresolved
    - target: dropdown.selectbox
      type: unresolved
    - target: ui.full_graph::create_full_graph_ui
      type: external
    - target: ui.report_details::create_report_details_ui
      type: external
    visibility: public
    node_id: unified-search-app/app/home_page.py::main
    called_by: []
  - name: on_click_reset
    start_line: 33
    end_line: 36
    code: "def on_click_reset(sv: SessionVariables):\n        sv.generated_questions.value\
      \ = []\n        sv.selected_question.value = \"\"\n        sv.show_text_input.value\
      \ = True"
    signature: 'def on_click_reset(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/home_page.py::on_click_reset
    called_by: []
  - name: on_change
    start_line: 38
    end_line: 39
    code: "def on_change(sv: SessionVariables):\n        sv.question.value = st.session_state[question_input]"
    signature: 'def on_change(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/home_page.py::on_change
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/knowledge_loader/data_prep.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: data_config
    name: null
    alias: config
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: knowledge_loader.data_sources.typing
    name: Datasource
    alias: null
  functions:
  - name: get_entity_data
    start_line: 23
    end_line: 29
    code: "def get_entity_data(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with entity data from the indexed-data.\"\"\"\n\
      \    entity_details_df = _datasource.read(config.entity_table)\n\n    print(f\"\
      Entity records: {len(entity_details_df)}\")  # noqa T201\n    print(f\"Dataset:\
      \ {dataset}\")  # noqa T201\n    return entity_details_df"
    signature: 'def get_entity_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    - target: print
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_entity_data
    called_by: []
  - name: get_relationship_data
    start_line: 33
    end_line: 38
    code: "def get_relationship_data(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with entity-entity relationship data from the\
      \ indexed-data.\"\"\"\n    relationship_df = _datasource.read(config.relationship_table)\n\
      \    print(f\"Relationship records: {len(relationship_df)}\")  # noqa T201\n\
      \    print(f\"Dataset: {dataset}\")  # noqa T201\n    return relationship_df"
    signature: 'def get_relationship_data(dataset: str, _datasource: Datasource) ->
      pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    - target: print
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_relationship_data
    called_by: []
  - name: get_covariate_data
    start_line: 42
    end_line: 47
    code: "def get_covariate_data(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with covariate data from the indexed-data.\"\"\
      \"\n    covariate_df = _datasource.read(config.covariate_table)\n    print(f\"\
      Covariate records: {len(covariate_df)}\")  # noqa T201\n    print(f\"Dataset:\
      \ {dataset}\")  # noqa T201\n    return covariate_df"
    signature: 'def get_covariate_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    - target: print
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_covariate_data
    called_by: []
  - name: get_text_unit_data
    start_line: 51
    end_line: 56
    code: "def get_text_unit_data(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with text units (i.e. chunks of text from the\
      \ raw documents) from the indexed-data.\"\"\"\n    text_unit_df = _datasource.read(config.text_unit_table)\n\
      \    print(f\"Text unit records: {len(text_unit_df)}\")  # noqa T201\n    print(f\"\
      Dataset: {dataset}\")  # noqa T201\n    return text_unit_df"
    signature: 'def get_text_unit_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    - target: print
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_text_unit_data
    called_by: []
  - name: get_community_report_data
    start_line: 60
    end_line: 67
    code: "def get_community_report_data(\n    _datasource: Datasource,\n) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with community report data from the indexed-data.\"\
      \"\"\n    report_df = _datasource.read(config.community_report_table)\n    print(f\"\
      Report records: {len(report_df)}\")  # noqa T201\n\n    return report_df"
    signature: "def get_community_report_data(\n    _datasource: Datasource,\n) ->\
      \ pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    - target: print
      type: builtin
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_community_report_data
    called_by: []
  - name: get_communities_data
    start_line: 71
    end_line: 75
    code: "def get_communities_data(\n    _datasource: Datasource,\n) -> pd.DataFrame:\n\
      \    \"\"\"Return a dataframe with communities data from the indexed-data.\"\
      \"\"\n    return _datasource.read(config.communities_table)"
    signature: "def get_communities_data(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=config.default_ttl)'
    raises: []
    calls:
    - target: _datasource.read
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_communities_data
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  imports:
  - module: io
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: io
    name: BytesIO
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: yaml
    name: null
    alias: null
  - module: azure.identity
    name: DefaultAzureCredential
    alias: null
  - module: azure.storage.blob
    name: BlobServiceClient
    alias: null
  - module: azure.storage.blob
    name: ContainerClient
    alias: null
  - module: knowledge_loader.data_sources.typing
    name: Datasource
    alias: null
  - module: graphrag.config.create_graphrag_config
    name: create_graphrag_config
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: _get_container
    start_line: 29
    end_line: 35
    code: "def _get_container(account_name: str, container_name: str) -> ContainerClient:\n\
      \    \"\"\"Return container from blob storage.\"\"\"\n    print(\"LOGIN---------------\"\
      )  # noqa T201\n    account_url = f\"https://{account_name}.blob.core.windows.net\"\
      \n    default_credential = DefaultAzureCredential()\n    blob_service_client\
      \ = BlobServiceClient(account_url, credential=default_credential)\n    return\
      \ blob_service_client.get_container_client(container_name)"
    signature: 'def _get_container(account_name: str, container_name: str) -> ContainerClient'
    decorators:
    - '@st.cache_data(ttl=60 * 60 * 24)'
    raises: []
    calls:
    - target: print
      type: builtin
    - target: azure.identity::DefaultAzureCredential
      type: external
    - target: azure.storage.blob::BlobServiceClient
      type: external
    - target: blob_service_client.get_container_client
      type: unresolved
    visibility: protected
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
    called_by:
    - source: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_prompt_config
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
      type: internal
  - name: load_blob_prompt_config
    start_line: 38
    end_line: 57
    code: "def load_blob_prompt_config(\n    dataset: str,\n    account_name: str\
      \ | None = blob_account_name,\n    container_name: str | None = blob_container_name,\n\
      ) -> dict[str, str]:\n    \"\"\"Load blob prompt configuration.\"\"\"\n    if\
      \ account_name is None or container_name is None:\n        return {}\n\n   \
      \ container_client = _get_container(account_name, container_name)\n    prompts\
      \ = {}\n\n    prefix = f\"{dataset}/prompts\"\n    for file in container_client.list_blobs(name_starts_with=prefix):\n\
      \        map_name = file.name.split(\"/\")[-1].split(\".\")[0]\n        prompts[map_name]\
      \ = (\n            container_client.download_blob(file.name).readall().decode(\"\
      utf-8\")\n        )\n\n    return prompts"
    signature: "def load_blob_prompt_config(\n    dataset: str,\n    account_name:\
      \ str | None = blob_account_name,\n    container_name: str | None = blob_container_name,\n\
      ) -> dict[str, str]"
    decorators: []
    raises: []
    calls:
    - target: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
      type: internal
    - target: container_client.list_blobs
      type: unresolved
    - target: file.name.split("/")[-1].split
      type: unresolved
    - target: file.name.split
      type: unresolved
    - target: container_client.download_blob(file.name).readall().decode
      type: unresolved
    - target: container_client.download_blob(file.name).readall
      type: unresolved
    - target: container_client.download_blob
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_prompt_config
    called_by: []
  - name: load_blob_file
    start_line: 60
    end_line: 78
    code: "def load_blob_file(\n    dataset: str | None,\n    file: str | None,\n\
      \    account_name: str | None = blob_account_name,\n    container_name: str\
      \ | None = blob_container_name,\n) -> BytesIO:\n    \"\"\"Load blob file from\
      \ container.\"\"\"\n    stream = io.BytesIO()\n\n    if account_name is None\
      \ or container_name is None:\n        logger.warning(\"No account name or container\
      \ name provided\")\n        return stream\n\n    container_client = _get_container(account_name,\
      \ container_name)\n    blob_path = f\"{dataset}/{file}\" if dataset is not None\
      \ else file\n\n    container_client.download_blob(blob_path).readinto(stream)\n\
      \n    return stream"
    signature: "def load_blob_file(\n    dataset: str | None,\n    file: str | None,\n\
      \    account_name: str | None = blob_account_name,\n    container_name: str\
      \ | None = blob_container_name,\n) -> BytesIO"
    decorators: []
    raises: []
    calls:
    - target: io::BytesIO
      type: stdlib
    - target: logger.warning
      type: unresolved
    - target: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
      type: internal
    - target: container_client.download_blob(blob_path).readinto
      type: unresolved
    - target: container_client.download_blob
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
    called_by:
    - source: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
      type: internal
  - name: __init__
    start_line: 84
    end_line: 86
    code: "def __init__(self, database: str):\n        \"\"\"Init method definition.\"\
      \"\"\n        self._database = database"
    signature: 'def __init__(self, database: str)'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.__init__
    called_by: []
  - name: read
    start_line: 88
    end_line: 104
    code: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame:\n\
      \        \"\"\"Read file from container.\"\"\"\n        try:\n            data\
      \ = load_blob_file(self._database, f\"{table}.parquet\")\n        except Exception\
      \ as err:\n            if throw_on_missing:\n                error_msg = f\"\
      Table {table} does not exist\"\n                raise FileNotFoundError(error_msg)\
      \ from err\n            logger.warning(\"Table %s does not exist\", table)\n\
      \            return pd.DataFrame(columns=columns) if columns else pd.DataFrame()\n\
      \n        return pd.read_parquet(data, columns=columns)"
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
      type: internal
    - target: FileNotFoundError
      type: builtin
    - target: logger.warning
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: pandas::read_parquet
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
    called_by: []
  - name: read_settings
    start_line: 106
    end_line: 127
    code: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None:\n        \"\"\"Read settings\
      \ from container.\"\"\"\n        try:\n            settings = load_blob_file(self._database,\
      \ file)\n            settings.seek(0)\n            str_settings = settings.read().decode(\"\
      utf-8\")\n            config = os.path.expandvars(str_settings)\n          \
      \  settings_yaml = yaml.safe_load(config)\n            graphrag_config = create_graphrag_config(values=settings_yaml)\n\
      \        except Exception as err:\n            if throw_on_missing:\n      \
      \          error_msg = f\"File {file} does not exist\"\n                raise\
      \ FileNotFoundError(error_msg) from err\n\n            logger.warning(\"File\
      \ %s does not exist\", file)\n            return None\n\n        return graphrag_config"
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
      type: internal
    - target: settings.seek
      type: unresolved
    - target: settings.read().decode
      type: unresolved
    - target: settings.read
      type: unresolved
    - target: os::path.expandvars
      type: stdlib
    - target: yaml::safe_load
      type: external
    - target: graphrag/config/create_graphrag_config.py::create_graphrag_config
      type: internal
    - target: FileNotFoundError
      type: builtin
    - target: logger.warning
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/default.py
  imports:
  - module: os
    name: null
    alias: null
  functions: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/loader.py
  imports:
  - module: json
    name: null
    alias: null
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: knowledge_loader.data_sources.blob_source
    name: BlobDatasource
    alias: null
  - module: knowledge_loader.data_sources.blob_source
    name: load_blob_file
    alias: null
  - module: knowledge_loader.data_sources.blob_source
    name: load_blob_prompt_config
    alias: null
  - module: knowledge_loader.data_sources.default
    name: LISTING_FILE
    alias: null
  - module: knowledge_loader.data_sources.default
    name: blob_account_name
    alias: null
  - module: knowledge_loader.data_sources.default
    name: local_data_root
    alias: null
  - module: knowledge_loader.data_sources.local_source
    name: LocalDatasource
    alias: null
  - module: knowledge_loader.data_sources.local_source
    name: load_local_prompt_config
    alias: null
  - module: knowledge_loader.data_sources.typing
    name: DatasetConfig
    alias: null
  - module: knowledge_loader.data_sources.typing
    name: Datasource
    alias: null
  functions:
  - name: _get_base_path
    start_line: 31
    end_line: 40
    code: "def _get_base_path(\n    dataset: str | None, root: str | None, extra_path:\
      \ str | None = None\n) -> str:\n    \"\"\"Construct and return the base path\
      \ for the given dataset and extra path.\"\"\"\n    return os.path.join(  # noqa:\
      \ PTH118\n        os.path.dirname(os.path.realpath(__file__)),  # noqa: PTH120\n\
      \        root if root else \"\",\n        dataset if dataset else \"\",\n  \
      \      *(extra_path.split(\"/\") if extra_path else []),\n    )"
    signature: "def _get_base_path(\n    dataset: str | None, root: str | None, extra_path:\
      \ str | None = None\n) -> str"
    decorators: []
    raises: []
    calls:
    - target: os::path.join
      type: stdlib
    - target: os::path.dirname
      type: stdlib
    - target: os::path.realpath
      type: stdlib
    - target: extra_path.split
      type: unresolved
    visibility: protected
    node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
    called_by:
    - source: unified-search-app/app/knowledge_loader/data_sources/loader.py::create_datasource
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_dataset_listing
      type: internal
    - source: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_prompts
      type: internal
  - name: create_datasource
    start_line: 43
    end_line: 49
    code: "def create_datasource(dataset_folder: str) -> Datasource:\n    \"\"\"Return\
      \ a datasource that reads from a local or blob storage parquet file.\"\"\"\n\
      \    if blob_account_name is not None and blob_account_name != \"\":\n     \
      \   return BlobDatasource(dataset_folder)\n\n    base_path = _get_base_path(dataset_folder,\
      \ local_data_root)\n    return LocalDatasource(base_path)"
    signature: 'def create_datasource(dataset_folder: str) -> Datasource'
    decorators: []
    raises: []
    calls:
    - target: knowledge_loader.data_sources.blob_source::BlobDatasource
      type: external
    - target: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
      type: internal
    - target: knowledge_loader.data_sources.local_source::LocalDatasource
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::create_datasource
    called_by: []
  - name: load_dataset_listing
    start_line: 52
    end_line: 69
    code: "def load_dataset_listing() -> list[DatasetConfig]:\n    \"\"\"Load dataset\
      \ listing file.\"\"\"\n    datasets = []\n    if blob_account_name is not None\
      \ and blob_account_name != \"\":\n        try:\n            blob = load_blob_file(None,\
      \ LISTING_FILE)\n            datasets_str = blob.getvalue().decode(\"utf-8\"\
      )\n            if datasets_str:\n                datasets = json.loads(datasets_str)\n\
      \        except Exception as e:  # noqa: BLE001\n            print(f\"Error\
      \ loading dataset config: {e}\")  # noqa T201\n            return []\n    else:\n\
      \        base_path = _get_base_path(None, local_data_root, LISTING_FILE)\n \
      \       with open(base_path, \"r\") as file:  # noqa: UP015, PTH123\n      \
      \      datasets = json.load(file)\n\n    return [DatasetConfig(**d) for d in\
      \ datasets]"
    signature: def load_dataset_listing() -> list[DatasetConfig]
    decorators: []
    raises: []
    calls:
    - target: knowledge_loader.data_sources.blob_source::load_blob_file
      type: external
    - target: blob.getvalue().decode
      type: unresolved
    - target: blob.getvalue
      type: unresolved
    - target: json::loads
      type: stdlib
    - target: print
      type: builtin
    - target: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
      type: internal
    - target: open
      type: builtin
    - target: json::load
      type: stdlib
    - target: knowledge_loader.data_sources.typing::DatasetConfig
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_dataset_listing
    called_by: []
  - name: load_prompts
    start_line: 72
    end_line: 78
    code: "def load_prompts(dataset: str) -> dict[str, str]:\n    \"\"\"Return the\
      \ prompts configuration for a specific dataset.\"\"\"\n    if blob_account_name\
      \ is not None and blob_account_name != \"\":\n        return load_blob_prompt_config(dataset)\n\
      \n    base_path = _get_base_path(dataset, local_data_root, \"prompts\")\n  \
      \  return load_local_prompt_config(base_path)"
    signature: 'def load_prompts(dataset: str) -> dict[str, str]'
    decorators: []
    raises: []
    calls:
    - target: knowledge_loader.data_sources.blob_source::load_blob_prompt_config
      type: external
    - target: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
      type: internal
    - target: knowledge_loader.data_sources.local_source::load_local_prompt_config
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_prompts
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  imports:
  - module: logging
    name: null
    alias: null
  - module: os
    name: null
    alias: null
  - module: pathlib
    name: Path
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: knowledge_loader.data_sources.typing
    name: Datasource
    alias: null
  - module: graphrag.config.load_config
    name: load_config
    alias: null
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: load_local_prompt_config
    start_line: 21
    end_line: 30
    code: "def load_local_prompt_config(base_path=\"\") -> dict[str, str]:\n    \"\
      \"\"Load local prompt configuration.\"\"\"\n    # for each file inside folder\
      \ base_path\n    prompts = {}\n\n    for path in os.listdir(base_path):  # noqa:\
      \ PTH208\n        with open(os.path.join(base_path, path), \"r\") as f:  # noqa:\
      \ UP015, PTH123, PTH118\n            map_name = path.split(\".\")[0]\n     \
      \       prompts[map_name] = f.read()\n    return prompts"
    signature: def load_local_prompt_config(base_path="") -> dict[str, str]
    decorators: []
    raises: []
    calls:
    - target: os::listdir
      type: stdlib
    - target: open
      type: builtin
    - target: os::path.join
      type: stdlib
    - target: path.split
      type: unresolved
    - target: f.read
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::load_local_prompt_config
    called_by: []
  - name: __init__
    start_line: 38
    end_line: 40
    code: "def __init__(self, base_path: str):\n        \"\"\"Init method definition.\"\
      \"\"\n        self._base_path = base_path"
    signature: 'def __init__(self, base_path: str)'
    decorators: []
    raises: []
    calls: []
    visibility: protected
    node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.__init__
    called_by: []
  - name: read
    start_line: 42
    end_line: 62
    code: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame:\n\
      \        \"\"\"Read file from local source.\"\"\"\n        table = os.path.join(self._base_path,\
      \ f\"{table}.parquet\")  # noqa: PTH118\n\n        if not os.path.exists(table):\
      \  # noqa: PTH110\n            if throw_on_missing:\n                error_msg\
      \ = f\"Table {table} does not exist\"\n                raise FileNotFoundError(error_msg)\n\
      \n            print(f\"Table {table} does not exist\")  # noqa T201\n      \
      \      return (\n                pd.DataFrame(data=[], columns=columns)\n  \
      \              if columns is not None\n                else pd.DataFrame()\n\
      \            )\n        return pd.read_parquet(table, columns=columns)"
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    decorators: []
    raises:
    - FileNotFoundError
    calls:
    - target: os::path.join
      type: stdlib
    - target: os::path.exists
      type: stdlib
    - target: FileNotFoundError
      type: builtin
    - target: print
      type: builtin
    - target: pandas::DataFrame
      type: external
    - target: pandas::read_parquet
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read
    called_by: []
  - name: read_settings
    start_line: 64
    end_line: 72
    code: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None:\n        \"\"\"Read settings\
      \ file from local source.\"\"\"\n        cwd = Path(__file__).parent\n     \
      \   root_dir = (cwd / self._base_path).resolve()\n        return load_config(root_dir=root_dir)"
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    decorators: []
    raises: []
    calls:
    - target: pathlib::Path
      type: stdlib
    - target: (cwd / self._base_path).resolve
      type: unresolved
    - target: graphrag/config/load_config.py::load_config
      type: internal
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/data_sources/typing.py
  imports:
  - module: abc
    name: ABC
    alias: null
  - module: abc
    name: abstractmethod
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: graphrag.config.models.graph_rag_config
    name: GraphRagConfig
    alias: null
  functions:
  - name: __call__
    start_line: 28
    end_line: 30
    code: "def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame:\n\
      \        \"\"\"Call method definition.\"\"\"\n        raise NotImplementedError"
    signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: protected
    node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.__call__
    called_by: []
  - name: read
    start_line: 33
    end_line: 40
    code: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame:\n\
      \        \"\"\"Read method definition.\"\"\"\n        raise NotImplementedError"
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read
    called_by: []
  - name: read_settings
    start_line: 43
    end_line: 45
    code: "def read_settings(self, file: str) -> GraphRagConfig | None:\n        \"\
      \"\"Read settings method definition.\"\"\"\n        raise NotImplementedError"
    signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
    decorators:
    - '@abstractmethod'
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read_settings
    called_by: []
  - name: write
    start_line: 47
    end_line: 51
    code: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
      \ | None = None\n    ) -> None:\n        \"\"\"Write method definition.\"\"\"\
      \n        raise NotImplementedError"
    signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
      \ | None = None\n    ) -> None"
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.write
    called_by: []
  - name: has_table
    start_line: 53
    end_line: 55
    code: "def has_table(self, table: str) -> bool:\n        \"\"\"Check if table\
      \ exists method definition.\"\"\"\n        raise NotImplementedError"
    signature: 'def has_table(self, table: str) -> bool'
    decorators: []
    raises:
    - NotImplementedError
    calls: []
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.has_table
    called_by: []
- file_name: unified-search-app/app/knowledge_loader/model.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: data_config
    name: default_ttl
    alias: null
  - module: knowledge_loader.data_prep
    name: get_communities_data
    alias: null
  - module: knowledge_loader.data_prep
    name: get_community_report_data
    alias: null
  - module: knowledge_loader.data_prep
    name: get_covariate_data
    alias: null
  - module: knowledge_loader.data_prep
    name: get_entity_data
    alias: null
  - module: knowledge_loader.data_prep
    name: get_relationship_data
    alias: null
  - module: knowledge_loader.data_prep
    name: get_text_unit_data
    alias: null
  - module: knowledge_loader.data_sources.typing
    name: Datasource
    alias: null
  functions:
  - name: load_entities
    start_line: 30
    end_line: 35
    code: "def load_entities(\n    dataset: str,\n    _datasource: Datasource,\n)\
      \ -> pd.DataFrame:\n    \"\"\"Return a list of Entity objects.\"\"\"\n    return\
      \ get_entity_data(dataset, _datasource)"
    signature: "def load_entities(\n    dataset: str,\n    _datasource: Datasource,\n\
      ) -> pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_entity_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_entities
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_entity_relationships
    start_line: 39
    end_line: 44
    code: "def load_entity_relationships(\n    dataset: str,\n    _datasource: Datasource,\n\
      ) -> pd.DataFrame:\n    \"\"\"Return lists of Entity and Relationship objects.\"\
      \"\"\n    return get_relationship_data(dataset, _datasource)"
    signature: "def load_entity_relationships(\n    dataset: str,\n    _datasource:\
      \ Datasource,\n) -> pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_relationship_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_entity_relationships
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_covariates
    start_line: 48
    end_line: 50
    code: "def load_covariates(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a dictionary of Covariate objects, with the key being the\
      \ covariate type.\"\"\"\n    return get_covariate_data(dataset, _datasource)"
    signature: 'def load_covariates(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_covariate_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_covariates
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_community_reports
    start_line: 54
    end_line: 58
    code: "def load_community_reports(\n    _datasource: Datasource,\n) -> pd.DataFrame:\n\
      \    \"\"\"Return a list of CommunityReport objects.\"\"\"\n    return get_community_report_data(_datasource)"
    signature: "def load_community_reports(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_community_report_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_community_reports
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_communities
    start_line: 62
    end_line: 66
    code: "def load_communities(\n    _datasource: Datasource,\n) -> pd.DataFrame:\n\
      \    \"\"\"Return a list of Communities objects.\"\"\"\n    return get_communities_data(_datasource)"
    signature: "def load_communities(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_communities_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_communities
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_text_units
    start_line: 70
    end_line: 72
    code: "def load_text_units(dataset: str, _datasource: Datasource) -> pd.DataFrame:\n\
      \    \"\"\"Return a list of TextUnit objects.\"\"\"\n    return get_text_unit_data(dataset,\
      \ _datasource)"
    signature: 'def load_text_units(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    decorators:
    - '@st.cache_data(ttl=default_ttl)'
    raises: []
    calls:
    - target: knowledge_loader.data_prep::get_text_unit_data
      type: external
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_text_units
    called_by:
    - source: unified-search-app/app/knowledge_loader/model.py::load_model
      type: internal
  - name: load_model
    start_line: 87
    end_line: 110
    code: "def load_model(\n    dataset: str,\n    datasource: Datasource,\n):\n \
      \   \"\"\"\n    Load all relevant graph-indexed data into collections of knowledge\
      \ model objects and store the model collections in the session variables.\n\n\
      \    This is a one-time data retrieval and preparation per session.\n    \"\"\
      \"\n    entities = load_entities(dataset, datasource)\n    relationships = load_entity_relationships(dataset,\
      \ datasource)\n    covariates = load_covariates(dataset, datasource)\n    community_reports\
      \ = load_community_reports(datasource)\n    communities = load_communities(datasource)\n\
      \    text_units = load_text_units(dataset, datasource)\n\n    return KnowledgeModel(\n\
      \        entities=entities,\n        relationships=relationships,\n        community_reports=community_reports,\n\
      \        communities=communities,\n        text_units=text_units,\n        covariates=(None\
      \ if covariates.empty else covariates),\n    )"
    signature: "def load_model(\n    dataset: str,\n    datasource: Datasource,\n)"
    decorators: []
    raises: []
    calls:
    - target: unified-search-app/app/knowledge_loader/model.py::load_entities
      type: internal
    - target: unified-search-app/app/knowledge_loader/model.py::load_entity_relationships
      type: internal
    - target: unified-search-app/app/knowledge_loader/model.py::load_covariates
      type: internal
    - target: unified-search-app/app/knowledge_loader/model.py::load_community_reports
      type: internal
    - target: unified-search-app/app/knowledge_loader/model.py::load_communities
      type: internal
    - target: unified-search-app/app/knowledge_loader/model.py::load_text_units
      type: internal
    - target: KnowledgeModel
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/knowledge_loader/model.py::load_model
    called_by: []
- file_name: unified-search-app/app/rag/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/rag/typing.py
  imports:
  - module: dataclasses
    name: dataclass
    alias: null
  - module: enum
    name: Enum
    alias: null
  - module: pandas
    name: null
    alias: pd
  functions: []
- file_name: unified-search-app/app/state/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/state/query_variable.py
  imports:
  - module: typing
    name: Any
    alias: null
  - module: streamlit
    name: null
    alias: st
  functions:
  - name: __init__
    start_line: 20
    end_line: 29
    code: "def __init__(self, key: str, default: Any | None):\n        \"\"\"Init\
      \ method definition.\"\"\"\n        self._key = key\n        val = st.query_params[key].lower()\
      \ if key in st.query_params else default\n        if val == \"true\":\n    \
      \        val = True\n        elif val == \"false\":\n            val = False\n\
      \        if key not in st.session_state:\n            st.session_state[key]\
      \ = val"
    signature: 'def __init__(self, key: str, default: Any | None)'
    decorators: []
    raises: []
    calls:
    - target: streamlit::query_params[key].lower
      type: external
    visibility: protected
    node_id: unified-search-app/app/state/query_variable.py::QueryVariable.__init__
    called_by: []
  - name: key
    start_line: 32
    end_line: 34
    code: "def key(self) -> str:\n        \"\"\"Key property definition.\"\"\"\n \
      \       return self._key"
    signature: def key(self) -> str
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/state/query_variable.py::QueryVariable.key
    called_by: []
  - name: value
    start_line: 37
    end_line: 39
    code: "def value(self) -> Any:\n        \"\"\"Value property definition.\"\"\"\
      \n        return st.session_state[self._key]"
    signature: def value(self) -> Any
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/state/query_variable.py::QueryVariable.value
    called_by: []
  - name: value
    start_line: 42
    end_line: 45
    code: "def value(self, value: Any) -> None:\n        \"\"\"Value setter definition.\"\
      \"\"\n        st.session_state[self._key] = value\n        st.query_params[self._key]\
      \ = f\"{value}\".lower()"
    signature: 'def value(self, value: Any) -> None'
    decorators:
    - '@value.setter'
    raises: []
    calls:
    - target: f"{value}".lower
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/state/query_variable.py::QueryVariable.value
    called_by: []
- file_name: unified-search-app/app/state/session_variable.py
  imports:
  - module: traceback
    name: null
    alias: null
  - module: typing
    name: Any
    alias: null
  - module: streamlit
    name: null
    alias: st
  functions:
  - name: __init__
    start_line: 15
    end_line: 34
    code: "def __init__(self, default: Any = \"\", prefix: str = \"\"):\n        \"\
      \"\"Create a managed session variable with a default value and a prefix.\n\n\
      \        The prefix is used to avoid collisions between variables with the same\
      \ name.\n\n        To modify the variable use the value property, for example:\
      \ `name.value = \"Bob\"`\n        To get the value use the variable itself,\
      \ for example: `name`\n\n        Use this class to avoid using st.session_state\
      \ dictionary directly and be able to\n        just use the variables. These\
      \ variables will share values across files as long as you use\n        the same\
      \ variable name and prefix.\n        \"\"\"\n        (_, _, _, text) = traceback.extract_stack()[-2]\n\
      \        var_name = text[: text.find(\"=\")].strip()\n\n        self._key =\
      \ \"_\".join(arg for arg in [prefix, var_name] if arg != \"\")\n        self._value\
      \ = default\n\n        if self._key not in st.session_state:\n            st.session_state[self._key]\
      \ = default"
    signature: 'def __init__(self, default: Any = "", prefix: str = "")'
    decorators: []
    raises: []
    calls:
    - target: traceback::extract_stack
      type: stdlib
    - target: 'text[: text.find("=")].strip'
      type: unresolved
    - target: text.find
      type: unresolved
    - target: '"_".join'
      type: unresolved
    visibility: protected
    node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__init__
    called_by: []
  - name: key
    start_line: 37
    end_line: 39
    code: "def key(self) -> str:\n        \"\"\"Key property definition.\"\"\"\n \
      \       return self._key"
    signature: def key(self) -> str
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/state/session_variable.py::SessionVariable.key
    called_by: []
  - name: value
    start_line: 42
    end_line: 44
    code: "def value(self) -> Any:\n        \"\"\"Value property definition.\"\"\"\
      \n        return st.session_state[self._key]"
    signature: def value(self) -> Any
    decorators:
    - '@property'
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/state/session_variable.py::SessionVariable.value
    called_by: []
  - name: value
    start_line: 47
    end_line: 49
    code: "def value(self, value: Any) -> None:\n        \"\"\"Value setter definition.\"\
      \"\"\n        st.session_state[self._key] = value"
    signature: 'def value(self, value: Any) -> None'
    decorators:
    - '@value.setter'
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/state/session_variable.py::SessionVariable.value
    called_by: []
  - name: __repr__
    start_line: 51
    end_line: 53
    code: "def __repr__(self) -> Any:\n        \"\"\"Repr method definition.\"\"\"\
      \n        return str(st.session_state[self._key])"
    signature: def __repr__(self) -> Any
    decorators: []
    raises: []
    calls:
    - target: str
      type: builtin
    visibility: protected
    node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__repr__
    called_by: []
- file_name: unified-search-app/app/state/session_variables.py
  imports:
  - module: data_config
    name: default_suggested_questions
    alias: null
  - module: state.query_variable
    name: QueryVariable
    alias: null
  - module: state.session_variable
    name: SessionVariable
    alias: null
  functions:
  - name: __init__
    start_line: 16
    end_line: 42
    code: "def __init__(self):\n        \"\"\"Init method definition.\"\"\"\n    \
      \    self.dataset = QueryVariable(\"dataset\", \"\")\n        self.datasets\
      \ = SessionVariable([])\n        self.dataset_config = SessionVariable()\n \
      \       self.datasource = SessionVariable()\n        self.graphrag_config =\
      \ SessionVariable()\n        self.question = QueryVariable(\"question\", \"\"\
      )\n        self.suggested_questions = SessionVariable(default_suggested_questions)\n\
      \        self.entities = SessionVariable([])\n        self.relationships = SessionVariable([])\n\
      \        self.covariates = SessionVariable({})\n        self.communities = SessionVariable([])\n\
      \        self.community_reports = SessionVariable([])\n        self.text_units\
      \ = SessionVariable([])\n        self.question_in_progress = SessionVariable(\"\
      \")\n        self.include_global_search = QueryVariable(\"include_global_search\"\
      , True)\n        self.include_local_search = QueryVariable(\"include_local_search\"\
      , True)\n        self.include_drift_search = QueryVariable(\"include_drift_search\"\
      , False)\n        self.include_basic_rag = QueryVariable(\"include_basic_rag\"\
      , False)\n\n        self.selected_report = SessionVariable()\n        self.graph_community_level\
      \ = SessionVariable(0)\n\n        self.selected_question = SessionVariable(\"\
      \")\n        self.generated_questions = SessionVariable([])\n        self.show_text_input\
      \ = SessionVariable(True)"
    signature: def __init__(self)
    decorators: []
    raises: []
    calls:
    - target: state.query_variable::QueryVariable
      type: external
    - target: state.session_variable::SessionVariable
      type: external
    visibility: protected
    node_id: unified-search-app/app/state/session_variables.py::SessionVariables.__init__
    called_by: []
- file_name: unified-search-app/app/ui/__init__.py
  imports: []
  functions: []
- file_name: unified-search-app/app/ui/full_graph.py
  imports:
  - module: altair
    name: null
    alias: alt
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: state.session_variables
    name: SessionVariables
    alias: null
  functions:
  - name: create_full_graph_ui
    start_line: 12
    end_line: 56
    code: "def create_full_graph_ui(sv: SessionVariables):\n    \"\"\"Return graph\
      \ UI object.\"\"\"\n    entities = sv.entities.value.copy()\n    communities\
      \ = sv.communities.value.copy()\n\n    if not communities.empty and not entities.empty:\n\
      \        communities_entities = (\n            communities.explode(\"entity_ids\"\
      )\n            .merge(\n                entities,\n                left_on=\"\
      entity_ids\",\n                right_on=\"id\",\n                suffixes=(\"\
      _entities\", \"_communities\"),\n            )\n            .dropna(subset=[\"\
      x\", \"y\"])\n        )\n    else:\n        communities_entities = pd.DataFrame()\n\
      \n    level = sv.graph_community_level.value\n    communities_entities_filtered\
      \ = communities_entities[\n        communities_entities[\"level\"] == level\n\
      \    ]\n\n    graph = (\n        alt.Chart(communities_entities_filtered)\n\
      \        .mark_circle()\n        .encode(\n            x=\"x\",\n          \
      \  y=\"y\",\n            color=alt.Color(\n                \"community\",\n\
      \                scale=alt.Scale(\n                    domain=communities_entities_filtered[\"\
      community\"].unique(),\n                    scheme=\"category10\",\n       \
      \         ),\n            ),\n            size=alt.Size(\"degree\", scale=alt.Scale(range=[50,\
      \ 1000]), legend=None),\n            tooltip=[\"id_entities\", \"type\", \"\
      description\", \"community\"],\n        )\n        .properties(height=1000)\n\
      \        .configure_axis(disable=True)\n    )\n    st.altair_chart(graph, use_container_width=True)\n\
      \    return graph"
    signature: 'def create_full_graph_ui(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: sv.entities.value.copy
      type: unresolved
    - target: sv.communities.value.copy
      type: unresolved
    - target: "communities.explode(\"entity_ids\")\n            .merge(\n        \
        \        entities,\n                left_on=\"entity_ids\",\n            \
        \    right_on=\"id\",\n                suffixes=(\"_entities\", \"_communities\"\
        ),\n            )\n            .dropna"
      type: unresolved
    - target: "communities.explode(\"entity_ids\")\n            .merge"
      type: unresolved
    - target: communities.explode
      type: unresolved
    - target: pandas::DataFrame
      type: external
    - target: "altair::Chart(communities_entities_filtered)\n        .mark_circle()\n\
        \        .encode(\n            x=\"x\",\n            y=\"y\",\n          \
        \  color=alt.Color(\n                \"community\",\n                scale=alt.Scale(\n\
        \                    domain=communities_entities_filtered[\"community\"].unique(),\n\
        \                    scheme=\"category10\",\n                ),\n        \
        \    ),\n            size=alt.Size(\"degree\", scale=alt.Scale(range=[50,\
        \ 1000]), legend=None),\n            tooltip=[\"id_entities\", \"type\", \"\
        description\", \"community\"],\n        )\n        .properties(height=1000)\n\
        \        .configure_axis"
      type: external
    - target: "altair::Chart(communities_entities_filtered)\n        .mark_circle()\n\
        \        .encode(\n            x=\"x\",\n            y=\"y\",\n          \
        \  color=alt.Color(\n                \"community\",\n                scale=alt.Scale(\n\
        \                    domain=communities_entities_filtered[\"community\"].unique(),\n\
        \                    scheme=\"category10\",\n                ),\n        \
        \    ),\n            size=alt.Size(\"degree\", scale=alt.Scale(range=[50,\
        \ 1000]), legend=None),\n            tooltip=[\"id_entities\", \"type\", \"\
        description\", \"community\"],\n        )\n        .properties"
      type: external
    - target: "altair::Chart(communities_entities_filtered)\n        .mark_circle()\n\
        \        .encode"
      type: external
    - target: "altair::Chart(communities_entities_filtered)\n        .mark_circle"
      type: external
    - target: altair::Chart
      type: external
    - target: altair::Color
      type: external
    - target: altair::Scale
      type: external
    - target: communities_entities_filtered["community"].unique
      type: unresolved
    - target: altair::Size
      type: external
    - target: streamlit::altair_chart
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/full_graph.py::create_full_graph_ui
    called_by: []
- file_name: unified-search-app/app/ui/questions_list.py
  imports:
  - module: streamlit
    name: null
    alias: st
  - module: state.session_variables
    name: SessionVariables
    alias: null
  functions:
  - name: create_questions_list_ui
    start_line: 10
    end_line: 23
    code: "def create_questions_list_ui(sv: SessionVariables):\n    \"\"\"Return question\
      \ list UI component.\"\"\"\n    selection = st.dataframe(\n        sv.generated_questions.value,\n\
      \        use_container_width=True,\n        hide_index=True,\n        selection_mode=\"\
      single-row\",\n        column_config={\"value\": \"question\"},\n        on_select=\"\
      rerun\",\n    )\n    rows = selection.selection.rows\n    if len(rows) > 0:\n\
      \        question_index = selection.selection.rows[0]\n        sv.selected_question.value\
      \ = sv.generated_questions.value[question_index]"
    signature: 'def create_questions_list_ui(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: streamlit::dataframe
      type: external
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/ui/questions_list.py::create_questions_list_ui
    called_by: []
- file_name: unified-search-app/app/ui/report_details.py
  imports:
  - module: json
    name: null
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: state.session_variables
    name: SessionVariables
    alias: null
  - module: ui.search
    name: display_graph_citations
    alias: null
  - module: ui.search
    name: format_response_hyperlinks
    alias: null
  - module: ui.search
    name: get_ids_per_key
    alias: null
  functions:
  - name: create_report_details_ui
    start_line: 18
    end_line: 98
    code: "def create_report_details_ui(sv: SessionVariables):\n    \"\"\"Return report\
      \ details UI component.\"\"\"\n    if sv.selected_report.value is not None and\
      \ sv.selected_report.value.empty is False:\n        text = \"\"\n        entity_ids\
      \ = []\n        relationship_ids = []\n        try:\n            report = json.loads(sv.selected_report.value.full_content_json)\n\
      \            title = report[\"title\"]\n            summary = report[\"summary\"\
      ]\n            rating = report[\"rating\"]\n            rating_explanation =\
      \ report[\"rating_explanation\"]\n            findings = report[\"findings\"\
      ]\n            text += f\"#### {title}\\n\\n{summary}\\n\\n\"\n            text\
      \ += f\"**Priority: {rating}**\\n\\n{rating_explanation}\\n\\n##### Key Findings\\\
      n\\n\"\n            if isinstance(findings, list):\n                for finding\
      \ in findings:\n                    # extract data for citations\n         \
      \           entity_ids.extend(\n                        get_ids_per_key(finding[\"\
      explanation\"], \"Entities\")\n                    )\n                    relationship_ids.extend(\n\
      \                        get_ids_per_key(finding[\"explanation\"], \"Relationships\"\
      )\n                    )\n\n                    formatted_text = format_response_hyperlinks(\n\
      \                        finding[\"explanation\"], \"graph\"\n             \
      \       )\n                    text += f\"\\n\\n**{finding['summary']}**\\n\\\
      n{formatted_text}\"\n            elif isinstance(findings, str):\n         \
      \       # extract data for citations\n                entity_ids.extend(get_ids_per_key(finding[\"\
      explanation\"], \"Entities\"))  # type: ignore\n                relationship_ids.extend(\n\
      \                    get_ids_per_key(finding[\"explanation\"], \"Relationships\"\
      )  # type: ignore\n                )\n\n                formatted_text = format_response_hyperlinks(findings,\
      \ \"graph\")\n                text += f\"\\n\\n{formatted_text}\"\n\n      \
      \  except json.JSONDecodeError:\n            st.write(\"Error parsing report.\"\
      )\n            st.write(sv.selected_report.value.full_content_json)\n      \
      \  text_replacement = (\n            text.replace(\"Entity_Relationships\",\
      \ \"Relationships\")\n            .replace(\"Entity_Claims\", \"Claims\")\n\
      \            .replace(\"Entity_Details\", \"Entities\")\n        )\n       \
      \ st.markdown(f\"{text_replacement}\", unsafe_allow_html=True)\n\n        #\
      \ extract entities\n        selected_entities = []\n        for _index, row\
      \ in sv.entities.value.iterrows():\n            if str(row[\"human_readable_id\"\
      ]) in entity_ids:\n                selected_entities.append({\n            \
      \        \"id\": str(row[\"human_readable_id\"]),\n                    \"title\"\
      : row[\"title\"],\n                    \"description\": row[\"description\"\
      ],\n                })\n\n        sorted_entities = sorted(selected_entities,\
      \ key=lambda x: int(x[\"id\"]))\n\n        # extract relationships\n       \
      \ selected_relationships = []\n        for _index, row in sv.relationships.value.iterrows():\n\
      \            if str(row[\"human_readable_id\"]) in relationship_ids:\n     \
      \           selected_relationships.append({\n                    \"id\": str(row[\"\
      human_readable_id\"]),\n                    \"source\": row[\"source\"],\n \
      \                   \"target\": row[\"target\"],\n                    \"description\"\
      : row[\"description\"],\n                })\n\n        sorted_relationships\
      \ = sorted(\n            selected_relationships, key=lambda x: int(x[\"id\"\
      ])\n        )\n\n        display_graph_citations(\n            pd.DataFrame(sorted_entities),\
      \ pd.DataFrame(sorted_relationships), \"graph\"\n        )\n    else:\n    \
      \    st.write(\"No report selected\")"
    signature: 'def create_report_details_ui(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: json::loads
      type: stdlib
    - target: isinstance
      type: builtin
    - target: entity_ids.extend
      type: unresolved
    - target: ui.search::get_ids_per_key
      type: external
    - target: relationship_ids.extend
      type: unresolved
    - target: ui.search::format_response_hyperlinks
      type: external
    - target: streamlit::write
      type: external
    - target: "text.replace(\"Entity_Relationships\", \"Relationships\")\n       \
        \     .replace(\"Entity_Claims\", \"Claims\")\n            .replace"
      type: unresolved
    - target: "text.replace(\"Entity_Relationships\", \"Relationships\")\n       \
        \     .replace"
      type: unresolved
    - target: text.replace
      type: unresolved
    - target: streamlit::markdown
      type: external
    - target: sv.entities.value.iterrows
      type: unresolved
    - target: str
      type: builtin
    - target: selected_entities.append
      type: unresolved
    - target: sorted
      type: builtin
    - target: int
      type: builtin
    - target: sv.relationships.value.iterrows
      type: unresolved
    - target: selected_relationships.append
      type: unresolved
    - target: ui.search::display_graph_citations
      type: external
    - target: pandas::DataFrame
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/report_details.py::create_report_details_ui
    called_by: []
- file_name: unified-search-app/app/ui/report_list.py
  imports:
  - module: streamlit
    name: null
    alias: st
  - module: state.session_variables
    name: SessionVariables
    alias: null
  functions:
  - name: create_report_list_ui
    start_line: 10
    end_line: 25
    code: "def create_report_list_ui(sv: SessionVariables):\n    \"\"\"Return report\
      \ list UI component.\"\"\"\n    selection = st.dataframe(\n        sv.community_reports.value,\n\
      \        height=1000,\n        hide_index=True,\n        column_order=[\"id\"\
      , \"title\"],\n        selection_mode=\"single-row\",\n        on_select=\"\
      rerun\",\n    )\n    rows = selection.selection.rows\n    if len(rows) > 0:\n\
      \        report_index = selection.selection.rows[0]\n        sv.selected_report.value\
      \ = sv.community_reports.value.iloc[report_index]\n    else:\n        sv.selected_report.value\
      \ = None"
    signature: 'def create_report_list_ui(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: streamlit::dataframe
      type: external
    - target: len
      type: builtin
    visibility: public
    node_id: unified-search-app/app/ui/report_list.py::create_report_list_ui
    called_by: []
- file_name: unified-search-app/app/ui/search.py
  imports:
  - module: json
    name: null
    alias: null
  - module: re
    name: null
    alias: null
  - module: dataclasses
    name: dataclass
    alias: null
  - module: pandas
    name: null
    alias: pd
  - module: streamlit
    name: null
    alias: st
  - module: rag.typing
    name: SearchResult
    alias: null
  - module: rag.typing
    name: SearchType
    alias: null
  - module: streamlit.delta_generator
    name: DeltaGenerator
    alias: null
  functions:
  - name: init_search_ui
    start_line: 16
    end_line: 27
    code: "def init_search_ui(\n    container: DeltaGenerator, search_type: SearchType,\
      \ title: str, caption: str\n):\n    \"\"\"Initialize search UI component.\"\"\
      \"\n    with container:\n        st.markdown(title)\n        st.caption(caption)\n\
      \n        ui_tag = search_type.value.lower()\n        st.session_state[f\"{ui_tag}_response_placeholder\"\
      ] = st.empty()\n        st.session_state[f\"{ui_tag}_context_placeholder\"]\
      \ = st.empty()\n        st.session_state[f\"{ui_tag}_container\"] = container"
    signature: "def init_search_ui(\n    container: DeltaGenerator, search_type: SearchType,\
      \ title: str, caption: str\n)"
    decorators: []
    raises: []
    calls:
    - target: streamlit::markdown
      type: external
    - target: streamlit::caption
      type: external
    - target: search_type.value.lower
      type: unresolved
    - target: streamlit::empty
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/search.py::init_search_ui
    called_by: []
  - name: display_search_result
    start_line: 39
    end_line: 60
    code: "def display_search_result(\n    container: DeltaGenerator, result: SearchResult,\
      \ stats: SearchStats | None = None\n):\n    \"\"\"Display search results data\
      \ into the UI.\"\"\"\n    response_placeholder_attr = (\n        result.search_type.value.lower()\
      \ + \"_response_placeholder\"\n    )\n\n    with container:\n        # display\
      \ response\n        response = format_response_hyperlinks(\n            result.response,\
      \ result.search_type.value.lower()\n        )\n\n        if stats is not None\
      \ and stats.completion_time is not None:\n            st.markdown(\n       \
      \         f\"*{stats.prompt_tokens:,} tokens used, {stats.llm_calls} LLM calls,\
      \ {int(stats.completion_time)} seconds elapsed.*\"\n            )\n        st.session_state[response_placeholder_attr]\
      \ = st.markdown(\n            f\"<div id='{result.search_type.value.lower()}-response'>{response}</div>\"\
      ,\n            unsafe_allow_html=True,\n        )"
    signature: "def display_search_result(\n    container: DeltaGenerator, result:\
      \ SearchResult, stats: SearchStats | None = None\n)"
    decorators: []
    raises: []
    calls:
    - target: result.search_type.value.lower
      type: unresolved
    - target: unified-search-app/app/ui/search.py::format_response_hyperlinks
      type: internal
    - target: streamlit::markdown
      type: external
    - target: int
      type: builtin
    visibility: public
    node_id: unified-search-app/app/ui/search.py::display_search_result
    called_by: []
  - name: display_citations
    start_line: 63
    end_line: 97
    code: "def display_citations(\n    container: DeltaGenerator | None = None, result:\
      \ SearchResult | None = None\n):\n    \"\"\"Display citations into the UI.\"\
      \"\"\n    if container is not None:\n        with container:\n            #\
      \ display context used for generating the response\n            if result is\
      \ not None:\n                context_data = result.context\n               \
      \ context_data = dict(sorted(context_data.items()))\n\n                st.markdown(\"\
      ---\")\n                st.markdown(\"### Citations\")\n                for\
      \ key, value in context_data.items():\n                    if len(value) > 0:\n\
      \                        key_type = key\n                        if key == \"\
      sources\":\n                            st.markdown(\n                     \
      \           f\"Relevant chunks of source documents **({len(value)})**:\"\n \
      \                           )\n                            key_type = \"sources\"\
      \n                        elif key == \"reports\":\n                       \
      \     st.markdown(\n                                f\"Relevant AI-generated\
      \ network reports **({len(value)})**:\"\n                            )\n   \
      \                     else:\n                            st.markdown(\n    \
      \                            f\"Relevant AI-extracted {key} **({len(value)})**:\"\
      \n                            )\n                        st.markdown(\n    \
      \                        render_html_table(\n                              \
      \  value, result.search_type.value.lower(), key_type\n                     \
      \       ),\n                            unsafe_allow_html=True,\n          \
      \              )"
    signature: "def display_citations(\n    container: DeltaGenerator | None = None,\
      \ result: SearchResult | None = None\n)"
    decorators: []
    raises: []
    calls:
    - target: dict
      type: builtin
    - target: sorted
      type: builtin
    - target: context_data.items
      type: unresolved
    - target: streamlit::markdown
      type: external
    - target: len
      type: builtin
    - target: unified-search-app/app/ui/search.py::render_html_table
      type: internal
    - target: result.search_type.value.lower
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/ui/search.py::display_citations
    called_by: []
  - name: format_response_hyperlinks
    start_line: 100
    end_line: 118
    code: "def format_response_hyperlinks(str_response: str, search_type: str = \"\
      \"):\n    \"\"\"Format response to show hyperlinks inside the response UI.\"\
      \"\"\n    results_with_hyperlinks = format_response_hyperlinks_by_key(\n   \
      \     str_response, \"Entities\", \"Entities\", search_type\n    )\n    results_with_hyperlinks\
      \ = format_response_hyperlinks_by_key(\n        results_with_hyperlinks, \"\
      Sources\", \"Sources\", search_type\n    )\n    results_with_hyperlinks = format_response_hyperlinks_by_key(\n\
      \        results_with_hyperlinks, \"Documents\", \"Sources\", search_type\n\
      \    )\n    results_with_hyperlinks = format_response_hyperlinks_by_key(\n \
      \       results_with_hyperlinks, \"Relationships\", \"Relationships\", search_type\n\
      \    )\n    results_with_hyperlinks = format_response_hyperlinks_by_key(\n \
      \       results_with_hyperlinks, \"Reports\", \"Reports\", search_type\n   \
      \ )\n\n    return results_with_hyperlinks  # noqa: RET504"
    signature: 'def format_response_hyperlinks(str_response: str, search_type: str
      = "")'
    decorators: []
    raises: []
    calls:
    - target: unified-search-app/app/ui/search.py::format_response_hyperlinks_by_key
      type: internal
    visibility: public
    node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks
    called_by:
    - source: unified-search-app/app/ui/search.py::display_search_result
      type: internal
  - name: format_response_hyperlinks_by_key
    start_line: 121
    end_line: 148
    code: "def format_response_hyperlinks_by_key(\n    str_response: str, key: str,\
      \ anchor: str, search_type: str = \"\"\n):\n    \"\"\"Format response to show\
      \ hyperlinks inside the response UI by key.\"\"\"\n    pattern = r\"\\(\\d+(?:,\\\
      s*\\d+)*(?:,\\s*\\+more)?\\)\"\n\n    citations_list = re.findall(f\"{key} {pattern}\"\
      , str_response)\n\n    results_with_hyperlinks = str_response\n    if len(citations_list)\
      \ > 0:\n        for occurrence in citations_list:\n            string_occurrence\
      \ = str(occurrence)\n            numbers_list = string_occurrence[\n       \
      \         string_occurrence.find(\"(\") + 1 : string_occurrence.find(\")\")\n\
      \            ].split(\",\")\n            string_occurrence_hyperlinks = string_occurrence\n\
      \            for number in numbers_list:\n                if number.lower().strip()\
      \ != \"+more\":\n                    string_occurrence_hyperlinks = string_occurrence_hyperlinks.replace(\n\
      \                        number,\n                        f'<a href=\"#{search_type.lower().strip()}-{anchor.lower().strip()}-{number.strip()}\"\
      >{number}</a>',\n                    )\n\n            results_with_hyperlinks\
      \ = results_with_hyperlinks.replace(\n                occurrence, string_occurrence_hyperlinks\n\
      \            )\n\n    return results_with_hyperlinks"
    signature: "def format_response_hyperlinks_by_key(\n    str_response: str, key:\
      \ str, anchor: str, search_type: str = \"\"\n)"
    decorators: []
    raises: []
    calls:
    - target: re::findall
      type: stdlib
    - target: len
      type: builtin
    - target: str
      type: builtin
    - target: "string_occurrence[\n                string_occurrence.find(\"(\") +\
        \ 1 : string_occurrence.find(\")\")\n            ].split"
      type: unresolved
    - target: string_occurrence.find
      type: unresolved
    - target: number.lower().strip
      type: unresolved
    - target: number.lower
      type: unresolved
    - target: string_occurrence_hyperlinks.replace
      type: unresolved
    - target: search_type.lower().strip
      type: unresolved
    - target: search_type.lower
      type: unresolved
    - target: anchor.lower().strip
      type: unresolved
    - target: anchor.lower
      type: unresolved
    - target: number.strip
      type: unresolved
    - target: results_with_hyperlinks.replace
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks_by_key
    called_by:
    - source: unified-search-app/app/ui/search.py::format_response_hyperlinks
      type: internal
  - name: format_suggested_questions
    start_line: 151
    end_line: 155
    code: "def format_suggested_questions(questions: str):\n    \"\"\"Format suggested\
      \ questions to the UI.\"\"\"\n    citations_pattern = r\"\\[.*?\\]\"\n    substring\
      \ = re.sub(citations_pattern, \"\", questions).strip()\n    return convert_numbered_list_to_array(substring)"
    signature: 'def format_suggested_questions(questions: str)'
    decorators: []
    raises: []
    calls:
    - target: re::sub(citations_pattern, "", questions).strip
      type: stdlib
    - target: re::sub
      type: stdlib
    - target: unified-search-app/app/ui/search.py::convert_numbered_list_to_array
      type: internal
    visibility: public
    node_id: unified-search-app/app/ui/search.py::format_suggested_questions
    called_by: []
  - name: convert_numbered_list_to_array
    start_line: 158
    end_line: 169
    code: "def convert_numbered_list_to_array(numbered_list_str):\n    \"\"\"Convert\
      \ numbered list result into an array of elements.\"\"\"\n    lines = numbered_list_str.strip().split(\"\
      \\n\")\n    items = []\n\n    for line in lines:\n        match = re.match(r\"\
      ^\\d+\\.\\s*(.*)\", line)\n        if match:\n            item = match.group(1).strip()\n\
      \            items.append(item)\n\n    return items"
    signature: def convert_numbered_list_to_array(numbered_list_str)
    decorators: []
    raises: []
    calls:
    - target: numbered_list_str.strip().split
      type: unresolved
    - target: numbered_list_str.strip
      type: unresolved
    - target: re::match
      type: stdlib
    - target: match.group(1).strip
      type: unresolved
    - target: match.group
      type: unresolved
    - target: items.append
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/ui/search.py::convert_numbered_list_to_array
    called_by:
    - source: unified-search-app/app/ui/search.py::format_suggested_questions
      type: internal
  - name: get_ids_per_key
    start_line: 172
    end_line: 184
    code: "def get_ids_per_key(str_response: str, key: str):\n    \"\"\"Filter ids\
      \ per key.\"\"\"\n    pattern = r\"\\(\\d+(?:,\\s*\\d+)*(?:,\\s*\\+more)?\\\
      )\"\n    citations_list = re.findall(f\"{key} {pattern}\", str_response)\n \
      \   numbers_list = []\n    if len(citations_list) > 0:\n        for occurrence\
      \ in citations_list:\n            string_occurrence = str(occurrence)\n    \
      \        numbers_list = string_occurrence[\n                string_occurrence.find(\"\
      (\") + 1 : string_occurrence.find(\")\")\n            ].split(\",\")\n\n   \
      \ return numbers_list"
    signature: 'def get_ids_per_key(str_response: str, key: str)'
    decorators: []
    raises: []
    calls:
    - target: re::findall
      type: stdlib
    - target: len
      type: builtin
    - target: str
      type: builtin
    - target: "string_occurrence[\n                string_occurrence.find(\"(\") +\
        \ 1 : string_occurrence.find(\")\")\n            ].split"
      type: unresolved
    - target: string_occurrence.find
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/ui/search.py::get_ids_per_key
    called_by: []
  - name: render_html_table
    start_line: 192
    end_line: 264
    code: "def render_html_table(df: pd.DataFrame, search_type: str, key: str):\n\
      \    \"\"\"Render HTML table into the UI.\"\"\"\n    table_container = \"\"\"\
      \n        max-width: 100%;\n        overflow: hidden;\n        margin: 0 auto;\n\
      \    \"\"\"\n\n    table_style = \"\"\"\n        width: 100%;\n        border-collapse:\
      \ collapse;\n        table-layout: fixed;\n    \"\"\"\n\n    th_style = \"\"\
      \"\n        word-wrap: break-word;\n        white-space: normal;\n    \"\"\"\
      \n\n    td_style = \"\"\"\n        border: 1px solid #efefef;\n        word-wrap:\
      \ break-word;\n        white-space: normal;\n    \"\"\"\n\n    table_html =\
      \ f'<div style=\"{table_container}\">'\n    table_html += f'<table style=\"\
      {table_style}\">'\n\n    table_html += \"<thead><tr>\"\n    for col in pd.DataFrame(df).columns:\n\
      \        table_html += f'<th style=\"{th_style}\">{col}</th>'\n    table_html\
      \ += \"</tr></thead>\"\n\n    table_html += \"<tbody>\"\n    for index, row\
      \ in pd.DataFrame(df).iterrows():\n        html_id = (\n            f\"{search_type.lower().strip()}-{key.lower().strip()}-{row.id.strip()}\"\
      \n            if \"id\" in row\n            else f\"row-{index}\"\n        )\n\
      \        table_html += f'<tr id=\"{html_id}\">'\n        for value in row:\n\
      \            if isinstance(value, str):\n                if value[0:1] == \"\
      {\":\n                    value_casted = json.loads(value)\n               \
      \     value = value_casted[\"summary\"]\n                value_array = str(value).split(\"\
      \ \")\n                td_value = (\n                    \" \".join(value_array[:SHORT_WORDS])\
      \ + \"...\"\n                    if len(value_array) >= SHORT_WORDS\n      \
      \              else value\n                )\n                title_value =\
      \ (\n                    \" \".join(value_array[:LONG_WORDS]) + \"...\"\n  \
      \                  if len(value_array) >= LONG_WORDS\n                    else\
      \ value\n                )\n                title_value = (\n              \
      \      title_value.replace('\"', \"&quot;\")\n                    .replace(\"\
      '\", \"&apos;\")\n                    .replace(\"\\n\", \" \")\n           \
      \         .replace(\"\\n\\n\", \" \")\n                    .replace(\"\\r\\\
      n\", \" \")\n                )\n                table_html += (\n          \
      \          f'<td style=\"{td_style}\" title=\"{title_value}\">{td_value}</td>'\n\
      \                )\n            else:\n                table_html += f'<td style=\"\
      {td_style}\" title=\"{value}\">{value}</td>'\n        table_html += \"</tr>\"\
      \n    table_html += \"</tbody></table></div>\"\n\n    return table_html"
    signature: 'def render_html_table(df: pd.DataFrame, search_type: str, key: str)'
    decorators: []
    raises: []
    calls:
    - target: pandas::DataFrame
      type: external
    - target: pandas::DataFrame(df).iterrows
      type: external
    - target: search_type.lower().strip
      type: unresolved
    - target: search_type.lower
      type: unresolved
    - target: key.lower().strip
      type: unresolved
    - target: key.lower
      type: unresolved
    - target: row.id.strip
      type: unresolved
    - target: isinstance
      type: builtin
    - target: json::loads
      type: stdlib
    - target: str(value).split
      type: unresolved
    - target: str
      type: builtin
    - target: '" ".join'
      type: unresolved
    - target: len
      type: builtin
    - target: "title_value.replace('\"', \"&quot;\")\n                    .replace(\"\
        '\", \"&apos;\")\n                    .replace(\"\\n\", \" \")\n         \
        \           .replace(\"\\n\\n\", \" \")\n                    .replace"
      type: unresolved
    - target: "title_value.replace('\"', \"&quot;\")\n                    .replace(\"\
        '\", \"&apos;\")\n                    .replace(\"\\n\", \" \")\n         \
        \           .replace"
      type: unresolved
    - target: "title_value.replace('\"', \"&quot;\")\n                    .replace(\"\
        '\", \"&apos;\")\n                    .replace"
      type: unresolved
    - target: "title_value.replace('\"', \"&quot;\")\n                    .replace"
      type: unresolved
    - target: title_value.replace
      type: unresolved
    visibility: public
    node_id: unified-search-app/app/ui/search.py::render_html_table
    called_by:
    - source: unified-search-app/app/ui/search.py::display_citations
      type: internal
    - source: unified-search-app/app/ui/search.py::display_graph_citations
      type: internal
  - name: display_graph_citations
    start_line: 267
    end_line: 284
    code: "def display_graph_citations(\n    entities: pd.DataFrame, relationships:\
      \ pd.DataFrame, citation_type: str\n):\n    \"\"\"Display graph citations into\
      \ the UI.\"\"\"\n    st.markdown(\"---\")\n    st.markdown(\"### Citations\"\
      )\n\n    st.markdown(f\"Relevant AI-extracted entities **({len(entities)})**:\"\
      )\n    st.markdown(\n        render_html_table(entities, citation_type, \"entities\"\
      ),\n        unsafe_allow_html=True,\n    )\n\n    st.markdown(f\"Relevant AI-extracted\
      \ relationships **({len(relationships)})**:\")\n    st.markdown(\n        render_html_table(relationships,\
      \ citation_type, \"relationships\"),\n        unsafe_allow_html=True,\n    )"
    signature: "def display_graph_citations(\n    entities: pd.DataFrame, relationships:\
      \ pd.DataFrame, citation_type: str\n)"
    decorators: []
    raises: []
    calls:
    - target: streamlit::markdown
      type: external
    - target: len
      type: builtin
    - target: unified-search-app/app/ui/search.py::render_html_table
      type: internal
    visibility: public
    node_id: unified-search-app/app/ui/search.py::display_graph_citations
    called_by: []
- file_name: unified-search-app/app/ui/sidebar.py
  imports:
  - module: streamlit
    name: null
    alias: st
  - module: app_logic
    name: dataset_name
    alias: null
  - module: app_logic
    name: load_dataset
    alias: null
  - module: state.session_variables
    name: SessionVariables
    alias: null
  functions:
  - name: reset_app
    start_line: 11
    end_line: 15
    code: "def reset_app():\n    \"\"\"Reset app to its original state.\"\"\"\n  \
      \  st.cache_data.clear()\n    st.session_state.clear()\n    st.rerun()"
    signature: def reset_app()
    decorators: []
    raises: []
    calls:
    - target: streamlit::cache_data.clear
      type: external
    - target: streamlit::session_state.clear
      type: external
    - target: streamlit::rerun
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::reset_app
    called_by: []
  - name: update_dataset
    start_line: 18
    end_line: 25
    code: "def update_dataset(sv: SessionVariables):\n    \"\"\"Update dataset from\
      \ the dropdown.\"\"\"\n    value = st.session_state[sv.dataset.key]\n    st.cache_data.clear()\n\
      \    if \"response_lengths\" not in st.session_state:\n        st.session_state.response_lengths\
      \ = []\n    st.session_state.response_lengths = []\n    load_dataset(value,\
      \ sv)"
    signature: 'def update_dataset(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: streamlit::cache_data.clear
      type: external
    - target: app_logic::load_dataset
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::update_dataset
    called_by: []
  - name: update_basic_rag
    start_line: 28
    end_line: 30
    code: "def update_basic_rag(sv: SessionVariables):\n    \"\"\"Update basic rag\
      \ state.\"\"\"\n    sv.include_basic_rag.value = st.session_state[sv.include_basic_rag.key]"
    signature: 'def update_basic_rag(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::update_basic_rag
    called_by: []
  - name: update_drift_search
    start_line: 33
    end_line: 35
    code: "def update_drift_search(sv: SessionVariables):\n    \"\"\"Update drift\
      \ rag state.\"\"\"\n    sv.include_drift_search.value = st.session_state[sv.include_drift_search.key]"
    signature: 'def update_drift_search(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::update_drift_search
    called_by: []
  - name: update_local_search
    start_line: 38
    end_line: 40
    code: "def update_local_search(sv: SessionVariables):\n    \"\"\"Update local\
      \ rag state.\"\"\"\n    sv.include_local_search.value = st.session_state[sv.include_local_search.key]"
    signature: 'def update_local_search(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::update_local_search
    called_by: []
  - name: update_global_search
    start_line: 43
    end_line: 45
    code: "def update_global_search(sv: SessionVariables):\n    \"\"\"Update global\
      \ rag state.\"\"\"\n    sv.include_global_search.value = st.session_state[sv.include_global_search.key]"
    signature: 'def update_global_search(sv: SessionVariables)'
    decorators: []
    raises: []
    calls: []
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::update_global_search
    called_by: []
  - name: create_side_bar
    start_line: 48
    end_line: 97
    code: "def create_side_bar(sv: SessionVariables):\n    \"\"\"Create a side bar\
      \ panel..\"\"\"\n    with st.sidebar:\n        st.subheader(\"Options\")\n\n\
      \        options = [d.key for d in sv.datasets.value]\n\n        def lookup_label(key:\
      \ str):\n            return dataset_name(key, sv)\n\n        st.selectbox(\n\
      \            \"Dataset\",\n            key=sv.dataset.key,\n            on_change=update_dataset,\n\
      \            kwargs={\"sv\": sv},\n            options=options,\n          \
      \  format_func=lookup_label,\n        )\n        st.number_input(\n        \
      \    \"Number of suggested questions\",\n            key=sv.suggested_questions.key,\n\
      \            min_value=1,\n            max_value=100,\n            step=1,\n\
      \        )\n        st.subheader(\"Search options:\")\n        st.toggle(\n\
      \            \"Include basic RAG\",\n            key=sv.include_basic_rag.key,\n\
      \            on_change=update_basic_rag,\n            kwargs={\"sv\": sv},\n\
      \        )\n        st.toggle(\n            \"Include local search\",\n    \
      \        key=sv.include_local_search.key,\n            on_change=update_local_search,\n\
      \            kwargs={\"sv\": sv},\n        )\n        st.toggle(\n         \
      \   \"Include global search\",\n            key=sv.include_global_search.key,\n\
      \            on_change=update_global_search,\n            kwargs={\"sv\": sv},\n\
      \        )\n        st.toggle(\n            \"Include drift search\",\n    \
      \        key=sv.include_drift_search.key,\n            on_change=update_drift_search,\n\
      \            kwargs={\"sv\": sv},\n        )"
    signature: 'def create_side_bar(sv: SessionVariables)'
    decorators: []
    raises: []
    calls:
    - target: streamlit::subheader
      type: external
    - target: streamlit::selectbox
      type: external
    - target: streamlit::number_input
      type: external
    - target: streamlit::toggle
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::create_side_bar
    called_by: []
  - name: lookup_label
    start_line: 55
    end_line: 56
    code: "def lookup_label(key: str):\n            return dataset_name(key, sv)"
    signature: 'def lookup_label(key: str)'
    decorators: []
    raises: []
    calls:
    - target: app_logic::dataset_name
      type: external
    visibility: public
    node_id: unified-search-app/app/ui/sidebar.py::lookup_label
    called_by: []
