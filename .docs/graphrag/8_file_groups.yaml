- file: graphrag/api/index.py
  functions:
  - node_id: graphrag/api/index.py::_get_method
    name: _get_method
    signature: 'def _get_method(method: IndexingMethod | str, is_update_run: bool)
      -> str'
    docstring: "Return the method name to use for the indexing pipeline, optionally\
      \ indicating an update run.\n\nArgs:\n    method: IndexingMethod | str\n   \
      \     The indexing method. If an IndexingMethod is provided, its value is used;\
      \ otherwise the string value is used directly.\n    is_update_run: bool\n  \
      \      True if this is an update run, in which case the method name will be\
      \ suffixed with \"-update\".\n\nReturns:\n    str: The computed method name,\
      \ possibly suffixed with \"-update\" when is_update_run is True."
  - node_id: graphrag/api/index.py::build_index
    name: build_index
    signature: "def build_index(\n    config: GraphRagConfig,\n    method: IndexingMethod\
      \ | str = IndexingMethod.Standard,\n    is_update_run: bool = False,\n    memory_profile:\
      \ bool = False,\n    callbacks: list[WorkflowCallbacks] | None = None,\n   \
      \ additional_context: dict[str, Any] | None = None,\n    verbose: bool = False,\n\
      \    input_documents: pd.DataFrame | None = None,\n) -> list[PipelineRunResult]"
    docstring: "Run the indexing pipeline with the given configuration.\n\nThe asynchronous\
      \ function orchestrates the indexing workflow: it initializes logging, prepares\
      \ a callback chain, determines the final indexing method (including update semantics),\
      \ builds the pipeline, executes the run, collects outputs, and returns them\
      \ as a list of PipelineRunResult.\n\nParameters\n----------\nconfig : GraphRagConfig\n\
      \    The configuration for GraphRAG indexing.\nmethod : IndexingMethod | str\
      \ default=IndexingMethod.Standard\n    Styling of indexing to perform (full\
      \ LLM, NLP + LLM, etc.). If a string is provided, it will be used directly.\n\
      is_update_run : bool\n    Indicates whether this is an incremental update run.\
      \ When True, the final method name will be suffixed with \"-update\".\nmemory_profile\
      \ : bool\n    Whether to enable memory profiling. Note: Memory profiling is\
      \ not yet supported and a warning is logged.\ncallbacks : list[WorkflowCallbacks]\
      \ | None default=None\n    A list of callbacks to register for the pipeline\
      \ lifecycle. If None, a NoopWorkflowCallbacks is used.\nadditional_context :\
      \ dict[str, Any] | None default=None\n    Additional context to pass to the\
      \ pipeline run. This can be accessed in the pipeline state under the 'additional_context'\
      \ key.\nverbose : bool\n    Enables verbose logging during initialization. Affects\
      \ the logging configuration.\ninput_documents : pd.DataFrame | None default=None\n\
      \    Override document loading and parsing and supply your own dataframe of\
      \ documents to index.\n\nReturns\n-------\nlist[PipelineRunResult]\n    The\
      \ list of pipeline run results\n\nRaises\n------\nException\n    Exceptions\
      \ raised by underlying components (e.g., logging initialization, pipeline execution)\
      \ may propagate to the caller.\n\nNotes\n-----\n- This function is asynchronous\
      \ and must be awaited.\n- The final method name is determined by _get_method(method,\
      \ is_update_run)."
  classes: []
- file: graphrag/api/prompt_tune.py
  functions:
  - node_id: graphrag/api/prompt_tune.py::generate_indexing_prompts
    name: generate_indexing_prompts
    signature: "def generate_indexing_prompts(\n    config: GraphRagConfig,\n    chunk_size:\
      \ PositiveInt = graphrag_config_defaults.chunks.size,\n    overlap: Annotated[\n\
      \        int, annotated_types.Gt(-1)\n    ] = graphrag_config_defaults.chunks.overlap,\n\
      \    limit: PositiveInt = 15,\n    selection_method: DocSelectionType = DocSelectionType.RANDOM,\n\
      \    domain: str | None = None,\n    language: str | None = None,\n    max_tokens:\
      \ int = MAX_TOKEN_COUNT,\n    discover_entity_types: bool = True,\n    min_examples_required:\
      \ PositiveInt = 2,\n    n_subset_max: PositiveInt = 300,\n    k: PositiveInt\
      \ = 15,\n    verbose: bool = False,\n) -> tuple[str, str, str]"
    docstring: "Generate indexing prompts.\n\nParameters\n----------\nconfig: GraphRagConfig\n\
      \    The GraphRag configuration.\nchunk_size: PositiveInt\n    The chunk token\
      \ size to use for input text units.\noverlap: Annotated[int, annotated_types.Gt(-1)]\n\
      \    The number of tokens to overlap between consecutive chunks (must be greater\
      \ than -1).\nlimit: PositiveInt\n    The limit of chunks to load.\nselection_method:\
      \ DocSelectionType\n    The chunk selection method.\ndomain: str | None\n  \
      \  The domain to map the input documents to.\nlanguage: str | None\n    The\
      \ language to use for the prompts.\nmax_tokens: int\n    The maximum number\
      \ of tokens to use on entity extraction prompts.\ndiscover_entity_types: bool\n\
      \    Generate entity types.\nmin_examples_required: PositiveInt\n    The minimum\
      \ number of examples required for entity extraction prompts.\nn_subset_max:\
      \ PositiveInt\n    The number of text chunks to embed when using auto selection\
      \ method.\nk: PositiveInt\n    The number of documents to select when using\
      \ auto selection method.\nverbose: bool\n    Whether to enable verbose logging.\n\
      \nReturns\n-------\ntuple[str, str, str]\n    entity extraction prompt, entity\
      \ summarization prompt, community summarization prompt\n\nRaises\n------\nValidationError\n\
      \    If input arguments fail validation (thrown by pydantic\u2019s validate_call).\n\
      RuntimeError\n    If any underlying operation (loading docs, language model\
      \ calls, or prompt generation steps) fails."
  classes: []
- file: graphrag/api/query.py
  functions:
  - node_id: graphrag/api/query.py::on_context
    name: on_context
    signature: 'def on_context(context: Any) -> None'
    docstring: "Store the given context in the enclosing scope's context_data.\n\n\
      Args:\n    context (Any): The context data to store in the enclosing scope.\n\
      \nReturns:\n    None: The function updates context_data in the outer scope and\
      \ returns no value.\n\nRaises:\n    None: This function does not raise exceptions\
      \ by itself."
  - node_id: graphrag/api/query.py::global_search_streaming
    name: global_search_streaming
    signature: "def global_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    community_level: int | None,\n    dynamic_community_selection: bool,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    docstring: "Perform a global search and stream the response in chunks via an async\
      \ generator.\n\nArgs:\n  config: GraphRagConfig: A graphrag configuration (from\
      \ settings.yaml).\n  entities: pd.DataFrame: A DataFrame containing the final\
      \ entities (from entities.parquet).\n  communities: pd.DataFrame: A DataFrame\
      \ containing the final communities (from communities.parquet).\n  community_reports:\
      \ pd.DataFrame: A DataFrame containing the final community reports (from community_reports.parquet).\n\
      \  community_level: int | None: The community level to search at.\n  dynamic_community_selection:\
      \ bool: Enable dynamic community selection instead of using all community reports\
      \ at a fixed level. Note that you can still provide community_level cap the\
      \ maximum level to search.\n  response_type: str: The type of response to return.\n\
      \  query: str: The user query to search for.\n  callbacks: list[QueryCallbacks]\
      \ | None: Optional callbacks to receive streaming events.\n  verbose: bool:\
      \ If True, enable verbose logging.\nReturns:\n  AsyncGenerator: An asynchronous\
      \ generator yielding strings; each yielded chunk is part of the streaming global\
      \ search response.\nRaises:\n  pydantic.ValidationError: If input arguments\
      \ fail validation via the validate_call decorator."
  - node_id: graphrag/api/query.py::global_search
    name: global_search
    signature: "def global_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    community_level:\
      \ int | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    docstring: "Perform a global search and return the full response and context data.\n\
      \nArgs:\n    config: GraphRagConfig. A graphrag configuration (from settings.yaml).\n\
      \    entities: pd.DataFrame. A DataFrame containing the final entities (from\
      \ entities.parquet).\n    communities: pd.DataFrame. A DataFrame containing\
      \ the final communities (from communities.parquet).\n    community_reports:\
      \ pd.DataFrame. A DataFrame containing the final community reports (from community_reports.parquet).\n\
      \    community_level: int | None. The community level to search at.\n    dynamic_community_selection:\
      \ bool. Enable dynamic community selection instead of using all community reports\
      \ at a fixed level. Note that you can still provide a community_level to cap\
      \ the maximum level to search.\n    response_type: str. The type of response\
      \ to return.\n    query: str. The user query to search for.\n    callbacks:\
      \ list[QueryCallbacks] | None. Optional list of QueryCallbacks to be invoked\
      \ during the search.\n    verbose: bool. Enable verbose logging during the search.\n\
      \nReturns:\n    tuple[str | dict[str, Any] | list[dict[str, Any]], str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame]]: \n        The first element is the aggregated\
      \ response produced by the search. This can be a string or a structured object\
      \ (e.g., dict or list) depending on response_type.\n        The second element\
      \ is the context data captured during the search. This may be a string, a list\
      \ of DataFrames, or a dictionary of DataFrames depending on how context was\
      \ populated.\n\nRaises:\n    pydantic.ValidationError: If input arguments fail\
      \ type validation.\n    Exception: If an unexpected error occurs during streaming\
      \ or processing."
  - node_id: graphrag/api/query.py::multi_index_global_search
    name: multi_index_global_search
    signature: "def multi_index_global_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int\
      \ | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
      \    streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    docstring: "Perform a global search across multiple indexes and return the results\
      \ and associated context data.\n\nDeprecated: Multi-index global search is deprecated\
      \ and will be removed in GraphRAG v3.\n\nParameters\n    config: GraphRagConfig.\
      \ A graphrag configuration (from settings.yaml).\n    entities_list: list[pd.DataFrame].\
      \ A list of DataFrames containing the final entities (from entities.parquet).\n\
      \    communities_list: list[pd.DataFrame]. A list of DataFrames containing the\
      \ final communities (from communities.parquet).\n    community_reports_list:\
      \ list[pd.DataFrame]. A list of DataFrames containing the final community reports\
      \ (from community_reports.parquet).\n    index_names: list[str]. A list of index\
      \ names.\n    community_level: int | None. The community level to search at.\n\
      \    dynamic_community_selection: bool. Enable dynamic community selection instead\
      \ of using all community reports at a fixed level. Note that you can still provide\
      \ a community_level cap to the maximum level to search.\n    response_type:\
      \ str. The type of response to return.\n    streaming: bool. Whether to stream\
      \ the results or not. Streaming is currently not supported and will raise NotImplementedError.\n\
      \    query: str. The user query to search for.\n    callbacks: list[QueryCallbacks]\
      \ | None. Optional callbacks to handle streaming results or events.\n    verbose:\
      \ bool. Verbose logging.\n\nReturns\n    tuple[\n        str | dict[str, Any]\
      \ | list[dict[str, Any]],\n        str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      \    ]\n    The first element is the search response payload, which may be a\
      \ plain string, a structured payload (dict), or a list of dicts depending on\
      \ the response_type. The second element is the updated context payload, which\
      \ can be a string, a list of DataFrames, or a mapping of keys to DataFrames\
      \ representing the contextual data for the response.\n\nRaises\n    NotImplementedError:\
      \ If streaming is requested (streaming == True).\n\nExample\n    Typical usage:\n\
      \    result, context = await multi_index_global_search(\n        config=config,\n\
      \        entities_list=entities_list,\n        communities_list=communities_list,\n\
      \        community_reports_list=community_reports_list,\n        index_names=index_names,\n\
      \        community_level=community_level,\n        dynamic_community_selection=dynamic_community_selection,\n\
      \        response_type=\"default\",\n        streaming=False,\n        query=\"\
      example query\",\n        callbacks=None,\n        verbose=False,\n    )\n\n\
      \    Streaming usage (not implemented):\n        streaming=True currently raises\
      \ NotImplementedError."
  - node_id: graphrag/api/query.py::basic_search
    name: basic_search
    signature: "def basic_search(\n    config: GraphRagConfig,\n    text_units: pd.DataFrame,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    docstring: "Perform a basic search and return the response and the context data.\n\
      \nParameters\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml).\n\
      \    text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet).\n    query (str): The user query to search for.\n\
      \    callbacks (list[QueryCallbacks] | None): Optional callbacks for processing\
      \ the search.\n    verbose (bool): If True, enable verbose logging.\n\nReturns\n\
      \    tuple[str, dict[str, Any]]: The first element is the concatenated response\
      \ string produced by streaming, and the second element is the context data dict\
      \ collected from the streaming callbacks.\n\nNotes\n    - The function initializes\
      \ logging via init_loggers and writes to query.log.\n    - It attaches a local\
      \ NoopQueryCallbacks that updates context_data through its on_context callback.\n\
      \    - The returned context_data reflects the most recent context produced by\
      \ the streaming process.\n\nRaises\n    pydantic.ValidationError: If input arguments\
      \ fail validation by the validate_call decorator."
  - node_id: graphrag/api/query.py::basic_search_streaming
    name: basic_search_streaming
    signature: "def basic_search_streaming(\n    config: GraphRagConfig,\n    text_units:\
      \ pd.DataFrame,\n    query: str,\n    callbacks: list[QueryCallbacks] | None\
      \ = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    docstring: "Stream results from a local search as an asynchronous generator.\n\
      \nArgs:\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
      \    text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet)\n    query (str): The user query to search for.\n\
      \    callbacks (list[QueryCallbacks] | None): Optional list of QueryCallbacks\
      \ to customize streaming behavior.\n    verbose (bool): If True, enable verbose\
      \ logging.\n\nReturns:\n    AsyncGenerator: An asynchronous generator yielding\
      \ chunks of the search response as strings.\n\nRaises:\n    Exception: Propagates\
      \ exceptions raised by underlying components (e.g., during embedding retrieval,\
      \ prompt loading, or streaming)."
  - node_id: graphrag/api/query.py::drift_search
    name: drift_search
    signature: "def drift_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level: int,\n\
      \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    docstring: 'Perform a DRIFT search and return the context data and response.


      Parameters

      ----------

      - config (GraphRagConfig): A graphrag configuration (from settings.yaml).

      - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet).

      - communities (pd.DataFrame): A DataFrame containing the final communities (from
      communities.parquet).

      - community_reports (pd.DataFrame): A DataFrame containing the final community
      reports (from community_reports.parquet).

      - text_units (pd.DataFrame): A DataFrame containing the final text units (from
      text_units.parquet).

      - relationships (pd.DataFrame): A DataFrame containing the final relationships
      (from relationships.parquet).

      - community_level (int): The community level to search at.

      - response_type (str): The type of response to generate (e.g., full, compact).

      - query (str): The user query to search for.

      - callbacks (list[QueryCallbacks] | None): Optional list of callback handlers
      to receive streaming and context information.

      - verbose (bool): Enable verbose logging for debugging.


      Returns

      ----------

      tuple[str | dict[str, Any] | list[dict[str, Any]], str | list[pd.DataFrame]
      | dict[str, pd.DataFrame]]: A tuple containing the search response and the context
      data. The first element is the generated response, which can be a string, a
      dictionary, or a list of dictionaries depending on the response_type. The second
      element is the context data, which may be a string, a list of DataFrames, or
      a mapping of strings to DataFrames.


      Raises

      ----------

      pydantic.ValidationError: If input arguments fail validation via the validate_call
      decorator.'
  - node_id: graphrag/api/query.py::drift_search_streaming
    name: drift_search_streaming
    signature: "def drift_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level:\
      \ int,\n    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
    docstring: "Perform a DRIFT streaming search and yield streaming response chunks.\n\
      \nArgs:\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml).\n\
      \    entities (pd.DataFrame): A DataFrame containing the final entities (from\
      \ entities.parquet).\n    communities (pd.DataFrame): A DataFrame containing\
      \ the final communities (from communities.parquet).\n    community_reports (pd.DataFrame):\
      \ A DataFrame containing the final community reports (from community_reports.parquet).\n\
      \    text_units (pd.DataFrame): A DataFrame containing the final text units\
      \ (from text_units.parquet).\n    relationships (pd.DataFrame): A DataFrame\
      \ containing the final relationships (from relationships.parquet).\n    community_level\
      \ (int): The community level to search at.\n    response_type (str): The response\
      \ type to shape the drift search output.\n    query (str): The user query to\
      \ search for.\n    callbacks (list[QueryCallbacks] | None): Optional list of\
      \ callbacks to customize the streaming search behavior. If None, a default empty\
      \ list is used.\n    verbose (bool): Enable verbose logging of the streaming\
      \ search process.\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous\
      \ generator yielding string chunks that together form the streaming search response.\
      \ Callers should consume chunks as they arrive and concatenate them to reconstruct\
      \ the full response."
  - node_id: graphrag/api/query.py::local_search
    name: local_search
    signature: "def local_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
      \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates: pd.DataFrame\
      \ | None,\n    community_level: int,\n    response_type: str,\n    query: str,\n\
      \    callbacks: list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n\
      ) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]"
    docstring: "Perform a local search and return the aggregated response and any\
      \ context data captured during streaming.\n\nThis function initializes logging,\
      \ wires a context-capturing callback, and streams results via local_search_streaming.\
      \ Chunks yielded by the streaming engine are concatenated into a single full_response\
      \ string. The latest context data emitted during streaming is returned as context_data.\n\
      \nArgs:\n  config: GraphRagConfig\n      A graphrag configuration (from settings.yaml).\n\
      \  entities: pandas.DataFrame\n      The final entities (from entities.parquet).\n\
      \  communities: pandas.DataFrame\n      The final communities (from communities.parquet).\n\
      \  community_reports: pandas.DataFrame\n      The final community reports (from\
      \ community_reports.parquet).\n  text_units: pandas.DataFrame\n      The final\
      \ text units (from text_units.parquet).\n  relationships: pandas.DataFrame\n\
      \      The final relationships (from relationships.parquet).\n  covariates:\
      \ pandas.DataFrame | None\n      The final covariates (from covariates.parquet),\
      \ or None if not available.\n  community_level: int\n      The community level\
      \ to search at.\n  response_type: str\n      The response type to return.\n\
      \  query: str\n      The user query to search for.\n  callbacks: list[QueryCallbacks]\
      \ | None\n      Optional list of callback handlers. Defaults to None.\n  verbose:\
      \ bool\n      Whether to enable verbose logging. Defaults to False.\n\nReturns:\n\
      \  tuple[str, dict[str, Any] | list[dict[str, Any]] | dict[str, pd.DataFrame]\
      \ | list[pd.DataFrame]]\n      A tuple containing:\n      - full_response: the\
      \ aggregated response string produced by streaming.\n      - context_data: the\
      \ latest context data captured from streaming (structure depends on the engine;\
      \ commonly a dict).\n\nExamples:\n  result, ctx = local_search(\n      config=config,\n\
      \      entities=entities_df,\n      communities=communities_df,\n      community_reports=reports_df,\n\
      \      text_units=text_units_df,\n      relationships=relationships_df,\n  \
      \    covariates=covariates_df,\n      community_level=2,\n      response_type=\"\
      full\",\n      query=\"quantum computing\",\n      verbose=True,\n  )\n\nRaises:\n\
      \  pydantic.ValidationError: If the input arguments do not satisfy the declared\
      \ types.\n  Exception: Any exception raised by the underlying streaming engine\
      \ or callback processing."
  - node_id: graphrag/api/query.py::local_search_streaming
    name: local_search_streaming
    signature: "def local_search_streaming(\n    config: GraphRagConfig,\n    entities:\
      \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
      \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates:\
      \ pd.DataFrame | None,\n    community_level: int,\n    response_type: str,\n\
      \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> AsyncGenerator"
    docstring: "Perform a local search and stream results via an asynchronous generator.\n\
      \nArgs:\n  config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
      \  entities (pd.DataFrame): A DataFrame containing the final entities (from\
      \ entities.parquet)\n  communities (pd.DataFrame): A DataFrame containing the\
      \ final communities (from communities.parquet)\n  community_reports (pd.DataFrame):\
      \ A DataFrame containing the final community reports (from community_reports.parquet)\n\
      \  text_units (pd.DataFrame): A DataFrame containing the final text units (from\
      \ text_units.parquet)\n  relationships (pd.DataFrame): A DataFrame containing\
      \ the final relationships (from relationships.parquet)\n  covariates (pd.DataFrame\
      \ | None): A DataFrame containing the final covariates (from covariates.parquet)\
      \ or None\n  community_level (int): The community level to search at.\n  response_type\
      \ (str): The response type to return.\n  query (str): The user query to search\
      \ for.\n  callbacks (list[QueryCallbacks] | None): Optional list of QueryCallbacks\
      \ to customize streaming behavior.\n  verbose (bool): Enable verbose logging\
      \ when true.\n\nReturns:\n  AsyncGenerator[str, None]: An asynchronous generator\
      \ yielding chunks of the streaming local search results as strings.\n\nEdge\
      \ cases:\n  - If covariates is None, covariates are treated as an empty list\
      \ for the search."
  - node_id: graphrag/api/query.py::multi_index_basic_search
    name: multi_index_basic_search
    signature: "def multi_index_basic_search(\n    config: GraphRagConfig,\n    text_units_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    streaming: bool,\n \
      \   query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
      \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
      \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
    docstring: "Perform a basic search across multiple indexes and return the search\
      \ results and associated context data.\n\nArgs:\n    config (GraphRagConfig):\
      \ A graphrag configuration (from settings.yaml).\n    text_units_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final text units (from text_units.parquet).\n\
      \    index_names (list[str]): A list of index names.\n    streaming (bool):\
      \ Whether to stream the results or not. Streaming is not implemented for this\
      \ function.\n    query (str): The user query to search for.\n    callbacks (list[QueryCallbacks]\
      \ | None): Optional callbacks for processing the search.\n    verbose (bool):\
      \ If True, enable verbose logging.\n\nReturns:\n    tuple[str | dict[str, Any]\
      \ | list[dict[str, Any]], str | list[pd.DataFrame] | dict[str, pd.DataFrame]]:\
      \ A tuple containing the search response and the context data. The first element\
      \ can be a string, a dict, or a list of dicts. The second element can be a string,\
      \ a list of DataFrames, or a dict mapping strings to DataFrames.\n\nRaises:\n\
      \    NotImplementedError: If streaming is True; streaming is not yet implemented\
      \ for multi_index_basic_search."
  - node_id: graphrag/api/query.py::multi_index_drift_search
    name: multi_index_drift_search
    signature: "def multi_index_drift_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int,\n\
      \    response_type: str,\n    streaming: bool,\n    query: str,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n) -> tuple[\n\
      \    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
      \ | dict[str, pd.DataFrame],\n]"
    docstring: "Perform a DRIFT search across multiple indexes and return the search\
      \ result and updated context data.\n\nArgs:\n    config (GraphRagConfig): A\
      \ graphrag configuration (from settings.yaml).\n    entities_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final entities (from entities.parquet).\n\
      \        Each DataFrame is expected to include columns such as human_readable_id,\
      \ id, title, and text_unit_ids.\n    communities_list (list[pd.DataFrame]):\
      \ A list of DataFrames containing the final communities (from communities.parquet).\n\
      \        Each DataFrame is expected to include columns such as community, entity_ids,\
      \ and human_readable_id.\n    community_reports_list (list[pd.DataFrame]): A\
      \ list of DataFrames containing the final community reports (from community_reports.parquet).\n\
      \        Each DataFrame is expected to include columns such as community, id,\
      \ and human_readable_id.\n    text_units_list (list[pd.DataFrame]): A list of\
      \ DataFrames containing the final text units (from text_units.parquet).\n  \
      \      Each DataFrame should include at least id and human_readable_id.\n  \
      \  relationships_list (list[pd.DataFrame]): A list of DataFrames containing\
      \ the final relationships (from relationships.parquet).\n        Each DataFrame\
      \ is expected to include human_readable_id, source, target, and text_unit_ids.\n\
      \    index_names (list[str]): A list of index names.\n    community_level (int):\
      \ The community level to search at.\n    response_type (str): The type of response\
      \ to return.\n    streaming (bool): Whether to stream the results or not. Streaming\
      \ is not yet implemented; if True a NotImplementedError will be raised.\n  \
      \  query (str): The user query to search for.\n    callbacks (list[QueryCallbacks]\
      \ | None): Optional callbacks to apply to the search.\n    verbose (bool): Enable\
      \ verbose logging.\n\nReturns:\n    tuple[\n        str | dict[str, Any] | list[dict[str,\
      \ Any]],\n        str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n    ]:\
      \ A tuple consisting of the search result payload and the updated context data.\n\
      \        - The first element (result) may be a string, a dictionary mapping\
      \ string keys to result data, or a list of result dictionaries.\n        - The\
      \ second element (context) is either a string, a list of DataFrames, or a dictionary\
      \ mapping keys to DataFrames.\n\nRaises:\n    NotImplementedError: If streaming\
      \ is requested, as streaming is not yet implemented for multi_index_drift_search.\n\
      \nExamples:\n    result, context = await multi_index_drift_search(\n       \
      \ config=cfg,\n        entities_list=entities_per_index,\n        communities_list=communities_per_index,\n\
      \        community_reports_list=reports_per_index,\n        text_units_list=text_units_per_index,\n\
      \        relationships_list=relationships_per_index,\n        index_names=[\"\
      idx_a\", \"idx_b\"],\n        community_level=1,\n        response_type=\"full\"\
      ,\n        streaming=False,\n        query=\"find drift\",\n        callbacks=None,\n\
      \        verbose=True,\n    )"
  - node_id: graphrag/api/query.py::multi_index_local_search
    name: multi_index_local_search
    signature: "def multi_index_local_search(\n    config: GraphRagConfig,\n    entities_list:\
      \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
      \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
      \ list[pd.DataFrame],\n    covariates_list: list[pd.DataFrame] | None,\n   \
      \ index_names: list[str],\n    community_level: int,\n    response_type: str,\n\
      \    streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
      \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      ]"
    docstring: "Perform a local search across multiple indexes and return the rendered\
      \ response and updated context data.\n\nArgs:\n  config: GraphRagConfig\n  \
      \    A graphrag configuration (from settings.yaml).\n  entities_list: list[pd.DataFrame]\n\
      \      A list of DataFrames containing the final entities (from entities.parquet).\n\
      \  communities_list: list[pd.DataFrame]\n      A list of DataFrames containing\
      \ the final communities (from communities.parquet).\n  community_reports_list:\
      \ list[pd.DataFrame]\n      A list of DataFrames containing the final community\
      \ reports (from community_reports.parquet).\n  text_units_list: list[pd.DataFrame]\n\
      \      A list of DataFrames containing the final text units (from text_units.parquet).\n\
      \  relationships_list: list[pd.DataFrame]\n      A list of DataFrames containing\
      \ the final relationships (from relationships.parquet).\n  covariates_list:\
      \ list[pd.DataFrame] | None\n      Optional; A list of DataFrames containing\
      \ the final covariates (from covariates.parquet).\n  index_names: list[str]\n\
      \      A list of index names.\n  community_level: int\n      The community level\
      \ to search at.\n  response_type: str\n      The response type to return.\n\
      \  streaming: bool\n      Whether to stream the results or not. Streaming is\
      \ not yet implemented for this multi-index search.\n  query: str\n      The\
      \ user query to search for.\n  callbacks: list[QueryCallbacks] | None\n    \
      \  Optional; Callbacks to customize query handling.\n  verbose: bool\n     \
      \ Enable verbose logging.\n\nReturns:\n  tuple[\n      str | dict[str, Any]\
      \ | list[dict[str, Any]],\n      str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
      \  ]\n  A tuple containing the rendered response and the updated context data.\
      \ The first element is\n  the response as a string or a structured object (dict\
      \ or list of dicts) depending on the\n  requested format. The second element\
      \ is the context data, which may be a string, a list of DataFrames,\n  or a\
      \ dict mapping string keys to DataFrames.\n\nRaises:\n  NotImplementedError\n\
      \      If streaming is requested (streaming == True), as streaming is not yet\
      \ implemented for this\n      multi-index local search."
  classes: []
- file: graphrag/callbacks/console_workflow_callbacks.py
  functions:
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.__init__
    name: __init__
    signature: def __init__(self, verbose=False)
    docstring: "Initialize ConsoleWorkflowCallbacks with an optional verbose mode.\n\
      \nArgs:\n  verbose: Enable verbose logging to the console.\n\nReturns:\n  None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Args:\n    results: A list of PipelineRunResult objects representing the results\
      \ of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "\"\"\"Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the workflows that started.\n\nReturns:\n\
      \    None\n\"\"\""
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
      \ The name of the workflow starting.\n    instance (object): The workflow instance\
      \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "\"\"\"Writes a live progress bar to stdout that updates in place as\
      \ progress events occur.\n\nThis callback renders a simple, in-place progress\
      \ bar on a single stdout line and\noverwrites the previous line using a carriage\
      \ return. It flushes the output to\nensure timely updates.\n\nProgress calculation:\n\
      - completed_items: number of items completed (defaults to 0 if None or falsy)\n\
      - total_items: total items to process (defaults to 1 if None or falsy to avoid\
      \ division by zero)\n- percent: integer percentage of completion, computed as\
      \ round((completed / total) * 100)\n\nOutput behavior:\n- Prints a line showing\
      \ the current progress as \"completed / total\" followed by a dot-filled\n \
      \ bar whose width is proportional to percent. The bar is created by left-justifying\
      \ the\n  start string within a field of width percent using '.' as the fill\
      \ character.\n- The line uses end=\"\\r\" to return the cursor to the start\
      \ of the line for in-place updates.\n\nArgs:\n    progress: Progress object\
      \ containing completed_items and total_items attributes.\n\nReturns:\n    None\n\
      \"\"\""
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None..."
  classes:
  - class_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks
    name: ConsoleWorkflowCallbacks
    docstring: "Console-based callback implementation that prints workflow and pipeline\
      \ events to the console.\n\nThis class inherits NoopWorkflowCallbacks and provides\
      \ a concrete, console-oriented implementation of the workflow callback interface.\
      \ It prints status messages for pipeline and workflow lifecycle events and renders\
      \ a live progress bar to stdout as progress updates are received. When verbose\
      \ mode is enabled, additional diagnostic information may be emitted to aid debugging.\n\
      \nArgs:\n    verbose (bool): Enable verbose logging to the console.\n\nAttributes:\n\
      \    _verbose (bool): Internal flag controlling verbose output.\n\nReturns:\n\
      \    None\n\nRaises:\n    May raise OSError or other I/O-related exceptions\
      \ raised by writing to stdout.\n\nMethods:\n    __init__(verbose: bool = False)\n\
      \    pipeline_start(names: list[str]) -> None\n    pipeline_end(results: list[PipelineRunResult])\
      \ -> None\n    workflow_start(name: str, instance: object) -> None\n    progress(progress:\
      \ Progress) -> None\n    workflow_end(name: str, instance: object) -> None"
    methods:
    - name: __init__
      signature: def __init__(self, verbose=False)
    - name: pipeline_end
      signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    - name: pipeline_start
      signature: 'def pipeline_start(self, names: list[str]) -> None'
    - name: workflow_start
      signature: 'def workflow_start(self, name: str, instance: object) -> None'
    - name: progress
      signature: 'def progress(self, progress: Progress) -> None'
    - name: workflow_end
      signature: 'def workflow_end(self, name: str, instance: object) -> None'
- file: graphrag/callbacks/llm_callbacks.py
  functions:
  - node_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback.on_llm_new_token
    name: on_llm_new_token
    signature: 'def on_llm_new_token(self, token: str)'
    docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n \
      \       The new token generated by the LLM.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback
    name: BaseLLMCallback
    docstring: 'BaseLLMCallback is a base interface for callbacks that respond to
      token generation events in an LLM pipeline.


      Purpose:

      Define the contract for handling LLM events, specifically when new tokens are
      produced. Subclasses should override on_llm_new_token(token) to implement custom
      behavior.


      Attributes:

      - None defined at the base level. Concrete implementations may add state as
      needed.


      Summary:

      This class provides a lightweight, extendable hook for observing or processing
      tokens emitted by the language model.


      Args:

      - token: str The new token generated by the LLM. This value is passed to on_llm_new_token
      when a token is produced.


      Returns:

      - None


      Raises:

      - None'
    methods:
    - name: on_llm_new_token
      signature: 'def on_llm_new_token(self, token: str)'
- file: graphrag/callbacks/noop_query_callbacks.py
  functions:
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_start
    name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    docstring: "Handle the start of a map operation.\n\nArgs:\n    map_response_contexts:\
      \ A list of strings representing contexts for the map response operation to\
      \ begin processing.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_llm_new_token
    name: on_llm_new_token
    signature: def on_llm_new_token(self, token)
    docstring: "Handle when a new token is generated.\n\nThis is a no-op callback\
      \ in NoopQueryCallbacks; calling this method does not modify state, perform\
      \ work, or produce side effects.\n\nArgs:\n    token: str\n        The new token\
      \ generated by the LLM.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_context
    name: on_context
    signature: 'def on_context(self, context: Any) -> None'
    docstring: "\"\"\"Handle when context data is constructed.\n\nThis no-op implementation\
      \ does not modify, store, or otherwise affect the given context.\n\nArgs:\n\
      \    context (Any): The context data provided to the callback. This implementation\
      \ performs no operations on it.\n\nReturns:\n    None: The function returns\
      \ no value and has no side effects.\n\nRaises:\n    None: This function does\
      \ not raise exceptions by itself.\n\"\"\""
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_start
    name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
      \ Context for the reduce response (str | dict[str, Any]).\n\nReturns:\n    None:\
      \ The function does not return a value."
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_end
    name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    docstring: "Handle the end of map operation.\n\nArgs:\n    map_response_outputs:\
      \ list[SearchResult] - The outputs produced by the map operation.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_end
    name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    docstring: "No-op callback for the end of the reduce operation in NoopQueryCallbacks.\n\
      \nThis method intentionally performs no action and exists solely to conform\
      \ to the NoopQueryCallbacks interface as a placeholder without side effects.\n\
      \nArgs:\n    reduce_response_output: str\n        The output produced by the\
      \ end of the reduce operation.\n\nReturns:\n    None: The function does not\
      \ return a value."
  classes:
  - class_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
    name: NoopQueryCallbacks
    docstring: "NoopQueryCallbacks is a no-op implementation of the QueryCallbacks\
      \ interface used for query callback events. It deliberately performs no actions\
      \ and maintains no internal state.\n\nArgs:\n  None: This class has no constructor\
      \ parameters.\n\nReturns:\n  None: The class does not return a value.\n\nRaises:\n\
      \  None: This class does not raise exceptions by itself.\n\nAttributes:\n  None:\
      \ This class maintains no internal state."
    methods:
    - name: on_map_response_start
      signature: 'def on_map_response_start(self, map_response_contexts: list[str])
        -> None'
    - name: on_llm_new_token
      signature: def on_llm_new_token(self, token)
    - name: on_context
      signature: 'def on_context(self, context: Any) -> None'
    - name: on_reduce_response_start
      signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
        \ str | dict[str, Any]\n    ) -> None"
    - name: on_map_response_end
      signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
        -> None'
    - name: on_reduce_response_end
      signature: 'def on_reduce_response_end(self, reduce_response_output: str) ->
        None'
- file: graphrag/callbacks/noop_workflow_callbacks.py
  functions:
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "Handle when progress occurs.\n\nArgs:\n    progress: Progress object\
      \ representing the current progress event.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Args:\n    results: A list of PipelineRunResult objects representing the results\
      \ of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
      \ The name of the workflow starting.\n    instance (object): The workflow instance\
      \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  classes:
  - class_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
    name: NoopWorkflowCallbacks
    docstring: 'Noop implementation of the WorkflowCallbacks interface that performs
      no operations.


      Purpose:

      Provide a safe, no-op callback implementation that conforms to the WorkflowCallbacks

      interface, suitable for tests or scenarios where callbacks are required but
      should

      not alter behavior.


      Attributes:

      - Stateless: This class does not maintain internal state between calls.


      Summary:

      All callback methods implemented by this class return None and perform no side
      effects.'
    methods:
    - name: progress
      signature: 'def progress(self, progress: Progress) -> None'
    - name: pipeline_end
      signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    - name: workflow_end
      signature: 'def workflow_end(self, name: str, instance: object) -> None'
    - name: pipeline_start
      signature: 'def pipeline_start(self, names: list[str]) -> None'
    - name: workflow_start
      signature: 'def workflow_start(self, name: str, instance: object) -> None'
- file: graphrag/callbacks/query_callbacks.py
  functions:
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_start
    name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
      \ Context for the reduce response (str | dict[str, Any]).\nReturns:\n    None:\
      \ The function does not return a value."
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_llm_new_token
    name: on_llm_new_token
    signature: def on_llm_new_token(self, token) -> None
    docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n \
      \       The new token generated by the LLM.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_end
    name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    docstring: "End of map operation callback. This default implementation is a no-op\
      \ and does not mutate state or produce side effects. Subclasses may override\
      \ this method to handle the map outputs as needed.\n\nArgs:\n    map_response_outputs\
      \ (list[SearchResult]): The outputs produced by the map operation.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_start
    name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    docstring: "Handle the start of map response operation.\n\nArgs:\n    map_response_contexts:\
      \ A list of strings representing contexts for the map response operation to\
      \ begin processing.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_context
    name: on_context
    signature: 'def on_context(self, context: Any) -> None'
    docstring: "Handle when context data is constructed.\n\nArgs:\n    context (Any):\
      \ The context data provided to the callback. This implementation performs no\
      \ operations on it.\n\nReturns:\n    None: The function returns no value.\n\n\
      Raises:\n    None: This function does not raise exceptions by itself."
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_end
    name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    docstring: "Handle the end of reduce operation.\n\nArgs:\n    reduce_response_output:\
      \ str\n        The output produced by the end of the reduce operation.\n\nReturns:\n\
      \    None: The function does not return a value."
  classes:
  - class_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks
    name: QueryCallbacks
    docstring: "QueryCallbacks is a base class that defines callback hooks used during\
      \ a query processing workflow involving map and reduce operations and interactions\
      \ with a language model.\n\nPurpose:\n    Provide default, overridable callback\
      \ methods for lifecycle events such as starting and ending map and reduce operations,\
      \ handling new tokens from the LLM, and processing context data.\n\nKey attributes:\n\
      \    None documented. This class does not define persistent state in the provided\
      \ data.\n\nSummary:\n    The class declares the following callback methods:\n\
      \    - on_reduce_response_start(reduce_response_context: str | dict[str, Any])\
      \ -> None\n    - on_llm_new_token(token) -> None\n    - on_map_response_end(map_response_outputs:\
      \ list[SearchResult]) -> None\n    - on_map_response_start(map_response_contexts:\
      \ list[str]) -> None\n    - on_context(context: Any) -> None\n    - on_reduce_response_end(reduce_response_output:\
      \ str) -> None\n\n    Subclasses may override these methods to implement custom\
      \ side effects. In particular, on_map_response_end and on_context describe no-op\
      \ default behavior."
    methods:
    - name: on_reduce_response_start
      signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
        \ str | dict[str, Any]\n    ) -> None"
    - name: on_llm_new_token
      signature: def on_llm_new_token(self, token) -> None
    - name: on_map_response_end
      signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
        -> None'
    - name: on_map_response_start
      signature: 'def on_map_response_start(self, map_response_contexts: list[str])
        -> None'
    - name: on_context
      signature: 'def on_context(self, context: Any) -> None'
    - name: on_reduce_response_end
      signature: 'def on_reduce_response_end(self, reduce_response_output: str) ->
        None'
- file: graphrag/callbacks/workflow_callbacks.py
  functions:
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "\"\"\"Handle when progress occurs.\n\nArgs:\n    progress: Progress\
      \ object representing the current progress event.\n\nReturns:\n    None\n\"\"\
      \""
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n   \
      \ name (str): The name of the workflow starting.\n    instance (object): The\
      \ workflow instance object associated with this start event.\n\nReturns:\n \
      \   None\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Parameters:\n    results: list[PipelineRunResult]. A list of PipelineRunResult\
      \ objects representing the results of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow ends.\n\nArgs:\n    name:\
      \ str\n        The name of the workflow.\n    instance: object\n        The\
      \ workflow instance object.\n\nReturns:\n    None...\n\"\"\""
  classes:
  - class_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks
    name: WorkflowCallbacks
    docstring: "WorkflowCallbacks defines a Protocol for observers of workflow and\
      \ pipeline lifecycle events. Implementations act as stateless observers that\
      \ react to progress updates and the start/end signals emitted by the orchestration\
      \ layer.\n\nMethods\n\nprogress(self, progress: Progress) -> None\n    Called\
      \ when a progress update occurs.\n    progress: Progress object representing\
      \ the current progress event.\n    Returns: None\n\nworkflow_start(self, name:\
      \ str, instance: object) -> None\n    Invoked when a workflow starts.\n    name:\
      \ The name of the workflow starting.\n    instance: The workflow instance object\
      \ associated with the start event.\n    Returns: None\n\npipeline_start(self,\
      \ names: list[str]) -> None\n    Invoked to signal that the entire pipeline\
      \ starts.\n    names: The names of the pipelines that started.\n    Returns:\
      \ None\n\npipeline_end(self, results: list[PipelineRunResult]) -> None\n   \
      \ Invoked to signal that the entire pipeline ends.\n    results: A list of PipelineRunResult\
      \ objects representing the pipeline results.\n    Returns: None\n\nworkflow_end(self,\
      \ name: str, instance: object) -> None\n    Invoked when a workflow ends.\n\
      \    name: The name of the workflow.\n    instance: The workflow instance object.\n\
      \    Returns: None\n\nUsage\nObservers implement this Protocol and register\
      \ with the orchestration system to receive these callbacks during workflow and\
      \ pipeline lifecycle events."
    methods:
    - name: progress
      signature: 'def progress(self, progress: Progress) -> None'
    - name: workflow_start
      signature: 'def workflow_start(self, name: str, instance: object) -> None'
    - name: pipeline_start
      signature: 'def pipeline_start(self, names: list[str]) -> None'
    - name: pipeline_end
      signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    - name: workflow_end
      signature: 'def workflow_end(self, name: str, instance: object) -> None'
- file: graphrag/callbacks/workflow_callbacks_manager.py
  functions:
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n   \
      \ name (str): The name of the workflow starting.\n    instance (object): The\
      \ workflow instance object associated with this start event.\n\nReturns:\n \
      \   None\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.__init__
    name: __init__
    signature: def __init__(self)
    docstring: 'Initialize a new WorkflowCallbacksManager.


      This manager holds registered WorkflowCallbacks instances in the _callbacks
      list and dispatches relevant lifecycle events to them (pipeline_start, pipeline_end,
      workflow_start, workflow_end, progress). The _callbacks list is initialized
      to an empty list during construction.


      Returns: None (implicit)'
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Dispatch the pipeline_start event to all registered callbacks.\n\n\
      As a manager, this forwards the pipeline_start event to every registered WorkflowCallbacks\
      \ instance that implements a pipeline_start method. The names argument is passed\
      \ unchanged to each callback's pipeline_start.\n\nArgs:\n    names: list[str]\
      \ The names of the pipelines that started.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "Forward progress events to registered callbacks that implement a progress\
      \ method.\n\nThis method iterates over the internal _callbacks collection and\
      \ propagates the\nprovided Progress event to each callback by invoking its progress\
      \ method when\npresent.\n\nArgs:\n    progress: Progress object representing\
      \ the current progress event.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.register
    name: register
    signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
    docstring: "Register a new WorkflowCallbacks instance.\n\nAppends the provided\
      \ WorkflowCallbacks instance to the internal registry (self._callbacks). This\
      \ allows multiple callbacks to be registered and receive lifecycle event callbacks.\
      \ There is no deduplication; registering the same instance more than once will\
      \ result in duplicates in the registry.\n\nArgs:\n    callbacks (WorkflowCallbacks):\
      \ The WorkflowCallbacks instance to register.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback when the entire pipeline ends.\n\nArgs:\n  \
      \  results: A list of PipelineRunResult objects representing the results of\
      \ the pipeline runs.\n\nReturns:\n    None..."
  classes:
  - class_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager
    name: WorkflowCallbacksManager
    docstring: "Manager that holds registered WorkflowCallbacks instances and dispatches\
      \ lifecycle events to them.\n\nPurpose:\nTo centralize the registration and\
      \ dispatch of workflow and pipeline lifecycle events to all registered callbacks.\n\
      \nKey attributes:\n- _callbacks: List[WorkflowCallbacks]\n    Internal registry\
      \ of callbacks to which events are forwarded.\n\nBrief summary:\nThe manager\
      \ maintains an internal registry of WorkflowCallbacks implementations and forwards\
      \ events such as pipeline_start, pipeline_end, workflow_start, workflow_end,\
      \ and progress to each registered callback that implements the corresponding\
      \ method. There is no deduplication when registering callbacks; the same instance\
      \ can be added multiple times.\n\nArgs:\n- self: The WorkflowCallbacksManager\
      \ instance. The constructor takes no external parameters.\n\nReturns:\nNone\n\
      \nRaises:\nNone"
    methods:
    - name: workflow_start
      signature: 'def workflow_start(self, name: str, instance: object) -> None'
    - name: __init__
      signature: def __init__(self)
    - name: workflow_end
      signature: 'def workflow_end(self, name: str, instance: object) -> None'
    - name: pipeline_start
      signature: 'def pipeline_start(self, names: list[str]) -> None'
    - name: progress
      signature: 'def progress(self, progress: Progress) -> None'
    - name: register
      signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
    - name: pipeline_end
      signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
- file: graphrag/cli/index.py
  functions:
  - node_id: graphrag/cli/index.py::handle_signal
    name: handle_signal
    signature: def handle_signal(signum, _)
    docstring: "Handle a system signal by cancelling all asyncio tasks and logging\
      \ exit messages.\n\nArgs:\n    signum: The signal number received.\n    _: The\
      \ current stack frame (unused).\n\nReturns:\n    None"
  - node_id: graphrag/cli/index.py::_register_signal_handlers
    name: _register_signal_handlers
    signature: def _register_signal_handlers()
    docstring: "Register signal handlers for graceful shutdown of the CLI.\n\nThis\
      \ function defines a signal handler that logs the received signal, cancels all\
      \ asyncio tasks, and logs that all tasks have been cancelled. It registers the\
      \ handler for SIGINT and, on non-Windows platforms, SIGHUP.\n\nReturns:\n  \
      \  None"
  - node_id: graphrag/cli/index.py::_run_index
    name: _run_index
    signature: "def _run_index(\n    config,\n    method,\n    is_update_run,\n  \
      \  verbose,\n    memprofile,\n    cache,\n    dry_run,\n    skip_validation,\n\
      )"
    docstring: "Run the indexing pipeline using the provided configuration.\n\nParameters\
      \ (with types):\n  config: GraphRagConfig - The GraphRagConfig instance containing\
      \ run configuration.\n  method: IndexingMethod - The indexing method to use\
      \ for this run.\n  is_update_run: bool - True if this is an update run, False\
      \ otherwise.\n  verbose: bool - Enable verbose logging/output.\n  memprofile:\
      \ bool - Enable memory profiling during execution.\n  cache: bool - Whether\
      \ to enable caching; if False, caching is disabled.\n  dry_run: bool - If True,\
      \ perform a dry run and exit before execution.\n  skip_validation: bool - If\
      \ True, skip validation of configuration names.\n\nReturns (None):\n  None\n\
      \nRaises:\n  SystemExit:\n    - If dry_run is True, the function exits with\
      \ status 0.\n    - At the end, the function exits with status 0 if no errors\
      \ were encountered, or 1 if errors occurred.\n    - If validate_config_names\
      \ raises SystemExit due to validation failures (only when skip_validation is\
      \ False)."
  - node_id: graphrag/cli/index.py::index_cli
    name: index_cli
    signature: "def index_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n\
      \    verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
      \ Path | None,\n    dry_run: bool,\n    skip_validation: bool,\n    output_dir:\
      \ Path | None,\n)"
    docstring: "Run the indexing pipeline with the given configuration.\n\nParameters:\n\
      \    root_dir (Path): The root directory of the project. Will search for the\
      \ configuration file in this directory.\n    method (IndexingMethod): The indexing\
      \ method to use for this run.\n    verbose (bool): Enable verbose logging/output.\n\
      \    memprofile (bool): Enable memory profiling during execution.\n    cache\
      \ (bool): Whether to enable caching; if False, caching is disabled.\n    config_filepath\
      \ (Path | None): Path to the configuration file. If None, searches for the config\
      \ file in root_dir.\n    dry_run (bool): If True, perform a dry run without\
      \ making changes.\n    skip_validation (bool): Skip validation of the loaded\
      \ configuration.\n    output_dir (Path | None): Directory to write outputs.\
      \ If provided, overrides base output/reporting directories.\n\nReturns:\n  \
      \  None\n\nRaises:\n    FileNotFoundError: If the config file is not found.\n\
      \    ValueError: If the configuration is invalid."
  - node_id: graphrag/cli/index.py::update_cli
    name: update_cli
    signature: "def update_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n\
      \    verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
      \ Path | None,\n    skip_validation: bool,\n    output_dir: Path | None,\n)"
    docstring: "Run the update pipeline with the given config.\n\nThis function applies\
      \ optional output directory overrides, loads the configuration, and executes\
      \ the update phase of the indexing pipeline.\n\nArgs:\n    root_dir: The root\
      \ directory of the project. Will search for the config file in this directory.\n\
      \    method: The indexing method to use for this run.\n    verbose: Enable verbose\
      \ logging/output.\n    memprofile: Enable memory profiling during execution.\n\
      \    cache: Whether to enable caching; if False, caching is disabled.\n    config_filepath:\
      \ The path to the config file. If None, searches for config file in root.\n\
      \    skip_validation: Whether to skip validation of the loaded configuration.\n\
      \    output_dir: Optional output directory to override base directories used\
      \ by the pipeline.\n\nReturns:\n    None\n\nRaises:\n    FileNotFoundError:\
      \ If the config file is not found.\n    ValueError: If the configuration is\
      \ invalid."
  classes: []
- file: graphrag/cli/initialize.py
  functions:
  - node_id: graphrag/cli/initialize.py::initialize_project_at
    name: initialize_project_at
    signature: 'def initialize_project_at(path: Path, force: bool) -> None'
    docstring: "Initialize the project at the given path.\n\nArgs:\n    path: The\
      \ path at which to initialize the project.\n    force: Whether to force initialization\
      \ even if the project already exists.\n\nReturns:\n    None\n\nRaises:\n   \
      \ ValueError: If the project already exists and force is False."
  classes: []
- file: graphrag/cli/main.py
  functions:
  - node_id: graphrag/cli/main.py::wildcard_match
    name: wildcard_match
    signature: 'def wildcard_match(string: str, pattern: str) -> bool'
    docstring: "Determine whether the entire string matches a wildcard pattern.\n\
      The pattern uses ? to match any single character and * to match any sequence\
      \ of characters.\n\nArgs:\n    string: The input string to test against the\
      \ pattern.\n    pattern: The wildcard pattern, where ? matches any single character\
      \ and * matches any sequence of characters.\n\nReturns:\n    bool: True if the\
      \ string matches the wildcard pattern, otherwise False.\n\nRaises:\n    TypeError:\
      \ If string or pattern are not of type str."
  - node_id: graphrag/cli/main.py::path_autocomplete
    name: path_autocomplete
    signature: "def path_autocomplete(\n    file_okay: bool = True,\n    dir_okay:\
      \ bool = True,\n    readable: bool = True,\n    writable: bool = False,\n  \
      \  match_wildcard: str | None = None,\n) -> Callable[[str], list[str]]"
    docstring: "Autocomplete file and directory paths.\n\nArgs:\n    file_okay: bool\n\
      \        If True, include files in the completions; otherwise, exclude files.\n\
      \    dir_okay: bool\n        If True, include directories in the completions;\
      \ otherwise, exclude directories.\n    readable: bool\n        If True, include\
      \ only items that are readable (os.R_OK).\n    writable: bool\n        If True,\
      \ include only items that are writable (os.W_OK).\n    match_wildcard: str |\
      \ None\n        Optional wildcard pattern to filter items; supports '?' and\
      \ '*' characters.\n\nReturns:\n    Callable[[str], list[str]]\n        A function\
      \ that takes the current incomplete string and returns a list of matching item\
      \ names.\n\nRaises:\n    OSError\n        If an I/O error occurs during directory\
      \ listing or permission checks."
  - node_id: graphrag/cli/main.py::completer
    name: completer
    signature: 'def completer(incomplete: str) -> list[str]'
    docstring: "Return a list of possible completions for the given incomplete input\
      \ from the current directory.\n\nArgs:\n    incomplete: str\n        The partial\
      \ string to match against directory item names in the current directory.\n\n\
      Returns:\n    list[str]\n        A list of completion strings that start with\
      \ the provided incomplete string, after applying\n        filtering based on\
      \ external configuration (file_okay, dir_okay, readable, writable) and\n   \
      \     optional wildcard matching.\n\nRaises:\n    TypeError\n        If the\
      \ underlying wildcard matching function is invoked with non-string arguments."
  - node_id: graphrag/cli/main.py::_initialize_cli
    name: _initialize_cli
    signature: "def _initialize_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    force: bool = typer.Option(\n\
      \        False,\n        \"--force\",\n        \"-f\",\n        help=\"Force\
      \ initialization even if the project already exists.\",\n    ),\n) -> None"
    docstring: "\"\"\"Generate a default configuration file.\n\nArgs:\n    root (Path):\
      \ The project root directory.\n    force (bool): Force initialization even if\
      \ the project already exists.\n\nReturns:\n    None: This function does not\
      \ return a value.\n\nRaises:\n    ValueError: If the project already exists\
      \ and force is False.\n\"\"\""
  - node_id: graphrag/cli/main.py::_query_cli
    name: _query_cli
    signature: "def _query_cli(\n    method: SearchMethod = typer.Option(\n      \
      \  ...,\n        \"--method\",\n        \"-m\",\n        help=\"The query algorithm\
      \ to use.\",\n    ),\n    query: str = typer.Option(\n        ...,\n       \
      \ \"--query\",\n        \"-q\",\n        help=\"The query to execute.\",\n \
      \   ),\n    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the query with verbose logging.\",\n\
      \    ),\n    data: Path | None = typer.Option(\n        None,\n        \"--data\"\
      ,\n        \"-d\",\n        help=\"Index output directory (contains the parquet\
      \ files).\",\n        exists=True,\n        dir_okay=True,\n        readable=True,\n\
      \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n   \
      \ ),\n    root: Path = typer.Option(\n        Path(),\n        \"--root\",\n\
      \        \"-r\",\n        help=\"The project root directory.\",\n        exists=True,\n\
      \        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
      \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    community_level: int\
      \ = typer.Option(\n        2,\n        \"--community-level\",\n        help=(\n\
      \            \"Leiden hierarchy level from which to load community reports.\
      \ \"\n            \"Higher values represent smaller communities.\"\n       \
      \ ),\n    ),\n    dynamic_community_selection: bool = typer.Option(\n      \
      \  False,\n        \"--dynamic-community-selection/--no-dynamic-selection\"\
      ,\n        help=\"Use global search with dynamic community selection.\",\n \
      \   ),\n    response_type: str = typer.Option(\n        \"Multiple Paragraphs\"\
      ,\n        \"--response-type\",\n        help=(\n            \"Free-form description\
      \ of the desired response format \"\n            \"(e.g. 'Single Sentence',\
      \ 'List of 3-7 Points', etc.).\"\n        ),\n    ),\n    streaming: bool =\
      \ typer.Option(\n        False,\n        \"--streaming/--no-streaming\",\n \
      \       help=\"Print the response in a streaming manner.\",\n    ),\n) -> None"
    docstring: "Query a knowledge graph index.\n\nArgs:\n    method: The query algorithm\
      \ to use.\n    query: The query to execute.\n    config: The configuration to\
      \ use.\n    verbose: Run the query with verbose logging.\n    data: Index output\
      \ directory (contains the parquet files).\n    root: The project root directory.\n\
      \    community_level: Leiden hierarchy level from which to load community reports.\
      \ Higher values represent smaller communities.\n    dynamic_community_selection:\
      \ Use global search with dynamic community selection.\n    response_type: Free-form\
      \ description of the desired response format (e.g. 'Single Sentence', 'List\
      \ of 3-7 Points', etc.).\n    streaming: Print the response in a streaming manner.\n\
      \nReturns:\n    None\n\nRaises:\n    ValueError"
  - node_id: graphrag/cli/main.py::_index_cli
    name: _index_cli
    signature: "def _index_cli(\n    config: Path | None = typer.Option(\n       \
      \ None,\n        \"--config\",\n        \"-c\",\n        help=\"The configuration\
      \ to use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging\",\n    ),\n    memprofile: bool = typer.Option(\n  \
      \      False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling\",\n    ),\n    dry_run: bool = typer.Option(\n    \
      \    False,\n        \"--dry-run\",\n        help=(\n            \"Run the indexing\
      \ pipeline without executing any steps \"\n            \"to inspect and validate\
      \ the configuration.\"\n        ),\n    ),\n    cache: bool = typer.Option(\n\
      \        True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None"
    docstring: "Build a knowledge graph index.\n\nArgs:\n    config: The configuration\
      \ to use.\n    root: The project root directory.\n    method: The indexing method\
      \ to use.\n    verbose: Run the indexing pipeline with verbose logging.\n  \
      \  memprofile: Run the indexing pipeline with memory profiling.\n    dry_run:\
      \ Run the indexing pipeline without executing any steps to inspect and validate\
      \ the configuration.\n    cache: Use LLM cache.\n    skip_validation: Skip any\
      \ preflight validation. Useful when running no LLM steps.\n    output: Indexing\
      \ pipeline output directory. Overrides output.base_dir in the configuration\
      \ file.\n\nReturns:\n    None\n\nRaises:\n    Exceptions raised by graphrag.cli.index.index_cli\
      \ may propagate."
  - node_id: graphrag/cli/main.py::_prompt_tune_cli
    name: _prompt_tune_cli
    signature: "def _prompt_tune_cli(\n    root: Path = typer.Option(\n        Path(),\n\
      \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
      ,\n        exists=True,\n        dir_okay=True,\n        writable=True,\n  \
      \      resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
      \    config: Path | None = typer.Option(\n        None,\n        \"--config\"\
      ,\n        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
      \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
      \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
      ,\n        \"-v\",\n        help=\"Run the prompt tuning pipeline with verbose\
      \ logging.\",\n    ),\n    domain: str | None = typer.Option(\n        None,\n\
      \        \"--domain\",\n        help=(\n            \"The domain your input\
      \ data is related to. \"\n            \"For example 'space science', 'microbiology',\
      \ 'environmental news'. \"\n            \"If not defined, a domain will be inferred\
      \ from the input data.\"\n        ),\n    ),\n    selection_method: DocSelectionType\
      \ = typer.Option(\n        DocSelectionType.RANDOM.value,\n        \"--selection-method\"\
      ,\n        help=\"The text chunk selection method.\",\n    ),\n    n_subset_max:\
      \ int = typer.Option(\n        N_SUBSET_MAX,\n        \"--n-subset-max\",\n\
      \        help=\"The number of text chunks to embed when --selection-method=auto.\"\
      ,\n    ),\n    k: int = typer.Option(\n        K,\n        \"--k\",\n      \
      \  help=\"The maximum number of documents to select from each centroid when\
      \ --selection-method=auto.\",\n    ),\n    limit: int = typer.Option(\n    \
      \    LIMIT,\n        \"--limit\",\n        help=\"The number of documents to\
      \ load when --selection-method={random,top}.\",\n    ),\n    max_tokens: int\
      \ = typer.Option(\n        MAX_TOKEN_COUNT,\n        \"--max-tokens\",\n   \
      \     help=\"The max token count for prompt generation.\",\n    ),\n    min_examples_required:\
      \ int = typer.Option(\n        2,\n        \"--min-examples-required\",\n  \
      \      help=\"The minimum number of examples to generate/include in the entity\
      \ extraction prompt.\",\n    ),\n    chunk_size: int = typer.Option(\n     \
      \   graphrag_config_defaults.chunks.size,\n        \"--chunk-size\",\n     \
      \   help=\"The size of each example text chunk. Overrides chunks.size in the\
      \ configuration file.\",\n    ),\n    overlap: int = typer.Option(\n       \
      \ graphrag_config_defaults.chunks.overlap,\n        \"--overlap\",\n       \
      \ help=\"The overlap size for chunking documents. Overrides chunks.overlap in\
      \ the configuration file.\",\n    ),\n    language: str | None = typer.Option(\n\
      \        None,\n        \"--language\",\n        help=\"The primary language\
      \ used for inputs and outputs in graphrag prompts.\",\n    ),\n    discover_entity_types:\
      \ bool = typer.Option(\n        True,\n        \"--discover-entity-types/--no-discover-entity-types\"\
      ,\n        help=\"Discover and extract unspecified entity types.\",\n    ),\n\
      \    output: Path = typer.Option(\n        Path(\"prompts\"),\n        \"--output\"\
      ,\n        \"-o\",\n        help=\"The directory to save prompts to, relative\
      \ to the project root directory.\",\n        dir_okay=True,\n        writable=True,\n\
      \        resolve_path=True,\n    ),\n) -> None"
    docstring: "Generate custom graphrag prompts for a project using provided configuration\
      \ and options.\n\nArgs:\n    root: The project root directory.\n    config:\
      \ The configuration to use.\n    verbose: Run the prompt tuning pipeline with\
      \ verbose logging.\n    domain: The domain your input data is related to. If\
      \ not defined, a domain will be inferred from the input data.\n    selection_method:\
      \ The text chunk selection method.\n    n_subset_max: The number of text chunks\
      \ to embed when --selection-method=auto.\n    k: The maximum number of documents\
      \ to select from each centroid when --selection-method=auto.\n    limit: The\
      \ number of documents to load when --selection-method={random,top}.\n    max_tokens:\
      \ The max token count for prompt generation.\n    min_examples_required: The\
      \ minimum number of examples to generate/include in the entity extraction prompt.\n\
      \    chunk_size: The size of each example text chunk. Overrides chunks.size\
      \ in the configuration file.\n    overlap: The overlap size for chunking documents.\
      \ Overrides chunks.overlap in the configuration file.\n    language: The primary\
      \ language used for inputs and outputs in graphrag prompts.\n    discover_entity_types:\
      \ Discover and extract unspecified entity types.\n    output: The directory\
      \ to save prompts to, relative to the project root directory.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/cli/main.py::_update_cli
    name: _update_cli
    signature: "def _update_cli(\n    config: Path | None = typer.Option(\n      \
      \  None,\n        \"--config\",\n        \"-c\",\n        help=\"The configuration\
      \ to use.\",\n        exists=True,\n        file_okay=True,\n        readable=True,\n\
      \        autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
      \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
      \ root directory.\",\n        exists=True,\n        dir_okay=True,\n       \
      \ writable=True,\n        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n\
      \    ),\n    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
      \        \"--method\",\n        \"-m\",\n        help=\"The indexing method\
      \ to use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n  \
      \      \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline\
      \ with verbose logging.\",\n    ),\n    memprofile: bool = typer.Option(\n \
      \       False,\n        \"--memprofile\",\n        help=\"Run the indexing pipeline\
      \ with memory profiling.\",\n    ),\n    cache: bool = typer.Option(\n     \
      \   True,\n        \"--cache/--no-cache\",\n        help=\"Use LLM cache.\"\
      ,\n    ),\n    skip_validation: bool = typer.Option(\n        False,\n     \
      \   \"--skip-validation\",\n        help=\"Skip any preflight validation. Useful\
      \ when running no LLM steps.\",\n    ),\n    output: Path | None = typer.Option(\n\
      \        None,\n        \"--output\",\n        \"-o\",\n        help=(\n   \
      \         \"Indexing pipeline output directory. \"\n            \"Overrides\
      \ output.base_dir in the configuration file.\"\n        ),\n        dir_okay=True,\n\
      \        writable=True,\n        resolve_path=True,\n    ),\n) -> None"
    docstring: "Update an existing knowledge graph index.\n\nApplies a default output\
      \ configuration (if not provided by config), saving the new index to the local\
      \ file system in the update_output folder. If an explicit output path is provided,\
      \ it overrides the base output directory specified in the configuration.\n\n\
      Args:\n    config: Path | None - The configuration file to use. If None, the\
      \ configuration is located via the project root.\n    root: Path - The project\
      \ root directory. Directory containing the configuration and output structure;\
      \ used to locate the config when not provided.\n    method: IndexingMethod -\
      \ The indexing method to use. Defaults to Standard.\n    verbose: bool - Run\
      \ the indexing pipeline with verbose logging for detailed output.\n    memprofile:\
      \ bool - Run the indexing pipeline with memory profiling.\n    cache: bool -\
      \ Use LLM cache during processing.\n    skip_validation: bool - Skip any preflight\
      \ validation. Useful when running no LLM steps.\n    output: Path | None - Output\
      \ directory for the indexing pipeline. Overrides output.base_dir in the configuration\
      \ file. If None, the configuration\u2019s defaults are used.\n\nReturns:\n \
      \   None\n\nRaises:\n    FileNotFoundError - If the specified config or root\
      \ path does not exist.\n    PermissionError - If the configured or chosen output\
      \ locations are not writable.\n    ValueError - If an invalid combination of\
      \ options is provided.\n    OSError - For generic I/O errors encountered during\
      \ update.\n    Other exceptions may propagate from the underlying update_cli\
      \ call."
  classes: []
- file: graphrag/cli/prompt_tune.py
  functions:
  - node_id: graphrag/cli/prompt_tune.py::prompt_tune
    name: prompt_tune
    signature: "def prompt_tune(\n    root: Path,\n    config: Path | None,\n    domain:\
      \ str | None,\n    verbose: bool,\n    selection_method: api.DocSelectionType,\n\
      \    limit: int,\n    max_tokens: int,\n    chunk_size: int,\n    overlap: int,\n\
      \    language: str | None,\n    discover_entity_types: bool,\n    output: Path,\n\
      \    n_subset_max: int,\n    k: int,\n    min_examples_required: int,\n)"
    docstring: "Prompt tune the model asynchronously.\n\nNote: This coroutine must\
      \ be awaited. It loads the configuration, applies any chunking overrides, initializes\
      \ the root logger according to the verbose flag, and generates indexing prompts.\
      \ It writes the resulting prompts to the specified output directory if an output\
      \ path is provided; otherwise it logs an error and skips writing. Returns None\
      \ upon successful completion.\n\nArgs:\n    root: Path \u2014 The root directory\
      \ to resolve relative paths from.\n    config: Path | None \u2014 Optional path\
      \ to the configuration file.\n    domain: str | None \u2014 The domain to map\
      \ the input documents to.\n    verbose: bool \u2014 Enable verbose logging.\n\
      \    selection_method: api.DocSelectionType \u2014 The chunk selection method.\n\
      \    limit: int \u2014 The limit of chunks to load.\n    max_tokens: int \u2014\
      \ The maximum number of tokens to use on entity extraction prompts.\n    chunk_size:\
      \ int \u2014 The chunk token size to use.\n    overlap: int \u2014 The number\
      \ of tokens to overlap between consecutive chunks.\n    language: str | None\
      \ \u2014 The language to use for the prompts.\n    discover_entity_types: bool\
      \ \u2014 Generate entity types.\n    output: Path \u2014 The output folder to\
      \ store the prompts.\n    n_subset_max: int \u2014 The number of text chunks\
      \ to embed when using auto selection method.\n    k: int \u2014 The number of\
      \ documents to select when using auto selection method.\n    min_examples_required:\
      \ int \u2014 The minimum number of examples required for entity extraction prompts.\n\
      \nReturns:\n    None\n\nRaises:\n    Exceptions raised by underlying IO or configuration\
      \ loading may propagate."
  classes: []
- file: graphrag/cli/query.py
  functions:
  - node_id: graphrag/cli/query.py::on_context
    name: on_context
    signature: 'def on_context(context: Any) -> None'
    docstring: "Stores the given context in the enclosing scope's nonlocal variable\
      \ context_data.\n\nThis function uses a nonlocal binding to the variable context_data\
      \ defined in the outer scope and assigns the provided context to it. As a result,\
      \ the outer scope's context_data is updated. This function returns None.\n\n\
      Args:\n    context (Any): The context data to store in the enclosing scope.\n\
      \nReturns:\n    None: The function updates the outer scope's context_data and\
      \ returns no value.\n\nRaises:\n    NameError: If the enclosing scope does not\
      \ define context_data (nonlocal binding cannot be resolved)."
  - node_id: graphrag/cli/query.py::run_streaming_search
    name: run_streaming_search
    signature: def run_streaming_search()
    docstring: 'Run a streaming search and collect the full response while printing
      streamed chunks.


      Args:

      - None: The function does not take any parameters.


      Returns:

      - tuple[str, dict[str, Any]]: The full streaming response string and the context
      data captured during streaming.


      Raises:

      - Exceptions raised by the underlying streaming API or asyncio operations may
      propagate to the caller.'
  - node_id: graphrag/cli/query.py::_resolve_output_files
    name: _resolve_output_files
    signature: "def _resolve_output_files(\n    config: GraphRagConfig,\n    output_list:\
      \ list[str],\n    optional_list: list[str] | None = None,\n) -> dict[str, Any]"
    docstring: "\"\"\"Read indexing output files to a dataframe dict.\n\nArgs:\n \
      \   config: GraphRagConfig The configuration for GraphRag, including outputs\
      \ for multi-index search.\n    output_list: list[str] Names of the output dataframe\
      \ keys to load from storage.\n    optional_list: list[str] | None Optional list\
      \ of additional output dataframe keys to load if present.\n\nReturns:\n    dict[str,\
      \ Any]: A dictionary containing the loaded dataframes and metadata describing\
      \ the indexing layout.\n        If config.outputs is truthy (multi-index search):\n\
      \          - \"multi-index\": True\n          - \"num_indexes\": int number\
      \ of indexes (len(config.outputs))\n          - \"index_names\": config.outputs.keys()\n\
      \          - For each name in output_list: a list of DataFrames loaded from\
      \ storage (one per index)\n          - For each optional_file in optional_list,\
      \ a key optional_file with a list of DataFrames if present, otherwise an empty\
      \ list\n        If config.outputs is falsy (single-index search):\n        \
      \  - \"multi-index\": False\n          - For each name in output_list: the loaded\
      \ DataFrame\n          - For each optional_file in optional_list: the loaded\
      \ DataFrame if present, otherwise None\n\"\"\""
  - node_id: graphrag/cli/query.py::run_global_search
    name: run_global_search
    signature: "def run_global_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int | None,\n    dynamic_community_selection:\
      \ bool,\n    response_type: str,\n    streaming: bool,\n    query: str,\n  \
      \  verbose: bool,\n)"
    docstring: "Perform a global search with a given query.\n\nLoads index files required\
      \ for global search and calls the Query API.\n\nArgs:\n    config_filepath:\
      \ Path to a config file, or None.\n    data_dir: Optional data directory to\
      \ override output.base_dir, or None.\n    root_dir: Root directory of the project.\n\
      \    community_level: Optional integer indicating the target community level.\n\
      \    dynamic_community_selection: Whether to dynamically select communities.\n\
      \    response_type: Type of response to return.\n    streaming: True to use\
      \ streaming mode; False for a standard response.\n    query: The search query.\n\
      \    verbose: If True, enable verbose output.\n\nReturns:\n    tuple[str, dict[str,\
      \ Any]]: The response as a string and a context data dictionary.\n\nRaises:\n\
      \    FileNotFoundError: If the config file cannot be found.\n    ValueError:\
      \ If the loaded configuration is invalid.\n    Exception: Exceptions raised\
      \ by the underlying API calls or asyncio operations may propagate to the caller."
  - node_id: graphrag/cli/query.py::run_local_search
    name: run_local_search
    signature: "def run_local_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
    docstring: "Perform a local search with a given query.\n\nLoads index files required\
      \ for local search and calls the Query API.\n\nArgs:\n    config_filepath: Path\
      \ | None\n        Path to the configuration file to use, or None to use the\
      \ default location.\n    data_dir: Path | None\n        Optional directory containing\
      \ precomputed data to override the base output directory.\n    root_dir: Path\n\
      \        Root directory of the project.\n    community_level: int\n        Target\
      \ community level for the search.\n    response_type: str\n        The type\
      \ of response to return.\n    streaming: bool\n        If True, perform a streaming\
      \ search; otherwise perform a non-streaming search.\n    query: str\n      \
      \  The search query string used to perform the local search.\n    verbose: bool\n\
      \        If True, enable verbose logging output.\n\nReturns:\n    tuple[str,\
      \ dict[str, Any]]\n        The textual response and the context data dictionary\
      \ produced by the search operation.\n\nRaises:\n    FileNotFoundError\n    \
      \    If the configuration file cannot be found.\n    ValueError\n        If\
      \ the configuration is invalid or required components are missing.\n    Exception\n\
      \        Exceptions raised by the underlying API calls or asyncio operations\
      \ may propagate to the caller."
  - node_id: graphrag/cli/query.py::run_drift_search
    name: run_drift_search
    signature: "def run_drift_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
      \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
    docstring: "Perform a local drift search for a given query across either a multi-index\
      \ or single-index dataset, with optional streaming of results.\n\nLoads index\
      \ files required for local search and calls the appropriate Query API depending\
      \ on the loaded index layout.\n\nArgs:\n    config_filepath (Path | None): Path\
      \ to the configuration file to use, or None to use the default configuration.\n\
      \    data_dir (Path | None): Optional directory containing precomputed data\
      \ to override the base output directory.\n    root_dir (Path): Root directory\
      \ of the project.\n    community_level (int): Target community level for the\
      \ drift search.\n    response_type (str): The type of response to request from\
      \ the API.\n    streaming (bool): If True, stream results as they are produced.\
      \ Streaming prints each received chunk to stdout (without buffering) and may\
      \ update the context data during streaming.\n    query (str): The drift search\
      \ query string used to perform the local search.\n    verbose (bool): If True,\
      \ print verbose progress and diagnostic information.\n\nReturns:\n    tuple[str,\
      \ dict[str, Any]]: A pair consisting of the final response as a string and a\
      \ dictionary containing context data captured during the operation.\n\nRaises:\n\
      \    FileNotFoundError: If the configuration file cannot be found.\n    ValueError:\
      \ If the configuration is invalid or required configuration data are missing.\n\
      \    Exceptions propagated from the underlying API calls or asyncio operations\
      \ may occur (e.g., during streaming or remote calls).\n\nNotes on behavior:\n\
      \    - Internal flow depends on the index layout loaded by _resolve_output_files:\n\
      \      \u2022 If dataframe_dict[\"multi-index\"] is True, this function uses\
      \ the Multi-Index Drift Search API\n        and returns the API response along\
      \ with context data. The response is printed to stdout.\n      \u2022 Otherwise,\
      \ a Single-Index Drift Search workflow is used:\n        - If streaming is enabled,\
      \ an asynchronous streaming search is performed. Chunks are printed as they\
      \ arrive,\n          and a final full response string along with context data\
      \ is returned.\n        - If streaming is disabled, a single non-streaming search\
      \ is performed; the final response is printed and\n          returned with context\
      \ data.\n\nNotes on side effects:\n    - Printing: In both multi-index and non-streaming\
      \ single-index paths, the final response is printed to stdout.\n    - Streaming\
      \ path: Each streamed chunk is printed immediately as it is received, followed\
      \ by a newline at the end.\n    - Context data: The context data is captured\
      \ via an on_context callback during streaming and returned with the response."
  - node_id: graphrag/cli/query.py::run_basic_search
    name: run_basic_search
    signature: "def run_basic_search(\n    config_filepath: Path | None,\n    data_dir:\
      \ Path | None,\n    root_dir: Path,\n    streaming: bool,\n    query: str,\n\
      \    verbose: bool,\n)"
    docstring: "Perform a basics search with a given query.\n\nLoads index files required\
      \ for basic search and calls the Query API.\n\nArgs:\n    config_filepath: Path\
      \ | None\n        Path to a config file to use, or None to locate one in root_dir.\n\
      \    data_dir: Path | None\n        Optional directory containing precomputed\
      \ data to override the base output directory.\n    root_dir: Path\n        Root\
      \ directory of the project.\n    streaming: bool\n        If True, perform a\
      \ streaming search and print chunks as they are produced.\n    query: str\n\
      \        The search query string used to perform the basic search.\n    verbose:\
      \ bool\n        If True, enable verbose output from the API calls.\n\nReturns:\n\
      \    tuple[str, dict[str, Any]]\n        The full response string and the context\
      \ data collected during the search.\n\nRaises:\n    FileNotFoundError\n    \
      \    If the config file cannot be found.\n    ValueError\n        If the config\
      \ is invalid.\n    Exception\n        Exceptions raised by the underlying streaming\
      \ or non-streaming API calls may propagate to the caller."
  classes: []
- file: graphrag/config/create_graphrag_config.py
  functions:
  - node_id: graphrag/config/create_graphrag_config.py::create_graphrag_config
    name: create_graphrag_config
    signature: "def create_graphrag_config(\n    values: dict[str, Any] | None = None,\n\
      \    root_dir: str | None = None,\n) -> GraphRagConfig"
    docstring: "\"\"\"Load Configuration Parameters from a dictionary.\n\nArgs:\n\
      \    values: dict[str, Any] | None\n        Dictionary of configuration values\
      \ to pass into pydantic model.\n    root_dir: str | None\n        Root directory\
      \ for the project.\n\nReturns:\n    GraphRagConfig\n        The configuration\
      \ object.\n\nRaises:\n    ValidationError\n        If the configuration values\
      \ do not satisfy pydantic validation.\n\"\"\""
  classes: []
- file: graphrag/config/embeddings.py
  functions:
  - node_id: graphrag/config/embeddings.py::create_index_name
    name: create_index_name
    signature: "def create_index_name(\n    container_name: str, embedding_name: str,\
      \ validate: bool = True\n) -> str"
    docstring: "Create an index name for the embedding store.\n\nWithin any given\
      \ vector store, we can have multiple sets of embeddings organized into projects.\n\
      The container_name parameter is used for this partitioning, and is added as\
      \ a prefix to the index name for differentiation.\n\nThe embedding_name is fixed,\
      \ with the available list defined in graphrag.index.config.embeddings\n\nNote\
      \ that we use dot notation in our names, but many vector stores do not support\
      \ this - so we convert to dashes.\n\nArgs:\n    container_name: The container\
      \ name used as a prefix for differentiation.\n    embedding_name: The embedding\
      \ name to include in the index name.\n    validate: Whether to validate embedding_name\
      \ against the allowed set before constructing the name.\n\nReturns:\n    str:\
      \ The constructed index name, with dots replaced by dashes.\n\nRaises:\n   \
      \ KeyError: If validate is True and embedding_name is not in all_embeddings."
  classes: []
- file: graphrag/config/enums.py
  functions:
  - node_id: graphrag/config/enums.py::ModelType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes."
  - node_id: graphrag/config/enums.py::SearchMethod.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "\"\"\"Return the string representation of the enum value.\n\nArgs:\n\
      \    self: The enum member.\n\nReturns:\n    str: The string representation\
      \ of the enum value.\n\"\"\""
  - node_id: graphrag/config/enums.py::ChunkStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this ChunkStrategyType enum member.\n\
      \nArgs:\n    self: ChunkStrategyType, the enum member to represent as a string.\n\
      \nReturns:\n    str: The enum member's value enclosed in double quotes."
  - node_id: graphrag/config/enums.py::InputFileType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enum member.\n\nArgs:\n \
      \   self: The enum member instance.\n\nReturns:\n    str: The string representation\
      \ of the enum member, with its value enclosed in double quotes.\n\"\"\""
  - node_id: graphrag/config/enums.py::CacheType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes.\n\"\"\""
  - node_id: graphrag/config/enums.py::ReportingType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes."
  - node_id: graphrag/config/enums.py::StorageType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes.\n\"\"\""
  classes:
  - class_id: graphrag/config/enums.py::ModelType
    name: ModelType
    docstring: 'ModelType is an enumeration of string-based model type identifiers
      used in graphrag''s configuration.


      Purpose:

      Provide a centralized, type-safe collection of ModelType Members that map to
      backend and component identifiers such as lancedb, azure_ai_search, cosmosdb,
      embedding, chat, and related utilities. This helps ensure consistency across
      configuration and usage wherever model types are referenced.


      Enum Members:

      - LanceDB: "lancedb"

      - AzureAISearch: "azure_ai_search"

      - CosmosDB: "cosmosdb"

      - OpenAIEmbedding: "openai_embedding"

      - AzureOpenAIEmbedding: "azure_openai_embedding"

      - Embedding: "embedding"

      - OpenAIChat: "openai_chat"

      - AzureOpenAIChat: "azure_openai_chat"

      - Chat: "chat"

      - MockChat: "mock_chat"

      - MockEmbedding: "mock_embedding"

      - APIKey: "api_key"

      - AzureManagedIdentity: "azure_managed_identity"

      - AsyncIO: "asyncio"

      - Threaded: "threaded"

      - LOCAL: "local"

      - GLOBAL: "global"

      - DRIFT: "drift"

      - BASIC: "basic"

      - Standard: "standard"

      - Fast: "fast"

      - StandardUpdate: "standard-update"

      - FastUpdate: "fast-update"

      - RegexEnglish: "regex_english"

      - Syntactic: "syntactic_parser"

      - CFG: "cfg"

      - Graph: "graph"

      - LCC: "lcc"

      - WeightedComponents: "weighted_components"


      Args:

      None: This Enum does not require constructor arguments.


      Returns:

      There is no return value for a class. To obtain the underlying string, use member.value.


      Raises:

      None


      Notes:

      - Access the string value via member.value; the member object itself is an Enum
      member.

      - __repr__ and __str__ representations of Enum members are not guaranteed; do
      not rely on a specific format.'
    methods:
    - name: __repr__
      signature: def __repr__(self)
  - class_id: graphrag/config/enums.py::SearchMethod
    name: SearchMethod
    docstring: 'SearchMethod enumeration of the available search methods.


      Purpose:

      Represents the set of string-backed identifiers used to select among different
      search backends or strategies in Graphrag''s configuration and runtime logic.


      Attributes:

      - value (str): The string identifier associated with the enum member (the enum''s
      underlying value).

      - name (str): The member''s name.


      Notes:

      The __str__ method returns the string representation of the enum value.'
    methods:
    - name: __str__
      signature: def __str__(self)
  - class_id: graphrag/config/enums.py::ChunkStrategyType
    name: ChunkStrategyType
    docstring: 'ChunkStrategyType is an Enum subclass that defines the available chunking
      strategies used by Graphrag configuration.


      This enum maps each strategy to a string value that appears in configuration
      and is intended to be used to configure chunking behavior.


      Enum members:

      - BASIC: basic

      - Standard: standard

      - Fast: fast

      - StandardUpdate: standard-update

      - FastUpdate: fast-update


      Accessing values:

      The literal string for a member is accessible via the value attribute. For example,
      ChunkStrategyType.BASIC.value yields basic.


      Inheritance:

      This class inherits from Enum; members are enumeration members rather than plain
      strings.


      Usage notes:

      Do not rely on the string representations of the members; use .value to obtain
      the underlying string used in configuration.'
    methods:
    - name: __repr__
      signature: def __repr__(self)
  - class_id: graphrag/config/enums.py::InputFileType
    name: InputFileType
    docstring: "InputFileType is an enumeration of input file types used by the configuration.\n\
      \nThis Enum defines string-valued members that identify various input sources.\
      \ Each member maps to a string key defined elsewhere in the configuration.\n\
      \nEnum members:\n    LanceDB\n    AzureAISearch\n    CosmosDB\n    OpenAIEmbedding\n\
      \    AzureOpenAIEmbedding\n    Embedding\n    OpenAIChat\n    AzureOpenAIChat\n\
      \    Chat\n    MockChat\n    MockEmbedding\n    APIKey\n    AzureManagedIdentity\n\
      \    AsyncIO\n    Threaded\n    LOCAL\n    GLOBAL\n    DRIFT\n    BASIC\n  \
      \  Standard\n    Fast\n    StandardUpdate\n    FastUpdate\n    RegexEnglish\n\
      \    Syntactic\n    CFG\n    Graph\n    LCC\n    WeightedComponents\n\nUsage:\n\
      \  - Access a member directly: InputFileType.LanceDB\n  - Construct by value:\
      \ InputFileType(value)\n\nRaises:\n  ValueError: If value is not a valid member\
      \ value."
    methods:
    - name: __repr__
      signature: def __repr__(self)
  - class_id: graphrag/config/enums.py::CacheType
    name: CacheType
    docstring: "\"CacheType\"  \nEnumeration of cache backends used by graphrag configuration.\n\
      \nPurpose:\nThis Enum provides a typed collection of string-valued identifiers\
      \ that correspond to the top-level configuration constants defined in graphrag.config.\
      \ Each member's value is the backend identifier string used in configuration.\
      \ The constants include LanceDB, AzureAISearch, CosmosDB, OpenAIEmbedding, AzureOpenAIEmbedding,\
      \ Embedding, OpenAIChat, AzureOpenAIChat, Chat, MockChat, MockEmbedding, APIKey,\
      \ AzureManagedIdentity, AsyncIO, Threaded, LOCAL, GLOBAL, DRIFT, BASIC, Standard,\
      \ Fast, StandardUpdate, FastUpdate, RegexEnglish, Syntactic, CFG, Graph, LCC,\
      \ WeightedComponents. The actual code uses AzureAISearch as the constant name\
      \ (not the spaced human-friendly form).\n\nAttributes:\n- Each member has a\
      \ string value that corresponds to the backend identifier used in configuration.\
      \ Examples:\n  \"lancedb\" (LanceDB), \"azure_ai_search\" (AzureAISearch), \"\
      cosmosdb\" (CosmosDB), \"openai_embedding\" (OpenAIEmbedding), \"azure_openai_embedding\"\
      \ (AzureOpenAIEmbedding), \"embedding\" (Embedding), \"openai_chat\" (OpenAIChat),\
      \ \"azure_openai_chat\" (AzureOpenAIChat), \"chat\" (Chat), \"mock_chat\" (MockChat),\
      \ \"mock_embedding\" (MockEmbedding), \"api_key\" (APIKey), \"azure_managed_identity\"\
      \ (AzureManagedIdentity), \"asyncio\" (AsyncIO), \"threaded\" (Threaded), \"\
      local\" (LOCAL), \"global\" (GLOBAL), \"drift\" (DRIFT), \"basic\" (BASIC),\
      \ \"standard\" (Standard), \"fast\" (Fast), \"standard-update\" (StandardUpdate),\
      \ \"fast-update\" (FastUpdate), \"regex_english\" (RegexEnglish), \"syntactic_parser\"\
      \ (Syntactic), \"cfg\" (CFG), \"graph\" (Graph), \"lcc\" (LCC), \"weighted_components\"\
      \ (WeightedComponents).\n\nRepresentation:\n- __repr__(self) -> str: Returns\
      \ a string representation of the enumeration member. Following standard Enum\
      \ semantics, this typically includes the member name and its value."
    methods:
    - name: __repr__
      signature: def __repr__(self)
  - class_id: graphrag/config/enums.py::ReportingType
    name: ReportingType
    docstring: "\"\"\"ReportingType is an enumeration of the available reporting configurations\
      \ used by the graphrag configuration system.\n\nSummary:\nProvides a type-safe\
      \ collection of reporter configuration options, each with a string value that\
      \ can be consumed by the codebase or serialized.\n\nAttributes:\n    value:\
      \ The string value associated with the enumeration member.\n\nMethods:\n   \
      \ __repr__(self):\n        Return a string representation of the enumeration\
      \ member. The current implementation returns the member's value wrapped in double\
      \ quotes (e.g., '\"standard\"'), which is convenient for certain serialization\
      \ scenarios but differs from Python's standard Enum representation. For conventional\
      \ display, consider implementing __str__ or using the .value attribute.\n\n\
      Examples:\n    from graphrag.config.enums import ReportingType\n    t = ReportingType.Standard\n\
      \    s = t.value           # \"standard\"\n    r = repr(t)           # '\"standard\"\
      ' (based on current __repr__)\n\"\"\""
    methods:
    - name: __repr__
      signature: def __repr__(self)
  - class_id: graphrag/config/enums.py::StorageType
    name: StorageType
    docstring: "Enumeration of storage and configuration key types used by graphrag.\n\
      \nRepresents the various storage backends and related configuration keys that\
      \ the project may utilize. Each member corresponds to a string key used in the\
      \ configuration system.\n\nNote: The __repr__ method returns the member's value\
      \ wrapped in double quotes.\n\nArgs:\n    None: The StorageType Enum does not\
      \ accept initialization parameters; members are defined on the class.\n\nReturns:\n\
      \    StorageType: The enumeration type itself; members represent specific storage/config\
      \ keys.\n\nRaises:\n    None: This class does not raise exceptions during normal\
      \ usage."
    methods:
    - name: __repr__
      signature: def __repr__(self)
- file: graphrag/config/environment_reader.py
  functions:
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader._read_env
    name: _read_env
    signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
      \ T, read: Callable[[str, T], T]\n    ) -> T | None"
    docstring: "Read environment value(s) using a reader function and return the first\
      \ non-default result.\n\nArgs:\n    env_key: str | list[str]. Environment key\
      \ or keys to look up. If a single string is provided, it will be treated as\
      \ a one-element list. Keys are checked in order and converted to upper-case\
      \ before reading.\n    default_value: T. The default value to return if no key\
      \ yields a non-default result.\n    read: Callable[[str, T], T]. A function\
      \ that takes an environment key (uppercase) and a default value, and returns\
      \ a value of type T.\n\nReturns:\n    T | None. The value returned by read for\
      \ the first key that yields a value different from default_value; otherwise\
      \ returns default_value.\n\nRaises:\n    Any exception raised by the read callable\
      \ is propagated to the caller."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.__init__
    name: __init__
    signature: 'def __init__(self, env: Env)'
    docstring: "Initialize the EnvironmentReader with the provided environment.\n\n\
      This constructor stores the given environment for later reads and initializes\
      \ an internal configuration stack to an empty list.\n\nArgs:\n    env: Environment\
      \ instance used to read configuration values.\n\nReturns:\n    None"
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.config_context
    name: config_context
    signature: def config_context()
    docstring: "Create a context manager to push a value into the config_stack for\
      \ the duration of the context.\n\nReturns:\n    A context manager that appends\
      \ the value (or {} if the value is falsy) to the internal _config_stack upon\
      \ entry, and pops it on exit."
  - node_id: graphrag/config/environment_reader.py::read_key
    name: read_key
    signature: 'def read_key(value: KeyValue) -> str'
    docstring: "Read a key value and normalize it to lowercase string.\n\nArgs:\n\
      \    value: KeyValue\n        The key to normalize. It can be a string or an\
      \ Enum. If value is a string, the result is the string converted to lowercase.\
      \ If value is an Enum, the Enum.value attribute is used and lowercased. The\
      \ Enum.value is assumed to be a string; otherwise a runtime error may occur.\n\
      \nReturns:\n    str\n        The lowercase representation of the key.\n\nRaises:\n\
      \    AttributeError\n        If value is not a string and does not have a value\
      \ attribute, or if value.value is not a string or cannot call lower()."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.section
    name: section
    signature: def section(self) -> dict
    docstring: "Get the current section.\n\nReturns:\n    dict: The current section\
      \ dictionary from the configuration stack, or an empty\n    dict if there is\
      \ no active section."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.use
    name: use
    signature: 'def use(self, value: Any | None)'
    docstring: "Create a context manager to push the value into the config_stack.\n\
      \nArgs:\n    value: Any | None. The value to push onto the internal config stack;\
      \ if None, an empty dict is pushed.\n\nReturns:\n    contextmanager. A context\
      \ manager that pushes the provided value (or an empty dict) onto the internal\
      \ config stack for the duration of the context and pops it on exit."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.env
    name: env
    signature: def env(self)
    docstring: "\"\"\"Get the environment object.\n\nReturns:\n    Env: The environment\
      \ object stored on this reader.\n\"\"\""
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
    name: envvar_prefix
    signature: 'def envvar_prefix(self, prefix: KeyValue)'
    docstring: "Set the environment variable prefix.\n\nThe provided prefix is normalized\
      \ via read_key and then transformed to uppercase with a trailing underscore,\
      \ and applied to the underlying Env instance using Env.prefixed.\n\nArgs:\n\
      \    prefix (KeyValue): The key to use as the environment variable prefix. It\
      \ can be a string or Enum. Strings are converted to lowercase; Enum values are\
      \ converted to their value and lowercased. The result is suffixed with an underscore\
      \ and uppercased before being applied as the prefix.\n\nReturns:\n    Env: An\
      \ environment object with the specified prefix applied."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.str
    name: str
    signature: "def str(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: str | None = None,\n    )\
      \ -> str | None"
    docstring: "Read a configuration value from the current section or environment.\n\
      \nArgs:\n    key: KeyValue\n        The key to read. It can be a string or an\
      \ Enum. It is normalized to a string using read_key.\n    env_key: EnvKeySet\
      \ | None\n        Optional environment variable name(s) to check if the key\
      \ is not found in the current section. If None, the environment key used is\
      \ the key.\n    default_value: str | None\n        Default value to return if\
      \ the key is not found in the current section or environment.\n\nReturns:\n\
      \    Any | None\n    The value read from the current section (returned as stored,\
      \ which may be non-string) or from the environment (typically a string), or\
      \ None if not found."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.int
    name: int
    signature: "def int(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: int | None = None,\n    )\
      \ -> int | None"
    docstring: "Read an integer configuration value.\n\nArgs:\n  key: KeyValue\n \
      \     The key to read. It is normalized to a string using read_key.\n  env_key:\
      \ EnvKeySet | None\n      Optional environment variable name(s) to check if\
      \ the key is not found in the current section. If None, the environment key\
      \ used is the key.\n  default_value: int | None\n      The default value to\
      \ return if the key is not found in either the current section or environment.\n\
      \nReturns:\n  int | None\n      The resulting integer value, or None if no value\
      \ is found and no default_value is provided.\n\nRaises:\n  AttributeError\n\
      \      If the provided key cannot be normalized to a string via read_key."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.bool
    name: bool
    signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: bool | None = None,\n   \
      \ ) -> bool | None"
    docstring: "Read a boolean configuration value.\n\nArgs:\n  key: KeyValue\n  \
      \    The key to read. It is normalized to a string using read_key.\n  env_key:\
      \ EnvKeySet | None\n      Optional environment variable name(s) to check if\
      \ the key is not found in the current section. If None, the environment key\
      \ used is the key.\n  default_value: bool | None\n      The default value to\
      \ return if the key is not found in either the section or environment.\nReturns:\n\
      \  bool | None\n      The boolean value read from the configuration, or None\
      \ if not found and no default is provided.\nRaises:\n  AttributeError\n    \
      \  If value is not a string and Enum (as raised by read_key)."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.float
    name: float
    signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: float | None = None,\n  \
      \  ) -> float | None"
    docstring: "\"\"\"Read a float configuration value.\n\nArgs:\n    key: KeyValue\n\
      \        The key to read. It can be a string or an Enum. It is normalized to\
      \ a string using read_key.\n    env_key: EnvKeySet | None\n        Optional\
      \ environment variable name(s) to check if the key is not found in the current\
      \ section. If None, the environment key used is the key.\n    default_value:\
      \ float | None\n        The default value to return if the key is not found\
      \ in either the current section or environment.\n\nReturns:\n    float | None\n\
      \        The configured float value, or None if not found and no default is\
      \ provided.\n\nRaises:\n    AttributeError\n        If the provided key cannot\
      \ be normalized to a string.\n\"\"\""
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.list
    name: list
    signature: "def list(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: list | None = None,\n   \
      \ ) -> list | None"
    docstring: "Parse a list configuration value.\n\nArgs:\n  key: KeyValue\n    \
      \  The key to read. It can be a string or an Enum. It is normalized to a string\
      \ using read_key.\n  env_key: EnvKeySet | None\n      Optional environment variable\
      \ name(s) to check if the key is not found in the current section. If None,\
      \ the environment key used is the key.\n  default_value: list | None\n     \
      \ The default value to return if the key is not found and no environment value\
      \ is available.\n\nReturns:\n  list | None\n      The parsed list from the configuration,\
      \ or default_value if not found.\n\nRaises:\n  AttributeError\n      If value\
      \ is not a string and ..."
  classes:
  - class_id: graphrag/config/environment_reader.py::EnvironmentReader
    name: EnvironmentReader
    docstring: "Reads configuration values by combining a per-context stack of configuration\
      \ sections with environment variables. The EnvironmentReader uses an Env instance\
      \ to access environment values and maintains a private _config_stack to support\
      \ context-based reads. Reads resolve keys against the current top-most section\
      \ on the stack, and if not found, fall back to environment values via an internal\
      \ helper. Context managers can push a new section onto the stack for the duration\
      \ of a with-block to scope reads.\n\nArgs:\n    env (Env): Environment instance\
      \ used to read configuration values.\n\nAttributes:\n    env (Env): The environment\
      \ instance used to read configuration values.\n    _config_stack (list): Internal\
      \ stack that stores configuration sections for the duration of a read context.\n\
      \nNotes:\n    - The private helper _read_env is used to fetch values from the\
      \ environment.\n    - Keys are normalized to strings via internal mechanisms\
      \ before lookup.\n    - Context management affects subsequent reads within the\
      \ with-block by extending the active configuration.\n    - Typical return values\
      \ are strings, integers, booleans, floats, lists, or None when a value cannot\
      \ be found.\n\nRaises:\n    - TypeError, ValueError: if arguments have invalid\
      \ types or the read context is misused.\n    - Exceptions from the underlying\
      \ Env reads may be raised as encountered.\n\nPublic API overview:\n    _read_env(env_key,\
      \ default_value, read) -> T | None\n    __init__(self, env) -> None\n    config_context()\
      \ -> contextmanager\n    section() -> dict\n    use(value) -> contextmanager\n\
      \    env() -> Env\n    envvar_prefix(prefix) -> None\n    str(key, env_key=None,\
      \ default_value=None) -> str | None\n    int(key, env_key=None, default_value=None)\
      \ -> int | None\n    bool(key, env_key=None, default_value=None) -> bool | None\n\
      \    float(key, env_key=None, default_value=None) -> float | None\n    list(key,\
      \ env_key=None, default_value=None) -> list | None"
    methods:
    - name: _read_env
      signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
        \ T, read: Callable[[str, T], T]\n    ) -> T | None"
    - name: __init__
      signature: 'def __init__(self, env: Env)'
    - name: config_context
      signature: def config_context()
    - name: section
      signature: def section(self) -> dict
    - name: use
      signature: 'def use(self, value: Any | None)'
    - name: env
      signature: def env(self)
    - name: envvar_prefix
      signature: 'def envvar_prefix(self, prefix: KeyValue)'
    - name: str
      signature: "def str(\n        self,\n        key: KeyValue,\n        env_key:\
        \ EnvKeySet | None = None,\n        default_value: str | None = None,\n  \
        \  ) -> str | None"
    - name: int
      signature: "def int(\n        self,\n        key: KeyValue,\n        env_key:\
        \ EnvKeySet | None = None,\n        default_value: int | None = None,\n  \
        \  ) -> int | None"
    - name: bool
      signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key:\
        \ EnvKeySet | None = None,\n        default_value: bool | None = None,\n \
        \   ) -> bool | None"
    - name: float
      signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
        \ EnvKeySet | None = None,\n        default_value: float | None = None,\n\
        \    ) -> float | None"
    - name: list
      signature: "def list(\n        self,\n        key: KeyValue,\n        env_key:\
        \ EnvKeySet | None = None,\n        default_value: list | None = None,\n \
        \   ) -> list | None"
- file: graphrag/config/errors.py
  functions:
  - node_id: graphrag/config/errors.py::AzureApiBaseMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
    docstring: "Init method for AzureApiBaseMissingError (internal API).\n\nArgs:\n\
      \    llm_type: The LLM type for which the API Base is required.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/config/errors.py::ApiKeyMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None = None)
      -> None'
    docstring: "Init method for ApiKeyMissingError (internal API).\n\nArgs:\n    llm_type:\
      \ The LLM type for which the API Key is required.\n    azure_auth_type: Optional\
      \ Azure authentication type; if provided, include in the message.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/config/errors.py::LanguageModelConfigMissingError.__init__
    name: __init__
    signature: 'def __init__(self, key: str = "") -> None'
    docstring: "Initialize LanguageModelConfigMissingError with provided key.\n\n\
      Args:\n    key: The key of the missing model configuration. Used to customize\
      \ the error message in settings.yaml.\n\nReturns:\n    None"
  - node_id: graphrag/config/errors.py::ConflictingSettingsError.__init__
    name: __init__
    signature: 'def __init__(self, msg: str) -> None'
    docstring: "Initialize error with the provided message.\n\nArgs:\n    msg: The\
      \ error message to pass to the base ValueError constructor.\n\nReturns:\n  \
      \  None"
  - node_id: graphrag/config/errors.py::AzureApiVersionMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
    docstring: "Init method for AzureApiVersionMissingError (internal API).\n\nThis\
      \ constructor formats the error message using the provided llm_type as: \"API\
      \ Version is required for {llm_type}. Please rerun graphrag init and set the\
      \ api_version.\" It initializes the base ValueError with that message. It returns\
      \ None and does not raise the exception by itself.\n\nArgs:\n    llm_type: The\
      \ LLM type for which the API Version is required.\n\nReturns:\n    None\n\n\
      Raises:\n    None"
  classes:
  - class_id: graphrag/config/errors.py::AzureApiBaseMissingError
    name: AzureApiBaseMissingError
    docstring: "AzureApiBaseMissingError is an internal exception raised when the\
      \ Azure API Base configuration is missing for the specified LLM type.\n\nThis\
      \ exception signals that the required Azure API Base is not configured for the\
      \ given LLM type.\n\nArgs:\n    llm_type (str): The LLM type for which the API\
      \ Base is required. The value is stored on the instance as llm_type and used\
      \ to construct the error message.\n\nAttributes:\n    llm_type (str): The LLM\
      \ type for which the API Base is required. Used to customize the error message.\n\
      \nNotes:\n    The __init__ method stores llm_type and creates a descriptive\
      \ message, typically \"Azure API Base missing for LLM type: {llm_type}\".\n\n\
      Examples:\n    raise AzureApiBaseMissingError('text-davinci-003') results in\
      \ an exception with a message indicating the missing API Base for that LLM type."
    methods:
    - name: __init__
      signature: 'def __init__(self, llm_type: str) -> None'
  - class_id: graphrag/config/errors.py::ApiKeyMissingError
    name: ApiKeyMissingError
    docstring: "Exception raised when an API key is missing for a specific LLM type;\
      \ this internal error is meant to be raised, not returned. The error message\
      \ constructed for this exception includes the llm_type and, if provided, the\
      \ azure_auth_type to aid diagnosis.\n\nAttributes:\n    llm_type: The LLM type\
      \ for which the API Key is required.\n    azure_auth_type: Optional Azure authentication\
      \ type; included in the message if provided.\n\nArgs:\n    llm_type: The LLM\
      \ type for which the API Key is required.\n    azure_auth_type: Optional Azure\
      \ authentication type; if provided, included in the message."
    methods:
    - name: __init__
      signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None =
        None) -> None'
  - class_id: graphrag/config/errors.py::LanguageModelConfigMissingError
    name: LanguageModelConfigMissingError
    docstring: "LanguageModelConfigMissingError is raised when a required language\
      \ model configuration key is missing from the configuration.\n\nArgs:\n    key:\
      \ The key of the missing model configuration. Used to customize the error message\
      \ in settings.yaml.\n\nReturns:\n    None\n\nRaises:\n    None\n\nAttributes:\n\
      \    key: The key of the missing model configuration. Stored on the instance\
      \ to provide context for the error message."
    methods:
    - name: __init__
      signature: 'def __init__(self, key: str = "") -> None'
  - class_id: graphrag/config/errors.py::ConflictingSettingsError
    name: ConflictingSettingsError
    docstring: "Exception raised when configuration contains conflicting or incompatible\
      \ settings.\n\nRepresents an error condition raised during GraphRAG configuration\
      \ when two or more settings conflict with each other. The error message provided\
      \ at initialization describes the specific conflict and is passed to the base\
      \ ValueError constructor.\n\nArgs:\n    msg: The error message to pass to the\
      \ base ValueError constructor.\n\nReturns:\n    None\n\nRaises:\n    None"
    methods:
    - name: __init__
      signature: 'def __init__(self, msg: str) -> None'
  - class_id: graphrag/config/errors.py::AzureApiVersionMissingError
    name: AzureApiVersionMissingError
    docstring: "AzureApiVersionMissingError is a specialized ValueError indicating\
      \ that an API version is required for a given LLM type.\n\nThe constructor formats\
      \ the error message using the provided llm_type as: \"API Version is required\
      \ for {llm_type}. Please rerun graphrag init and set the api_version.\" It initializes\
      \ the base ValueError with that message.\n\nArgs:\n  llm_type: The LLM type\
      \ for which the API version is required.\n\nReturns:\n  None. The __init__ method\
      \ returns None and initializes the base ValueError with the constructed message.\n\
      \nRaises:\n  None"
    methods:
    - name: __init__
      signature: 'def __init__(self, llm_type: str) -> None'
- file: graphrag/config/get_embedding_settings.py
  functions:
  - node_id: graphrag/config/get_embedding_settings.py::get_embedding_settings
    name: get_embedding_settings
    signature: "def get_embedding_settings(\n    settings: GraphRagConfig,\n    vector_store_params:\
      \ dict | None = None,\n) -> dict"
    docstring: "Transform GraphRAG config into settings for workflows.\n\nArgs:\n\
      \    settings: GraphRagConfig\n        GraphRagConfig containing embed_text\
      \ and vector_store configuration.\n    vector_store_params: dict | None\n  \
      \      Optional dictionary of vector store parameters to override defaults.\n\
      \nReturns:\n    dict\n        A dictionary with a single key \"strategy\" containing\
      \ the embedding strategy\n        configured using language model settings and\
      \ merged vector store settings\n        from both the config and any provided\
      \ vector_store_params.\n\nRaises:\n    Exceptions propagated from GraphRagConfig\
      \ methods or underlying calls may occur."
  classes: []
- file: graphrag/config/load_config.py
  functions:
  - node_id: graphrag/config/load_config.py::_load_dotenv
    name: _load_dotenv
    signature: 'def _load_dotenv(config_path: Path | str) -> None'
    docstring: "Load the .env file if it exists in the same directory as the config\
      \ file.\n\nArgs:\n    config_path (Path | str): The path to the config file.\n\
      \nReturns:\n    None"
  - node_id: graphrag/config/load_config.py::_apply_overrides
    name: _apply_overrides
    signature: 'def _apply_overrides(data: dict[str, Any], overrides: dict[str, Any])
      -> None'
    docstring: "Apply the overrides to the raw configuration.\n\nArgs:\n    data:\
      \ dict[str, Any]\n        The raw configuration dictionary to be updated in\
      \ place.\n    overrides: dict[str, Any]\n        A flat mapping of dot-separated\
      \ keys to values to override in the configuration.\n\nReturns:\n    None\n\n\
      Raises:\n    TypeError\n        If attempting to override a non-dict value along\
      \ the path."
  - node_id: graphrag/config/load_config.py::_parse
    name: _parse
    signature: 'def _parse(file_extension: str, contents: str) -> dict[str, Any]'
    docstring: "Parse configuration contents based on the file extension.\n\nParses\
      \ YAML (.yaml/.yml) using yaml.safe_load and JSON (.json) using json.loads.\n\
      \nArgs:\n    file_extension: The file extension determining the parsing method\
      \ (e.g., \".yaml\", \".yml\", \".json\").\n    contents: The raw string content\
      \ of the configuration file to parse.\n\nReturns:\n    Any: The parsed configuration.\
      \ Typically a dict[str, Any], but the result can be None (for empty content)\
      \ or other JSON/YAML types depending on the input.\n\nRaises:\n    ValueError:\
      \ If the file extension is not supported.\n    yaml.YAMLError: If YAML parsing\
      \ fails.\n    json.JSONDecodeError: If JSON parsing fails."
  - node_id: graphrag/config/load_config.py::_search_for_config_in_root_dir
    name: _search_for_config_in_root_dir
    signature: 'def _search_for_config_in_root_dir(root: str | Path) -> Path | None'
    docstring: "Resolve the config path from the given root directory.\n\nArgs:\n\
      \    root: str | Path\n        The path to the root directory containing the\
      \ config file. Searches for a default config file (settings.{yaml,yml,json}).\n\
      \nReturns:\n    Path | None: The Path to the config file if one exists in the\
      \ root directory; otherwise None.\n\nRaises:\n    FileNotFoundError: If the\
      \ provided root is not a directory."
  - node_id: graphrag/config/load_config.py::_parse_env_variables
    name: _parse_env_variables
    signature: 'def _parse_env_variables(text: str) -> str'
    docstring: "\"\"\"Parse environment variables in the configuration text.\n\nArgs:\n\
      \    text: The configuration text.\n\nReturns:\n    The configuration text with\
      \ environment variables parsed.\n\nRaises:\n    KeyError: If an environment\
      \ variable is not found.\n\"\"\""
  - node_id: graphrag/config/load_config.py::_get_config_path
    name: _get_config_path
    signature: 'def _get_config_path(root_dir: Path, config_filepath: Path | None)
      -> Path'
    docstring: "Resolve and return the configuration file path.\n\nArgs:\n    root_dir\
      \ (Path): The root directory of the project to search when config_filepath is\
      \ not provided.\n    config_filepath (Path | None): The explicit path to the\
      \ config file. If None, the config file will be searched for in root_dir.\n\n\
      Returns:\n    Path: The resolved configuration file path.\n\nRaises:\n    FileNotFoundError:\
      \ If the specified config file does not exist or if no configuration file can\
      \ be found in the root directory."
  - node_id: graphrag/config/load_config.py::load_config
    name: load_config
    signature: "def load_config(\n    root_dir: Path,\n    config_filepath: Path |\
      \ None = None,\n    cli_overrides: dict[str, Any] | None = None,\n) -> GraphRagConfig"
    docstring: "Load configuration from a file.\n\nArgs:\n    root_dir: The root directory\
      \ of the project. Will search for the config file in this directory.\n    config_filepath:\
      \ The path to the config file. If None, searches for config file in root.\n\
      \    cli_overrides: A flat dictionary of cli overrides. Example: {'output.base_dir':\
      \ 'override_value'}\n\nReturns:\n    GraphRagConfig\n        The loaded configuration.\n\
      \nRaises:\n    FileNotFoundError\n        If the config file is not found.\n\
      \    ValueError\n        If the config file extension is not supported.\n  \
      \  TypeError\n        If applying cli overrides to the config fails.\n    KeyError\n\
      \        If config file references a non-existent environment variable.\n  \
      \  ValidationError\n        If there are pydantic validation errors when instantiating\
      \ the config."
  classes: []
- file: graphrag/config/models/community_reports_config.py
  functions:
  - node_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "\"\"\"Get the resolved community report extraction strategy.\n\nArgs:\n\
      \    root_dir: The root directory used to resolve the graph and text prompt\
      \ file paths.\n    model_config: The LanguageModelConfig instance containing\
      \ the model configuration; its\n        model_dump() result is included in the\
      \ strategy as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy\
      \ is provided, it is returned as-is; otherwise\n        a default strategy dictionary\
      \ is constructed with the following keys:\n        type, llm, graph_prompt,\
      \ text_prompt, max_report_length, max_input_length.\n        graph_prompt is\
      \ the contents of the file at the path root_dir / self.graph_prompt when\n \
      \       self.graph_prompt is provided, otherwise None. text_prompt similarly\
      \ uses root_dir / self.text_prompt.\n\nRaises:\n    FileNotFoundError: If a\
      \ provided graph_prompt or text_prompt path does not exist.\n    OSError: If\
      \ an I/O error occurs while reading prompt files.\n\"\"\""
  classes:
  - class_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig
    name: CommunityReportsConfig
    docstring: "Configuration model for extracting and summarizing community reports\
      \ in GraphRag.\n\nOverview:\n- This Pydantic BaseModel stores the configuration\
      \ that governs how community reports are produced and prepared for downstream\
      \ processing. It supports resolving a concrete strategy to be used by other\
      \ components, either by returning a provided strategy or by constructing a default\
      \ one from configured defaults and the supplied language model configuration.\n\
      \nAttributes:\n- strategy: Optional[CreateCommunityReportsStrategyType]. The\
      \ optional strategy configuration for community report extraction. If provided,\
      \ the resolved_strategy method will return this strategy as-is. If omitted,\
      \ a default strategy is constructed from graphrag_config_defaults and later\
      \ augmented with the llm from a LanguageModelConfig when resolved.\n\nResolved\
      \ strategy behavior:\n- If strategy is provided, resolved_strategy returns the\
      \ provided strategy unchanged.\n- If strategy is None, resolved_strategy builds\
      \ a default strategy from graphrag_config_defaults and populates its llm field\
      \ with the serialized model configuration obtained from model_config.model_dump().\
      \ This enables downstream components to know which language model to use during\
      \ execution.\n\nMethods:\n- resolved_strategy(root_dir: str, model_config: LanguageModelConfig)\
      \ -> dict\n  Returns: A dictionary describing the resolved community report\
      \ extraction strategy, ready for downstream processing.\n\nRaises:\n- ValidationError:\
      \ If invalid data is provided to initialize the model (e.g., missing required\
      \ fields or inappropriate types).\n\nExamples:\n- Provided strategy:\n  config\
      \ = CommunityReportsConfig(strategy={\"type\": \"custom\", \"params\": {\"foo\"\
      : 1}})\n  resolved = config.resolved_strategy(\"/root\", some_model_config)\n\
      \  # resolved is exactly config.strategy\n\n- No strategy provided:\n  config\
      \ = CommunityReportsConfig(strategy=None)\n  resolved = config.resolved_strategy(\"\
      /root\", some_model_config)\n  # resolved is a dict built from graphrag_config_defaults\
      \ with llm populated from some_model_config.model_dump()."
    methods:
    - name: resolved_strategy
      signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
        \ LanguageModelConfig\n    ) -> dict"
- file: graphrag/config/models/extract_claims_config.py
  functions:
  - node_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved claim extraction strategy.\n\nArgs:\n    root_dir:\
      \ The root directory used to resolve the graph and text prompt file paths.\n\
      \    model_config: The LanguageModelConfig instance containing the model configuration;\
      \ its model_dump() result is included in the strategy as llm.\n\nReturns:\n\
      \    dict: The resolved strategy. If self.strategy is provided, it is returned\
      \ as-is; otherwise, a dict with the following keys:\n        llm: The result\
      \ of model_config.model_dump().\n        extraction_prompt: The contents of\
      \ the prompt file located at Path(root_dir) / self.prompt read as UTF-8, or\
      \ None if no prompt is configured.\n        claim_description: The description\
      \ from self.description.\n        max_gleanings: The max_gleanings value from\
      \ self.max_gleanings.\n\nRaises:\n    FileNotFoundError: If a prompt is configured\
      \ and the prompt file cannot be read."
  classes:
  - class_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig
    name: ClaimExtractionConfig
    docstring: 'Configuration container for the claim extraction strategy used during
      claim extraction.


      This class stores the claim extraction strategy and exposes resolution logic
      to produce a concrete strategy at runtime via the resolved_strategy method.


      Attributes:

      - strategy: The configured claim extraction strategy. If provided, resolved_strategy
      returns it unchanged.


      Methods:

      - resolved_strategy(root_dir: str, model_config: LanguageModelConfig) -> dict:
      Get the resolved claim extraction strategy. Args: root_dir: The root directory
      used to resolve the graph and text prompt file paths. model_config: The LanguageModelConfig
      instance containing the model configuration; its model_dump() result is included
      in the strategy as llm. Returns: dict: The resolved strategy. If self.strategy
      is provided, it is returned as-is; otherwise, a dict representing the resolved
      strategy is constructed, including the model_config dump as llm.'
    methods:
    - name: resolved_strategy
      signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
        \ LanguageModelConfig\n    ) -> dict"
- file: graphrag/config/models/extract_graph_config.py
  functions:
  - node_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved entity extraction strategy.\n\nArgs:\n    root_dir\
      \ (str): The root directory used to resolve the graph and text prompt file paths.\n\
      \    model_config (LanguageModelConfig): The LanguageModelConfig instance containing\
      \ the model configuration; its model_dump() result is included in the strategy\
      \ as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy is\
      \ provided, it is returned as-is; otherwise, a default strategy dictionary is\
      \ returned with the following keys:\n        type: The strategy type (ExtractEntityStrategyType.graph_intelligence).\n\
      \        llm: model_config.model_dump().\n        extraction_prompt: The contents\
      \ of the prompt file located at Path(root_dir) / self.prompt, read as UTF-8,\
      \ if self.prompt is set; otherwise None.\n        max_gleanings: self.max_gleanings."
  classes:
  - class_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig
    name: ExtractGraphConfig
    docstring: 'ExtractGraphConfig is a configuration container for graph extraction
      settings and the resolution of the final entity extraction strategy.


      Purpose:

      This class stores an optional strategy configuration and provides a method to
      resolve the active strategy using a root directory and a LanguageModelConfig.
      The resolved strategy is returned as a dict and may incorporate the LanguageModelConfig
      data via its model_dump() as llm.


      Attributes:

      strategy: Optional dict representing the base configuration for the extraction
      strategy. If provided, it participates in determining the final resolved strategy.


      Args:

      strategy: Optional dict. Base configuration for the extraction strategy used
      during resolution.


      Returns:

      dict: The final, resolved extraction strategy produced by resolving the configuration
      against the given root_dir and LanguageModelConfig. If a base strategy was provided,
      the result is derived from that configuration; otherwise a default strategy
      is constructed using graphrag_config_defaults and the provided inputs.


      Raises:

      ValueError: If root_dir is not a valid non-empty string or the model_config
      is invalid.

      TypeError: If model_config is not an instance of LanguageModelConfig.

      NotImplementedError: If strategy resolution is not implemented for the given
      inputs.


      Example:

      config = ExtractGraphConfig(strategy={''type'': ''entity'', ''params'': {}})

      resolved = config.resolved_strategy(''/path/to/root'', LanguageModelConfig(...))'
    methods:
    - name: resolved_strategy
      signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
        \ LanguageModelConfig\n    ) -> dict"
- file: graphrag/config/models/graph_rag_config.py
  functions:
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_base_dir
    name: _validate_input_base_dir
    signature: def _validate_input_base_dir(self) -> None
    docstring: "Validate the input base directory.\n\nArgs:\n    self: The instance\
      \ of the configuration model containing input configuration and root_dir.\n\n\
      Returns:\n    None. This method updates input.storage.base_dir to an absolute\
      \ path derived from root_dir joined with the provided base_dir.\n\nRaises:\n\
      \    ValueError: If the input storage type is file and the input storage base_dir\
      \ is empty."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_rate_limiter_services
    name: _validate_rate_limiter_services
    signature: def _validate_rate_limiter_services(self) -> None
    docstring: "Validate the rate limiter services configuration.\n\nThis method checks\
      \ each model's rate_limit_strategy. For each model with a configured strategy,\
      \ it verifies the strategy is registered with RateLimiterFactory and, if rpm\
      \ or tpm values are provided, creates a corresponding rate limiter instance\
      \ to validate configuration.\n\nArgs:\n  self: The instance containing the models\
      \ configuration to validate.\n\nReturns:\n  None: This method does not return\
      \ a value.\n\nRaises:\n  ValueError: If a rate limiter strategy for a model\
      \ is not registered."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_reporting_base_dir
    name: _validate_reporting_base_dir
    signature: def _validate_reporting_base_dir(self) -> None
    docstring: "Validate the reporting base directory.\n\nArgs:\n    self (GraphRagConfig):\
      \ The instance of the configuration model containing the reporting configuration\
      \ and root_dir.\n\nReturns:\n    None. When the reporting type is file, this\
      \ method validates that the base_dir is non-empty and then converts it to an\
      \ absolute path using root_dir.\n\nRaises:\n    ValueError: If the reporting\
      \ type is file and the reporting.base_dir is empty or whitespace."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_factories
    name: _validate_factories
    signature: def _validate_factories(self) -> None
    docstring: "Validate the factories used in the configuration.\n\nArgs:\n    self:\
      \ The GraphRagConfig instance.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    Exception: If validation fails in the underlying\
      \ retry or rate limiter validations."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model configuration after the initial schema validation.\n\
      \nThis is a post-schema, after-hook validator that returns the same instance\
      \ after performing a series of internal checks to ensure the model configuration\
      \ is consistent and ready for use.\n\nArgs:\n    self: The GraphRagConfig instance\
      \ being validated.\n\nReturns:\n    GraphRagConfig: The same instance after\
      \ validation.\n\nRaises:\n    ValueError: If a required configuration value\
      \ is missing or invalid during validation.\n\nValidations performed:\n- root_dir\
      \ is valid\n- models configuration is present and valid\n- input_pattern is\
      \ correct\n- input_base_dir is valid\n- reporting_base_dir is valid\n- output_base_dir\
      \ is valid\n- multi_output_base_dirs are valid\n- update_index_output_base_dir\
      \ is valid\n- vector_store_db_uri is valid\n- factories configuration is valid"
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_output_base_dir
    name: _validate_output_base_dir
    signature: def _validate_output_base_dir(self) -> None
    docstring: "Validate the output base directory.\n\nArgs:\n    self: The instance\
      \ of the configuration model containing the output configuration and root_dir.\n\
      \nReturns:\n    None. This method updates output.base_dir to an absolute path\
      \ derived from root_dir joined with the provided base_dir when the output type\
      \ is file.\n\nRaises:\n    ValueError: If the output storage type is file and\
      \ the output base_dir is empty."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_retry_services
    name: _validate_retry_services
    signature: def _validate_retry_services(self) -> None
    docstring: "\"\"\"Validate the retry services configuration.\n\nThis method iterates\
      \ over all language model configurations contained in self.models\nand validates\
      \ each model's retry_strategy. If a model's retry_strategy is \"none\",\n the\
      \ strategy is skipped. For any other strategy, the strategy must be registered\
      \ with\nRetryFactory; otherwise a ValueError is raised. The error message lists\
      \ the available\nretry strategies. When a registered strategy is found, a retry\
      \ object is instantiated by\nRetryFactory.create using the model's max_retries\
      \ and max_retry_wait to validate the\nconfiguration.\n\nArgs:\n  self: GraphRagConfig.\
      \ The instance containing the models dictionary to validate.\n\nReturns:\n \
      \ None\n\nRaises:\n  ValueError: If a model's retry_strategy is not registered\
      \ with RetryFactory. The exception\n    message includes the model id and the\
      \ list of available strategies.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "\"\"\"\nGet a string representation of this GraphRagConfig as a JSON\
      \ string.\n\nThis representation is produced by Pydantic's model_dump_json with\
      \ an indentation of 4 spaces, yielding a pretty-printed JSON string.\n\nArgs:\n\
      \    self: GraphRagConfig. The instance to serialize. Note: self is implicit\
      \ in Python method calls; this description is informational.\n\nReturns:\n \
      \   str: JSON string representation of the GraphRagConfig with indent=4.\n\n\
      Raises:\n    ValueError, TypeError: If the underlying serialization fails due\
      \ to non-serializable fields or data.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_vector_store_config
    name: get_vector_store_config
    signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
    docstring: '"""Get a vector store configuration by ID.


      Args:

      vector_store_id: The ID of the vector store to get. Should match an ID in the
      vector_store list.


      Returns:

      VectorStoreConfig: The vector store configuration if found.


      Raises:

      ValueError: If the vector store ID is not found in the configuration.

      """'
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_vector_store_db_uri
    name: _validate_vector_store_db_uri
    signature: def _validate_vector_store_db_uri(self) -> None
    docstring: "Validate the vector store configuration for LanceDB vector stores\
      \ and normalize their db_uri paths to absolute paths.\n\nArgs:\n    self: The\
      \ Graph Rag configuration instance being validated.\n\nReturns:\n    None\n\n\
      Raises:\n    ValueError: If a LanceDB vector store has a missing or empty db_uri.\
      \ The error message is: Vector store URI is required for LanceDB. Please rerun\
      \ graphrag init and set the vector store configuration."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_pattern
    name: _validate_input_pattern
    signature: def _validate_input_pattern(self) -> None
    docstring: "Mutates the input file_pattern when it is empty by applying a default\
      \ pattern based on the input type.\n\nIf the pattern is empty:\n- for text input:\
      \ sets the pattern to \".*\\\\.txt$\"\n- for other input types: sets the pattern\
      \ to \".*\\\\.{extension}$\" where extension is the value of input.file_type.value\n\
      \nArgs:\n    self: The GraphRagConfig instance.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_multi_output_base_dirs
    name: _validate_multi_output_base_dirs
    signature: def _validate_multi_output_base_dirs(self) -> None
    docstring: "Validate the outputs dict base directories.\n\nArgs:\n    self: The\
      \ instance of the configuration model containing the outputs dictionary and\
      \ root_dir.\n\nReturns:\n    None. This method updates file outputs' base_dir\
      \ to an absolute path by resolving it relative to root_dir.\n\nRaises:\n   \
      \ ValueError: If any file-type output has an empty base_dir."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_language_model_config
    name: get_language_model_config
    signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
    docstring: "\"\"\"Get a model configuration by ID.\n\nArgs:\n    model_id (str):\
      \ The ID of the model to get. Should match an ID in the models list.\n\nReturns:\n\
      \    LanguageModelConfig: The model configuration if found.\n\nRaises:\n   \
      \ ValueError: If the model ID is not found in the configuration.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__repr__
    name: __repr__
    signature: def __repr__(self) -> str
    docstring: "Get a string representation of this GraphRagConfig instance.\n\nArgs:\n\
      \  self: GraphRagConfig, the GraphRagConfig instance to represent as a string.\n\
      \nReturns:\n  str: The string representation of this GraphRagConfig instance."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_models
    name: _validate_models
    signature: def _validate_models(self) -> None
    docstring: "Validate the models configuration.\n\nEnsure both a default chat model\
      \ and default embedding model have been defined. Other models may also be defined\
      \ but defaults are required for the time being as places of the code fallback\
      \ to default model configs instead of specifying a specific model.\n\nTODO:\
      \ Don't fallback to default models elsewhere in the code. Forcing code to specify\
      \ a model to use and allowing for any names for model configurations.\n\nArgs:\n\
      \    self: GraphRagConfig instance being validated.\n\nReturns:\n    None: This\
      \ method does not return a value.\n\nRaises:\n    LanguageModelConfigMissingError:\
      \ If the default chat model ID or the default embedding model ID is not present\
      \ in self.models."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_update_index_output_base_dir
    name: _validate_update_index_output_base_dir
    signature: def _validate_update_index_output_base_dir(self) -> None
    docstring: "Validate the update index output base directory.\n\nArgs:\n  self\
      \ (GraphRagConfig): The instance of the graph rag configuration model containing\
      \ the update_index_output and root_dir attributes.\n\nReturns:\n  None. This\
      \ method updates update_index_output.base_dir to an absolute path derived from\
      \ root_dir when the update_index_output.type is file.\n\nRaises:\n  ValueError:\
      \ If update_index_output.type is file and update_index_output.base_dir is blank\
      \ or whitespace."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_root_dir
    name: _validate_root_dir
    signature: def _validate_root_dir(self) -> None
    docstring: "Validate the root directory.\n\nArgs:\n  self: GraphRagConfig. The\
      \ instance containing the root_dir attribute.\n\nReturns:\n  None. This method\
      \ updates self.root_dir to an absolute path of an existing directory, defaulting\
      \ to the current working directory if root_dir is blank.\n\nRaises:\n  FileNotFoundError:\
      \ If the resolved root_dir is not an existing directory."
  classes:
  - class_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig
    name: GraphRagConfig
    docstring: 'GraphRagConfig is a Pydantic-based configuration model that aggregates
      and validates GraphRag''s diverse configuration options.


      Args:

      - None: This model is constructed via Pydantic from internal fields representing
      sub-configs; there are no explicit __init__ parameters exposed.


      Returns:

      - GraphRagConfig: The same instance after initialization and internal validation.


      Raises:

      - ValueError: If a configuration value is invalid or required paths are missing.

      - FileNotFoundError: If a required root_dir or other directory does not exist.

      - LanguageModelConfigMissingError: If a default language model is not configured
      when required.


      Key attributes:

      - Sub-config models such as BasicSearchConfig, CacheConfig, ChunkingConfig,
      ClusterGraphConfig, CommunityReportsConfig, DRIFTSearchConfig, EmbedGraphConfig,
      ClaimExtractionConfig, ExtractGraphConfig, ExtractGraphNLPConfig, GlobalSearchConfig,
      InputConfig, LanguageModelConfig, LocalSearchConfig, PruneGraphConfig, ReportingConfig,
      SnapshotsConfig, StorageConfig, SummarizeDescriptionsConfig, TextEmbeddingConfig,
      UmapConfig, VectorStoreConfig.

      - Factories RateLimiterFactory and RetryFactory used for validating rate limiting
      and retry strategies.


      Summary:

      GraphRagConfig acts as the centralized, validated container for the GraphRag
      configuration, coordinating multiple sub-configs and providing utilities for
      path normalization, model validation, and access to vector store and language
      model configurations.'
    methods:
    - name: _validate_input_base_dir
      signature: def _validate_input_base_dir(self) -> None
    - name: _validate_rate_limiter_services
      signature: def _validate_rate_limiter_services(self) -> None
    - name: _validate_reporting_base_dir
      signature: def _validate_reporting_base_dir(self) -> None
    - name: _validate_factories
      signature: def _validate_factories(self) -> None
    - name: _validate_model
      signature: def _validate_model(self)
    - name: _validate_output_base_dir
      signature: def _validate_output_base_dir(self) -> None
    - name: _validate_retry_services
      signature: def _validate_retry_services(self) -> None
    - name: __str__
      signature: def __str__(self)
    - name: get_vector_store_config
      signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
    - name: _validate_vector_store_db_uri
      signature: def _validate_vector_store_db_uri(self) -> None
    - name: _validate_input_pattern
      signature: def _validate_input_pattern(self) -> None
    - name: _validate_multi_output_base_dirs
      signature: def _validate_multi_output_base_dirs(self) -> None
    - name: get_language_model_config
      signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
    - name: __repr__
      signature: def __repr__(self) -> str
    - name: _validate_models
      signature: def _validate_models(self) -> None
    - name: _validate_update_index_output_base_dir
      signature: def _validate_update_index_output_base_dir(self) -> None
    - name: _validate_root_dir
      signature: def _validate_root_dir(self) -> None
- file: graphrag/config/models/language_model_config.py
  functions:
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_key
    name: _validate_api_key
    signature: def _validate_api_key(self) -> None
    docstring: "Validate the API key.\n\nAPI Key is required when using OpenAI API\
      \ or when using Azure API with API Key authentication.\nFor the time being,\
      \ this check is extra verbose for clarity.\nIt will also raise an exception\
      \ if an API Key is provided when one is not expected such as the case of using\
      \ Azure Managed Identity.\n\nArgs:\n    self: The LanguageModelConfig instance.\n\
      \nReturns:\n    None\n\nRaises:\n    ApiKeyMissingError: If the API key is missing\
      \ and is required.\n    ConflictingSettingsError: If an API Key is provided\
      \ when using Azure Managed Identity."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_encoding_model
    name: _validate_encoding_model
    signature: def _validate_encoding_model(self) -> None
    docstring: 'Validate the encoding model.


      If encoding_model is not provided (empty or whitespace) and the type is not
      Chat or Embedding, derive

      the encoding model name for the configured LLM model using tiktoken.encoding_name_for_model
      and assign it

      to self.encoding_model. This updates the attribute in place and returns None.'
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_deployment_name
    name: _validate_deployment_name
    signature: def _validate_deployment_name(self) -> None
    docstring: "Validate the deployment name for Azure-hosted models.\n\nThis internal\
      \ validator checks whether a deployment_name is provided when using Azure OpenAI\
      \ (AOI) configurations. If the deployment_name is missing or consists only of\
      \ whitespace for Azure-hosted models (AzureOpenAIChat, AzureOpenAIEmbedding,\
      \ or when model_provider == \"azure\"), it logs a debug message stating that\
      \ deployment_name is not set and that the service will default to the model\
      \ name. No exception is raised; the behavior is a soft default.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_azure_settings
    name: _validate_azure_settings
    signature: def _validate_azure_settings(self) -> None
    docstring: "Validate the Azure settings.\n\nArgs:\n    self: The instance of the\
      \ language model configuration.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
      \ If the API base is missing and is required.\n    AzureApiVersionMissingError:\
      \ If the API version is missing and is required.\n    AzureDeploymentNameMissingError:\
      \ If the deployment name is missing and is required."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model_provider
    name: _validate_model_provider
    signature: def _validate_model_provider(self) -> None
    docstring: "Validate the model provider.\n\nThis validation applies only when\
      \ the model type is Chat or Embedding. If the model type is Chat or Embedding\
      \ and the model_provider is missing or blank, a KeyError is raised indicating\
      \ that a model provider must be specified for that type. For other model types,\
      \ this method performs no validation.\n\nArgs:\n    self: The instance of the\
      \ language model configuration. Note that self is an implicit parameter for\
      \ Python methods.\n\nReturns:\n    None\n\nRaises:\n    KeyError: If the model\
      \ provider is missing when the model type is Chat or Embedding."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_requests_per_minute
    name: _validate_requests_per_minute
    signature: def _validate_requests_per_minute(self) -> None
    docstring: "Validate the requests per minute.\n\nThis is a Pydantic model validator\
      \ invoked on the LanguageModelConfig instance. Accepted values are integers\
      \ >= 1, the string 'auto', or null. When the type is 'Chat' or 'Embedding' and\
      \ rate_limit_strategy is not None, 'auto' is not allowed.\n\nRaises\n------\n\
      ValueError\n    If the requests_per_minute is less than 1.\n    If requests_per_minute\
      \ is 'auto' when using type 'Chat' or 'Embedding' with a non-null rate_limit_strategy."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_max_retries
    name: _validate_max_retries
    signature: def _validate_max_retries(self) -> None
    docstring: "\"\"\"Validate the maximum retries.\n\nArgs:\n    self: The instance\
      \ of the language model configuration.\n\nReturns:\n    None: This method does\
      \ not return a value.\n\nRaises:\n    ValueError: If the maximum retries is\
      \ less than 1.\n\"\"\""
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_tokens_per_minute
    name: _validate_tokens_per_minute
    signature: def _validate_tokens_per_minute(self) -> None
    docstring: "Validate the tokens per minute.\n\nThis in-place validator ensures\
      \ tokens_per_minute is one of: an integer >= 1, the string \"auto\", or None.\
      \ It also enforces that tokens_per_minute cannot be set to 'auto' when using\
      \ a Chat or Embedding model type and a rate_limit_strategy is provided.\n\n\
      Returns:\n    None: This method validates in place on the LanguageModelConfig\
      \ instance and returns None.\n\nRaises:\n    ValueError: If tokens_per_minute\
      \ is an integer and less than 1.\n    ValueError: If tokens_per_minute is 'auto'\
      \ when using type 'Chat' or 'Embedding' and rate_limit_strategy is not None."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_base
    name: _validate_api_base
    signature: def _validate_api_base(self) -> None
    docstring: "Validate the API base.\n\nRequired when using AOI.\n\nArgs:\n    self:\
      \ The LanguageModelConfig instance.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
      \ If the API base is missing and is required."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_type
    name: _validate_type
    signature: def _validate_type(self) -> None
    docstring: "Validate the model type.\n\nArgs:\n    self: The instance being validated.\n\
      \nReturns:\n    None\n        The function does not return a value.\n\nRaises:\n\
      \    KeyError: If the model name is not recognized."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_auth_type
    name: _validate_auth_type
    signature: def _validate_auth_type(self) -> None
    docstring: "Validate the authentication type.\n\nauth_type must be api_key when\
      \ using OpenAI and\ncan be either api_key or azure_managed_identity when using\
      \ AOI.\n\nArgs:\n    self: The instance being validated.\n\nReturns:\n    None\n\
      \        The function does not return a value.\n\nRaises:\n    ConflictingSettingsError:\
      \ If the Azure authentication type conflicts with the model being used."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the LanguageModelConfig after the initial Pydantic schema\
      \ validation as a model_validator(mode='after') post-hook; this reflects the\
      \ actual validation sequence (after initial schema validation) and may raise\
      \ additional errors.\n\nArgs:\n    self (LanguageModelConfig): The LanguageModelConfig\
      \ instance being validated.\n\nReturns:\n    LanguageModelConfig: The same instance\
      \ after validation.\n\nRaises:\n    ApiKeyMissingError: If the API key is missing\
      \ when required.\n    AzureApiBaseMissingError: If the Azure API base is missing\
      \ when required.\n    AzureApiVersionMissingError: If the Azure API version\
      \ is missing when required.\n    AzureDeploymentNameMissingError: If the Azure\
      \ deployment name is missing when required.\n    ConflictingSettingsError: If\
      \ there are conflicting settings detected during validation."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_version
    name: _validate_api_version
    signature: def _validate_api_version(self) -> None
    docstring: "Validate the API version.\n\n        Required when using AOI.\n\n\
      \        Args:\n            self: The LanguageModelConfig instance.\n\n    \
      \    Returns:\n            None\n\n        Raises:\n            AzureApiVersionMissingError:\
      \ If the API version is missing and is required."
  classes:
  - class_id: graphrag/config/models/language_model_config.py::LanguageModelConfig
    name: LanguageModelConfig
    docstring: "LanguageModelConfig encapsulates configuration for language model\
      \ integration and performs validation for API keys, Azure AOI settings, rate\
      \ limits, and model selection.\n\nArgs:\n  model_type: The type of language\
      \ model to configure, as defined by ModelType.\n  model_provider: The provider\
      \ of the model (e.g., \"azure\").\n  encoding_model: The encoding model name\
      \ used for tokenization. If omitted, it may be derived from the configured model.\n\
      \  deployment_name: Azure OpenAI deployment name when using AOI; may be optional\
      \ for non-AOI configurations.\n  api_base: Base URL for API requests when AOI\
      \ or OpenAI endpoints are used.\n  api_version: API version for AOI usage.\n\
      \  rate_limit_strategy: Strategy for rate limiting (e.g., None or \"auto\").\n\
      \  requests_per_minute: Allowed requests per minute; integer >= 1, \"auto\"\
      , or None.\n  max_retries: Maximum number of retry attempts.\n  tokens_per_minute:\
      \ Allowed tokens per minute; integer >= 1, \"auto\", or None.\n  auth_type:\
      \ Authentication type for API access (e.g., ApiKey, AzureManagedIdentity).\n\
      \nReturns:\n  LanguageModelConfig: The same instance after validation.\n\nRaises:\n\
      \  ApiKeyMissingError: If an API key is required but not provided.\n  AzureApiBaseMissingError:\
      \ If the API base is required but missing.\n  AzureApiVersionMissingError: If\
      \ the API version is required but missing.\n  ConflictingSettingsError: If settings\
      \ conflict (e.g., auth/type mismatch or incompatible provider)."
    methods:
    - name: _validate_api_key
      signature: def _validate_api_key(self) -> None
    - name: _validate_encoding_model
      signature: def _validate_encoding_model(self) -> None
    - name: _validate_deployment_name
      signature: def _validate_deployment_name(self) -> None
    - name: _validate_azure_settings
      signature: def _validate_azure_settings(self) -> None
    - name: _validate_model_provider
      signature: def _validate_model_provider(self) -> None
    - name: _validate_requests_per_minute
      signature: def _validate_requests_per_minute(self) -> None
    - name: _validate_max_retries
      signature: def _validate_max_retries(self) -> None
    - name: _validate_tokens_per_minute
      signature: def _validate_tokens_per_minute(self) -> None
    - name: _validate_api_base
      signature: def _validate_api_base(self) -> None
    - name: _validate_type
      signature: def _validate_type(self) -> None
    - name: _validate_auth_type
      signature: def _validate_auth_type(self) -> None
    - name: _validate_model
      signature: def _validate_model(self)
    - name: _validate_api_version
      signature: def _validate_api_version(self) -> None
- file: graphrag/config/models/storage_config.py
  functions:
  - node_id: graphrag/config/models/storage_config.py::StorageConfig.validate_base_dir
    name: validate_base_dir
    signature: def validate_base_dir(cls, value, info)
    docstring: "Normalize base_dir to a filesystem path string for local storage.\n\
      \nThis validator does not verify that the path exists or is valid beyond conversion\
      \ to a string. It only performs normalization when the storage type is local\
      \ (StorageType.file); for all other storage types, the input value is returned\
      \ unchanged.\n\nArgs:\n    cls (type): The class that defines the validator.\n\
      \    value (Any): The input base_dir value to normalize.\n    info (object):\
      \ Validation context containing other field values, including the 'type' field.\n\
      \nReturns:\n    str: The normalized path string when using local storage; otherwise\
      \ returns the input value.\n\nRaises:\n    None"
  classes:
  - class_id: graphrag/config/models/storage_config.py::StorageConfig
    name: StorageConfig
    docstring: "StorageConfig manages storage-related configuration for GraphRAG backends.\n\
      \nPurpose:\n    Encapsulates settings for storage backends, including the storage\
      \ type and\n    the base directory for local storage. It reads defaults from\
      \ graphrag_config_defaults\n    and uses StorageType to determine behavior.\
      \ It provides a validator to normalize\n    the base_dir when using local storage.\n\
      \nAttributes:\n    base_dir: The filesystem path for local storage base directory.\
      \ Used when storage_type\n        indicates local file storage.\n    storage_type:\
      \ The StorageType enum value that selects the storage backend.\n\nBrief summary:\n\
      \    Centralizes storage configuration for GraphRAG, ensuring consistent handling\
      \ of\n    base directories for local storage and pass-through for non-local\
      \ storage types.\n\nMethods:\n    validate_base_dir(cls, value, info):\n   \
      \     Normalize base_dir to a filesystem path string for local storage. This\
      \ validator\n        does not verify that the path exists or is valid beyond\
      \ conversion to a string.\n        It only performs normalization when the storage\
      \ type is local (StorageType.file);\n        for all other storage types, the\
      \ input value is returned unchanged.\n\nArgs:\n    cls (type): The class that\
      \ defines the validator.\n    value (Any): The input value to be validated and\
      \ normalized.\n    info (dict): Validation information provided by Pydantic.\n\
      \nReturns:\n    str: The normalized base_dir path when storage_type is local;\
      \ otherwise, the input value\n        unchanged.\n\nRaises:\n    None"
    methods:
    - name: validate_base_dir
      signature: def validate_base_dir(cls, value, info)
- file: graphrag/config/models/summarize_descriptions_config.py
  functions:
  - node_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved description summarization strategy.\n\nArgs:\n  \
      \  root_dir: The root directory used to resolve the graph and text prompt file\
      \ paths.\n    model_config: The LanguageModelConfig instance containing the\
      \ model configuration; its model_dump() result is included in the strategy as\
      \ llm.\n\nReturns:\n    dict: The resolved strategy. If a custom strategy is\
      \ provided via self.strategy, that is returned; otherwise, a default strategy\
      \ dictionary is returned with the following keys:\n        type: The strategy\
      \ type (graph_intelligence)\n        llm: The serialized language model configuration\
      \ from model_config.model_dump()\n        summarize_prompt: The contents of\
      \ the prompt file located at Path(root_dir) / self.prompt when self.prompt is\
      \ set; otherwise None\n        max_summary_length: The maximum length for the\
      \ summary from self.max_length\n        max_input_tokens: The maximum input\
      \ tokens from self.max_input_tokens\n\nRaises:\n    IOError or OSError: If reading\
      \ the summarize_prompt file fails (e.g., prompt file is missing or unreadable)\
      \ when self.prompt is provided."
  classes:
  - class_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig
    name: SummarizeDescriptionsConfig
    docstring: "Config model that controls how descriptions are summarized and resolves\
      \ the concrete summarization strategy at runtime.\n\nAttributes:\n  strategy:\
      \ Optional[SummarizeStrategyType]. Custom strategy to override the default description\
      \ summarization strategy. If provided, this strategy will be used during strategy\
      \ resolution by resolved_strategy(). Default: None.\n\nMethods:\n  resolved_strategy(self,\
      \ root_dir: str, model_config: LanguageModelConfig) -> Any\n    Get the resolved\
      \ description summarization strategy.\n    root_dir: The root directory used\
      \ to resolve graph and text prompt file paths.\n    model_config: The LanguageModelConfig\
      \ instance; its model_dump() result is included in the strategy under the key\
      \ llm.\n    Returns: The resolved strategy representation. The exact type depends\
      \ on the underlying components; it could be a dict or another strategy object.\
      \ If a custom strategy was provided via self.strategy, that strategy participates\
      \ in resolution and may appear in the final result.\n    Raises: Exceptions\
      \ raised by underlying components during strategy resolution may propagate to\
      \ callers."
    methods:
    - name: resolved_strategy
      signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
        \ LanguageModelConfig\n    ) -> dict"
- file: graphrag/config/models/text_embedding_config.py
  functions:
  - node_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig.resolved_strategy
    name: resolved_strategy
    signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) ->
      dict'
    docstring: "Get the resolved text embedding strategy.\n\nArgs:\n    model_config:\
      \ The language model configuration used to resolve the strategy.\n\nReturns:\n\
      \    dict: The resolved text embedding strategy. If a custom strategy is provided\
      \ via self.strategy, that is returned; otherwise, a default strategy dictionary\
      \ is returned with the following keys:\n        type: The strategy type (TextEmbedStrategyType.openai)\n\
      \        llm: The serialized language model configuration (model_config.model_dump())\n\
      \        num_threads: The number of concurrent requests (model_config.concurrent_requests)\n\
      \        batch_size: The configured batch size (self.batch_size)\n        batch_max_tokens:\
      \ The configured max tokens per batch (self.batch_max_tokens)\n\nRaises:\n \
      \   ImportError: If the import of TextEmbedStrategyType fails."
  classes:
  - class_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig
    name: TextEmbeddingConfig
    docstring: 'TextEmbeddingConfig: Configuration holder for the text embedding strategy
      used to embed text in Graphrag. It centralizes the selection between a user-provided
      custom strategy and a default strategy resolved from LanguageModelConfig and
      graphrag_config_defaults.


      Initialization: __init__(strategy: dict | None = None) initializes the configuration
      with an optional custom strategy. If a strategy is provided, resolved_strategy(model_config)
      will return this value; otherwise, a default strategy is resolved based on the
      given LanguageModelConfig and the configured defaults.


      Attributes:

      - strategy: dict | None - Custom text embedding strategy to use when provided;
      otherwise None.


      Methods:

      - resolved_strategy(model_config: LanguageModelConfig) -> dict - Returns the
      resolved text embedding strategy. If a custom strategy is provided via self.strategy,
      that value is returned; otherwise, a default strategy dictionary is produced.
      The default includes at least the key ''type'' set to TextEmbedStrategyType.openai
      and other keys derived from the language model configuration and graphrag_config_defaults.


      Raises:

      - pydantic.ValidationError: if initialization receives an invalid strategy type
      or shape.

      - Exception during resolution if required configuration is missing or invalid.'
    methods:
    - name: resolved_strategy
      signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) ->
        dict'
- file: graphrag/config/models/vector_store_config.py
  functions:
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_db_uri
    name: _validate_db_uri
    signature: def _validate_db_uri(self) -> None
    docstring: "Validate the database URI. If the vector store type is LanceDB and\
      \ db_uri is missing or empty, set it to the default value from vector_store_defaults.\n\
      \nArgs:\n    self: The VectorStoreConfig instance being validated.\n\nReturns:\n\
      \    None\n\nRaises:\n    ValueError: If vector_store.type is not LanceDB and\
      \ a non-empty db_uri is provided. The error message is: vector_store.db_uri\
      \ is only used when vector_store.type is LanceDB. Please rerun graphrag init\
      \ and select the correct vector store type."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_embeddings_schema
    name: _validate_embeddings_schema
    signature: def _validate_embeddings_schema(self) -> None
    docstring: "Validate the embeddings schema. This method performs two checks:\n\
      1) Each entry in embeddings_schema must correspond to a known embedding schema\
      \ name present in all_embeddings. If any name is invalid, it raises a ValueError\
      \ with the message: vector_store.embeddings_schema contains an invalid embedding\
      \ schema name: {name}. Please update your settings.yaml and select the correct\
      \ embedding schema names.\n2) When vector_store.type == CosmosDB, every key\
      \ in embeddings_schema must be 'id'. If any key differs, it raises a ValueError\
      \ with the message: When using CosmosDB, the id_field in embeddings_schema must\
      \ be 'id'. Please update your settings.yaml and set the id_field to 'id'.\n\n\
      This method does not return a value. It raises ValueError on invalid configurations,\
      \ which are handled by the caller during model validation.\nArgs:\n    self:\
      \ The instance containing embeddings_schema and type attributes.\nReturns:\n\
      \    None\nRaises:\n    ValueError: vector_store.embeddings_schema contains\
      \ an invalid embedding schema name: {name}. Please update your settings.yaml\
      \ and select the correct embedding schema names.\n    ValueError: When using\
      \ CosmosDB, the id_field in embeddings_schema must be 'id'. Please update your\
      \ settings.yaml and set the id_field to 'id'."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_url
    name: _validate_url
    signature: def _validate_url(self) -> None
    docstring: "Validate the database URL.\n\nArgs:\n    self: The VectorStoreConfig\
      \ instance being validated.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
      \ If vector_store.type == azure_ai_search and vector_store.url is missing or\
      \ empty. The error message is: vector_store.url is required when vector_store.type\
      \ == azure_ai_search. Please rerun graphrag init and select the correct vector\
      \ store type.\n    ValueError: If vector_store.type == cosmos_db and vector_store.url\
      \ is missing or empty. The error message is: vector_store.url is required when\
      \ vector_store.type == cosmos_db. Please rerun graphrag init and select the\
      \ correct vector store type.\n    ValueError: If vector_store.type == LanceDB\
      \ and vector_store.url is provided (non-empty). The error message is: vector_store.url\
      \ is only used when vector_store.type == azure_ai_search or vector_store.type\
      \ == cosmos_db. Please rerun graphrag init and select the correct vector store\
      \ type."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model after the initial schema validation.\n\nArgs:\n\
      \    self: The instance being validated.\n\nReturns:\n    The same instance\
      \ after validation.\n\nRaises:\n    ValueError: If an invalid database URI,\
      \ URL, or embeddings schema is encountered during validation."
  classes:
  - class_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig
    name: VectorStoreConfig
    docstring: "Configuration model for vector store settings used by graphrag.\n\n\
      It centralizes validation and handling of vector store configuration, including\
      \ the store type, connection details (such as db_uri and url), embeddings schema\
      \ validation, and the vector store schema configuration. This ensures consistent,\
      \ validated settings are available for downstream operations.\n\nKey attributes:\n\
      \  vector_store: The vector store configuration block that includes the store\
      \ type and connection details.\n  db_uri: The database URI for the vector store;\
      \ may be set to a default when the type is LanceDB.\n  url: The URL for the\
      \ vector store; required when vector_store.type == azure_ai_search.\n  embeddings_schema:\
      \ A collection of embedding schema names to use; validated against all_embeddings.\n\
      \  vector_store_schema_config: The schema configuration for the vector store,\
      \ typically an instance of VectorStoreSchemaConfig.\n\nArgs:\n  vector_store:\
      \ The vector store configuration block containing the store type and connection\
      \ details.\n  db_uri: The database URI for the vector store; may be defaulted\
      \ for LanceDB.\n  url: The connection URL for the vector store.\n  embeddings_schema:\
      \ The list of embedding schema names to validate.\n  vector_store_schema_config:\
      \ The VectorStoreSchemaConfig object for schema validation.\n\nReturns:\n  None\n\
      \nRaises:\n  ValueError: If vector_store.type is not LanceDB and a non-empty\
      \ db_uri is provided.\n  ValueError: If any entry in embeddings_schema is not\
      \ a known embedding schema name present in all_embeddings.\n  ValueError: If\
      \ vector_store.type == azure_ai_search and vector_store.url is missing or empty.\n\
      \  ValueError: If an invalid database URI, URL, or embeddings schema is encountered\
      \ during validation."
    methods:
    - name: _validate_db_uri
      signature: def _validate_db_uri(self) -> None
    - name: _validate_embeddings_schema
      signature: def _validate_embeddings_schema(self) -> None
    - name: _validate_url
      signature: def _validate_url(self) -> None
    - name: _validate_model
      signature: def _validate_model(self)
- file: graphrag/config/models/vector_store_schema_config.py
  functions:
  - node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model after the initial schema validation.\n\nArgs:\n\
      \    self: The instance being validated.\n\nReturns:\n    The instance after\
      \ validation.\n\nRaises:\n    ValueError: If an unsafe or invalid field name\
      \ is encountered during validation."
  - node_id: graphrag/config/models/vector_store_schema_config.py::is_valid_field_name
    name: is_valid_field_name
    signature: 'def is_valid_field_name(field: str) -> bool'
    docstring: "Check if a field name is valid for CosmosDB.\n\nArgs:\n    field:\
      \ The field name to validate.\n\nReturns:\n    bool: True if the field name\
      \ is valid for CosmosDB, False otherwise."
  - node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
    name: _validate_schema
    signature: def _validate_schema(self) -> None
    docstring: "\"\"\"Validate the schema.\n\nArgs:\n    self (VectorStoreSchemaConfig):\
      \ The instance containing schema field names to validate.\n\nReturns:\n    None\n\
      \nRaises:\n    ValueError: If any of the id_field, vector_field, text_field,\
      \ or attributes_field contains an unsafe or invalid field name.\n\"\""
  classes:
  - class_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
    name: VectorStoreSchemaConfig
    docstring: 'VectorStoreSchemaConfig defines and validates the mapping of schema
      field names used by the vector store.


      Purpose: Centralizes the configuration of field names for id, vector, text,
      and attributes and provides validation to ensure field names are safe and valid.


      Key attributes:

      - id_field: The field name used for the unique identifier in the schema.

      - vector_field: The field name containing the vector data.

      - text_field: The field name containing associated text data.

      - attributes_field: The field name containing additional attributes.


      Args:

      - id_field: The field name used as the unique identifier (must be a valid identifier).

      - vector_field: The field name for the vector data (must be a valid identifier).

      - text_field: The field name for the associated text (must be a valid identifier).

      - attributes_field: The field name for additional attributes (must be a valid
      identifier).


      Returns:

      - None


      Raises:

      - ValueError: If an unsafe or invalid field name is encountered during validation.'
    methods:
    - name: _validate_model
      signature: def _validate_model(self)
    - name: _validate_schema
      signature: def _validate_schema(self) -> None
- file: graphrag/config/read_dotenv.py
  functions:
  - node_id: graphrag/config/read_dotenv.py::read_dotenv
    name: read_dotenv
    signature: 'def read_dotenv(root: str) -> None'
    docstring: "Read a .env file in the given root path.\n\nLoads environment variables\
      \ from a .env file located at the specified root into the process environment\
      \ (os.environ). Only variables that are not already defined in os.environ are\
      \ set, so existing environment variables are preserved.\n\nSide effects:\n \
      \   - Mutates os.environ by adding new variables from the .env file without\
      \ overwriting existing ones.\n\nLogging:\n    - Logs an info message when a\
      \ .env file is found and loaded.\n    - Logs an info message if no .env file\
      \ is found at the given root.\n\nArgs:\n    root: str\n        Path to the root\
      \ directory containing a .env file to load.\n\nReturns:\n    None\n        This\
      \ function does not return a value.\n\nNotes:\n    - If the .env file contains\
      \ invalid lines or is unreadable, dotenv_values will skip invalid lines or\n\
      \      otherwise perform its internal handling; this function does not perform\
      \ additional error handling."
  classes: []
- file: graphrag/data_model/community.py
  functions:
  - node_id: graphrag/data_model/community.py::Community.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
      \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n      \
      \  entities_key: str = \"entity_ids\",\n        relationships_key: str = \"\
      relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n    \
      \    covariates_key: str = \"covariate_ids\",\n        parent_key: str = \"\
      parent\",\n        children_key: str = \"children\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"Community\""
    docstring: "Create a new Community from the dict data.\n\nArgs:\n  cls (type):\
      \ The class.\n  d (dict[str, Any]): The source dictionary containing the values\
      \ for the Community fields.\n  id_key (str): Key in d for the community's identifier.\
      \ Defaults to \"id\".\n  title_key (str): Key in d for the community title.\
      \ Defaults to \"title\".\n  short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n  level_key (str): Key in\
      \ d for the community level. Defaults to \"level\".\n  entities_key (str): Key\
      \ in d for the related entity IDs. Defaults to \"entity_ids\".\n  relationships_key\
      \ (str): Key in d for the related relationship IDs. Defaults to \"relationship_ids\"\
      .\n  text_units_key (str): Key in d for the related text unit IDs. Defaults\
      \ to \"text_unit_ids\".\n  covariates_key (str): Key in d for covariate IDs.\
      \ Defaults to \"covariate_ids\".\n  parent_key (str): Key in d for the parent\
      \ community's ID. Defaults to \"parent\".\n  children_key (str): Key in d for\
      \ the child community IDs. Defaults to \"children\".\n  attributes_key (str):\
      \ Key in d for additional attributes. Defaults to \"attributes\".\n  size_key\
      \ (str): Key in d for the size of the community. Defaults to \"size\".\n  period_key\
      \ (str): Key in d for the period. Defaults to \"period\".\n\nReturns:\n  Community:\
      \ The newly created Community instance.\n\nRaises:\n  KeyError: If required\
      \ keys are missing from the input dictionary."
  classes:
  - class_id: graphrag/data_model/community.py::Community
    name: Community
    docstring: 'Dataclass representing a Community in the graph data model, extending
      Named with community-specific metadata and relationships. It encapsulates identity,
      hierarchical placement, and associations to entities, relationships, text units,
      and covariates.


      Inherits from Named to provide consistent identity and naming semantics across
      the graph model. This class is a dataclass and is intended to be instantiated
      directly or via the from_dict factory method.


      Attributes:

      - id (str): Unique identifier of the community.

      - title (str): Human-readable title of the community.

      - human_readable_id (Optional[str]): Optional short identifier for human readability
      (default: None).

      - level (int): Hierarchical level of the community within the graph.

      - entity_ids (List[str]): Identifiers of entities belonging to the community.

      - relationship_ids (List[str]): Identifiers of relationships associated with
      the community.

      - text_unit_ids (List[str]): Identifiers of text units in the community.

      - covariate_ids (List[str]): Identifiers of covariates associated with the community.

      - parent (Optional[str]): Identifier of the parent community, if any (default:
      None).

      - children (List[str]): Identifiers of child communities.

      - attributes (Dict[str, Any]): Additional attributes describing the community
      (default: empty dict).

      - size (Optional[int]): Size metric for the community (default: None).

      - period (Optional[str]): Time period associated with the community (default:
      None).


      Factory methods:

      - from_dict: Classmethod that constructs a Community from a dictionary using
      configurable keys. It maps dict keys to the corresponding fields (with defaults
      for key names) and returns a Community instance populated from the provided
      data.'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
        \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n    \
        \    entities_key: str = \"entity_ids\",\n        relationships_key: str =\
        \ \"relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n\
        \        covariates_key: str = \"covariate_ids\",\n        parent_key: str\
        \ = \"parent\",\n        children_key: str = \"children\",\n        attributes_key:\
        \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
        \ str = \"period\",\n    ) -> \"Community\""
- file: graphrag/data_model/community_report.py
  functions:
  - node_id: graphrag/data_model/community_report.py::CommunityReport.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
      \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        summary_key: str = \"summary\",\n        full_content_key: str = \"\
      full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"CommunityReport\""
    docstring: "Create a CommunityReport instance from the dict data.\n\nArgs:\n \
      \ cls (type): The class.\n  d (dict[str, Any]): The source dictionary containing\
      \ the values for the CommunityReport fields.\n  id_key (str): Key in d for the\
      \ report's identifier. Defaults to \"id\".\n  title_key (str): Key in d for\
      \ the report title. Defaults to \"title\".\n  community_id_key (str): Key in\
      \ d for the associated community's id. Defaults to \"community\".\n  short_id_key\
      \ (str): Key in d for the optional short identifier. Defaults to \"human_readable_id\"\
      .\n  summary_key (str): Key in d for the summary. Defaults to \"summary\".\n\
      \  full_content_key (str): Key in d for the full content. Defaults to \"full_content\"\
      .\n  rank_key (str): Key in d for the rank value. Defaults to \"rank\".\n  attributes_key\
      \ (str): Key in d for optional attributes dictionary. Defaults to \"attributes\"\
      .\n  size_key (str): Key in d for the size value. Defaults to \"size\".\n  period_key\
      \ (str): Key in d for the period. Defaults to \"period\".\n\nReturns:\n  CommunityReport:\
      \ The constructed CommunityReport instance.\n\nRaises:\n  KeyError: If a required\
      \ key is missing from d (e.g., id_key, title_key, community_id_key, summary_key,\
      \ full_content_key, rank_key)."
  classes:
  - class_id: graphrag/data_model/community_report.py::CommunityReport
    name: CommunityReport
    docstring: 'Dataclass-based model representing a community report, storing identifiers,
      metadata, and content for a specific community. The class inherits from Named
      and acts as a lightweight data container with a convenient from_dict constructor.


      Args:

      - id (str): The report''s identifier.

      - title (str): The report title.

      - community_id (str): The associated community''s id.

      - human_readable_id (Optional[str]): A short, human-friendly identifier.

      - summary (Optional[str]): A brief summary of the report.

      - full_content (Optional[str]): The full content of the report.

      - rank (Optional[int]): The report''s ranking or order.

      - attributes (Optional[dict[str, Any]]): Additional arbitrary attributes.

      - size (Optional[int]): A size-related metric.

      - period (Optional[str]): The time period the report covers.


      Returns:

      - CommunityReport: A new CommunityReport instance.


      Raises:

      - ValueError: If required fields are missing or have invalid types.


      From_dict:

      - cls (type): The class used for construction (typically CommunityReport).

      - d (dict[str, Any]): Source dictionary containing field values.

      - id_key (str): Key in d for the report''s identifier. Defaults to ''id''.

      - title_key (str): Key in d for the report title. Defaults to ''title''.

      - community_id_key (str): Key in d for the associated community''s id. Defaults
      to ''community''.

      - short_id_key (str): Key in d for the human-readable id. Defaults to ''human_readable_id''.

      - summary_key (str): Key in d for the summary. Defaults to ''summary''.

      - full_content_key (str): Key in d for the full content. Defaults to ''full_content''.

      - rank_key (str): Key in d for the rank. Defaults to ''rank''.

      - attributes_key (str): Key in d for additional attributes. Defaults to ''attributes''.

      - size_key (str): Key in d for the size. Defaults to ''size''.

      - period_key (str): Key in d for the period. Defaults to ''period''.


      Returns:

      - CommunityReport: A new instance constructed from d.


      Raises:

      - KeyError or ValueError: If required keys are missing or values are invalid.'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
        \ str = \"community\",\n        short_id_key: str = \"human_readable_id\"\
        ,\n        summary_key: str = \"summary\",\n        full_content_key: str\
        \ = \"full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
        \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
        \ str = \"period\",\n    ) -> \"CommunityReport\""
- file: graphrag/data_model/covariate.py
  functions:
  - node_id: graphrag/data_model/covariate.py::Covariate.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        subject_id_key: str = \"subject_id\",\n   \
      \     covariate_type_key: str = \"covariate_type\",\n        short_id_key: str\
      \ = \"human_readable_id\",\n        text_unit_ids_key: str = \"text_unit_ids\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Covariate\""
    docstring: "Create a new Covariate from the dict data.\n\nArgs:\n    cls (type):\
      \ The Covariate class; this is a classmethod.\n    d (dict[str, Any]): The dictionary\
      \ containing covariate fields. The function reads keys including id_key, subject_id_key,\
      \ covariate_type_key, short_id_key, text_unit_ids_key, and attributes_key to\
      \ construct the Covariate.\n    id_key (str): The key in d that corresponds\
      \ to the covariate's id.\n    subject_id_key (str): The key in d that corresponds\
      \ to the subject's id.\n    covariate_type_key (str): The key in d for the covariate\
      \ type.\n    short_id_key (str): The key in d that corresponds to the covariate's\
      \ short id (human readable).\n    text_unit_ids_key (str): The key in d that\
      \ contains text unit ids (optional).\n    attributes_key (str): The key in d\
      \ that contains additional attributes (optional).\n\nReturns:\n    Covariate:\
      \ The Covariate instance created from the dictionary.\n\nRaises:\n    KeyError:\
      \ If d does not contain the required keys identified by id_key or subject_id_key."
  classes:
  - class_id: graphrag/data_model/covariate.py::Covariate
    name: Covariate
    docstring: 'Covariate model representing a covariate linked to a subject in the
      graph-based data model.


      This class encapsulates a covariate''s identity, its association with a subject,
      its covariate_type, a human_readable_id, related text_unit_ids, and additional
      attributes. The id attribute is inherited from the Identified base class.


      Key attributes

      - id: Unique identifier for the covariate (inherited from Identified)

      - subject_id: Identifier of the subject this covariate pertains to

      - covariate_type: Type or category of the covariate

      - human_readable_id: Short, human-readable identifier

      - text_unit_ids: Collection of text unit identifiers related to this covariate

      - attributes: Additional arbitrary attributes for the covariate


      Brief summary

      Describes a covariate and its relationships to subjects and text units, with
      a utility to construct instances from a dictionary via from_dict.


      From_dict

      This classmethod constructs a Covariate from a dictionary. It reads and uses
      the following keys by default: id ("id"), subject_id ("subject_id"), covariate_type
      ("covariate_type"), human_readable_id ("human_readable_id"), text_unit_ids ("text_unit_ids"),
      attributes ("attributes"). The keys can be customized via id_key, subject_id_key,
      covariate_type_key, short_id_key, text_unit_ids_key, and attributes_key.


      Args

      - cls (type Covariate): The Covariate class; this is a classmethod.

      - d (dict[str, Any]): The dictionary containing covariate fields.

      - id_key (str): The key in d that corresponds to the covariate id (default "id")

      - subject_id_key (str): The key in d for the subject identifier (default "subject_id")

      - covariate_type_key (str): The key in d for the covariate type (default "covariate_type")

      - short_id_key (str): The key in d for the human-readable id (default "human_readable_id")

      - text_unit_ids_key (str): The key in d for the text unit identifiers (default
      "text_unit_ids")

      - attributes_key (str): The key in d for the attributes dictionary (default
      "attributes")


      Returns

      - Covariate: An instance populated from the provided data


      Raises

      - KeyError: If required keys are missing from the input dictionary

      - TypeError: If any field has an inappropriate type'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        subject_id_key: str = \"subject_id\",\n\
        \        covariate_type_key: str = \"covariate_type\",\n        short_id_key:\
        \ str = \"human_readable_id\",\n        text_unit_ids_key: str = \"text_unit_ids\"\
        ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Covariate\""
- file: graphrag/data_model/document.py
  functions:
  - node_id: graphrag/data_model/document.py::Document.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      text_key: str = \"text\",\n        text_units_key: str = \"text_units\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Document\""
    docstring: "Create a new document from the dict data.\n\nArgs:\n    cls (type):\
      \ The class.\n    d (dict[str, Any]): The source dictionary containing the values\
      \ for the Document fields.\n    id_key (str): Key in d for the document's identifier.\
      \ Defaults to \"id\".\n    short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n    title_key (str): Key in\
      \ d for the title. Defaults to \"title\".\n    type_key (str): Key in d for\
      \ the document type. Defaults to \"type\".\n    text_key (str): Key in d for\
      \ the text content. Defaults to \"text\".\n    text_units_key (str): Key in\
      \ d for the list of text unit ids. Defaults to \"text_units\".\n    attributes_key\
      \ (str): Key in d for optional attributes dictionary. Defaults to \"attributes\"\
      .\n\nReturns:\n    Document: A Document instance created from the dictionary\
      \ data.\n\nRaises:\n    KeyError: If a required key is missing from d."
  classes:
  - class_id: graphrag/data_model/document.py::Document
    name: Document
    docstring: 'Document data model representing a document in the GraphRag data model.


      Purpose:

      Encapsulates identifiers, metadata, and content for a document, supporting construction
      from a dictionary.


      Key attributes:

      - id: The document''s identifier.

      - human_readable_id: Optional short identifier.

      - title: The document title.

      - type: The document type.

      - text: The document text content.

      - text_units: Units describing text content.

      - attributes: Additional attributes associated with the document.


      From dictionary construction:

      The class provides a from_dict classmethod to create a Document instance from
      a dictionary. The method accepts keys for each field, with sensible defaults:

      - id_key: Key in d for the document''s identifier. Defaults to "id".

      - short_id_key: Key in d for the optional short identifier. Defaults to "human_readable_id".

      - title_key: Key in d for the title. Defaults to "title".

      - type_key: Key in d for the document''s type. Defaults to "type".

      - text_key: Key in d for the document''s text. Defaults to "text".

      - text_units_key: Key in d for the text units. Defaults to "text_units".

      - attributes_key: Key in d for the document''s attributes. Defaults to "attributes".


      Returns:

      Document: The constructed Document instance.


      Raises:

      None documented.'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\"\
        ,\n        title_key: str = \"title\",\n        type_key: str = \"type\",\n\
        \        text_key: str = \"text\",\n        text_units_key: str = \"text_units\"\
        ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Document\""
- file: graphrag/data_model/entity.py
  functions:
  - node_id: graphrag/data_model/entity.py::Entity.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      description_key: str = \"description\",\n        description_embedding_key:\
      \ str = \"description_embedding\",\n        name_embedding_key: str = \"name_embedding\"\
      ,\n        community_key: str = \"community\",\n        text_unit_ids_key: str\
      \ = \"text_unit_ids\",\n        rank_key: str = \"degree\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Entity\""
    docstring: "Create a new entity from the dict data.\n\nArgs:\n  cls (type): The\
      \ class.\n  d (dict[str, Any]): The source dictionary containing the values\
      \ for the Entity fields.\n  id_key (str): Key in d for the entity's identifier.\
      \ Defaults to \"id\".\n  short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n  title_key (str): Key in\
      \ d for the title. Defaults to \"title\".\n  type_key (str): Key in d for the\
      \ type. Defaults to \"type\".\n  description_key (str): Key in d for the description.\
      \ Defaults to \"description\".\n  description_embedding_key (str): Key in d\
      \ for the description embedding. Defaults to \"description_embedding\".\n  name_embedding_key\
      \ (str): Key in d for the name embedding. Defaults to \"name_embedding\".\n\
      \  community_key (str): Key in d for the community IDs. Defaults to \"community\"\
      .\n  text_unit_ids_key (str): Key in d for the text unit IDs. Defaults to \"\
      text_unit_ids\".\n  rank_key (str): Key in d for the rank. Defaults to \"degree\"\
      .\n  attributes_key (str): Key in d for the attributes. Defaults to \"attributes\"\
      .\n\nReturns:\n  Entity: The newly created Entity instance.\n\nRaises:\n  KeyError:\
      \ If the dictionary does not contain the keys specified by id_key and title_key."
  classes:
  - class_id: graphrag/data_model/entity.py::Entity
    name: Entity
    docstring: 'Entity represents a graph entity with identifying information and
      metadata in the GraphRag data model.


      This class encapsulates identifiers, descriptive metadata, embeddings, and relationships
      to related data such as text units and communities. It can be constructed from
      a dictionary via the from_dict class method, which supports configurable keys
      for mapping.


      Key attributes:

      - id: The unique identifier for the entity.

      - human_readable_id: Optional short identifier for human-friendly display.

      - title: Title of the entity.

      - type: Classification type of the entity.

      - description: Textual description of the entity.

      - description_embedding: Embedding representation of the description.

      - name_embedding: Embedding representation of the name.

      - community: Community the entity belongs to.

      - text_unit_ids: Identifiers of related text units.

      - degree: Rank or degree of the entity.

      - attributes: Additional attributes associated with the entity.


      From dictionary construction:

      - from_dict(cls, d, id_key="id", short_id_key="human_readable_id", title_key="title",
      type_key="type", description_key="description", description_embedding_key="description_embedding",
      name_embedding_key="name_embedding", community_key="community", text_unit_ids_key="text_unit_ids",
      rank_key="degree", attributes_key="attributes") creates an Entity from the provided
      dictionary using the given key mappings and defaults.


      Returns: An Entity instance created from the input dictionary.


      Raises: Exceptions may be raised by from_dict if the input data are invalid
      or required fields are missing.'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\"\
        ,\n        title_key: str = \"title\",\n        type_key: str = \"type\",\n\
        \        description_key: str = \"description\",\n        description_embedding_key:\
        \ str = \"description_embedding\",\n        name_embedding_key: str = \"name_embedding\"\
        ,\n        community_key: str = \"community\",\n        text_unit_ids_key:\
        \ str = \"text_unit_ids\",\n        rank_key: str = \"degree\",\n        attributes_key:\
        \ str = \"attributes\",\n    ) -> \"Entity\""
- file: graphrag/data_model/relationship.py
  functions:
  - node_id: graphrag/data_model/relationship.py::Relationship.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        source_key: str = \"source\",\n        target_key: str = \"target\"\
      ,\n        description_key: str = \"description\",\n        rank_key: str =\
      \ \"rank\",\n        weight_key: str = \"weight\",\n        text_unit_ids_key:\
      \ str = \"text_unit_ids\",\n        attributes_key: str = \"attributes\",\n\
      \    ) -> \"Relationship\""
    docstring: "Create a new Relationship from the dictionary data.\n\nArgs:\n  cls\
      \ (type): The class.\n  d (dict[str, Any]): The source dictionary containing\
      \ the values for the Relationship fields.\n  id_key (str): Key in d for the\
      \ relationship's identifier. Defaults to \"id\".\n  short_id_key (str): Key\
      \ in d for the optional short identifier. Defaults to \"human_readable_id\"\
      .\n  source_key (str): Key in d for the source entity. Defaults to \"source\"\
      .\n  target_key (str): Key in d for the target entity. Defaults to \"target\"\
      .\n  description_key (str): Key in d for the description. Defaults to \"description\"\
      .\n  rank_key (str): Key in d for the rank. Defaults to \"rank\".\n  weight_key\
      \ (str): Key in d for the weight. Defaults to \"weight\".\n  text_unit_ids_key\
      \ (str): Key in d for text unit IDs. Defaults to \"text_unit_ids\".\n  attributes_key\
      \ (str): Key in d for additional attributes. Defaults to \"attributes\".\n\n\
      Returns:\n  Relationship: A Relationship instance constructed from the dictionary\
      \ data.\n\nRaises:\n  KeyError: If id_key is not found in d."
  classes:
  - class_id: graphrag/data_model/relationship.py::Relationship
    name: Relationship
    docstring: "Represents a relationship between two entities in the graph data model.\n\
      \nThis dataclass captures metadata about a relationship, including identifiers,\
      \ source\nand target references, descriptive text, ranking and weight, related\
      \ text units, and\narbitrary attributes.\n\nArgs:\n    id: Unique identifier\
      \ for the relationship.\n    short_id: Optional short identifier (human_readable_id).\n\
      \    source: Reference to the source entity in the relationship.\n    target:\
      \ Reference to the target entity in the relationship.\n    description: Description\
      \ of the relationship.\n    rank: Ranking value for ordering or prioritization.\n\
      \    weight: Weight or strength of the relationship.\n    text_unit_ids: Identifiers\
      \ of related text units.\n    attributes: Arbitrary additional attributes associated\
      \ with the relationship.\n\nReturns:\n    Relationship: A Relationship instance\
      \ created from dictionary data via from_dict.\n\nRaises:\n    KeyError: If required\
      \ keys are missing from the input dictionary during construction.\n    TypeError:\
      \ If input data types are not as expected."
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\"\
        ,\n        source_key: str = \"source\",\n        target_key: str = \"target\"\
        ,\n        description_key: str = \"description\",\n        rank_key: str\
        \ = \"rank\",\n        weight_key: str = \"weight\",\n        text_unit_ids_key:\
        \ str = \"text_unit_ids\",\n        attributes_key: str = \"attributes\",\n\
        \    ) -> \"Relationship\""
- file: graphrag/data_model/text_unit.py
  functions:
  - node_id: graphrag/data_model/text_unit.py::TextUnit.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        text_key: str = \"text\",\n        entities_key: str = \"entity_ids\"\
      ,\n        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
      \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n    \
      \    document_ids_key: str = \"document_ids\",\n        attributes_key: str\
      \ = \"attributes\",\n    ) -> \"TextUnit\""
    docstring: "Create a new TextUnit from the dict data.\n\nArgs:\n    cls: The class.\n\
      \    d (dict[str, Any]): The source dictionary containing the values for the\
      \ TextUnit fields.\n    id_key (str): Key in d for the text unit's identifier.\
      \ Defaults to \"id\".\n    short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n    text_key (str): Key in\
      \ d for the text content. Defaults to \"text\".\n    entities_key (str): Key\
      \ in d for the related entity IDs. Defaults to \"entity_ids\".\n    relationships_key\
      \ (str): Key in d for the related relationship IDs. Defaults to \"relationship_ids\"\
      .\n    covariates_key (str): Key in d for covariate IDs. Defaults to \"covariate_ids\"\
      .\n    n_tokens_key (str): Key in d for the number of tokens. Defaults to \"\
      n_tokens\".\n    document_ids_key (str): Key in d for the document IDs. Defaults\
      \ to \"document_ids\".\n    attributes_key (str): Key in d for additional attributes.\
      \ Defaults to \"attributes\".\n\nReturns:\n    TextUnit: A new TextUnit instance\
      \ populated with values from d.\n\nRaises:\n    KeyError: If id_key is not present\
      \ in d."
  classes:
  - class_id: graphrag/data_model/text_unit.py::TextUnit
    name: TextUnit
    docstring: 'TextUnit is a data model that encapsulates a unit of text and its
      metadata for graph-based data handling.


      Purpose:

      - Store the text content together with identifiers linking it to entities, relationships,
      covariates, and related documents.


      Inherits:

      - Identified to provide a stable, unique identifier for the text unit.


      Key attributes:

      - id: Text unit identifier.

      - human_readable_id: Optional short identifier.

      - text: The actual text content.

      - entity_ids: Identifiers of entities present in the text.

      - relationship_ids: Identifiers of relationships associated with the text.

      - covariate_ids: Identifiers of covariates linked to the text.

      - n_tokens: Number of tokens in the text.

      - document_ids: Identifiers of documents containing the text unit.

      - attributes: Additional arbitrary attributes.


      From dictionary construction:

      - This class provides a from_dict classmethod to construct a TextUnit from a
      dictionary, mapping configured keys to the corresponding fields.


      Args:

      - cls: The class.

      - d (dict[str, Any]): The source dictionary containing the values for the TextUnit
      fields.

      - id_key (str): Key in d for the text unit''s identifier. Defaults to "id".

      - short_id_key (str): Key in d for the optional short identifier. Defaults to
      "human_readable_id".

      - text_key (str): Key in d for the text content. Defaults to "text".

      - entities_key (str): Key in d for the entity identifiers. Defaults to "entity_ids".

      - relationships_key (str): Key in d for the relationship identifiers. Defaults
      to "relationship_ids".

      - covariates_key (str): Key in d for the covariate identifiers. Defaults to
      "covariate_ids".

      - n_tokens_key (str): Key in d for the token count. Defaults to "n_tokens".

      - document_ids_key (str): Key in d for the document identifiers. Defaults to
      "document_ids".

      - attributes_key (str): Key in d for the attributes. Defaults to "attributes".


      Returns:

      - TextUnit: A new TextUnit instance constructed from the provided dictionary
      data.


      Raises:

      - May raise exceptions stemming from dictionary access or type mismatches depending
      on input data.'
    methods:
    - name: from_dict
      signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n     \
        \   id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\"\
        ,\n        text_key: str = \"text\",\n        entities_key: str = \"entity_ids\"\
        ,\n        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
        \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n  \
        \      document_ids_key: str = \"document_ids\",\n        attributes_key:\
        \ str = \"attributes\",\n    ) -> \"TextUnit\""
- file: graphrag/factory/factory.py
  functions:
  - node_id: graphrag/factory/factory.py::Factory.create
    name: create
    signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
    docstring: "Create a service instance based on the strategy.\n\nArgs:\n    strategy\
      \ (str): The name of the strategy.\n    kwargs (Any): Additional arguments to\
      \ pass to the service initializer.\n\nReturns:\n    T: An instance of T.\n\n\
      Raises:\n    ValueError: If the strategy is not registered."
  - node_id: graphrag/factory/factory.py::Factory.__contains__
    name: __contains__
    signature: 'def __contains__(self, strategy: str) -> bool'
    docstring: "\"\"\"Check if a strategy is registered.\n\nArgs:\n    strategy: str\
      \ The name of the strategy.\n\nReturns:\n    bool: True if the strategy is registered,\
      \ False otherwise.\n\"\"\""
  - node_id: graphrag/factory/factory.py::Factory.__new__
    name: __new__
    signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
    docstring: "Return the per-subclass singleton instance for the class that invokes\
      \ __new__.\n\nIf an instance for this class has not been created yet, create\
      \ one using the base object's __new__ and store it on the class. Subsequent\
      \ calls return the same instance.\n\nArgs\n----\n    cls: The class for which\
      \ to obtain the singleton instance. This is the class that invoked __new__,\
      \ and may be a subclass of Factory.\n    *args: Additional positional arguments\
      \ forwarded to the class's constructor.\n    **kwargs: Additional keyword arguments\
      \ forwarded to the class's constructor.\n\nReturns\n----\n    The singleton\
      \ instance of the calling class (cls). Each subclass maintains its own _instance,\
      \ so the return value is an instance of the subclass rather than the base Factory."
  - node_id: graphrag/factory/factory.py::Factory.register
    name: register
    signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
      T]) -> None'
    docstring: "Register a new service factory for a strategy.\n\nStores a factory\
      \ (callable) under the given strategy name. The factory is not invoked at registration\
      \ time; it will be called later by create(**kwargs) to produce an instance of\
      \ T.\n\nIf a factory is already registered under the same strategy name, it\
      \ will be overwritten with the new factory.\n\nArgs:\n    strategy (str): The\
      \ name of the strategy.\n    service_initializer (Callable[..., T]): A callable\
      \ that, when invoked via create(**kwargs), returns an instance of T.\n\nReturns:\n\
      \    None: This method does not return a value."
  - node_id: graphrag/factory/factory.py::Factory.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize internal state for the Factory singleton instance on first\
      \ initialization.\n\nArgs\n----\nself: The Factory instance being initialized.\
      \ The internal state is created only on the first initialization due to the\
      \ singleton __new__-based pattern.\n\nReturns\n-------\nNone\n\nRaises\n-------\n\
      None\n\nAttributes\n- _services: dict[str, Callable[..., T]] \u2014 registry\
      \ mapping strategy names to callables that return T.\n- _initialized: bool \u2014\
      \ initialization flag."
  - node_id: graphrag/factory/factory.py::Factory.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"Get a list of registered strategy names.\n\nArgs:\n    self:\
      \ The instance containing registered strategies.\n\nReturns:\n    list[str]:\
      \ A list of the registered strategy names.\n\"\"\""
  classes:
  - class_id: graphrag/factory/factory.py::Factory
    name: Factory
    docstring: "Factory is a generic, per-subclass singleton that registers and creates\
      \ service instances by strategy name.\n\nThe Factory maintains a registry of\
      \ strategy names to callables that return instances of T. Strategies can be\
      \ registered with register, queried with __contains__, listed with keys, and\
      \ used to create instances with create. Each subclass uses its own singleton\
      \ instance.\n\nAttributes:\n    _services: dict[str, Callable[..., T]] \u2014\
      \ registry mapping strategy names to callables that produce T instances."
    methods:
    - name: create
      signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
    - name: __contains__
      signature: 'def __contains__(self, strategy: str) -> bool'
    - name: __new__
      signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
    - name: register
      signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
        T]) -> None'
    - name: __init__
      signature: def __init__(self)
    - name: keys
      signature: def keys(self) -> list[str]
- file: graphrag/index/input/csv.py
  functions:
  - node_id: graphrag/index/input/csv.py::load_csv
    name: load_csv
    signature: "def load_csv(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    docstring: "Load csv inputs from a directory.\n\nParameters:\n    config: InputConfig\n\
      \        Configuration for the CSV input, including encoding and storage settings.\n\
      \    storage: PipelineStorage\n        Storage backend used to retrieve CSV\
      \ files and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
      \ containing the data from loaded CSV files with any\n        additional processing\
      \ applied (data columns, and creation_date).\n\nRaises:\n    Exception\n   \
      \     Propagates exceptions from the underlying storage or pandas operations;\
      \ per-file\n        failures are logged and skipped by the loader used to load\
      \ files."
  - node_id: graphrag/index/input/csv.py::load_file
    name: load_file
    signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
    docstring: "Asynchronously load a CSV file from storage and return it as a DataFrame.\n\
      \nThis function reads the CSV using the configured encoding (config.encoding).\n\
      If grouping data is provided via 'group', the corresponding keys are added as\
      \ new columns to every row (one column per key), rather than merging or stacking\
      \ rows.\n\nThe DataFrame is then augmented by process_data_columns to include\
      \ additional configured metadata columns (for example, id, text, and title)\
      \ as defined by the configuration.\n\nA creation_date column is added to all\
      \ rows, with the same value derived from storage.get_creation_date(path) for\
      \ the given path.\n\nArgs:\n  path (str): Path to the CSV file in storage.\n\
      \  group (dict | None): Optional mapping of grouping metadata. If None, treated\
      \ as {}.\n\nReturns:\n  pd.DataFrame: Loaded CSV data with grouping columns\
      \ (if provided), processed data columns, and a creation_date column added to\
      \ every row.\n\nRaises:\n  Exception: If storage access, CSV parsing, or data\
      \ processing fails."
  classes: []
- file: graphrag/index/input/factory.py
  functions:
  - node_id: graphrag/index/input/factory.py::create_input
    name: create_input
    signature: "def create_input(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    docstring: "Instantiate input data for a pipeline.\n\nArgs:\n    config: InputConfig\
      \ containing input configuration (such as file_type and metadata) and storage\
      \ base_dir information.\n    storage: PipelineStorage used to access the input\
      \ data.\n\nReturns:\n    pandas.DataFrame: The loaded input data as a DataFrame.\
      \ If config.metadata is provided and all metadata columns exist in the DataFrame,\
      \ a new column named \"metadata\" is added containing a JSON object per row\
      \ built from the metadata columns, and the original metadata columns are converted\
      \ to strings.\n\nRaises:\n    ValueError: If one or more metadata columns listed\
      \ in config.metadata are not found in the DataFrame, or if the input type specified\
      \ in config.file_type is unknown."
  classes: []
- file: graphrag/index/input/json.py
  functions:
  - node_id: graphrag/index/input/json.py::load_json
    name: load_json
    signature: "def load_json(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    docstring: "Load json inputs from a directory.\n\nArgs:\n    config: InputConfig\n\
      \        Configuration for the JSON input, including encoding and storage settings.\n\
      \    storage: PipelineStorage\n        Storage backend used to retrieve JSON\
      \ files and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
      \ containing the data from loaded JSON files with any\n        additional processing\
      \ applied (including adding per-file group keys as columns\n        and a creation_date\
      \ column).\n\nRaises:\n    json.JSONDecodeError\n        If a JSON payload cannot\
      \ be decoded from the loaded text."
  - node_id: graphrag/index/input/json.py::load_file
    name: load_file
    signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
    docstring: "Asynchronously load a JSON input file from storage and return it as\
      \ a DataFrame.\n\nArgs:\n  path (str): Path to the JSON file.\n  group (dict\
      \ | None): Optional grouping metadata to merge with the item. If None, an empty\
      \ dict is used.\n\nReturns:\n  pd.DataFrame: DataFrame loaded from the JSON\
      \ content, augmented with grouping keys (if any), processed by process_data_columns,\
      \ and annotated with a creation_date column for each row.\n\nRaises:\n  json.JSONDecodeError:\
      \ If the loaded content cannot be decoded as JSON."
  classes: []
- file: graphrag/index/input/text.py
  functions:
  - node_id: graphrag/index/input/text.py::load_text
    name: load_text
    signature: "def load_text(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    docstring: "Load text inputs from a directory.\n\nArgs:\n    config: InputConfig\n\
      \        Configuration for the text input, including encoding and storage settings.\n\
      \    storage: PipelineStorage\n        Storage backend used to retrieve text\
      \ files and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
      \ containing the data from loaded text files with any\n        additional processing\
      \ applied.\n\nRaises:\n    None\n        The per-file loading failures are logged\
      \ and skipped by the internal loader\n        (load_files) rather than raised\
      \ to the caller."
  - node_id: graphrag/index/input/text.py::load_file
    name: load_file
    signature: 'def load_file(path: str, group: dict | None = None) -> pd.DataFrame'
    docstring: "Load a text input from storage and return it as a DataFrame containing\
      \ metadata.\n\nArgs:\n  path (str): Path to the text file.\n  group (dict |\
      \ None): Optional grouping metadata to merge with the item. If None, an empty\
      \ dict is used.\n\nReturns:\n  pd.DataFrame: A DataFrame containing a single\
      \ row with the loaded text and metadata, including id, title, and creation_date.\n\
      \nRaises:\n  KeyError: If a key from the hashcode used for hashing is not present\
      \ in the item."
  classes: []
- file: graphrag/index/input/util.py
  functions:
  - node_id: graphrag/index/input/util.py::load_files
    name: load_files
    signature: "def load_files(\n    loader: Any,\n    config: InputConfig,\n    storage:\
      \ PipelineStorage,\n) -> pd.DataFrame"
    docstring: "Load files from storage asynchronously and apply a loader function,\
      \ then concatenate the results into a single pandas DataFrame.\n\nThe loader\
      \ is awaited for each file. Failures are logged and the corresponding file is\
      \ skipped rather than raised.\n\nArgs:\n    loader: Any\n        Async loader\
      \ callable that accepts (file, group) and returns a value compatible with pandas.concat.\n\
      \        The loader will be awaited for each file. If it raises, the file is\
      \ skipped with a warning.\n    config: InputConfig\n        Configuration for\
      \ input files, including file_pattern, file_filter, and file_type/storage details\
      \ used to locate files.\n    storage: PipelineStorage\n        Storage backend\
      \ used to locate and read files.\n\nReturns:\n    pd.DataFrame\n        A DataFrame\
      \ formed by concatenating all successfully loaded data.\n\nRaises:\n    ValueError\n\
      \        If no files matching the pattern are found in the configured storage\
      \ location.\n\nNotes:\n    - The final concatenation uses pd.concat on the list\
      \ of successfully loaded objects. If none are loaded, pd.concat may raise a\
      \ ValueError for no objects to concatenate. This edge case should be considered\
      \ by callers.\n    - The function logs the number of files found and the number\
      \ successfully loaded."
  - node_id: graphrag/index/input/util.py::process_data_columns
    name: process_data_columns
    signature: "def process_data_columns(\n    documents: pd.DataFrame, config: InputConfig,\
      \ path: str\n) -> pd.DataFrame"
    docstring: "Process configured data columns of a DataFrame by augmenting it with\
      \ id, text, and title columns according to the provided configuration. Warnings\
      \ are logged if a configured text or title column is not found in the data.\n\
      \nThe function mutates the input DataFrame and returns it.\n\nArgs:\n  documents\
      \ (pd.DataFrame): DataFrame to augment with id, text, and title columns as configured.\n\
      \  config (InputConfig): Configuration containing optional text_column and title_column.\
      \ If text_column is provided, a text column will be created from that column\
      \ if present; otherwise a warning is logged and no text column is created. If\
      \ text_column is None, no text column is created.\n  path (str): File path used\
      \ for logging warnings and as the default title when no title column is specified.\n\
      \nReturns:\n  pd.DataFrame: The input DataFrame augmented with id, text, and\
      \ title columns as configured.\n\nRaises:\n  Exception: If hashing or data processing\
      \ fails due to underlying data issues or library operations. The exact exception\
      \ depends on the hashing function or pandas operations."
  classes: []
- file: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
    name: _extract_edges
    signature: "def _extract_edges(\n    nodes_df: pd.DataFrame,\n    normalize_edge_weights:\
      \ bool = True,\n) -> pd.DataFrame"
    docstring: "Extract edges between nodes by linking nouns that co-occur in the\
      \ same text unit.\n\nNodes that appear in the same text unit are connected.\
      \ If normalize_edge_weights is True, PMI-based weights are computed via calculate_pmi_edge_weights.\n\
      \nArgs:\n  nodes_df (pd.DataFrame): DataFrame containing node information with\
      \ columns including id, title, frequency, and text_unit_ids.\n  normalize_edge_weights\
      \ (bool): If True, PMI-based weights are computed instead of raw counts. Default:\
      \ True.\n\nReturns:\n  pd.DataFrame: Edges DataFrame with columns [source, target,\
      \ weight, text_unit_ids]."
  - node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::extract
    name: extract
    signature: def extract(row)
    docstring: "Extract noun phrases from a row's text using a cache-backed analyzer.\n\
      \nArgs:\n  row: dict[str, Any] or pandas.Series: A mapping with a \"text\" key\
      \ containing the input text to analyze.\n\nReturns:\n  list[str]: The noun phrases\
      \ extracted from the text, or the cached result if available.\n\nRaises:\n \
      \ KeyError: If the input row does not contain the required \"text\" key, or\
      \ if a required key is missing during hashing in gen_sha512_hash."
  - node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
    name: _extract_nodes
    signature: "def _extract_nodes(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    num_threads: int = 4,\n    async_mode: AsyncType\
      \ = AsyncType.Threaded,\n    cache: PipelineCache | None = None,\n) -> pd.DataFrame"
    docstring: "Extract noun-phrase nodes from text units.\n\nThis internal helper\
      \ asynchronously extracts noun phrases from each text unit and aggregates them\
      \ into a node DataFrame. It does not create edges; edge creation is handled\
      \ by _extract_edges elsewhere.\n\nArgs:\n  text_unit_df (pd.DataFrame): Input\
      \ text units with schema [id, text].\n  text_analyzer (BaseNounPhraseExtractor):\
      \ Noun-phrase extractor used to determine noun phrases from text.\n  num_threads\
      \ (int): Number of worker threads to use for parallel processing.\n  async_mode\
      \ (AsyncType): Scheduling type to use for asynchronous operations.\n  cache\
      \ (PipelineCache | None): Optional cache for intermediate results; if None,\
      \ NoopPipelineCache is used.\n\nReturns:\n  pd.DataFrame: DataFrame with columns\
      \ [title, frequency, text_unit_ids].\n\nRaises:\n  Exceptions raised by derive_from_rows\
      \ and text_analyzer.extract propagate to the caller.\n\nNotes:\n  This function\
      \ is focused on noun-phrase extraction only; edge construction is performed\
      \ by _extract_edges elsewhere."
  - node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
    name: build_noun_graph
    signature: "def build_noun_graph(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
      \ BaseNounPhraseExtractor,\n    normalize_edge_weights: bool,\n    num_threads:\
      \ int = 4,\n    async_mode: AsyncType = AsyncType.Threaded,\n    cache: PipelineCache\
      \ | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    docstring: "Build a noun graph from text units.\n\nArgs:\n    text_unit_df (pd.DataFrame):\
      \ Input text units with at least columns 'id' and 'text'; used to extract noun\
      \ phrases for graph construction.\n    text_analyzer (BaseNounPhraseExtractor):\
      \ Noun-phrase extractor used to determine noun phrases from text.\n    normalize_edge_weights\
      \ (bool): If True, compute PMI-based weights for edges; otherwise use raw co-occurrence\
      \ counts.\n    num_threads (int): Number of worker threads to use for parallel\
      \ processing.\n    async_mode (AsyncType): Async processing mode to use (e.g.,\
      \ Threaded).\n    cache (PipelineCache | None): Optional cache for results;\
      \ if None, a NoopPipelineCache is used.\n\nReturns:\n    tuple[pd.DataFrame,\
      \ pd.DataFrame]: A pair of DataFrames (nodes_df, edges_df) representing extracted\
      \ noun nodes and their connecting edges (with optional weights)."
  classes: []
- file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.load_spacy_model
    name: load_spacy_model
    signature: "def load_spacy_model(\n        model_name: str, exclude: list[str]\
      \ | None = None\n    ) -> spacy.language.Language"
    docstring: "Load a SpaCy model.\n\nArgs:\n    model_name: Name of the SpaCy model\
      \ to load.\n    exclude: Optional list of components to exclude from loading.\n\
      \nReturns:\n    spacy.language.Language: The loaded SpaCy language object.\n\
      \nRaises:\n    OSError: If the model cannot be loaded (after attempting to download\
      \ if necessary)."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str | None,\n  \
      \      exclude_nouns: list[str] | None = None,\n        max_word_length: int\
      \ = 15,\n        word_delimiter: str = \" \",\n    ) -> None"
    docstring: "Initialize the base noun phrase extractor.\n\nArgs:\n    model_name:\
      \ The name of the SpaCy model to use, or None.\n    exclude_nouns: List of nouns\
      \ to exclude from extraction. If None, an empty list is used. Excluded nouns\
      \ are stored in uppercase.\n    max_word_length: Maximum length of a word to\
      \ consider when forming noun phrases.\n    word_delimiter: Delimiter used to\
      \ join words within a noun phrase.\n\nReturns:\n    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.extract
    name: extract
    signature: 'def extract(self, text: str) -> list[str]'
    docstring: "Extract noun phrases from text.\n\nArgs:\n    text (str): Text.\n\n\
      Returns:\n    list[str]: List of noun phrases."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the extractor.\n\nThis is an abstract\
      \ method and must be implemented by concrete subclasses. The base\nclass cannot\
      \ be instantiated due to ABCMeta and abstractmethod usage; the actual\nstring\
      \ representation is therefore defined by subclass implementations. At runtime,\n\
      the behavior depends on the subclass implementation rather than raising an error\
      \ in\nthe base class.\n\nArgs:\n    self (BaseNounPhraseExtractor): The instance\
      \ of the extractor.\n\nReturns:\n    str: The string representation used for\
      \ cache key generation, encoding the extractor's\n    configuration (e.g., model_name,\
      \ max_word_length, exclude_nouns, word_delimiter)."
  classes:
  - class_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor
    name: BaseNounPhraseExtractor
    docstring: 'BaseNounPhraseExtractor is an abstract base class that defines the
      interface and shared configuration for noun phrase extraction using a SpaCy
      model. Concrete subclasses implement the actual extraction logic and provide
      a meaningful string representation.


      Attributes:

      - model_name: The name of the SpaCy model to use, or None to avoid loading a
      model at initialization time.

      - exclude_nouns: Optional list of nouns to exclude from extraction. If None,
      an empty list is used. Subclasses may normalize or store this differently.

      - max_word_length: Maximum length of a word to consider when forming noun phrases.

      - word_delimiter: Delimiter used to join words within a noun phrase.


      Args:

      - model_name: The name of the SpaCy model to use, or None.

      - exclude_nouns: List of nouns to exclude from extraction. If None, an empty
      list is used.

      - max_word_length: Maximum length of a word to consider when forming noun phrases.

      - word_delimiter: Delimiter used to join words within a noun phrase.


      Returns:

      - None


      Raises:

      - TypeError: If the abstract base class is instantiated directly (enforced by
      the abstract base class mechanism).


      Notes:

      - Instantiation of this class is prevented by the abstract base class mechanism;
      concrete subclasses must implement extract and __str__.


      Load/spacy model helper:

      - load_spacy_model model_name with optional exclude: Load a SpaCy model by name
      and optionally exclude components. Returns a SpaCy language object.


      Subclass contract:

      - Concrete subclasses must implement extract to return a list of noun phrases
      from input text and __str__ to provide a meaningful string representation. The
      base class provides a helper for loading the SpaCy model but does not dictate
      specific extraction logic.'
    methods:
    - name: load_spacy_model
      signature: "def load_spacy_model(\n        model_name: str, exclude: list[str]\
        \ | None = None\n    ) -> spacy.language.Language"
    - name: __init__
      signature: "def __init__(\n        self,\n        model_name: str | None,\n\
        \        exclude_nouns: list[str] | None = None,\n        max_word_length:\
        \ int = 15,\n        word_delimiter: str = \" \",\n    ) -> None"
    - name: extract
      signature: 'def extract(self, text: str) -> list[str]'
    - name: __str__
      signature: def __str__(self) -> str
- file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "String representation of the extractor, used for cache key generation.\n\
      \nArgs:\n    self: The instance of the extractor.\n\nReturns:\n    The string\
      \ representation used for cache key generation."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
      \        noun_phrase_tags: list[str],\n    )"
    docstring: "\"\"\"Noun phrase extractor combining CFG-based noun-chunk extraction\
      \ and NER.\n\nCFG-based extraction was based on TextBlob's fast NP extractor\
      \ implementation:\nThis extractor tends to be faster than the dependency-parser-based\
      \ extractors but grammars may need to be changed for different languages.\n\n\
      Args:\n    model_name (str): SpaCy model name.\n    max_word_length (int): Maximum\
      \ length (in character) of each extracted word.\n    include_named_entities\
      \ (bool): Whether to include named entities in noun phrases\n    exclude_entity_tags\
      \ (list[str]): list of named entity tags to exclude in noun phrases.\n    exclude_pos_tags\
      \ (list[str]): List of POS tags to remove in noun phrases.\n    word_delimiter\
      \ (str): Delimiter for joining words.\n    noun_phrase_grammars (dict[tuple,\
      \ str]): CFG for matching noun phrases.\n\nReturns:\n    None\n\nRaises:\n \
      \   None\n\"\"\"\n}"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text using a combination of CFG-based noun\
      \ phrase matching and optional named-entity recognition (NER), with post-filtering\
      \ rules applied to remove unlikely phrases.\n\nThis extractor is configurable\
      \ via instance attributes that influence its behavior and the NLP model loaded:\n\
      \n- include_named_entities (bool): If True, include named entities in the noun\
      \ phrase results; otherwise, NER is disabled by loading the model with the ner\
      \ component excluded.\n- exclude_entity_tags (list[str]): Named entity labels\
      \ to exclude from consideration (e.g., PERSON, ORG).\n- exclude_pos_tags (list[str]):\
      \ POS tags to remove from consideration when forming CFG-based noun phrases.\n\
      - noun_phrase_grammars (dict[tuple[str,str], str]): CFG rules used to merge\
      \ adjacent words into noun phrases based on their POS tags.\n- noun_phrase_tags\
      \ (set[str]): The POS-like labels that qualify a token as a noun phrase after\
      \ CFG merging.\n- word_delimiter (str): Delimiter used to join tokens when merging\
      \ matched words.\n- model_name, max_word_length, exclude_nouns, etc.: Other\
      \ configuration inherited from the base extractor that affect tokenization,\
      \ length filtering, and joining.\n\nReturns:\n    list[str]: A deduplicated\
      \ list of noun phrases extracted from the input text. Duplicates are removed\
      \ by using a set; the resulting order is not guaranteed and may appear in arbitrary\
      \ order. If you need deterministic ordering, post-process the results (e.g.,\
      \ by preserving original offsets or sorting).\n\nRaises:\n    RuntimeError,\
      \ ValueError, or spaCy-related exceptions that may be raised by the underlying\
      \ NLP pipeline if the configured model cannot be loaded, if processing fails,\
      \ or if input text is not valid."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract_cfg_matches
    name: extract_cfg_matches
    signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
    docstring: "Return noun phrases that match a given context-free grammar.\n\nArgs:\n\
      \    doc (Doc): The spaCy Doc to process for noun phrase extraction.\n\nReturns:\n\
      \    list[tuple[str, str]]: A list of noun phrases as (text, tag) pairs, where\
      \ text is the merged noun phrase string and tag is the corresponding noun phrase\
      \ tag."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str],\
      \ entities: set[str] | None = None\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\n\
      Args:\n  noun_chunk: tuple[str, str] - The noun chunk as (text, pos) where text\
      \ is the chunk text and pos is its part-of-speech tag.\n  entities: set[str]\
      \ | None - Optional set of entity strings to consider when validating the noun\
      \ chunk. If provided and the noun_chunk[0] is in entities, is_valid_entity is\
      \ computed.\n\nReturns:\n  dict[str, Any] - A dictionary containing the following\
      \ keys:\n      cleaned_tokens: List[str] - Tokens after removing excluded nouns.\n\
      \      cleaned_text: str - The cleaned tokens joined by self.word_delimiter,\
      \ with newline characters removed, and converted to upper-case.\n      is_valid_entity:\
      \ bool - True if the noun chunk corresponds to a valid entity given the tokens\
      \ and optional entities.\n      has_proper_nouns: bool - True if noun_chunk[1]\
      \ == \"PROPN\".\n      has_compound_words: bool - True if cleaned_tokens form\
      \ a hyphenated compound.\n      has_valid_tokens: bool - True if all cleaned_tokens\
      \ pass the maximum word length constraint.\n\nRaises:\n  None"
  classes:
  - class_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor
    name: CFGNounPhraseExtractor
    docstring: CFGNounPhraseExtractor is a fast noun phrase extractor that combines
      CFG-based noun-chunk matching with optional named-entity recognition (NER) to
      support noun-phrase extraction for graph-building. It processes text with a
      SpaCy model, applies configured CFG grammars to identify candidate noun phrases,
      and then filters and merges results according to the configured rules. The extractor
      is configurable via instance attributes and aims to be robust across languages
      with grammar-driven matching and optional NER enrichment.
    methods:
    - name: __str__
      signature: def __str__(self) -> str
    - name: __init__
      signature: "def __init__(\n        self,\n        model_name: str,\n       \
        \ max_word_length: int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
        \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns:\
        \ list[str],\n        word_delimiter: str,\n        noun_phrase_grammars:\
        \ dict[tuple, str],\n        noun_phrase_tags: list[str],\n    )"
    - name: extract
      signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    - name: extract_cfg_matches
      signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
    - name: _tag_noun_phrases
      signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str],\
        \ entities: set[str] | None = None\n    ) -> dict[str, Any]"
- file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.get_np_extractor
    name: get_np_extractor
    signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
    docstring: "Get the noun phrase extractor instance based on the configured type.\n\
      \nArgs:\n    cls: The class (used as a classmethod parameter).\n    config:\
      \ TextAnalyzerConfig containing extractor_type and related options such as model_name,\
      \ max_word_length, include_named_entities, exclude_entity_tags, exclude_pos_tags,\
      \ exclude_nouns, word_delimiter, noun_phrase_grammars, and noun_phrase_tags.\n\
      \nReturns:\n    BaseNounPhraseExtractor: An instance of the selected noun phrase\
      \ extractor (Syntactic, CFG, or RegexEnglish)."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.register
    name: register
    signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
    docstring: "Register a noun phrase extractor in NounPhraseExtractorFactory by\
      \ adding it to the class-level registry np_extractor_types. This is a classmethod\
      \ that updates the mapping from extractor type identifiers to extractor classes,\
      \ enabling get_np_extractor to instantiate the correct extractor based on configuration.\n\
      \nArgs:\n    cls: The class on which this classmethod is invoked.\n    np_extractor_type:\
      \ str The identifier for the noun phrase extractor type.\n    np_extractor:\
      \ type The extractor class/type to register for the given type.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::create_noun_phrase_extractor
    name: create_noun_phrase_extractor
    signature: "def create_noun_phrase_extractor(\n    analyzer_config: TextAnalyzerConfig,\n\
      ) -> BaseNounPhraseExtractor"
    docstring: "Create a noun phrase extractor from a configuration.\n\nArgs:\n  \
      \  analyzer_config (TextAnalyzerConfig): Configuration for text analysis used\
      \ to configure the noun phrase extractor.\n\nReturns:\n    BaseNounPhraseExtractor:\
      \ An instance of a noun phrase extractor created according to the given configuration.\n\
      \nRaises:\n    Exception: If the underlying factory fails to create the extractor\
      \ (propagates from NounPhraseExtractorFactory.get_np_extractor)."
  classes:
  - class_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory
    name: NounPhraseExtractorFactory
    docstring: "Factory for selecting and instantiating noun phrase extractors based\
      \ on configuration.\n\nThis class provides a registry of available noun phrase\
      \ extractors and a single entry point to instantiate the appropriate extractor\
      \ according to a TextAnalyzerConfig. It relies on a class-level mapping from\
      \ extractor type identifiers to extractor classes to resolve and instantiate\
      \ the requested extractor.\n\nKey attributes\n- np_extractor_types: ClassVar\
      \ mapping from extractor type identifiers (str) to extractor classes (the registry).\n\
      \nMethods\n- get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor:\
      \ Get the noun phrase extractor instance based on the configured type.\n  Args:\
      \ cls: The class (used as a classmethod parameter). config: TextAnalyzerConfig\
      \ containing extractor_type and related options such as model_name, max_word_length,\
      \ include_named_entities, exclude_entity_tags, exclude_pos_tags, exclude_nouns,\
      \ word_delimiter, noun_phrase_grammars, and noun_phrase_tags.\n  Returns: BaseNounPhraseExtractor\n\
      \  Raises: Exceptions raised by underlying extractors or by configuration issues\
      \ may propagate.\n\n- register(cls, np_extractor_type: str, np_extractor: type):\
      \ Register a noun phrase extractor in NounPhraseExtractorFactory by adding it\
      \ to the class-level registry np_extractor_types.\n  Args: cls: The class on\
      \ which this classmethod is invoked. np_extractor_type: The string identifier\
      \ for the extractor type. np_extractor: The extractor class to register.\n \
      \ Returns: None\n  Raises: Exceptions raised during registration may propagate."
    methods:
    - name: get_np_extractor
      signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
    - name: register
      signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
- file: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
    name: is_compound
    signature: 'def is_compound(tokens: list[str]) -> bool'
    docstring: "Return True if any token in the provided list is a hyphenated compound\
      \ token.\n\nArgs:\n    tokens: List[str] - The list of tokens to inspect.\n\n\
      Returns:\n    bool - True if at least one token contains a hyphen, has length\
      \ greater than 1 after stripping whitespace, and splits into more than one part\
      \ when split by hyphen; otherwise False.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
    name: has_valid_token_length
    signature: 'def has_valid_token_length(tokens: list[str], max_length: int) ->
      bool'
    docstring: "\"\"\"Check if all tokens have valid length.\n\nArgs:\n    tokens:\
      \ List of tokens to validate lengths for.\n    max_length: Maximum allowed length\
      \ for any token.\n\nReturns:\n    bool: True if all tokens have length <= max_length,\
      \ otherwise False.\n\"\"\""
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
    name: is_valid_entity
    signature: 'def is_valid_entity(entity: tuple[str, str], tokens: list[str]) ->
      bool'
    docstring: "Check if the given entity is valid with respect to the provided tokens.\n\
      \nArgs:\n    entity: tuple[str, str] - The entity as (text, label). The label\
      \ indicates the category of the entity, e.g., CARDINAL or ORDINAL.\n    tokens:\
      \ list[str] - The tokens associated with the entity used to determine validity.\n\
      \nReturns:\n    bool - True if the entity is valid according to the validation\
      \ rules; otherwise False.\n\nRaises:\n    None"
  classes: []
- file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text using TextBlob's noun phrase extractor\
      \ with post-filtering.\n\nThis English-only extractor relies on TextBlob and\
      \ NLTK data and downloads required corpora on first use\n(brown, treebank, averaged_perceptron_tagger_eng)\
      \ and tokenizers (punkt, punkt_tab). It uses instance\nproperties from the base\
      \ extractor (exclude_nouns, max_word_length, word_delimiter) to filter and format\
      \ results.\n\nThe method collects noun phrases from TextBlob and applies filtering\
      \ based on:\n- presence of a proper noun within the phrase\n- number of cleaned\
      \ tokens\n- presence of a compound word\nand ensures all tokens are valid and\
      \ within max_word_length. The resulting phrases are normalized by removing\n\
      excluded tokens, joining with the configured delimiter, converting to uppercase,\
      \ and deduplicating.\n\nArgs:\n    text: The input text to extract noun phrases\
      \ from.\n\nReturns:\n    List[str]: A list of cleaned noun phrases. Duplicates\
      \ are removed; the order is not guaranteed.\n\nRaises:\n    May propagate exceptions\
      \ from TextBlob or NLTK if required resources cannot be downloaded or loaded."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
      \ list[str] | None = None\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun phrase for filtering.\n\nArgs:\n    noun_phrase:\
      \ The noun phrase to analyze.\n    all_proper_nouns: Optional list of proper\
      \ nouns (uppercase) to consider when detecting proper nouns within the phrase.\n\
      \nReturns:\n    dict[str, Any]: A dictionary containing the following keys:\n\
      \        cleaned_tokens: List[str] of tokens after removing excluded nouns.\n\
      \        cleaned_text: str of cleaned tokens joined by the configured delimiter,\
      \ with newline removed and uppercased.\n        has_proper_nouns: True if any\
      \ cleaned token matches an entry in all_proper_nouns.\n        has_compound_words:\
      \ True if any cleaned token is a hyphenated compound word.\n        has_valid_tokens:\
      \ True if all cleaned tokens are valid per regex and length constraints."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the regex extractor, used for cache\
      \ key generation.\n\nArgs:\n    self: The instance of the extractor.\n\nReturns:\n\
      \    str: The cache key string encoding the extractor's configuration, built\
      \ from exclude_nouns, max_word_length, and word_delimiter.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n\
      \        max_word_length: int,\n        word_delimiter: str,\n    )"
    docstring: "Initialize a regular-expression-based noun phrase extractor for English.\n\
      \nNOTE: This is the extractor used in the first benchmarking of LazyGraphRAG\
      \ but it only works for English. It is much faster but likely less accurate\
      \ than the syntactic parser-based extractor. TODO: Reimplement this using SpaCy\
      \ to remove TextBlob dependency.\n\nArgs:\n    exclude_nouns: list[str] \u2014\
      \ List of nouns to exclude from extraction.\n    max_word_length: int \u2014\
      \ Maximum length (in characters) of each extracted word.\n    word_delimiter:\
      \ str \u2014 Delimiter for joining words.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor
    name: RegexENNounPhraseExtractor
    docstring: "Regular-expression-based English noun phrase extractor used for building\
      \ noun graphs.\n\nNOTE: This extractor was used in the first benchmarking of\
      \ LazyGraphRAG and only works for English. It is much faster but likely less\
      \ accurate than the syntactic parser-based extractor. TODO: Reimplement this\
      \ using SpaCy to remove TextBlob dependency.\n\nArgs:\n  exclude_nouns: Nouns\
      \ to exclude from extraction.\n  max_word_length: Maximum length of words to\
      \ consider when forming noun phrases.\n  word_delimiter: Delimiter used to join\
      \ tokens within noun phrases."
    methods:
    - name: extract
      signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    - name: _tag_noun_phrases
      signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
        \ list[str] | None = None\n    ) -> dict[str, Any]"
    - name: __str__
      signature: def __str__(self) -> str
    - name: __init__
      signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n\
        \        max_word_length: int,\n        word_delimiter: str,\n    )"
- file: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py::download_if_not_exists
    name: download_if_not_exists
    signature: def download_if_not_exists(resource_name) -> bool
    docstring: "Download nltk resources if they haven't been already.\n\nArgs:\n \
      \   resource_name: The name of the nltk resource to locate or download.\n\n\
      Returns:\n    bool: True if the resource was found without downloading; False\
      \ if the resource was not found and had to be downloaded."
  classes: []
- file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  functions:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text. Noun phrases may include named entities\
      \ and noun chunks, which are filtered based on some heuristics.\n\nArgs:\n \
      \   text: Text.\n\nReturns:\n    List of noun phrases."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Returns the string representation used for cache key generation.\n\
      \nThe returned string encodes the extractor's configuration to uniquely identify\n\
      cache entries. It is constructed from the following attributes: model_name,\n\
      max_word_length, include_named_entities, exclude_entity_tags, exclude_pos_tags,\n\
      exclude_nouns, and word_delimiter.\n\nParameters:\n    self: The instance of\
      \ the extractor.\n\nReturns:\n    str: The cache-key string in the form:\n \
      \       syntactic_<model_name>_<max_word_length>_<include_named_entities>_<exclude_entity_tags>_<exclude_pos_tags>_<exclude_nouns>_<word_delimiter>\n\
      \nThere are no other parameters beyond self."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n    )"
    docstring: "Initialize the SyntacticNounPhraseExtractor.\n\nThis initializer configures\
      \ the base class and prepares the SpaCy NLP pipeline used for noun phrase extraction\
      \ via dependency parsing and optional NER.\n\nIt delegates to the base class\
      \ BaseNounPhraseExtractor to set common configuration, including excluding nouns.\
      \ The call to the base initializer passes model_name, max_word_length, exclude_nouns,\
      \ and word_delimiter.\n\nArgs:\n    model_name: SpaCy model name.\n    max_word_length:\
      \ Maximum length in character of each extracted word.\n    include_named_entities:\
      \ Whether to include named entities in noun phrases. When True, named entities\
      \ are loaded and considered; when False, NER is disabled in the pipeline.\n\
      \    exclude_entity_tags: List of named entity tags to exclude in noun phrases.\n\
      \    exclude_pos_tags: List of POS tags to remove in noun phrases.\n    exclude_nouns:\
      \ List of nouns to exclude (passed to the base initializer).\n    word_delimiter:\
      \ Delimiter for joining words.\n\nReturns:\n    None\n\nNotes:\n    This initializer\
      \ sets up the self.nlp pipeline and stores configuration attributes for later\
      \ use.\n\nDynamic model loading behavior:\n    If include_named_entities is\
      \ False: load_spacy_model with exclude including lemmatizer and ner to disable\
      \ NER.\n    If include_named_entities is True: load_spacy_model with exclude\
      \ including lemmatizer to keep NER enabled."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities:\
      \ list[Span]\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\n\
      Args:\n  noun_chunk: Span - The noun chunk to analyze.\n  entities: list[Span]\
      \ - Entities to consider when validating the noun chunk; used to determine if\
      \ the chunk matches an entity and, if so, whether it's a valid entity.\n\nReturns:\n\
      \  dict[str, Any] - A dictionary containing the following keys:\n    cleaned_tokens:\
      \ List[Token] - The tokens from the noun_chunk after filtering out tokens based\
      \ on excluded POS tags, excluded nouns, spaces, and punctuation.\n    cleaned_text:\
      \ str - The tokens joined using the configured delimiter (word_delimiter), with\
      \ newline characters removed and converted to uppercase.\n    is_valid_entity:\
      \ bool - True if the noun_chunk corresponds to a valid entity among the provided\
      \ entities; otherwise False.\n    has_proper_nouns: bool - True if any of the\
      \ cleaned tokens is a proper noun.\n    has_compound_words: bool - True if the\
      \ cleaned token texts form a hyphenated compound (as determined by is_compound).\n\
      \    has_valid_tokens: bool - True if all cleaned token texts satisfy the maximum\
      \ token length constraint (has_valid_token_length with self.max_word_length).\n\
      \nRaises:\n  None."
  classes:
  - class_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor
    name: SyntacticNounPhraseExtractor
    docstring: "SyntacticNounPhraseExtractor extracts noun phrases from text using\
      \ syntactic parsing with SpaCy, with configurable filters and optional named-entity\
      \ integration.\n\nArgs:\n    model_name: The name of the NLP model used by the\
      \ underlying SpaCy pipeline.\n    max_word_length: Maximum number of words allowed\
      \ in a noun phrase.\n    include_named_entities: Whether to include named entities\
      \ as noun phrases.\n    exclude_entity_tags: List of entity tags to exclude\
      \ from consideration.\n    exclude_pos_tags: List of part-of-speech tags to\
      \ exclude from noun phrases.\n    exclude_nouns: List of noun strings to exclude\
      \ from results.\n    word_delimiter: Delimiter used to join tokens into a noun\
      \ phrase.\n\nReturns:\n    None\n\nAttributes:\n    model_name: The NLP model\
      \ identifier used for parsing.\n    max_word_length: Maximum length of a noun\
      \ phrase by word count.\n    include_named_entities: Flag indicating whether\
      \ named entities are included.\n    exclude_entity_tags: Entity tags to skip\
      \ during extraction.\n    exclude_pos_tags: POS tags to skip when forming noun\
      \ phrases.\n    exclude_nouns: Specific nouns to exclude from results.\n   \
      \ word_delimiter: Delimiter used to join tokens inside a noun phrase."
    methods:
    - name: extract
      signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    - name: __str__
      signature: def __str__(self) -> str
    - name: __init__
      signature: "def __init__(\n        self,\n        model_name: str,\n       \
        \ max_word_length: int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
        \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns:\
        \ list[str],\n        word_delimiter: str,\n    )"
    - name: _tag_noun_phrases
      signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities:\
        \ list[Span]\n    ) -> dict[str, Any]"
- file: graphrag/index/operations/chunk_text/bootstrap.py
  functions:
  - node_id: graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
    name: bootstrap
    signature: def bootstrap()
    docstring: "Bootstrap initialization for NLTK resources.\n\nDownloads and prepares\
      \ the required NLTK data on the first call, and sets the module-level flag initialized_nltk\
      \ to True to prevent repeated work.\n\nThis function downloads the following\
      \ resources and ensures WordNet is loaded: punkt, punkt_tab, averaged_perceptron_tagger,\
      \ averaged_perceptron_tagger_eng, maxent_ne_chunker, maxent_ne_chunker_tab,\
      \ words, and wordnet; it also calls wn.ensure_loaded().\n\nReturns:\n    None\n\
      \nRaises:\n    ImportError: If the nltk package or required submodules are unavailable."
  classes: []
- file: graphrag/index/operations/chunk_text/chunk_text.py
  functions:
  - node_id: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
    name: _get_num_total
    signature: 'def _get_num_total(output: pd.DataFrame, column: str) -> int'
    docstring: 'Compute the total number of elements in a DataFrame column, counting
      strings as a single element and non-string entries by their length.


      Args:

      output: pandas.DataFrame The DataFrame containing the target column.

      column: str The name of the column to process.


      Returns:

      int The total number of elements in the specified column; strings contribute
      1 each, non-string entries contribute their length.'
  - node_id: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
    name: run_strategy
    signature: "def run_strategy(\n    strategy_exec: ChunkStrategy,\n    input: ChunkInput,\n\
      \    config: ChunkingConfig,\n    tick: ProgressTicker,\n) -> list[str | tuple[list[str]\
      \ | None, str, int]]"
    docstring: "Run the given chunking strategy on the input data and return the produced\
      \ chunks.\n\nArgs:\n    strategy_exec: ChunkStrategy\n        The strategy function\
      \ to execute to generate text chunks.\n    input: ChunkInput\n        The input\
      \ data to chunk. May be a string or a list of strings, or a list\n        of\
      \ tuples of (document_id, text content).\n    config: ChunkingConfig\n     \
      \   Configuration for chunking, including size, overlap, and encoding model.\n\
      \    tick: ProgressTicker\n        Progress ticker used to report progress.\n\
      \nReturns:\n    list[str | tuple[list[str] | None, str, int]]\n        A list\
      \ of results. If the input was a simple string, returns a list of\n        text_chunk\
      \ strings. Otherwise each element is either:\n        - a text_chunk string,\
      \ or\n        - a tuple (doc_ids, text_chunk, n_tokens) where doc_ids is a list\
      \ of\n          document ids corresponding to the source documents for that\
      \ chunk,\n          text_chunk is the chunk text, and n_tokens is the number\
      \ of tokens\n          in the chunk."
  - node_id: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
    name: load_strategy
    signature: 'def load_strategy(strategy: ChunkStrategyType) -> ChunkStrategy'
    docstring: "Load the strategy method for the given chunking strategy.\n\nArgs:\n\
      \    strategy (ChunkStrategyType): The type of chunk strategy to load. If ChunkStrategyType.tokens,\
      \ the tokens strategy is returned. If ChunkStrategyType.sentence, NLP resources\
      \ are bootstrapped and the sentences strategy is returned.\n\nReturns:\n   \
      \ ChunkStrategy: The loaded strategy callable corresponding to the provided\
      \ strategy type.\n\nRaises:\n    ValueError: If an unknown strategy is provided."
  - node_id: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
    name: chunk_text
    signature: "def chunk_text(\n    input: pd.DataFrame,\n    column: str,\n    size:\
      \ int,\n    overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    callbacks: WorkflowCallbacks,\n) -> pd.Series"
    docstring: "Chunk a piece of text into smaller pieces.\n\nArgs:\n    input: DataFrame\
      \ containing the data to chunk.\n    column: The name of the column containing\
      \ the text to chunk, this can either be a column with text, or a column with\
      \ a list[tuple[doc_id, str]].\n    size: The chunk size to use.\n    overlap:\
      \ The chunk overlap to use.\n    encoding_model: The encoding model to use for\
      \ chunking.\n    strategy: The strategy to use to chunk the text, see below\
      \ for more details.\n    callbacks: WorkflowCallbacks for progress reporting.\n\
      \nReturns:\n    A pandas Series where each element corresponds to the chunked\
      \ result for the input row. Each element is a list of chunks; for string inputs,\
      \ each item is a string text chunk. For inputs with document IDs, each item\
      \ is a tuple of (doc_ids, text_chunk, n_tokens).\n\nRaises:\n    ValueError:\
      \ If an unknown strategy is provided."
  classes: []
- file: graphrag/index/operations/chunk_text/strategies.py
  functions:
  - node_id: graphrag/index/operations/chunk_text/strategies.py::encode
    name: encode
    signature: 'def encode(text: str) -> list[int]'
    docstring: "Encode the input text into token IDs using the configured encoding\
      \ model.\n\nArgs:\n    text (str): The input to encode. If not a string, it\
      \ will be converted to a string.\n\nReturns:\n    list[int]: The encoded token\
      \ IDs produced by the encoding model.\n\nRaises:\n    Exception: If encoding\
      \ fails with the configured encoding model."
  - node_id: graphrag/index/operations/chunk_text/strategies.py::decode
    name: decode
    signature: 'def decode(tokens: list[int]) -> str'
    docstring: "\"\"\"Decode a list of tokens back into a string.\n\nArgs:\n    tokens\
      \ (list[int]): A list of tokens to decode.\n\nReturns:\n    str: The decoded\
      \ string from the list of tokens.\n\nRaises:\n    Exception: If decoding fails\
      \ due to an underlying error in the encoding.\n\"\"\""
  - node_id: graphrag/index/operations/chunk_text/strategies.py::run_sentences
    name: run_sentences
    signature: "def run_sentences(\n    input: list[str], _config: ChunkingConfig,\
      \ tick: ProgressTicker\n) -> Iterable[TextChunk]"
    docstring: "Chunks text into multiple parts by sentence.\n\nArgs:\n  input: list[str]\
      \ - list of input documents to chunk into sentences.\n  _config: ChunkingConfig\
      \ - chunking configuration (unused by this strategy).\n  tick: ProgressTicker\
      \ - progress reporter; invoked to indicate progress after processing each input\
      \ document.\n\nReturns:\n  Iterable[TextChunk] - yields TextChunk objects for\
      \ each sentence, with text_chunk set to the sentence and source_doc_indices\
      \ containing the index of the source document.\n\nRaises:\n  Exceptions raised\
      \ by nltk.sent_tokenize during sentence tokenization."
  - node_id: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
    name: get_encoding_fn
    signature: def get_encoding_fn(encoding_name)
    docstring: "Get encoding functions for a given encoding model.\n\nArgs:\n- encoding_name:\
      \ str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
      \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
      \ list[int]] - Encodes input text into token ids using the selected encoding;\
      \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
      \ str] - Decodes a list of token ids back into a string using the selected encoding.\n\
      \nRaises:\n- Exception: Propagates exceptions raised by tiktoken.get_encoding\
      \ when an invalid encoding_name is provided."
  - node_id: graphrag/index/operations/chunk_text/strategies.py::run_tokens
    name: run_tokens
    signature: "def run_tokens(\n    input: list[str],\n    config: ChunkingConfig,\n\
      \    tick: ProgressTicker,\n) -> Iterable[TextChunk]"
    docstring: "Chunks text into chunks based on encoding tokens.\n\nArgs:\n    input:\
      \ list[str] - The input texts to be chunked.\n    config: ChunkingConfig - Chunking\
      \ configuration. Uses:\n        size: number of tokens per chunk,\n        overlap:\
      \ number of overlapping tokens between consecutive chunks,\n        encoding_model:\
      \ name of the encoding model used to tokenize.\n    tick: ProgressTicker - Progress\
      \ reporter; invoked to indicate progress.\n\nReturns:\n    Iterable[TextChunk]\
      \ - An iterable of TextChunk objects representing the resulting chunks.\n\n\
      Raises:\n    Exception - Propagates exceptions raised by underlying encoding\
      \ retrieval or tokenization."
  classes: []
- file: graphrag/index/operations/cluster_graph.py
  functions:
  - node_id: graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
    name: _compute_leiden_communities
    signature: "def _compute_leiden_communities(\n    graph: nx.Graph | nx.DiGraph,\n\
      \    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int | None = None,\n\
      ) -> tuple[dict[int, dict[str, int]], dict[int, int]]"
    docstring: "Compute Leiden root communities for a graph and return level wise\
      \ node to community mappings and a cluster hierarchy.\n\nArgs:\n    graph: nx.Graph\
      \ | nx.DiGraph\n        Input graph from which to compute Leiden communities.\
      \ If use_lcc is True, the graph's largest connected component is used.\n   \
      \ max_cluster_size: int\n        Maximum size of a cluster allowed by Leiden\
      \ algorithm.\n    use_lcc: bool\n        If True, compute on the largest connected\
      \ component of the input graph.\n    seed: int | None\n        Random seed for\
      \ reproducibility in the Leiden clustering.\n\nReturns:\n    tuple[dict[int,\
      \ dict[str, int]], dict[int, int]]\n        A tuple containing:\n        - node_id_to_community_map:\
      \ dict[int, dict[str, int]]\n            Mapping from level to a dictionary\
      \ that maps node_id to its community id.\n        - parent_mapping: dict[int,\
      \ int]\n            Mapping from cluster_id to its parent cluster id, or -1\
      \ if there is no parent.\n\nRaises:\n    Propagates exceptions from the underlying\
      \ libraries or graph processing steps; there are no explicit raises documented\
      \ for this function."
  - node_id: graphrag/index/operations/cluster_graph.py::cluster_graph
    name: cluster_graph
    signature: "def cluster_graph(\n    graph: nx.Graph,\n    max_cluster_size: int,\n\
      \    use_lcc: bool,\n    seed: int | None = None,\n) -> Communities"
    docstring: "Compute hierarchical Leiden-based clusters for the input graph and\
      \ return them as a list of (level, cluster_id, parent_cluster_id, nodes).\n\n\
      Args:\n    graph: nx.Graph\n        Input graph on which to perform clustering.\n\
      \    max_cluster_size: int\n        Maximum size of a cluster allowed by Leiden\
      \ algorithm.\n    use_lcc: bool\n        If True, operate on the largest connected\
      \ component of the input graph.\n    seed: int | None\n        Random seed for\
      \ reproducibility of the clustering process.\n\nReturns:\n    Communities\n\
      \        A list of tuples (level, cluster_id, parent_cluster_id, nodes) describing\
      \ the\n        clusters found at different hierarchical levels, the cluster's\
      \ parent, and the\n        member node identifiers."
  classes: []
- file: graphrag/index/operations/compute_degree.py
  functions:
  - node_id: graphrag/index/operations/compute_degree.py::compute_degree
    name: compute_degree
    signature: 'def compute_degree(graph: nx.Graph) -> pd.DataFrame'
    docstring: "Create a new DataFrame with the degree of each node in the graph.\n\
      \nArgs:\n    graph (nx.Graph): NetworkX graph from which to compute node degrees.\n\
      \nReturns:\n    pd.DataFrame: DataFrame with one row per node, containing the\
      \ columns:\n        title: the node identifier\n        degree: the degree of\
      \ the node as an integer."
  classes: []
- file: graphrag/index/operations/compute_edge_combined_degree.py
  functions:
  - node_id: graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
    name: _degree_colname
    signature: 'def _degree_colname(column: str) -> str'
    docstring: "Return the degree column name derived from the given column.\n\nArgs:\n\
      \    column: The original column name.\n\nReturns:\n    The degree column name\
      \ as a string, formed by appending '_degree' to the input column name."
  - node_id: graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
    name: join_to_degree
    signature: 'def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame'
    docstring: "Join the input DataFrame with the node degree information for the\
      \ specified column and return the result augmented with a degree column.\n\n\
      Args:\n  df: The input DataFrame to augment.\n  column: The column name in df\
      \ used to align with the node degree data.\n\nReturns:\n  A DataFrame with an\
      \ additional column named '<column>_degree' containing the degree values; missing\
      \ degrees are filled with 0."
  - node_id: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
    name: compute_edge_combined_degree
    signature: "def compute_edge_combined_degree(\n    edge_df: pd.DataFrame,\n  \
      \  node_degree_df: pd.DataFrame,\n    node_name_column: str,\n    node_degree_column:\
      \ str,\n    edge_source_column: str,\n    edge_target_column: str,\n) -> pd.Series"
    docstring: "Compute the combined degree for each edge in a graph.\n\nArgs:\n \
      \   edge_df: pd.DataFrame\n        The DataFrame containing edges with columns\
      \ for source and target nodes.\n    node_degree_df: pd.DataFrame\n        DataFrame\
      \ containing node degree information, keyed by node name.\n    node_name_column:\
      \ str\n        The column in node_degree_df that contains node names to join\
      \ on.\n    node_degree_column: str\n        The column in node_degree_df that\
      \ contains the degree values to join.\n    edge_source_column: str\n       \
      \ The column name in edge_df that identifies the source node for each edge.\n\
      \    edge_target_column: str\n        The column name in edge_df that identifies\
      \ the target node for each edge.\n\nReturns:\n    pd.Series\n        A Series\
      \ of the per-edge combined degree values (sum of the source and target node\
      \ degrees; missing degrees are treated as 0).\n\nRaises:\n    Propagates exceptions\
      \ raised by pandas operations or invalid inputs."
  classes: []
- file: graphrag/index/operations/create_graph.py
  functions:
  - node_id: graphrag/index/operations/create_graph.py::create_graph
    name: create_graph
    signature: "def create_graph(\n    edges: pd.DataFrame,\n    edge_attr: list[str\
      \ | int] | None = None,\n    nodes: pd.DataFrame | None = None,\n    node_id:\
      \ str = \"title\",\n) -> nx.Graph"
    docstring: "Create a NetworkX graph from edges and optional nodes dataframes.\n\
      \nArgs:\n    edges (pd.DataFrame): DataFrame containing edge information for\
      \ the graph.\n    edge_attr (list[str | int] | None): List of edge attribute\
      \ column names (or None) to include as edge attributes.\n    nodes (pd.DataFrame\
      \ | None): Optional DataFrame containing node attributes to add to the graph.\
      \ If provided, nodes are added with attributes from this DataFrame.\n    node_id\
      \ (str): Column name to use as the node identifier when adding nodes from the\
      \ nodes DataFrame.\n\nReturns:\n    graph (nx.Graph): The constructed NetworkX\
      \ Graph.\n\nRaises:\n    KeyError: If the specified node_id column does not\
      \ exist in the nodes DataFrame when indexing.\n    ValueError: If the edges\
      \ DataFrame is missing required columns or edge_attr is invalid for from_pandas_edgelist.\n\
      \    TypeError: If input data types are incompatible with the underlying operations."
  classes: []
- file: graphrag/index/operations/embed_graph/embed_graph.py
  functions:
  - node_id: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
    name: embed_graph
    signature: "def embed_graph(\n    graph: nx.Graph,\n    config: EmbedGraphConfig,\n\
      ) -> NodeEmbeddings"
    docstring: "Embed a graph into a vector space using node2vec.\n\nThe graph is\
      \ expected to be in nx.Graph format. The operation outputs a mapping between\
      \ node name and vector.\n\nArgs:\n    graph: Input graph to embed.\n    config:\
      \ Embedding configuration, including dimensions, num_walks, walk_length, window_size,\
      \ iterations, random_seed, and use_lcc.\n\nReturns:\n    NodeEmbeddings: Mapping\
      \ from node name to embedding vector.\n\nRaises:\n    Exception: If underlying\
      \ operations fail."
  classes: []
- file: graphrag/index/operations/embed_graph/embed_node2vec.py
  functions:
  - node_id: graphrag/index/operations/embed_graph/embed_node2vec.py::embed_node2vec
    name: embed_node2vec
    signature: "def embed_node2vec(\n    graph: nx.Graph | nx.DiGraph,\n    dimensions:\
      \ int = 1536,\n    num_walks: int = 10,\n    walk_length: int = 40,\n    window_size:\
      \ int = 2,\n    iterations: int = 3,\n    random_seed: int = 86,\n) -> NodeEmbeddings"
    docstring: "Generate node embeddings using Node2Vec.\n\nArgs:\n    graph: Graph\
      \ or DiGraph on which to compute node embeddings.\n    dimensions: Dimensionality\
      \ of the embeddings.\n    num_walks: Number of random walks to start at every\
      \ node.\n    walk_length: Length of each random walk.\n    window_size: Window\
      \ size parameter for embedding model.\n    iterations: Number of training iterations.\n\
      \    random_seed: Seed for the random number generator.\n\nReturns:\n    NodeEmbeddings:\
      \ Object containing embeddings (np.ndarray) and the corresponding list of node\
      \ identifiers (list[str])."
  classes: []
- file: graphrag/index/operations/embed_text/embed_text.py
  functions:
  - node_id: graphrag/index/operations/embed_text/embed_text.py::load_strategy
    name: load_strategy
    signature: 'def load_strategy(strategy: TextEmbedStrategyType) -> TextEmbeddingStrategy'
    docstring: "Load the embedding strategy callable for the given strategy type.\n\
      \nArgs:\n    strategy: TextEmbedStrategyType - The strategy type used to determine\
      \ which embedding strategy to load.\n\nReturns:\n    TextEmbeddingStrategy:\
      \ The loaded strategy callable corresponding to the provided strategy.\n\nRaises:\n\
      \    ValueError: If an unknown strategy is provided."
  - node_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of this TextEmbedStrategyType enum\
      \ member.\n\nArgs:\n    self: TextEmbedStrategyType, the enum member to represent\
      \ as a string.\n\nReturns:\n    str: The enum member's value enclosed in double\
      \ quotes.\n\"\"\""
  - node_id: graphrag/index/operations/embed_text/embed_text.py::_create_vector_store
    name: _create_vector_store
    signature: "def _create_vector_store(\n    vector_store_config: dict, index_name:\
      \ str, embedding_name: str | None = None\n) -> BaseVectorStore"
    docstring: "Create and configure a vector store from the provided configuration.\n\
      \nArgs:\n    vector_store_config (dict): Configuration for the vector store,\
      \ including the type, embeddings_schema, and other parameters forwarded to the\
      \ vector store constructor and connect().\n    index_name (str): The index name\
      \ to assign if not provided by the embedding config.\n    embedding_name (str\
      \ | None): Optional embedding name to select a specific embedding configuration\
      \ from embeddings_schema.\n\nReturns:\n    BaseVectorStore: A connected vector\
      \ store instance created according to the configuration.\n\nRaises:\n    Exception:\
      \ If vector store creation or connection fails due to misconfiguration or underlying\
      \ storage errors."
  - node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
    name: _text_embed_in_memory
    signature: "def _text_embed_in_memory(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column: str,\n  \
      \  strategy: dict,\n)"
    docstring: "Embed a piece of text into a vector space in memory using the specified\
      \ embedding strategy.\n\nArgs:\n    input: DataFrame containing the text data\
      \ to embed\n    callbacks: WorkflowCallbacks used during embedding\n    cache:\
      \ PipelineCache used for caching intermediary results\n    embed_column: Name\
      \ of the column in input containing the text to embed\n    strategy: Dictionary\
      \ describing the embedding strategy to use (must include a \"type\" key)\n\n\
      Returns:\n    embeddings: The embeddings produced by the embedding strategy\n\
      \nRaises:\n    ValueError: If an unknown strategy is provided\n    KeyError:\
      \ If embed_column is not found in the input dataframe"
  - node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
    name: _text_embed_with_vector_store
    signature: "def _text_embed_with_vector_store(\n    input: pd.DataFrame,\n   \
      \ callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column:\
      \ str,\n    strategy: dict[str, Any],\n    vector_store: BaseVectorStore,\n\
      \    vector_store_config: dict,\n    id_column: str = \"id\",\n    title_column:\
      \ str | None = None,\n)"
    docstring: "Embed text from a DataFrame into a vector store using a specified\
      \ embedding strategy and load the resulting vectors into the provided vector\
      \ store.\n\nArgs:\n    input (pd.DataFrame): Input DataFrame containing the\
      \ data to embed; must include the embed_column and id_column, and may include\
      \ the title column.\n    callbacks (WorkflowCallbacks): Callbacks used during\
      \ embedding.\n    cache (PipelineCache): Cache object used by the embedding\
      \ strategy.\n    embed_column (str): Name of the DataFrame column containing\
      \ the text to embed (or lists of texts per row).\n    strategy (dict[str, Any]):\
      \ Embedding strategy configuration, including the type of strategy to load.\n\
      \    vector_store (BaseVectorStore): Vector store where embeddings will be loaded.\n\
      \    vector_store_config (dict): Configuration for the vector store (e.g., batch_size,\
      \ overwrite).\n    id_column (str): Name of the DataFrame column containing\
      \ the identifier for each row.\n    title_column (str | None): Optional column\
      \ name to use as the title; defaults to embed_column when None.\n\nReturns:\n\
      \    list[Any]: Aggregated embeddings produced by the embedding strategy across\
      \ all batches.\n\nRaises:\n    ValueError: If required columns are missing from\
      \ the input DataFrame or if the necessary columns cannot be found (embed_column,\
      \ id_column, and title_column as applicable)."
  - node_id: graphrag/index/operations/embed_text/embed_text.py::_get_index_name
    name: _get_index_name
    signature: 'def _get_index_name(vector_store_config: dict, embedding_name: str)
      -> str'
    docstring: "Get the index name for the embedding in the vector store.\n\nArgs:\n\
      \    vector_store_config (dict): Configuration for the vector store; may include\
      \ container_name (defaults to \"default\") and type.\n    embedding_name (str):\
      \ The embedding name used to construct the index name.\n\nReturns:\n    str:\
      \ The computed index name.\n\nRaises:\n    Exception: Propagates exceptions\
      \ raised by create_index_name or other internal calls."
  - node_id: graphrag/index/operations/embed_text/embed_text.py::embed_text
    name: embed_text
    signature: "def embed_text(\n    input: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    embed_column: str,\n    strategy: dict,\n  \
      \  embedding_name: str,\n    id_column: str = \"id\",\n    title_column: str\
      \ | None = None,\n)"
    docstring: "Embed text from a DataFrame into a vector space and return per-row\
      \ embeddings. If a vector store is configured in the provided strategy, embeddings\
      \ are generated and stored in that vector store using the given id and optional\
      \ title information; otherwise embeddings are computed in memory. The function\
      \ returns a sequence of embedding vectors, one per input row corresponding to\
      \ embed_column.\n\nArgs:\n    input (pd.DataFrame): Input data; must include\
      \ embed_column and id_column, and may include title_column.\n    callbacks (WorkflowCallbacks):\
      \ Callbacks used during embedding.\n    cache (PipelineCache): Cache object\
      \ used by the embedding strategy.\n    embed_column (str): Name of the DataFrame\
      \ column to embed.\n    strategy (dict): Embedding strategy configuration; must\
      \ include a \"type\" key and may include vector_store settings.\n    embedding_name\
      \ (str): The embedding configuration name used to determine indexing in the\
      \ vector store.\n    id_column (str): The ID column name. Defaults to \"id\"\
      .\n    title_column (str | None): Optional column containing the title for each\
      \ document; may be None.\n\nReturns:\n    embeddings (List[List[float]]): The\
      \ embeddings produced by the embedding process, one vector per input row. The\
      \ shape is (N, D) where N is the number of rows and D is the embedding dimension.\n\
      \nRaises:\n    Exception: Propagates exceptions raised by internal operations\
      \ such as vector store creation or embedding strategy execution."
  classes:
  - class_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType
    name: TextEmbedStrategyType
    docstring: 'Enum describing the available text embedding strategies used by the
      embedding operation.


      Purpose:

      Represent the different strategies that can generate text embeddings, enabling
      the embedding workflow to select and apply the appropriate strategy implementation
      (for example, OpenAI-based or mock strategies as evidenced by related modules).


      Key attributes:

      Enum members correspond to specific embedding strategies used by the system,
      as seen in the openai and mock strategy modules.'
    methods:
    - name: __repr__
      signature: def __repr__(self)
- file: graphrag/index/operations/embed_text/strategies/mock.py
  functions:
  - node_id: graphrag/index/operations/embed_text/strategies/mock.py::_embed_text
    name: _embed_text
    signature: 'def _embed_text(_cache: PipelineCache, _text: str, tick: ProgressTicker)
      -> list[float]'
    docstring: "Embed a single piece of text.\n\nArgs:\n    _cache: PipelineCache:\
      \ Cache used for embedding operations.\n    _text: str: Text to embed.\n   \
      \ tick: ProgressTicker: Progress ticker to report progress.\n\nReturns:\n  \
      \  list[float]: Embedding vector as a list of three floats.\n\nRaises:\n   \
      \ This function does not raise any exceptions."
  - node_id: graphrag/index/operations/embed_text/strategies/mock.py::run
    name: run
    signature: "def run(  # noqa RUF029 async is required for interface\n    input:\
      \ list[str],\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    _args: dict[str, Any],\n) -> TextEmbeddingResult"
    docstring: "Run the embedding generation for the given texts using a mock strategy.\
      \ This asynchronous function processes an input collection of texts and returns\
      \ a TextEmbeddingResult containing embeddings for each input text. It reports\
      \ progress via a progress ticker and uses the _embed_text helper to generate\
      \ a 3-dimensional embedding per text.\n\nParameters\n    input: Iterable[str]\n\
      \        The input texts to embed. Note: strings are Iterable; if a single string\
      \ is passed, it will be treated as an iterable of characters unless wrapped\
      \ in a collection of strings.\n    callbacks: WorkflowCallbacks\n        Callback\
      \ hooks invoked for progress updates.\n    cache: PipelineCache\n        Cache\
      \ used for embedding operations.\n    _args: dict[str, Any]\n        Additional\
      \ optional arguments.\n\nReturns\n    TextEmbeddingResult: Result containing\
      \ embeddings for the input texts. Each embedding is a list[float] of length\
      \ 3.\n\nRaises\n    This function may raise exceptions propagated from the progress\
      \ ticker or the embedding operation. No exceptions are guaranteed."
  classes: []
- file: graphrag/index/operations/embed_text/strategies/openai.py
  functions:
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::_reconstitute_embeddings
    name: _reconstitute_embeddings
    signature: "def _reconstitute_embeddings(\n    raw_embeddings: list[list[float]],\
      \ sizes: list[int]\n) -> list[list[float] | None]"
    docstring: "Reconstitute the embeddings into the original input texts.\n\nArgs:\n\
      \    raw_embeddings: list of embeddings, where each embedding is a list of floats\n\
      \    sizes: list of ints indicating the number of embeddings that belong to\
      \ each original input text\n\nReturns:\n    list of embeddings corresponding\
      \ to each input text. Each element is either:\n    - a list of floats representing\
      \ the embedding for that input, or\n    - None if the corresponding input text\
      \ had size 0\n    For entries with size > 1, the returned embedding is the normalized\
      \ average of the associated raw embeddings.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::_prepare_embed_texts
    name: _prepare_embed_texts
    signature: "def _prepare_embed_texts(\n    input: list[str], splitter: TokenTextSplitter\n\
      ) -> tuple[list[str], list[int]]"
    docstring: "Prepare a flat list of text snippets to embed and their per-input\
      \ sizes by splitting each input text.\n\nArgs:\n    input: The list of input\
      \ strings to process.\n    splitter: The TokenTextSplitter used to split each\
      \ input string into chunks.\n\nReturns:\n    tuple[list[str], list[int]]: A\
      \ tuple (snippets, sizes) where:\n        snippets: The concatenated list of\
      \ non-empty split_texts produced from all inputs.\n        sizes: A list containing,\
      \ for each input string, the number of split_texts produced.\n\nRaises:\n  \
      \  Propagates exceptions raised by splitter.split_text."
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::_create_text_batches
    name: _create_text_batches
    signature: "def _create_text_batches(\n    texts: list[str],\n    max_batch_size:\
      \ int,\n    max_batch_tokens: int,\n    splitter: TokenTextSplitter,\n) -> list[list[str]]"
    docstring: "Create batches of texts to embed.\n\nThis function groups input texts\
      \ into batches that respect the given batch constraints. A batch is closed when\
      \ adding the next text would exceed the maximum number of texts or the maximum\
      \ token count for the batch.\n\nArgs:\n    texts: List of input texts to be\
      \ batched.\n    max_batch_size: Maximum number of texts per batch.\n    max_batch_tokens:\
      \ Maximum total tokens per batch, as measured by the splitter.\n    splitter:\
      \ TokenTextSplitter used to count tokens per text.\n\nReturns:\n    A list of\
      \ batches, where each batch is a list of strings."
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::embed
    name: embed
    signature: 'def embed(chunk: list[str])'
    docstring: "Async helper to embed a batch of text chunks using the embedding model\
      \ with a concurrency guard.\n\nArgs:\n  chunk: A batch of text chunks to embed.\n\
      \nReturns:\n  numpy.ndarray: The embeddings for the input chunks as a 2D NumPy\
      \ array.\n\nRaises:\n  Exceptions raised by the embedding model (via model.aembed_batch)\
      \ may be propagated to the caller."
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
    name: _get_splitter
    signature: "def _get_splitter(\n    config: LanguageModelConfig, batch_max_tokens:\
      \ int\n) -> TokenTextSplitter"
    docstring: "Get a TokenTextSplitter configured for the given language model configuration.\n\
      \nArgs:\n    config: LanguageModelConfig describing the model configuration\
      \ used to select the tokenizer.\n    batch_max_tokens: int representing the\
      \ maximum number of tokens per chunk.\n\nReturns:\n    TokenTextSplitter: A\
      \ text splitter initialized with a tokenizer built from the provided config\
      \ and chunk_size equal to batch_max_tokens.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::_execute
    name: _execute
    signature: "def _execute(\n    model: EmbeddingModel,\n    chunks: list[list[str]],\n\
      \    tick: ProgressTicker,\n    semaphore: asyncio.Semaphore,\n) -> list[list[float]]"
    docstring: "Asynchronously embed batches of text chunks using the provided EmbeddingModel,\
      \ honoring the concurrency limit with the supplied semaphore and reporting progress\
      \ through the tick callback after processing each batch. The embeddings from\
      \ all input chunks are flattened into a single 1D list of floats in batch order\
      \ and returned.\n\nArgs:\n  model: EmbeddingModel - The embedding model to use.\n\
      \  chunks: list[list[str]] - A list of text chunks grouped into batches to embed.\n\
      \  tick: ProgressTicker - Callback to report progress after each batch is processed.\n\
      \  semaphore: asyncio.Semaphore - Semaphore controlling concurrent embedding\
      \ calls.\n\nReturns:\n  list[float] - A flat list of embedding floats for all\
      \ inputs, concatenated in batch order.\n\nRaises:\n  Exceptions raised by the\
      \ embedding model (via model.aembed_batch) may be propagated to the caller."
  - node_id: graphrag/index/operations/embed_text/strategies/openai.py::run
    name: run
    signature: "def run(\n    input: list[str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    args: dict[str, Any],\n) -> TextEmbeddingResult"
    docstring: "Run the Claim extraction chain.\n\nArgs:\n    input: list[str] The\
      \ input texts to process.\n    callbacks: WorkflowCallbacks The callbacks interface\
      \ for progress and other hooks.\n    cache: PipelineCache The cache to use for\
      \ model embeddings and related data.\n    args: dict[str, Any] Additional arguments\
      \ for configuring the run (e.g., batch_size, batch_max_tokens, llm).\n\nReturns:\n\
      \    TextEmbeddingResult The embedding results for the input texts, or embeddings=None\
      \ if the input is null.\n\nRaises:\n    None"
  classes: []
- file: graphrag/index/operations/extract_covariates/claim_extractor.py
  functions:
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._clean_claim
    name: _clean_claim
    signature: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
      \ dict\n    ) -> dict"
    docstring: 'Update a claim''s object and subject identifiers in place using a
      resolved_entities mapping. This function does not filter by status and does
      not remove claims with status = False.


      Args:

      - claim (dict): The claim dictionary to update. The function reads the object_id
      (or object) and subject_id (or subject), substitutes them using resolved_entities,
      and writes the resulting values back to object_id and subject_id in the input
      dict. If keys are missing, existing values are preserved.

      - document_id (str): Identifier of the document containing the claim. This parameter
      is unused by this function.

      - resolved_entities (dict): Mapping of original entity identifiers to resolved
      identifiers; used to substitute the object and subject when present.


      Returns:

      - dict: The updated claim dictionary (the same object, mutated in place).


      Notes:

      - The function updates only the object_id and subject_id keys.

      - No status filtering is performed; claims with status = False are not removed.

      - When reading, object is considered as a fallback for object_id and subject
      as a fallback for subject_id.'
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._process_document
    name: _process_document
    signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
      \ int\n    ) -> list[dict]"
    docstring: "Process a single document to extract claims from the text using the\
      \ configured extraction prompt and parse the results into dictionaries.\n\n\
      Args:\n  prompt_args: A dictionary of additional arguments used to configure\
      \ the extraction prompts and behavior.\n  doc: The document content to process.\n\
      \  doc_index: The zero-based index of the document within the input collection.\n\
      \nReturns:\n  A list of dictionaries representing parsed claims as produced\
      \ by _parse_claim_tuples. Each dictionary typically includes keys such as subject_id,\
      \ object_id, and type.\n\nRaises:\n  Exceptions raised by the underlying language\
      \ model interactions (e.g., self._model.achat) or by parsing/processing logic\
      \ can propagate to callers."
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._parse_claim_tuples
    name: _parse_claim_tuples
    signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
      \ dict\n    ) -> list[dict[str, Any]]"
    docstring: "Parse claim tuples.\n\nArgs:\n    claims: The raw claims text to parse\
      \ into structured claim dictionaries. The text is\n        expected to contain\
      \ multiple claims separated by the configured record delimiter,\n        with\
      \ fields within each claim separated by the configured tuple delimiter. The\n\
      \        trailing completion delimiter is ignored during parsing.\n    prompt_variables:\
      \ A mapping used to determine the delimiters for records, tuples, and\n    \
      \    completions. The method looks up keys corresponding to internal delimiter\
      \ keys and\n        falls back to DEFAULT_RECORD_DELIMITER, DEFAULT_TUPLE_DELIMITER,\
      \ and\n        DEFAULT_COMPLETION_DELIMITER when not present.\n\nReturns:\n\
      \    list[dict[str, Any]]: A list of dictionaries where each dictionary represents\
      \ a parsed claim\n        with the following keys: subject_id, object_id, type,\
      \ status, start_date, end_date,\n        description, source_text. Values are\
      \ strings or None depending on whether a field was\n        present.\n\nRaises:\n\
      \    None: This method does not raise any documented exceptions during normal\
      \ operation."
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        input_text_key: str\
      \ | None = None,\n        input_entity_spec_key: str | None = None,\n      \
      \  input_claim_description_key: str | None = None,\n        input_resolved_entities_key:\
      \ str | None = None,\n        tuple_delimiter_key: str | None = None,\n    \
      \    record_delimiter_key: str | None = None,\n        completion_delimiter_key:\
      \ str | None = None,\n        max_gleanings: int | None = None,\n        on_error:\
      \ ErrorHandlerFn | None = None,\n    )"
    docstring: "Initialize ClaimExtractor.\n\nArgs:\n  model_invoker: ChatModel\n\
      \      The model invoker used to run prompts.\n  extraction_prompt: str | None\n\
      \      Custom prompt for extraction. If None, defaults to EXTRACT_CLAIMS_PROMPT.\n\
      \  input_text_key: str | None\n      Key in inputs for the input text. Defaults\
      \ to \"input_text\".\n  input_entity_spec_key: str | None\n      Key for the\
      \ entity specifications. Defaults to \"entity_specs\".\n  input_claim_description_key:\
      \ str | None\n      Key for the claim description. Defaults to \"claim_description\"\
      .\n  input_resolved_entities_key: str | None\n      Key for resolved entities.\
      \ Defaults to \"resolved_entities\".\n  tuple_delimiter_key: str | None\n  \
      \    Key for the tuple delimiter. Defaults to \"tuple_delimiter\".\n  record_delimiter_key:\
      \ str | None\n      Key for the record delimiter. Defaults to \"record_delimiter\"\
      .\n  completion_delimiter_key: str | None\n      Key for the completion delimiter.\
      \ Defaults to \"completion_delimiter\".\n  max_gleanings: int | None\n     \
      \ Maximum number of gleanings to perform. If None, uses graphrag_config_defaults.extract_claims.max_gleanings.\n\
      \  on_error: ErrorHandlerFn | None\n      Error handler function. If None, a\
      \ no-op handler is used.\nReturns:\n  None"
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.pull_field
    name: pull_field
    signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
    docstring: "\"\"\"Pull a field from a list of strings by index.\n\nArgs:\n   \
      \ index: The position of the field to extract from fields.\n    fields: The\
      \ list of string fields from which to pull the value.\n\nReturns:\n    str |\
      \ None: The trimmed field at the given index if present; otherwise None.\n\"\
      \"\""
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
      \ dict | None = None\n    ) -> ClaimExtractorResult"
    docstring: "Process a collection of input texts to extract claims and return the\
      \ structured results.\n\nArgs:\n  inputs: dict[str, Any] - The inputs containing\
      \ the texts to process and related fields such as input_text key, entity specs,\
      \ and claim descriptions.\n  prompt_variables: dict | None - Optional mapping\
      \ of prompt variables to customize extraction prompts and delimiters. If None,\
      \ defaults are used.\n\nReturns:\n  ClaimExtractorResult - The result object\
      \ containing:\n    output: list[dict] - The cleaned claim dictionaries.\n  \
      \  source_docs: dict[str, str] - Mapping from document_id to the original text\
      \ for each processed document.\n\nRaises:\n  KeyError - If required input keys\
      \ are missing from inputs."
  classes:
  - class_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor
    name: ClaimExtractor
    docstring: "ClaimExtractor\n\nClass responsible for orchestrating claim extraction\
      \ from input texts using configurable prompts and parsing the results into structured\
      \ dictionaries. It updates claim objects with resolved entity identifiers and\
      \ supports customizable delimiters and error handling.\n\nArgs:\n  model_invoker:\
      \ ChatModel\n    The model invoker used to run prompts.\n  extraction_prompt:\
      \ str | None\n    Custom prompt for extraction. If None, defaults to EXTRACT_CLAIMS_PROMPT.\n\
      \  input_text_key: str | None\n    Key in the inputs for the input text.\n \
      \ input_entity_spec_key: str | None\n    Key for the entity specifications.\n\
      \  input_claim_description_key: str | None\n    Key for the claim description.\n\
      \  input_resolved_entities_key: str | None\n    Key for the resolved entities\
      \ used to update claims.\n  tuple_delimiter_key: str | None\n    Key for the\
      \ tuple delimiter used when parsing claims.\n  record_delimiter_key: str | None\n\
      \    Key for the record delimiter used when parsing claims.\n  completion_delimiter_key:\
      \ str | None\n    Key for the completion delimiter used to signal end of extraction.\n\
      \  max_gleanings: int | None\n    Maximum number of gleanings (iterations) to\
      \ perform during extraction.\n  on_error: ErrorHandlerFn | None\n    Optional\
      \ error handler callback invoked on errors.\n\nReturns:\n  None\n    Initializes\
      \ the instance; no return value is produced.\n\nRaises:\n  Exception\n    Exceptions\
      \ raised during initialization or by underlying components may be propagated\
      \ unless handled by on_error."
    methods:
    - name: _clean_claim
      signature: "def _clean_claim(\n        self, claim: dict, document_id: str,\
        \ resolved_entities: dict\n    ) -> dict"
    - name: _process_document
      signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
        \ int\n    ) -> list[dict]"
    - name: _parse_claim_tuples
      signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
        \ dict\n    ) -> list[dict[str, Any]]"
    - name: __init__
      signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
        \        extraction_prompt: str | None = None,\n        input_text_key: str\
        \ | None = None,\n        input_entity_spec_key: str | None = None,\n    \
        \    input_claim_description_key: str | None = None,\n        input_resolved_entities_key:\
        \ str | None = None,\n        tuple_delimiter_key: str | None = None,\n  \
        \      record_delimiter_key: str | None = None,\n        completion_delimiter_key:\
        \ str | None = None,\n        max_gleanings: int | None = None,\n        on_error:\
        \ ErrorHandlerFn | None = None,\n    )"
    - name: pull_field
      signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
    - name: __call__
      signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
        \ dict | None = None\n    ) -> ClaimExtractorResult"
- file: graphrag/index/operations/extract_covariates/extract_covariates.py
  functions:
  - node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_covariate
    name: create_covariate
    signature: 'def create_covariate(item: dict[str, Any]) -> Covariate'
    docstring: "Create a Covariate instance from the provided item.\n\nArgs:\n   \
      \ item: dict[str, Any]\n        The dictionary containing covariate fields.\
      \ The function reads\n        keys such as subject_id, object_id, type, status,\
      \ start_date, end_date,\n        description, source_text, record_id, and id\
      \ to construct the Covariate.\n\nReturns:\n    Covariate\n        The Covariate\
      \ object created from the item."
  - node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_row_from_claim_data
    name: create_row_from_claim_data
    signature: 'def create_row_from_claim_data(row, covariate_data: Covariate, covariate_type:
      str)'
    docstring: "Create a row from claim data and the input row.\n\nArgs:\n  row: The\
      \ input row to extend with covariate data.\n  covariate_data: Covariate data\
      \ to be merged into the row (converted to a dict via asdict).\n  covariate_type:\
      \ The covariate type to include in the returned row.\n\nReturns:\n  dict: A\
      \ new dictionary containing the original row data, the covariate data fields,\
      \ and the covariate_type field.\n\nRaises:\n  TypeError: If row is not a mapping\
      \ that can be expanded with **, or if covariate_data cannot be converted to\
      \ a dict via asdict."
  - node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
    name: run_extract_claims
    signature: "def run_extract_claims(\n    input: str | Iterable[str],\n    entity_types:\
      \ list[str],\n    resolved_entities_map: dict[str, str],\n    callbacks: WorkflowCallbacks,\n\
      \    cache: PipelineCache,\n    strategy_config: dict[str, Any],\n) -> CovariateExtractionResult"
    docstring: "Run the Claim extraction chain to derive covariates from input text.\n\
      \nArgs:\n    input: str | Iterable[str]\n        The input text or collection\
      \ of texts to process.\n    entity_types: list[str]\n        The entity types\
      \ to consider when extracting claims.\n    resolved_entities_map: dict[str,\
      \ str]\n        Mapping of resolved entities for the extraction process.\n \
      \   callbacks: WorkflowCallbacks\n        Callbacks used during model invocation\
      \ and extraction.\n    cache: PipelineCache\n        Cache to use for model\
      \ invocations.\n    strategy_config: dict[str, Any]\n        Strategy configuration\
      \ for the extraction, including llm settings,\n        prompts, and claim description.\n\
      \nReturns:\n    CovariateExtractionResult\n        The extraction result containing\
      \ covariates derived from the claims.\n\nRaises:\n    ValueError\n        If\
      \ claim_description is missing from strategy_config (i.e., claim_description\n\
      \        is required for claim extraction)."
  - node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
    name: run_strategy
    signature: def run_strategy(row)
    docstring: "Run the strategy on a single input row to asynchronously extract covariates\
      \ from text.\n\nArgs:\n  row: The input row to process. The text to analyze\
      \ is read from row[column].\n\nReturns:\n  List[dict[str, Any]]: A list of rows\
      \ augmented with covariate data. Each item is produced by merging the original\
      \ row with the corresponding covariate data (converted to a dict) and including\
      \ a covariate_type field set to covariate_type.\n\nRaises:\n  Exception: Propagates\
      \ exceptions raised during claim extraction or row construction."
  - node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
    name: extract_covariates
    signature: "def extract_covariates(\n    input: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    column: str,\n    covariate_type:\
      \ str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    entity_types: list[str] | None = None,\n    num_threads: int = 4,\n)"
    docstring: "Process a DataFrame by applying covariate extraction to the text in\
      \ a specified column for each row, returning covariate rows as a DataFrame.\n\
      \nArgs:\n  input: The input DataFrame to process.\n  callbacks: Workflow callbacks\
      \ for progress reporting and events.\n  cache: Cache to use for model and data\
      \ storage.\n  column: The name of the column containing text to extract covariates\
      \ from.\n  covariate_type: The type/tag to assign to produced covariate rows.\n\
      \  strategy: Strategy configuration for covariate extraction; if None, an empty\
      \ configuration is used.\n  async_mode: Asynchronous mode to use when applying\
      \ the strategy (default: AsyncType.AsyncIO).\n  entity_types: Entity types to\
      \ consider when extracting covariates; defaults to DEFAULT_ENTITY_TYPES if None.\n\
      \  num_threads: Number of concurrent workers to use.\n\nReturns:\n  A DataFrame\
      \ containing the extracted covariate rows, where each input row may contribute\
      \ zero or more covariate rows. Each covariate row includes the original row\
      \ fields, covariate data fields, and a covariate_type column.\n\nRaises:\n \
      \ Propagates exceptions raised by underlying operations (e.g., derive_from_rows,\
      \ run_extract_claims)."
  classes: []
- file: graphrag/index/operations/extract_graph/extract_graph.py
  functions:
  - node_id: graphrag/index/operations/extract_graph/extract_graph.py::_load_strategy
    name: _load_strategy
    signature: 'def _load_strategy(strategy_type: ExtractEntityStrategyType) -> EntityExtractStrategy'
    docstring: "Load the strategy method implementation for the given strategy type.\n\
      \nArgs:\n    strategy_type (ExtractEntityStrategyType): The type of extraction\
      \ strategy to load.\n\nReturns:\n    EntityExtractStrategy: The loaded strategy\
      \ callable.\n\nRaises:\n    ValueError: If an unknown strategy_type is provided."
  - node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_entities
    name: _merge_entities
    signature: def _merge_entities(entity_dfs) -> pd.DataFrame
    docstring: "Merge and aggregate multiple entity DataFrames into a single aggregated\
      \ entities DataFrame by title and type.\n\nArgs:\n  entity_dfs: List[pandas.DataFrame]\
      \ List of DataFrames containing entity information. Each DataFrame is expected\
      \ to include the columns: \"title\", \"type\", \"description\", and \"source_id\"\
      .\n\nReturns:\n  pandas.DataFrame A DataFrame with one row per (title, type)\
      \ pair containing:\n    - title: entity title\n    - type: entity type\n   \
      \ - description: list of descriptions for this key\n    - text_unit_ids: list\
      \ of source_id values for this key\n    - frequency: number of source_id entries\
      \ for this key"
  - node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_relationships
    name: _merge_relationships
    signature: def _merge_relationships(relationship_dfs) -> pd.DataFrame
    docstring: "Merge multiple relationship DataFrames into a single aggregated DataFrame\
      \ by source and target.\n\nArgs:\n    relationship_dfs: Iterable of pandas.DataFrame.\
      \ Each DataFrame should contain at least the columns\n        'source', 'target',\
      \ 'description', 'source_id', and 'weight'.\n\nReturns:\n    pd.DataFrame: A\
      \ DataFrame grouped by 'source' and 'target' with the following aggregated columns:\n\
      \        - description: list of descriptions for each (source, target) pair\n\
      \        - text_unit_ids: list of source_id values within the group\n      \
      \  - weight: sum of weights within the group\n    The result has 'source' and\
      \ 'target' as columns with the index reset.\n\nRaises:\n    TypeError: If relationship_dfs\
      \ is not a suitable iterable of DataFrames.\n    KeyError: If required columns\
      \ are missing from any input DataFrame."
  - node_id: graphrag/index/operations/extract_graph/extract_graph.py::run_strategy
    name: run_strategy
    signature: def run_strategy(row)
    docstring: "\"\"\"Run a strategy on a single input row to extract graph data.\n\
      \nArgs:\n    row: A row from the input DataFrame containing the values for text_column\
      \ and id_column. The text is read from row[text_column] and the id from row[id_column].\n\
      \nReturns:\n    A list with three elements: the entities, relationships, and\
      \ graph returned by the strategy execution for this row.\n\nRaises:\n    Exceptions\
      \ raised by the underlying strategy execution (strategy_exec) are propagated.\n\
      \"\"\""
  - node_id: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
    name: extract_graph
    signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    text_column: str,\n   \
      \ id_column: str,\n    strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types=DEFAULT_ENTITY_TYPES,\n    num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    docstring: "Extract a graph from a piece of text using a language model.\n\nArgs:\n\
      \    text_units: The input DataFrame containing the text data to process.\n\
      \    callbacks: WorkflowCallbacks used for progress reporting and event handling.\n\
      \    cache: PipelineCache instance used for caching results.\n    text_column:\
      \ Name of the column in text_units that contains the text to analyze.\n    id_column:\
      \ Name of the column in text_units that contains a unique identifier for each\
      \ row.\n    strategy: Strategy configuration dictionary; may be None to use\
      \ defaults.\n    async_mode: AsyncType controlling how tasks are scheduled.\n\
      \    entity_types: List of entity types to extract; if None, defaults to DEFAULT_ENTITY_TYPES.\n\
      \    num_threads: Number of worker threads to use for processing.\n\nReturns:\n\
      \    tuple[pd.DataFrame, pd.DataFrame]: A pair of DataFrames: the first contains\
      \ aggregated entities and the second contains aggregated relationships.\n\n\
      Raises:\n    ValueError: If an unknown strategy type is provided."
  classes: []
- file: graphrag/index/operations/extract_graph/graph_extractor.py
  functions:
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_source_ids
    name: _unpack_source_ids
    signature: 'def _unpack_source_ids(data: Mapping) -> list[str]'
    docstring: "Unpack source_id values from a mapping.\n\nArgs:\n    data (Mapping):\
      \ A mapping that may contain the key \"source_id\" whose value is a string of\
      \ IDs separated by comma-space.\n\nReturns:\n    list[str]: The list of source\
      \ IDs. If the key is missing or the value is None, returns an empty list."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_descriptions
    name: _unpack_descriptions
    signature: 'def _unpack_descriptions(data: Mapping) -> list[str]'
    docstring: "Unpack descriptions from a mapping by splitting the description string\
      \ into lines.\n\nArgs:\n    data (Mapping): input mapping that may contain a\
      \ \"description\" key with a string value.\n\nReturns:\n    list[str]: The list\
      \ of description lines. If no description is provided, returns an empty list.\n\
      \nRaises:\n    AttributeError: If the description value exists but does not\
      \ support the split method."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self, texts: list[str], prompt_variables: dict[str,\
      \ Any] | None = None\n    ) -> GraphExtractionResult"
    docstring: "Asynchronously run graph extraction on a list of input texts and return\
      \ the results.\n\nArgs:\n  texts: List[str] - List of input texts to process;\
      \ each element is treated as a separate document.\n  prompt_variables: dict[str,\
      \ Any] | None - Optional mapping of prompt variables to customize the extraction\
      \ prompts and delimiters. If None, defaults are used.\n\nReturns:\n  GraphExtractionResult\
      \ - An object containing the aggregated extraction output and a mapping of document\
      \ indices to their source texts.\n\nRaises:\n  Exception - If an error occurs\
      \ during document processing or result aggregation."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_document
    name: _process_document
    signature: "def _process_document(\n        self, text: str, prompt_variables:\
      \ dict[str, str]\n    ) -> str"
    docstring: "Process a single document to extract entities using the configured\
      \ extraction prompts, and accumulate the results. If gleanings are enabled (max_gleanings\
      \ > 0), this may perform multiple continuation prompts to extract additional\
      \ entities until the limit is reached or the model indicates there are no more\
      \ entities.\n\nArgs:\n  text: str - The document content to process.\n  prompt_variables:\
      \ dict[str, str] - A dictionary of prompt variables used to configure the extraction\
      \ prompts and behavior.\n\nReturns:\n  str - The concatenated extraction results\
      \ for the document.\n\nRaises:\n  Exception - If an error occurs calling the\
      \ model or during continuation prompts."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
      \ str | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        prompt: str | None = None,\n        join_descriptions=True,\n     \
      \   max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn | None\
      \ = None,\n    )"
    docstring: "Initializes a GraphExtractor with the given configuration.\n\nCreates\
      \ and configures a GraphExtractor instance using the provided model_invoker\
      \ and optional configuration values. This constructor assigns the model to use\
      \ for prompt execution, defines default keys for various prompt variables (e.g.,\
      \ tuple_delimiter, record_delimiter, input_text, entity_types, and completion_delimiter),\
      \ selects the extraction prompt, and establishes max_gleanings and an optional\
      \ on_error handler. It does not return a value.\n\nArgs:\n  model_invoker (ChatModel):\n\
      \      The model invoker used to run prompts.\n  tuple_delimiter_key (str |\
      \ None):\n      Key in prompt_variables for the tuple delimiter. Defaults to\
      \ \"tuple_delimiter\".\n  record_delimiter_key (str | None):\n      Key in prompt_variables\
      \ for the record delimiter. Defaults to \"record_delimiter\".\n  input_text_key\
      \ (str | None):\n      Key in inputs for the input text. Defaults to \"input_text\"\
      .\n  entity_types_key (str | None):\n      Key for the entity types in prompt\
      \ variables. Defaults to \"entity_types\".\n  completion_delimiter_key (str\
      \ | None):\n      Key for the completion delimiter in prompt variables. Defaults\
      \ to \"completion_delimiter\".\n  prompt (str | None):\n      Custom extraction\
      \ prompt to use. If None, defaults to GRAPH_EXTRACTION_PROMPT.\n  join_descriptions\
      \ (bool):\n      Whether to join descriptions in the extraction.\n  max_gleanings\
      \ (int | None):\n      Maximum number of gleanings. If None, defaults to graphrag_config_defaults.extract_graph.max_gleanings.\n\
      \  on_error (ErrorHandlerFn | None):\n      Optional error handler function.\
      \ If None, a no-op handler is used."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
    name: _process_results
    signature: "def _process_results(\n        self,\n        results: dict[int, str],\n\
      \        tuple_delimiter: str,\n        record_delimiter: str,\n    ) -> nx.Graph"
    docstring: "Parse the result string to create an undirected unipartite graph.\n\
      \nArgs:\n    results (dict[int, str]): dict of results from the extraction chain\n\
      \    tuple_delimiter (str): delimiter between tuples in an output record, default\
      \ is '<|>'\n    record_delimiter (str): delimiter between records, default is\
      \ '##'\n\nReturns:\n    nx.Graph: The undirected graph constructed from the\
      \ results."
  classes:
  - class_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor
    name: GraphExtractor
    docstring: "GraphExtractor orchestrates graph extraction from text using a language\
      \ model and structured prompts, accumulating results into graphs and supporting\
      \ iterative gleaning via continuation prompts when configured.\n\nKey attributes\
      \ include: model_invoker (the ChatModel used for prompt execution) and configuration\
      \ for delimiter/prompt keys (tuple_delimiter_key, record_delimiter_key, input_text_key,\
      \ entity_types_key, completion_delimiter_key), as well as optional prompt, join_descriptions,\
      \ max_gleanings, and on_error.\n\nArgs:\n  model_invoker: The model invoker\
      \ used to run the extraction prompts against the language model.\n  tuple_delimiter_key:\
      \ Prompt variable key for the delimiter between tuples.\n  record_delimiter_key:\
      \ Prompt variable key for the delimiter between records.\n  input_text_key:\
      \ Prompt variable key for the input text content.\n  entity_types_key: Prompt\
      \ variable key for the entity types to extract.\n  completion_delimiter_key:\
      \ Prompt variable key for the delimiter that marks completion of an extraction\
      \ loop.\n  prompt: Optional prompt template to override defaults.\n  join_descriptions:\
      \ Flag indicating whether to join entity descriptions in the results.\n  max_gleanings:\
      \ Maximum number of gleaning iterations allowed.\n  on_error: Error handler\
      \ to be invoked on errors during processing.\n\nReturns:\n  GraphExtractionResult:\
      \ The result of running graph extraction on the input texts.\n\nRaises:\n  Exception:\
      \ Exceptions may be raised by the underlying model invocation or processing,\
      \ which may be handled by the on_error callback."
    methods:
    - name: __call__
      signature: "def __call__(\n        self, texts: list[str], prompt_variables:\
        \ dict[str, Any] | None = None\n    ) -> GraphExtractionResult"
    - name: _process_document
      signature: "def _process_document(\n        self, text: str, prompt_variables:\
        \ dict[str, str]\n    ) -> str"
    - name: __init__
      signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
        \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
        \ str | None = None,\n        input_text_key: str | None = None,\n       \
        \ entity_types_key: str | None = None,\n        completion_delimiter_key:\
        \ str | None = None,\n        prompt: str | None = None,\n        join_descriptions=True,\n\
        \        max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn\
        \ | None = None,\n    )"
    - name: _process_results
      signature: "def _process_results(\n        self,\n        results: dict[int,\
        \ str],\n        tuple_delimiter: str,\n        record_delimiter: str,\n \
        \   ) -> nx.Graph"
- file: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py
  functions:
  - node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
    name: run_extract_graph
    signature: "def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
      \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
    docstring: "async def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
      \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult:\n\
      \    \"\"\"Run the entity extraction chain.\"\"\"\n\n    Args:\n        model:\
      \ ChatModel\n            The chat model instance used to invoke the extraction.\n\
      \        docs: list[Document]\n            The input documents from which to\
      \ extract entities.\n        entity_types: EntityTypes\n            The types\
      \ of entities to extract.\n        args: StrategyConfig\n            Strategy\
      \ configuration for extraction. May include:\n                tuple_delimiter:\
      \ delimiter for grouping tuples (or None)\n                record_delimiter:\
      \ delimiter for grouping records (or None)\n                completion_delimiter:\
      \ delimiter for completing extractions (or None)\n                extraction_prompt:\
      \ optional prompt used by the extractor\n                max_gleanings: maximum\
      \ number of gleanings; defaults to graphrag_config_defaults.extract_graph.max_gleanings\n\
      \n    Returns:\n        EntityExtractionResult\n            The extracted entities,\
      \ relationships, and the graph representing the extraction.\n\n    Raises:\n\
      \        Exception\n            If an error occurs during extraction (propagated\
      \ from the underlying GraphExtractor or processing steps)."
  - node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_graph_intelligence
    name: run_graph_intelligence
    signature: "def run_graph_intelligence(\n    docs: list[Document],\n    entity_types:\
      \ EntityTypes,\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
    docstring: "Run the graph intelligence entity extraction strategy.\n\nArgs:\n\
      \    docs: list[Document] - The input documents to process.\n    entity_types:\
      \ EntityTypes - The types of entities to extract.\n    cache: PipelineCache\
      \ - Cache to use for the language model and computations.\n    args: StrategyConfig\
      \ - Strategy configuration, including llm settings and extraction prompts.\n\
      \nReturns:\n    EntityExtractionResult - The extraction results.\n\nRaises:\n\
      \    Exceptions propagated from LanguageModelConfig initialization, ModelManager.get_or_create_chat_model,\
      \ or run_extract_graph."
  classes: []
- file: graphrag/index/operations/extract_graph/typing.py
  functions:
  - node_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this ExtractEntityStrategyType enum\
      \ member.\n\nArgs:\n    self: ExtractEntityStrategyType, the enum member to\
      \ represent as a string.\n\nReturns:\n    str: The enum member's value enclosed\
      \ in double quotes."
  classes:
  - class_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType
    name: ExtractEntityStrategyType
    docstring: 'ExtractEntityStrategyType is an Enum subclass that defines the available
      strategies for extracting entities in the graphrag index''s extraction workflow.


      Purpose:

      This enumeration provides a type-safe way to refer to a specific extraction
      strategy. Each member represents one strategy and has an associated value defined
      by the enum. The class itself does not implement behavior; it serves as a collection
      of named constants used by the extraction logic.


      Members:

      The enum''s members are defined in the source file. Each member has a unique
      name and value. The exact set of members may evolve as features are added, but
      all members share the Enum semantics.


      Usage:

      - You can enumerate or inspect the defined members at runtime via ExtractEntityStrategyType.__members__.keys().

      - To use a strategy, reference a concrete member in your code (for example,
      strategy == ExtractEntityStrategyType.SOME_MEMBER). Replace SOME_MEMBER with
      the actual member defined in your project.


      Notes:

      - __repr__ and __str__ representations follow standard Python Enum behavior
      unless overridden in the enum definition.

      - This docstring intentionally avoids asserting specific values or representations,
      since those depend on the actual member definitions in the codebase.'
    methods:
    - name: __repr__
      signature: def __repr__(self)
- file: graphrag/index/operations/finalize_community_reports.py
  functions:
  - node_id: graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
    name: finalize_community_reports
    signature: "def finalize_community_reports(\n    reports: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n) -> pd.DataFrame"
    docstring: "Merge input reports with communities to create final community reports.\n\
      \nArgs:\n    reports: The input reports data to be enriched with community metadata.\n\
      \    communities: The communities dataset containing metadata used for enrichment\
      \ (including fields used for the merge: 'community', 'parent', 'children', 'size',\
      \ 'period').\n\nReturns:\n    The finalized community reports DataFrame containing\
      \ only the columns defined by COMMUNITY_REPORTS_FINAL_COLUMNS, augmented with\
      \ a human_readable_id and an id per row.\n\nRaises:\n    KeyError: If required\
      \ columns are missing from the inputs or if the final column set referenced\
      \ by COMMUNITY_REPORTS_FINAL_COLUMNS is not present in the merged result."
  classes: []
- file: graphrag/index/operations/finalize_entities.py
  functions:
  - node_id: graphrag/index/operations/finalize_entities.py::finalize_entities
    name: finalize_entities
    signature: "def finalize_entities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
      \ bool = False,\n) -> pd.DataFrame"
    docstring: "Transforms input entities into final entity records by building a\
      \ graph from relationships, optionally embedding, applying a layout, and attaching\
      \ identifiers.\n\nArgs:\n    entities (pd.DataFrame): Input entities to be transformed.\
      \ The function merges on the 'title' column and augments with layout and degree\
      \ information.\n    relationships (pd.DataFrame): DataFrame containing edge\
      \ information used to construct the graph; edge attributes include 'weight'.\n\
      \    embed_config (EmbedGraphConfig | None): Embedding configuration. If provided\
      \ and embed_config.enabled is True, graph embeddings are computed.\n    layout_enabled\
      \ (bool): If True, a layout is applied to the graph to compute node positions.\n\
      \nReturns:\n    pd.DataFrame: The final entities DataFrame containing the columns\
      \ defined by ENTITIES_FINAL_COLUMNS.\n\nRaises:\n    Exception: Propagates exceptions\
      \ raised by underlying operations such as create_graph, embed_graph, layout_graph,\
      \ and compute_degree, as well as DataFrame operations."
  classes: []
- file: graphrag/index/operations/finalize_relationships.py
  functions:
  - node_id: graphrag/index/operations/finalize_relationships.py::finalize_relationships
    name: finalize_relationships
    signature: "def finalize_relationships(\n    relationships: pd.DataFrame,\n) ->\
      \ pd.DataFrame"
    docstring: "Transform input relationships into finalized relationship records.\n\
      \nThis function builds a graph from the input relationships, computes node degrees,\n\
      deduplicates edges, computes a combined degree for each edge, assigns stable\n\
      human-readable and UUID-based identifiers, and returns a DataFrame containing\n\
      the expected final columns.\n\nArgs:\n    relationships: DataFrame containing\
      \ edge information for relationships to finalize.\n\nReturns:\n    A DataFrame\
      \ containing the finalized relationships, including the columns\n    defined\
      \ in RELATIONSHIPS_FINAL_COLUMNS, as well as additional columns:\n    - human_readable_id:\
      \ the index value assigned prior to UUID generation\n    - id: a string UUID\
      \ for each row\n    - combined_degree: the computed combined degree per edge\n\
      \nRaises:\n    Propagates exceptions from underlying operations such as graph\
      \ creation,\n    degree computation, edge degree computation, and DataFrame\
      \ manipulation."
  classes: []
- file: graphrag/index/operations/graph_to_dataframes.py
  functions:
  - node_id: graphrag/index/operations/graph_to_dataframes.py::graph_to_dataframes
    name: graph_to_dataframes
    signature: "def graph_to_dataframes(\n    graph: nx.Graph,\n    node_columns:\
      \ list[str] | None = None,\n    edge_columns: list[str] | None = None,\n   \
      \ node_id: str = \"title\",\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    docstring: 'Deconstructs an nx.Graph into two pandas DataFrames: one for nodes
      and one for edges.


      Args:

      - graph: input graph

      - node_columns: optional list of node attribute column names to include in the
      nodes DataFrame

      - edge_columns: optional list of edge attribute column names to include in the
      edges DataFrame

      - node_id: name of the column to store node identifiers in the nodes DataFrame
      (default "title")


      Returns:

      - tuple[pd.DataFrame, pd.DataFrame]: pair of DataFrames (nodes, edges). The
      nodes DataFrame includes a column named node_id containing node identifiers;
      if node_columns is provided, only those columns are included. The edges DataFrame
      contains an undirected edge representation with columns source and target, and
      any edge attributes; if edge_columns are provided, only those columns are included.


      Raises:

      - TypeError: If inputs are not of expected types (e.g., graph is not an nx.Graph)
      or required attributes are missing.'
  classes: []
- file: graphrag/index/operations/layout_graph/layout_graph.py
  functions:
  - node_id: graphrag/index/operations/layout_graph/layout_graph.py::_run_layout
    name: _run_layout
    signature: "def _run_layout(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings,\n) -> GraphLayout"
    docstring: "Run the layout algorithm on a graph and return the resulting GraphLayout.\n\
      \nArgs:\n    graph: nx.Graph\n        The graph to layout.\n    enabled: bool\n\
      \        If True, use the UMAP-based layout; otherwise fall back to the Zero\
      \ layout.\n    embeddings: NodeEmbeddings\n        Embeddings for each node\
      \ in the graph.\n\nReturns:\n    GraphLayout\n        The resulting layout as\
      \ a GraphLayout."
  - node_id: graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
    name: layout_graph
    signature: "def layout_graph(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
      \ NodeEmbeddings | None,\n)"
    docstring: "Apply a layout algorithm to a nx.Graph. The method returns a dataframe\
      \ containing the node positions.\n\nArgs:\n    graph: The nx.Graph to layout.\n\
      \    enabled: If True, use the UMAP-based layout; otherwise fall back to the\
      \ Zero layout.\n    embeddings: NodeEmbeddings | None. Embeddings for each node\
      \ in the graph. If None, embeddings are treated as empty.\n\nReturns:\n    pandas.DataFrame:\
      \ A DataFrame containing the layout with columns 'label', 'x', 'y', 'size'.\n\
      \nRaises:\n    Exceptions raised by the underlying layout implementations (UMAP\
      \ or Zero) during layout computation."
  classes: []
- file: graphrag/index/operations/layout_graph/typing.py
  functions:
  - node_id: graphrag/index/operations/layout_graph/typing.py::NodePosition.to_pandas
    name: to_pandas
    signature: def to_pandas(self) -> tuple[str, float, float, str, float]
    docstring: "\"\"\"Convert this NodePosition to a pandas-friendly 5-tuple.\n\n\
      Args:\n    self (NodePosition): The NodePosition instance to convert.\n\nReturns:\n\
      \    tuple[str, float, float, str, float]: The tuple containing label, x, y,\
      \ cluster, and size.\n\n\"\"\""
  classes:
  - class_id: graphrag/index/operations/layout_graph/typing.py::NodePosition
    name: NodePosition
    docstring: "NodePosition represents the position and rendering properties of a\
      \ node in the layout graph.\n\nAttributes:\n  label: str\n  x: float\n  y: float\n\
      \  cluster: int | str\n  size: float\n\nArgs:\n  label: The node's display label.\n\
      \  x: The x-coordinate of the node in the layout.\n  y: The y-coordinate of\
      \ the node in the layout.\n  cluster: The cluster identifier (int or str) to\
      \ which the node belongs.\n  size: The visual size of the node.\n\nReturns:\n\
      \  None: The dataclass constructor initializes a NodePosition instance and does\
      \ not return a value.\n\nto_pandas:\n  def to_pandas(self) -> tuple[str, float,\
      \ float, str, float]:\n    Converts this NodePosition to a pandas-friendly 5-tuple\
      \ in the order: (label, x, y, cluster, size).\n    The 4th element (cluster)\
      \ is represented as a string to maintain consistency in pandas DataFrames.\n\
      \    Returns: A 5-tuple containing (label, x, y, cluster, size)."
    methods:
    - name: to_pandas
      signature: def to_pandas(self) -> tuple[str, float, float, str, float]
- file: graphrag/index/operations/layout_graph/umap.py
  functions:
  - node_id: graphrag/index/operations/layout_graph/umap.py::_filter_raw_embeddings
    name: _filter_raw_embeddings
    signature: 'def _filter_raw_embeddings(embeddings: NodeEmbeddings) -> NodeEmbeddings'
    docstring: "Filter out None entries from the input node embeddings mapping.\n\n\
      Args:\n    embeddings: NodeEmbeddings - Mapping of node identifiers to embedding\
      \ vectors; may contain None values.\n\nReturns:\n    NodeEmbeddings - A new\
      \ mapping with all entries whose embeddings are not None."
  - node_id: graphrag/index/operations/layout_graph/umap.py::compute_umap_positions
    name: compute_umap_positions
    signature: "def compute_umap_positions(\n    embedding_vectors: np.ndarray,\n\
      \    node_labels: list[str],\n    node_categories: list[int] | None = None,\n\
      \    node_sizes: list[int] | None = None,\n    min_dist: float = 0.75,\n   \
      \ n_neighbors: int = 5,\n    spread: int = 1,\n    metric: str = \"euclidean\"\
      ,\n    n_components: int = 2,\n    random_state: int = 86,\n) -> list[NodePosition]"
    docstring: "Project embedding vectors down to 2D/3D coordinates using UMAP.\n\n\
      Args:\n  embedding_vectors: Embedding vectors to project, provided as a numpy\
      \ array.\n  node_labels: Labels for each node.\n  node_categories: Optional\
      \ per-node category identifiers. If None, defaults to 1 for all nodes.\n  node_sizes:\
      \ Optional per-node sizes. If None, defaults to 1 for all nodes.\n  min_dist:\
      \ UMAP min_dist hyperparameter controlling the minimum distance between embedded\
      \ points.\n  n_neighbors: UMAP n_neighbors hyperparameter controlling the local\
      \ connectivity.\n  spread: UMAP spread parameter influencing the layout.\n \
      \ metric: Distance metric used by UMAP (e.g., \"euclidean\").\n  n_components:\
      \ Number of output dimensions (2 or 3) for the embedding.\n  random_state: Seed\
      \ for random number generation to ensure reproducibility.\n\nReturns:\n  list[NodePosition]:\
      \ A list of NodePosition objects corresponding to each input label. Each NodePosition\
      \ includes coordinates (x, y) for 2D layouts or x, y, z for 3D layouts, along\
      \ with label, cluster (from node_categories or 1), and size (from node_sizes\
      \ or 1).\n\nRaises:\n  ImportError: If the umap package is not installed or\
      \ cannot be imported."
  - node_id: graphrag/index/operations/layout_graph/umap.py::run
    name: run
    signature: "def run(\n    graph: nx.Graph,\n    embeddings: NodeEmbeddings,\n\
      \    on_error: ErrorHandlerFn,\n) -> GraphLayout"
    docstring: "Compute a UMAP-based layout for the given graph using node embeddings\
      \ and optional per-node attributes, with a fallback layout if the UMAP computation\
      \ fails.\n\nArgs:\n  graph: nx.Graph\n      The input graph. Nodes may have\
      \ attributes such as cluster (or community) and degree (or size) that are used\
      \ to influence the layout.\n  embeddings: NodeEmbeddings\n      Mapping of node\
      \ identifiers to embedding vectors; entries with None are ignored.\n  on_error:\
      \ ErrorHandlerFn\n      Callback invoked when an error occurs during layout\
      \ computation.\n\nReturns:\n  GraphLayout\n      The resulting layout as a list\
      \ of NodePosition objects representing node positions. If UMAP succeeds, positions\
      \ reflect the embeddings; otherwise a fallback layout with all nodes at (0,0)\
      \ is returned.\n\nRaises:\n  None\n      This function does not raise exceptions;\
      \ errors are reported via on_error and a fallback layout is returned."
  classes: []
- file: graphrag/index/operations/layout_graph/zero.py
  functions:
  - node_id: graphrag/index/operations/layout_graph/zero.py::get_zero_positions
    name: get_zero_positions
    signature: "def get_zero_positions(\n    node_labels: list[str],\n    node_categories:\
      \ list[int] | None = None,\n    node_sizes: list[int] | None = None,\n    three_d:\
      \ bool | None = False,\n) -> list[NodePosition]"
    docstring: "Create zero-coordinate positions for nodes\n\nThis function returns\
      \ a list of NodePosition objects for every label in node_labels. No embedding\
      \ or projection is performed; coordinates are initialized to zeros.\n\nArgs:\n\
      \  node_labels (list[str]): Labels for each node.\n  node_categories (list[int]\
      \ | None): Optional list of category/cluster identifiers. If None, defaults\
      \ to 1 for all nodes.\n  node_sizes (list[int] | None): Optional list of node\
      \ sizes. If None, defaults to 1 for all nodes.\n  three_d (bool | None): If\
      \ False or None, return 2D positions (x and y). If True, return 3D positions\
      \ (x, y, z).\n\nReturns:\n  list[NodePosition]: A list of NodePosition objects,\
      \ one per input label. Each position includes:\n    label: str(node label)\n\
      \    x, y (and z if three_d is True): coordinates initialized to 0\n    cluster:\
      \ str(int(node_category))\n    size: int(node_size)\n\nNotes:\n  - If node_categories\
      \ or node_sizes are provided and their lengths are shorter than the number of\
      \ labels, an IndexError may be raised when indexing into these lists.\n  - The\
      \ cluster is derived from node_categories (default 1) and converted to a string."
  - node_id: graphrag/index/operations/layout_graph/zero.py::run
    name: run
    signature: "def run(\n    graph: nx.Graph,\n    on_error: ErrorHandlerFn,\n) ->\
      \ GraphLayout"
    docstring: "Compute a zero-coordinate GraphLayout for the given graph, optionally\
      \ using per-node cluster/category and size hints, and fall back to a default\
      \ layout if an error occurs.\n\nArgs:\n  graph: nx.Graph\n      The input graph.\
      \ Nodes may have attributes such as cluster (or community) and degree (or size)\
      \ that are used to influence the layout.\n\n  on_error: ErrorHandlerFn\n   \
      \   Callback invoked when an error occurs. It is called with the exception,\
      \ a formatted traceback string, and None.\n\nReturns:\n  GraphLayout\n     \
      \ A layout (list of NodePosition) for all nodes in the graph. Coordinates are\
      \ initialized to zero; cluster and size metadata are derived from node attributes\
      \ when available. If an error occurs during layout computation, a fallback layout\
      \ with all nodes at (0,0) is returned.\n\nRaises:\n  None"
  classes: []
- file: graphrag/index/operations/prune_graph.py
  functions:
  - node_id: graphrag/index/operations/prune_graph.py::_get_upper_threshold_by_std
    name: _get_upper_threshold_by_std
    signature: "def _get_upper_threshold_by_std(\n    data: list[float] | list[int],\
      \ std_trim: float\n) -> float"
    docstring: "Get upper threshold by standard deviation.\n\nArgs:\n    data: list[float]\
      \ | list[int], a list of numeric values used to compute the threshold.\n   \
      \ std_trim: float, multiplier for the standard deviation to offset the mean.\n\
      \nReturns:\n    float: The upper threshold computed as mean + std_trim * std\
      \ of the data."
  - node_id: graphrag/index/operations/prune_graph.py::prune_graph
    name: prune_graph
    signature: "def prune_graph(\n    graph: nx.Graph,\n    min_node_freq: int = 1,\n\
      \    max_node_freq_std: float | None = None,\n    min_node_degree: int = 1,\n\
      \    max_node_degree_std: float | None = None,\n    min_edge_weight_pct: float\
      \ = 40,\n    remove_ego_nodes: bool = False,\n    lcc_only: bool = False,\n\
      ) -> nx.Graph"
    docstring: "Prune graph by removing nodes that are out of frequency/degree ranges\
      \ and edges with low weights.\n\nArgs:\n    graph (nx.Graph): The graph to prune.\n\
      \    min_node_freq (int): Minimum node frequency threshold; nodes with frequency\
      \ below this value are removed.\n    max_node_freq_std (float | None): If provided,\
      \ upper threshold is mean + max_node_freq_std * std of node frequencies; nodes\
      \ with frequency above this threshold are removed.\n    min_node_degree (int):\
      \ Minimum degree threshold; nodes with degree below this value are removed.\n\
      \    max_node_degree_std (float | None): If provided, upper threshold is mean\
      \ + max_node_degree_std * std of node degrees; nodes with degree above this\
      \ threshold are removed.\n    min_edge_weight_pct (float): Percentile for edge\
      \ weights; edges with weight below this percentile are removed.\n    remove_ego_nodes\
      \ (bool): If True, remove the ego node (the node with the highest degree) before\
      \ pruning.\n    lcc_only (bool): If True, return only the largest connected\
      \ component of the pruned graph.\n\nReturns:\n    nx.Graph: The pruned graph.\
      \ If lcc_only is True, returns the largest connected component of the pruned\
      \ graph."
  classes: []
- file: graphrag/index/operations/snapshot_graphml.py
  functions:
  - node_id: graphrag/index/operations/snapshot_graphml.py::snapshot_graphml
    name: snapshot_graphml
    signature: "def snapshot_graphml(\n    input: str | nx.Graph,\n    name: str,\n\
      \    storage: PipelineStorage,\n) -> None"
    docstring: "Take a snapshot of a graph in GraphML format and persist it to storage.\n\
      \nThis function accepts either a GraphML content string or a NetworkX Graph.\
      \ If input is a string, it is treated as GraphML content (not a file path).\
      \ If input is a Graph, it is converted to GraphML using NetworkX's GraphML generator.\
      \ The resulting GraphML content is written asynchronously to the provided storage\
      \ backend under the key name + \".graphml\".\n\nArgs:\n    input: str | nx.Graph\n\
      \        GraphML content as a string, or a NetworkX graph object to be converted\
      \ to GraphML.\n    name: str\n        Base name for the stored GraphML entry;\
      \ the actual storage key will be name + \".graphml\".\n    storage: PipelineStorage\n\
      \        Storage backend used to persist the GraphML representation asynchronously.\n\
      \nReturns:\n    None\n        The function completes the storage operation asynchronously\
      \ and does not return a value.\n\nRaises:\n    Exceptions raised by the storage\
      \ backend or by GraphML generation may be propagated to the caller."
  classes: []
- file: graphrag/index/operations/summarize_communities/build_mixed_context.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
    name: build_mixed_context
    signature: "def build_mixed_context(\n    context: list[dict], tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> str"
    docstring: "Builds the parent context by concatenating all sub-communities' contexts,\
      \ with a fallback to sub-community reports if the combined context would exceed\
      \ the token limit.\n\nArgs:\n    context: list[dict]\n        List of sub-community\
      \ contexts to process.\n    tokenizer: Tokenizer\n        Tokenizer used to\
      \ count tokens to enforce max_context_tokens.\n    max_context_tokens: int\n\
      \        Maximum number of tokens allowed for the resulting context.\n\nReturns:\n\
      \    str\n        The resulting context as a string; may be a concatenation\
      \ of local contexts, or a CSV of substitute reports if limits are reached."
  classes: []
- file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__call__
    name: __call__
    signature: 'def __call__(self, input_text: str)'
    docstring: "Generate a community report for the given input text using the configured\
      \ model and return both structured and text outputs.\n\nArgs:\n  input_text:\
      \ str - The input text to generate the report from.\n\nReturns:\n  CommunityReportsResult\
      \ - The result containing:\n    structured_output: CommunityReportResponse |\
      \ None - The parsed structured report from the model.\n    output: str - The\
      \ human-readable text representation of the report.\n\nRaises:\n  None"
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor._get_text_output
    name: _get_text_output
    signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
    docstring: "\"\"\"Get the text output for a CommunityReportResponse.\n\nArgs:\n\
      \    report: CommunityReportResponse\n        The report object containing a\
      \ title, a summary, and a list of findings. Each finding provides a summary\
      \ and an explanation.\n\nReturns:\n    str\n        A markdown-formatted string.\
      \ It starts with a top-level header using the report title, includes the report\
      \ summary, and appends a section for each finding using its summary as a subheader\
      \ and its explanation as content.\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n        max_report_length: int | None = None,\n    )"
    docstring: "Initialize a CommunityReportsExtractor with the provided configuration.\n\
      \nArgs:\n  model_invoker: ChatModel\n    The model invoker used to run prompts.\n\
      \  extraction_prompt: str | None\n    Custom prompt to use for extraction. If\
      \ None, defaults to COMMUNITY_REPORT_PROMPT.\n  on_error: ErrorHandlerFn | None\n\
      \    Function to handle errors. If None, a no-op is used.\n  max_report_length:\
      \ int | None\n    Maximum length of the generated report. If None, defaults\
      \ to 1500.\n\nReturns:\n  None"
  classes:
  - class_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor
    name: CommunityReportsExtractor
    docstring: "CommunityReportsExtractor orchestrates generation of a markdown-formatted\
      \ community report from input text by invoking a chat model with a configurable\
      \ extraction prompt, producing both a structured report and a human-readable\
      \ markdown output.\n\nArgs:\n    model_invoker: ChatModel - The model invoker\
      \ used to run prompts.\n    extraction_prompt: str | None - Custom prompt to\
      \ use for extraction. If None, defaults to COMMUNITY_REPORT_PROMPT.\n    on_error:\
      \ ErrorHandlerFn | None - Function to handle errors. If None, a no-op is used.\n\
      \    max_report_length: int | None - Maximum length for the generated report.\n\
      \nReturns:\n    CommunityReportsResult - The result containing:\n      structured_output:\
      \ CommunityReportResponse | None - The parsed structured report from the model.\n\
      \      output: str - The human-readable markdown text report.\n\nRaises:\n \
      \   Exceptions raised by the underlying model invocation or error handler may\
      \ propagate.\n\nAttributes:\n    model_invoker: The model invoker used to run\
      \ prompts (ChatModel).\n    extraction_prompt: str | None - Custom prompt to\
      \ use for extraction. If None, defaults to COMMUNITY_REPORT_PROMPT.\n    on_error:\
      \ ErrorHandlerFn | None - Function to handle errors. If None, a no-op is used.\n\
      \    max_report_length: int | None - Maximum report length."
    methods:
    - name: __call__
      signature: 'def __call__(self, input_text: str)'
    - name: _get_text_output
      signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
    - name: __init__
      signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
        \        extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
        \ | None = None,\n        max_report_length: int | None = None,\n    )"
- file: graphrag/index/operations/summarize_communities/explode_communities.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
    name: explode_communities
    signature: "def explode_communities(\n    communities: pd.DataFrame, entities:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    docstring: "Explode a list of communities into nodes for filtering.\n\nArgs:\n\
      \    communities: pd.DataFrame\n        DataFrame containing an entity_ids column\
      \ with the IDs of entities in each community, along with community and level\
      \ metadata used after exploding.\n    entities: pd.DataFrame\n        DataFrame\
      \ containing an id column used to join with the exploded entity_ids, and a community\
      \ identifier column (the one named by COMMUNITY_ID) for filtering.\n\nReturns:\n\
      \    pd.DataFrame\n        A DataFrame of entities enriched with community information\
      \ after the explode and merge, filtered to exclude rows where the community\
      \ identifier equals -1.\n\nRaises:\n    KeyError\n        If required columns\
      \ are missing from the input DataFrames (for example entity_ids in communities,\
      \ id in entities, or the COMMUNITY_ID column)."
  classes: []
- file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
    name: _drop_community_level
    signature: 'def _drop_community_level(df: pd.DataFrame) -> pd.DataFrame'
    docstring: "Drop the community level column from the dataframe.\n\nArgs:\n   \
      \ df (pd.DataFrame): The DataFrame from which to drop the community level column.\n\
      \nReturns:\n    pd.DataFrame: The DataFrame with the COMMUNITY_LEVEL column\
      \ dropped.\n\nRaises:\n    KeyError: If the COMMUNITY_LEVEL column does not\
      \ exist in df."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
    name: _antijoin_reports
    signature: 'def _antijoin_reports(df: pd.DataFrame, reports: pd.DataFrame) ->
      pd.DataFrame'
    docstring: "Return records in df that are not in reports.\n\nArgs:\n    df: The\
      \ DataFrame to apply the exclusion to.\n    reports: The DataFrame containing\
      \ rows to remove from df.\n\nReturns:\n    pd.DataFrame: The rows from df whose\
      \ COMMUNITY_ID value is not present in reports."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
    name: _at_level
    signature: 'def _at_level(level: int, df: pd.DataFrame) -> pd.DataFrame'
    docstring: "Return records at the given level.\n\nArgs:\n    level: The level\
      \ to filter by (int).\n    df: DataFrame containing community records, expected\
      \ to have a COMMUNITY_LEVEL column.\n\nReturns:\n    pd.DataFrame: A DataFrame\
      \ containing only records where COMMUNITY_LEVEL equals level.\n\nRaises:\n \
      \   KeyError: If the COMMUNITY_LEVEL column is not present in df."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
    name: _sort_and_trim_context
    signature: "def _sort_and_trim_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series"
    docstring: "Sort and trim the context to fit the token limit.\n\nArgs:\n    df:\
      \ DataFrame containing the contexts, with a column named by schemas.ALL_CONTEXT\
      \ that holds the per-row context data to be processed.\n    tokenizer: Tokenizer.\
      \ Tokenizer used to count tokens when trimming.\n    max_context_tokens: int.\
      \ Maximum number of tokens allowed for each context after trimming.\n\nReturns:\n\
      \    pd.Series. A Series containing the processed contexts after sorting and\
      \ trimming per entry.\n\nRaises:\n    Exception: Propagates any exception raised\
      \ by the transformation (e.g., via transform_series or sort_context) when processing\
      \ the contexts."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
    name: _get_subcontext_df
    signature: "def _get_subcontext_df(\n    level: int, report_df: pd.DataFrame,\
      \ local_context_df: pd.DataFrame\n) -> pd.DataFrame"
    docstring: "Get sub-community context for each community.\n\nArgs:\n    level:\
      \ int\n        The level to extract sub-context for.\n    report_df: pd.DataFrame\n\
      \        DataFrame containing the reports for communities at the given level.\n\
      \    local_context_df: pd.DataFrame\n        DataFrame containing local context\
      \ information for communities.\n\nReturns:\n    pd.DataFrame\n        DataFrame\
      \ containing sub-context for each community, with COMMUNITY_ID renamed to SUB_COMMUNITY.\n\
      \nRaises:\n    KeyError: If the COMMUNITY_LEVEL column is not present in report_df\
      \ or local_context_df."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
    name: _build_mixed_context
    signature: "def _build_mixed_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
      \ max_context_tokens: int\n) -> pd.Series"
    docstring: "Build mixed context for each row by applying build_mixed_context to\
      \ the ALL_CONTEXT data and trimming to the token limit.\n\nArgs:\n    df: DataFrame\
      \ containing ALL_CONTEXT column to be processed.\n    tokenizer: Tokenizer used\
      \ to count tokens during trimming.\n    max_context_tokens: int Maximum number\
      \ of tokens allowed for the resulting context.\n\nReturns:\n    pd.Series: A\
      \ Series of processed mixed contexts per row.\n\nRaises:\n    Exception: Any\
      \ exception raised by the transformation function will be propagated to the\
      \ caller."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
    name: _prepare_reports_at_level
    signature: "def _prepare_reports_at_level(\n    node_df: pd.DataFrame,\n    edge_df:\
      \ pd.DataFrame,\n    claim_df: pd.DataFrame | None,\n    tokenizer: Tokenizer,\n\
      \    level: int,\n    max_context_tokens: int = 16_000,\n) -> pd.DataFrame"
    docstring: "Prepare reports at a given level.\n\nArgs:\n    node_df: pd.DataFrame\n\
      \        DataFrame containing node details; filtered to the specified level\
      \ using schemas.COMMUNITY_LEVEL.\n    edge_df: pd.DataFrame\n        DataFrame\
      \ containing edge details between nodes.\n    claim_df: pd.DataFrame | None\n\
      \        Optional DataFrame containing claims related to nodes; may be None.\n\
      \    tokenizer: Tokenizer\n        Tokenizer used to compute context token counts\
      \ for context generation.\n    level: int\n        Target community level for\
      \ which to prepare reports.\n    max_context_tokens: int\n        Maximum number\
      \ of tokens allowed for the generated context strings (default 16_000).\n\n\
      Returns:\n    pd.DataFrame\n        DataFrame containing prepared reports at\
      \ the given level, with node and edge details\n        aggregated, optional\
      \ claim details merged, ALL_CONTEXT populated, and context strings\n       \
      \ generated for each community by parallel_sort_context_batch.\n\nRaises:\n\
      \    Exception\n        If an error occurs during processing (for example, due\
      \ to missing expected columns or invalid data)."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
    name: _get_community_df
    signature: "def _get_community_df(\n    level: int,\n    invalid_context_df: pd.DataFrame,\n\
      \    sub_context_df: pd.DataFrame,\n    community_hierarchy_df: pd.DataFrame,\n\
      \    tokenizer: Tokenizer,\n    max_context_tokens: int,\n) -> pd.DataFrame"
    docstring: "Get community context for each community.\n\nArgs:\n  level: The level\
      \ to process.\n  invalid_context_df: DataFrame containing IDs of communities\
      \ considered invalid at this level.\n  sub_context_df: DataFrame containing\
      \ sub-community context data for each community.\n  community_hierarchy_df:\
      \ DataFrame representing the community hierarchy from which communities at the\
      \ given level are selected.\n  tokenizer: Tokenizer used to build and trim mixed\
      \ context strings.\n  max_context_tokens: Maximum number of tokens allowed for\
      \ the resulting context.\n\nReturns:\n  pd.DataFrame: DataFrame containing one\
      \ row per community at the specified level, including:\n    - COMMUNITY_ID\n\
      \    - ALL_CONTEXT: a list of dictionaries with keys SUB_COMMUNITY, ALL_CONTEXT,\
      \ FULL_CONTENT, CONTEXT_SIZE\n    - CONTEXT_STRING: mixed context string built\
      \ from ALL_CONTEXT\n    - COMMUNITY_LEVEL: the level value\n\nRaises:\n  KeyError:\
      \ If required columns are missing from the input DataFrames."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
    name: build_local_context
    signature: "def build_local_context(\n    nodes,\n    edges,\n    claims,\n  \
      \  tokenizer: Tokenizer,\n    callbacks: WorkflowCallbacks,\n    max_context_tokens:\
      \ int = 16_000,\n)"
    docstring: "Prepare initial local context for all communities, processing level-by-level.\n\
      \nThis function computes per-level local context data and concatenates the results.\
      \ It determines the processing levels with get_levels, iterates through them\
      \ with progress_iterable using callbacks.progress, and for each level builds\
      \ a per-level DataFrame by calling _prepare_reports_at_level with the provided\
      \ inputs and the max_context_tokens budget. The resulting per-level DataFrame\
      \ has its COMMUNITY_LEVEL column set to the corresponding level, and all per-level\
      \ DataFrames are concatenated into a single DataFrame that represents the initial\
      \ local context for all communities.\n\nArgs:\n  nodes: DataFrame containing\
      \ community node details, including a COMMUNITY_LEVEL column.\n  edges: DataFrame\
      \ containing edge details between nodes.\n  claims: DataFrame or None containing\
      \ claims related to communities.\n  tokenizer: Tokenizer used to compute context\
      \ token counts during per-level processing.\n  callbacks: WorkflowCallbacks\
      \ used for progress reporting.\n  max_context_tokens: int, maximum number of\
      \ tokens allocated for per-level processing; defaults to 16,000.\n\nReturns:\n\
      \  pd.DataFrame: A concatenated DataFrame with prepared local context for all\
      \ levels; each row includes COMMUNITY_LEVEL indicating its level.\n\nRaises:\n\
      \  None."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
    name: build_level_context
    signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n  \
      \  community_hierarchy_df: pd.DataFrame,\n    local_context_df: pd.DataFrame,\n\
      \    tokenizer: Tokenizer,\n    level: int,\n    max_context_tokens: int,\n\
      ) -> pd.DataFrame"
    docstring: "Prepare context for each community at a given level.\n\nThis function\
      \ selects the communities at the specified level from local_context_df, then\
      \ classifies their local contexts into valid (CONTEXT_EXCEED_FLAG is False)\
      \ and invalid (CONTEXT_EXCEED_FLAG is True) records. It returns a DataFrame\
      \ containing the prepared context for that level, including the contextual strings\
      \ and metadata required for downstream processing. Note that this function may\
      \ create or update columns such as CONTEXT_STRING, CONTEXT_SIZE, and CONTEXT_EXCEED_FLAG\
      \ in the resulting DataFrame. The level scoping ensures only communities at\
      \ the given level are processed.\n\nProcessing flow by branch:\n- Early return\
      \ when there are no invalid contexts: if invalid_context_df is empty, returns\
      \ valid_context_df unchanged.\n- No available reports (report_df is None or\
      \ empty): for all invalid contexts, the context string is trimmed to fit max_context_tokens\
      \ via _sort_and_trim_context, CONTEXT_SIZE is computed, CONTEXT_EXCEED_FLAG\
      \ is set to False, and the function returns the union of valid_context_df and\
      \ the trimmed invalid contexts.\n- Reports are available: remove the observed\
      \ reports from level_context_df (level_context_df = _antijoin_reports(level_context_df,\
      \ report_df)); for each remaining invalid context, attempt substitution with\
      \ sub-community reports by computing sub_context_df and building community_df;\
      \ any remaining invalids not covered by sub-communities are collected in remaining_df,\
      \ trimmed via _sort_and_trim_context, and finally all parts are united via union(valid_context_df,\
      \ community_df, remaining_df). The resulting CONTEXT_SIZE is computed from CONTEXT_STRING,\
      \ and CONTEXT_EXCEED_FLAG is set to False in the result.\n\nReturns:\n    pd.DataFrame:\
      \ A DataFrame containing prepared contexts for communities at the specified\
      \ level. The rows include CONTEXT_STRING and metadata columns such as CONTEXT_SIZE\
      \ and CONTEXT_EXCEED_FLAG; the output is filtered to the given level.\n\nRaises:\n\
      \    Propagated exceptions from helper utilities may be raised (e.g., KeyError\
      \ if required columns are missing); the function does not raise its own explicit\
      \ errors."
  classes: []
- file: graphrag/index/operations/summarize_communities/graph_context/sort_context.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::_get_context_string
    name: _get_context_string
    signature: "def _get_context_string(\n        entities: list[dict],\n        edges:\
      \ list[dict],\n        claims: list[dict],\n        sub_community_reports: list[dict]\
      \ | None = None,\n    ) -> str"
    docstring: "Concatenate structured data into a context string.\n\nArgs:\n    entities:\
      \ List of entity dictionaries to include in the context.\n    edges: List of\
      \ edge/relationship dictionaries to include in the context.\n    claims: List\
      \ of claim dictionaries to include in the context.\n    sub_community_reports:\
      \ Optional list of dictionaries for sub-community reports to include at the\
      \ top.\n\nReturns:\n    str: The concatenated context string with optional reports\
      \ and sections for Entities, Claims, and Relationships, formatted as CSV blocks."
  - node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
    name: sort_context
    signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer:\
      \ Tokenizer,\n    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n    node_name_column: str = schemas.TITLE,\n    node_details_column:\
      \ str = schemas.NODE_DETAILS,\n    edge_id_column: str = schemas.SHORT_ID,\n\
      \    edge_details_column: str = schemas.EDGE_DETAILS,\n    edge_degree_column:\
      \ str = schemas.EDGE_DEGREE,\n    edge_source_column: str = schemas.EDGE_SOURCE,\n\
      \    edge_target_column: str = schemas.EDGE_TARGET,\n    claim_details_column:\
      \ str = schemas.CLAIM_DETAILS,\n) -> str"
    docstring: "Sorts context by degree in descending order, optimizing for performance.\n\
      \nArgs:\n    local_context: list[dict]. Local context data; each entry may contain\
      \ edge details under edge_details_column and associated node and claim information\
      \ as defined by the surrounding schema.\n    tokenizer: Tokenizer. Tokenizer\
      \ used to count tokens for max_context_tokens to enforce length constraints.\n\
      \    sub_community_reports: list[dict] | None. Optional list of sub-community\
      \ reports to include at the top of the context.\n    max_context_tokens: int\
      \ | None. Optional maximum number of tokens for the produced context; if exceeded,\
      \ the context is truncated accordingly.\n    node_name_column: str. Column name\
      \ used to identify a node's display name.\n    node_details_column: str. Column\
      \ name for the node's details payload.\n    edge_id_column: str. Column name\
      \ for the edge identifier.\n    edge_details_column: str. Column name for the\
      \ edge details payload.\n    edge_degree_column: str. Column name for the edge\
      \ degree measure.\n    edge_source_column: str. Column name for the edge source\
      \ node.\n    edge_target_column: str. Column name for the edge target node.\n\
      \    claim_details_column: str. Column name for the claim details associated\
      \ with nodes.\n\nReturns:\n    str. The consolidated context string built from\
      \ Entities, Claims, and Relationships, optionally prefixed with sub-community\
      \ reports, truncated to max_context_tokens if specified.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
    name: parallel_sort_context_batch
    signature: "def parallel_sort_context_batch(\n    community_df, tokenizer: Tokenizer,\
      \ max_context_tokens, parallel=False\n)"
    docstring: "Calculate context strings for each community entry, optionally using\
      \ parallel execution, and populate related context columns.\n\nArgs:\n  community_df:\
      \ DataFrame containing community data to be enriched with context strings.\n\
      \  tokenizer: Tokenizer used to count tokens for context strings.\n  max_context_tokens:\
      \ Maximum number of tokens allowed for a context string.\n  parallel: Whether\
      \ to enable parallel computation of context strings using ThreadPoolExecutor.\n\
      \nReturns:\n  The input DataFrame with additional context-related columns populated:\n\
      \  - CONTEXT_STRING: the computed context string for each row.\n  - CONTEXT_SIZE:\
      \ token length of CONTEXT_STRING.\n  - CONTEXT_EXCEED_FLAG: whether CONTEXT_SIZE\
      \ exceeds max_context_tokens.\n\nRaises:\n  Exceptions raised by sort_context\
      \ or by the parallel execution machinery if parallel is enabled."
  classes: []
- file: graphrag/index/operations/summarize_communities/strategies.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/strategies.py::_run_extractor
    name: _run_extractor
    signature: "def _run_extractor(\n    model: ChatModel,\n    community: str | int,\n\
      \    input: str,\n    level: int,\n    args: StrategyConfig,\n) -> CommunityReport\
      \ | None"
    docstring: "Run the CommunityReportsExtractor to produce a CommunityReport from\
      \ the given input.\n\nArgs:\n  model (ChatModel): The chat model instance used\
      \ to perform extraction.\n  community (str | int): Identifier for the community\
      \ being processed.\n  input (str): The input text to extract information from.\n\
      \  level (int): The reporting level to assign to the resulting CommunityReport.\n\
      \  args (StrategyConfig): Strategy configuration containing optional keys such\
      \ as extraction_prompt and max_report_length.\n\nReturns:\n  CommunityReport\
      \ | None: The constructed CommunityReport, or None if no structured report is\
      \ produced or an error occurs during extraction."
  - node_id: graphrag/index/operations/summarize_communities/strategies.py::run_graph_intelligence
    name: run_graph_intelligence
    signature: "def run_graph_intelligence(\n    community: str | int,\n    input:\
      \ str,\n    level: int,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    args: StrategyConfig,\n) -> CommunityReport | None"
    docstring: "Run the graph intelligence entity extraction strategy.\n\nArgs:\n\
      \    community: Identifier for the community being processed.\n    input: The\
      \ input text to extract information from.\n    level: The reporting level to\
      \ assign to the resulting CommunityReport.\n    callbacks: WorkflowCallbacks\
      \ instance providing callback hooks during processing.\n    cache: PipelineCache\
      \ instance used for caching language model results and computations.\n    args:\
      \ StrategyConfig containing strategy settings, including llm configuration and\
      \ extraction prompts.\n\nReturns:\n    CommunityReport | None: The produced\
      \ CommunityReport if extraction succeeds, otherwise None."
  classes: []
- file: graphrag/index/operations/summarize_communities/summarize_communities.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::load_strategy
    name: load_strategy
    signature: "def load_strategy(\n    strategy: CreateCommunityReportsStrategyType,\n\
      ) -> CommunityReportsStrategy"
    docstring: "Load the strategy method for community reports based on the provided\
      \ type.\n\nArgs:\n    strategy (CreateCommunityReportsStrategyType): The strategy\
      \ type used to determine which community reports strategy to load.\n\nReturns:\n\
      \    CommunityReportsStrategy: The callable strategy function corresponding\
      \ to the supplied strategy type.\n\nRaises:\n    ValueError: If an unknown strategy\
      \ type is provided."
  - node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::_generate_report
    name: _generate_report
    signature: "def _generate_report(\n    runner: CommunityReportsStrategy,\n   \
      \ callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy: dict,\n\
      \    community_id: int,\n    community_level: int,\n    community_context: str,\n\
      ) -> CommunityReport | None"
    docstring: "Generate a report for a single community.\n\nArgs:\n  runner: The\
      \ strategy function used to generate the report for the community.\n  callbacks:\
      \ Callbacks to use during report generation.\n  cache: Cache instance used by\
      \ the report generation process.\n  strategy: Strategy configuration for the\
      \ report generation.\n  community_id: Identifier of the community for which\
      \ to generate the report.\n  community_level: Level of the community.\n  community_context:\
      \ Context string describing the community.\nReturns:\n  CommunityReport | None\n\
      \      The generated CommunityReport, or None if no report was produced.\nRaises:\n\
      \  Exception: Propagates exceptions raised by the underlying runner."
  - node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::run_generate
    name: run_generate
    signature: def run_generate(record)
    docstring: "Generate a community summary for a single record.\n\nArgs:\n  record:\
      \ dict-like containing the keys defined by schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL,\
      \ and schemas.CONTEXT_STRING. The function uses record[schemas.COMMUNITY_ID],\
      \ record[schemas.COMMUNITY_LEVEL], and record[schemas.CONTEXT_STRING] to generate\
      \ the report.\n\nReturns:\n  CommunityReport | None: The generated report for\
      \ the given community, or None if no report could be produced.\n\nRaises:\n\
      \  Exception: May raise exceptions propagated from _generate_report and the\
      \ asynchronous operations involved in generating the report."
  - node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
    name: summarize_communities
    signature: "def summarize_communities(\n    nodes: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    local_contexts,\n    level_context_builder: Callable,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy:\
      \ dict,\n    tokenizer: Tokenizer,\n    max_input_length: int,\n    async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    num_threads: int = 4,\n)"
    docstring: "Generate community summaries across all levels and return a DataFrame\
      \ of CommunityReport records.\n\nArgs:\n    nodes: pd.DataFrame\n        DataFrame\
      \ containing node data used to determine levels and contexts.\n    communities:\
      \ pd.DataFrame\n        DataFrame containing community definitions and hierarchical\
      \ relationships.\n    local_contexts:\n        Local context data used to build\
      \ level contexts; passed to level_context_builder.\n    level_context_builder:\
      \ Callable\n        Function used to construct context objects for each level.\n\
      \    callbacks: WorkflowCallbacks\n        Callbacks for progress reporting\
      \ and other workflow events.\n    cache: PipelineCache\n        Cache to store\
      \ intermediate results during report generation.\n    strategy: dict\n     \
      \   Strategy configuration for report generation; expects a 'type' key to select\
      \ the strategy.\n    tokenizer: Tokenizer\n        Tokenizer used to process\
      \ text during context construction.\n    max_input_length: int\n        Maximum\
      \ number of tokens allowed in the combined input for generation.\n    async_mode:\
      \ AsyncType\n        Async scheduling mode to use (default AsyncType.AsyncIO).\n\
      \    num_threads: int\n        Number of concurrent worker threads to use.\n\
      \nReturns:\n    pd.DataFrame\n        DataFrame of generated CommunityReport\
      \ records. Each row corresponds to a\n        report for a specific community\
      \ at a specific level; columns match the fields\n        defined by the CommunityReport\
      \ type.\n\nNotes:\n    - Strategy loading is performed dynamically via load_strategy\
      \ based on strategy['type'];\n      an unsupported type will be resolved at\
      \ runtime by the strategy loader."
  classes: []
- file: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
    name: build_local_context
    signature: "def build_local_context(\n    community_membership_df: pd.DataFrame,\n\
      \    text_units_df: pd.DataFrame,\n    node_df: pd.DataFrame,\n    tokenizer:\
      \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame"
    docstring: "Prepare local context data for community report generation using text\
      \ unit data.\n\nComputes per-community local context by enriching text units\
      \ with degree information, merging with community membership, and producing\
      \ a per-community context string sorted by relevance. The function relies on\
      \ prep_text_units to obtain text unit details (including short_id, text, and\
      \ entity_degree) and merges these details with membership information to build\
      \ a per-community ALL_CONTEXT list. The resulting DataFrame includes a sorted\
      \ CONTEXT_STRING, its token size CONTEXT_SIZE, and a flag CONTEXT_EXCEED_FLAG\
      \ indicating whether the context exceeds max_context_tokens.\n\nArgs:\n  community_membership_df:\
      \ DataFrame containing community membership data with columns including COMMUNITY_ID,\
      \ COMMUNITY_LEVEL, and TEXT_UNIT_IDS.\n  text_units_df: DataFrame of text units\
      \ used to enrich with degree information via prep_text_units.\n  node_df: DataFrame\
      \ of nodes used to compute entity degrees for text units.\n  tokenizer: Tokenizer\
      \ used to compute token counts and to sort contexts.\n  max_context_tokens:\
      \ Maximum number of tokens allowed for a community's local context.\n\nReturns:\n\
      \  A pandas DataFrame containing per-community local context data, keyed by\
      \ COMMUNITY_ID and COMMUNITY_LEVEL, including ALL_CONTEXT (list of dictionaries\
      \ with id, text, entity_degree), CONTEXT_STRING (sorted context), CONTEXT_SIZE\
      \ (token count), and CONTEXT_EXCEED_FLAG (whether CONTEXT_SIZE exceeds max_context_tokens)."
  - node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
    name: build_level_context
    signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n  \
      \  community_hierarchy_df: pd.DataFrame,\n    local_context_df: pd.DataFrame,\n\
      \    level: int,\n    tokenizer: Tokenizer,\n    max_context_tokens: int = 16000,\n\
      ) -> pd.DataFrame"
    docstring: "Prep context for each community in a given level.\n\nFor each community:\n\
      - Check if local context fits within the limit, if yes use local context\n-\
      \ If local context exceeds the limit, iteratively replace local context with\
      \ sub-community reports, starting from the biggest sub-community\n\nArgs:\n\
      \    report_df: pd.DataFrame | None\n        DataFrame with reports for communities.\
      \ May be None if no reports are available.\n    community_hierarchy_df: pd.DataFrame\n\
      \        DataFrame describing the community hierarchy.\n    local_context_df:\
      \ pd.DataFrame\n        DataFrame containing the local context per community.\n\
      \    level: int\n        The level in the community hierarchy to prepare context\
      \ for.\n    tokenizer: Tokenizer\n        Tokenizer used to count tokens and\
      \ enforce the max_context_tokens limit.\n    max_context_tokens: int\n     \
      \   Maximum number of tokens allowed for the resulting context. Defaults to\
      \ 16000.\n\nReturns:\n    pd.DataFrame\n        DataFrame containing prepared\
      \ context for communities at the specified level, with context strings, sizes\
      \ and exceed flags as computed."
  classes: []
- file: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py::prep_text_units
    name: prep_text_units
    signature: "def prep_text_units(\n    text_unit_df: pd.DataFrame,\n    node_df:\
      \ pd.DataFrame,\n) -> pd.DataFrame"
    docstring: "Calculate text unit degree and concatenate text unit details.\n\n\
      Args:\n    text_unit_df (pd.DataFrame): DataFrame containing text unit information\
      \ to be enriched with degree information.\n    node_df (pd.DataFrame): DataFrame\
      \ of nodes. Each row should include TEXT_UNIT_IDS, TITLE, COMMUNITY_ID, and\
      \ NODE_DEGREE used to compute text unit degrees.\n\nReturns:\n    pd.DataFrame:\
      \ DataFrame with columns [COMMUNITY_ID, TEXT_UNIT_ID, ALL_DETAILS]."
  classes: []
- file: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::get_context_string
    name: get_context_string
    signature: "def get_context_string(\n    text_units: list[dict],\n    sub_community_reports:\
      \ list[dict] | None = None,\n) -> str"
    docstring: "Concatenate structured data into a context string.\n\nArgs:\n    text_units\
      \ (list[dict]): List of text unit dictionaries to include in the context. Each\
      \ dictionary should have an \"id\" key with a non-empty value.\n    sub_community_reports\
      \ (list[dict] | None): Optional list of dictionaries for sub-community reports\
      \ to include at the top. Only reports containing a non-empty community id (defined\
      \ by schemas.COMMUNITY_ID) are considered.\n\nReturns:\n    str: The context\
      \ string built by optionally including a reports section followed by a sources\
      \ section, separated by blank lines.\n\nRaises:\n    None: This function does\
      \ not raise exceptions."
  - node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
    name: sort_context
    signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer:\
      \ Tokenizer,\n    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
      \ int | None = None,\n) -> str"
    docstring: "Sort local context (list of text units) by total degree of associated\
      \ nodes in descending order.\n\nArgs:\n    local_context: list[dict] - Local\
      \ context data; a list of dictionaries representing text units.\n    tokenizer:\
      \ Tokenizer - Tokenizer used to count tokens for max_context_tokens to enforce\
      \ length constraints.\n    sub_community_reports: list[dict] | None - Optional\
      \ list of dictionaries for sub-community reports to include at the top of the\
      \ resulting context string.\n    max_context_tokens: int | None - Maximum number\
      \ of tokens allowed in the resulting context string; if provided, text units\
      \ are added until the limit would be exceeded.\n\nReturns:\n    str: The context\
      \ string built from the selected text units; if max_context_tokens is provided,\
      \ the string includes as many units as fit within the token limit; otherwise,\
      \ the full sorted context is returned, including sub_community_reports if present."
  classes: []
- file: graphrag/index/operations/summarize_communities/typing.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this CreateCommunityReportsStrategyType\
      \ enum member.\n\nArgs:\n    self: CreateCommunityReportsStrategyType, the enum\
      \ member to represent as a string.\n\nReturns:\n    str: The enum member's value\
      \ enclosed in double quotes."
  classes:
  - class_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType
    name: CreateCommunityReportsStrategyType
    docstring: 'Enum describing the strategies for creating community reports in the
      summarize_communities operation.


      Description:

      CreateCommunityReportsStrategyType is an enumeration of the available strategies
      used to create community reports in the summarize_communities workflow. Each
      member represents a concrete strategy and exposes its identity via the member''s
      name and its associated representation via the member''s value. The exact type
      of member.value is defined by the enum''s members and may be a string, a callable,
      or another object that encodes the strategy.


      Notes:

      - Access member.name for the programmer-friendly identifier and member.value
      for the underlying representation.

      - This docstring follows Python Enum semantics: unless overridden, __repr__
      and __str__ reflect Enum behavior, and the value attached to each member is
      whatever was assigned.

      - If you need a human-facing description of a member, consider maintaining a
      separate mapping or documentation since Enum members themselves typically only
      provide name and value.


      Examples:

      - Access attributes of a member: strategy.name and strategy.value

      - Iterate over all members: for s in CreateCommunityReportsStrategyType: ...'
    methods:
    - name: __repr__
      signature: def __repr__(self)
- file: graphrag/index/operations/summarize_communities/utils.py
  functions:
  - node_id: graphrag/index/operations/summarize_communities/utils.py::get_levels
    name: get_levels
    signature: "def get_levels(\n    df: pd.DataFrame, level_column: str = schemas.COMMUNITY_LEVEL\n\
      ) -> list[int]"
    docstring: "Get the levels of the communities.\n\nArgs:\n    df (pd.DataFrame):\
      \ The data frame containing community data.\n    level_column (str): The name\
      \ of the column that contains the level values. Defaults to schemas.COMMUNITY_LEVEL.\n\
      \nReturns:\n    list[int]: A list of integer levels in descending order, with\
      \ -1 and NaN values ignored.\n\nRaises:\n    KeyError: If level_column is not\
      \ a column in df."
  classes: []
- file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  functions:
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions_with_llm
    name: _summarize_descriptions_with_llm
    signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
      \ str] | list[str], descriptions: list[str]\n    )"
    docstring: "Summarize descriptions using a large language model (LLM).\n\nArgs:\n\
      \    id: str | tuple[str, str] | list[str] - Identifier(s) for the entity or\
      \ entities.\n    descriptions: list[str] - Descriptions to be summarized.\n\n\
      Returns:\n    str - The summarized descriptions as a string.\n\nRaises:\n  \
      \  Exception - If the underlying LLM call fails or processing encounters an\
      \ error."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions
    name: _summarize_descriptions
    signature: "def _summarize_descriptions(\n        self, id: str | tuple[str, str],\
      \ descriptions: list[str]\n    ) -> str"
    docstring: "Asynchronously summarize descriptions into a single description.\n\
      \nArgs:\n  id: str | tuple[str, str] - Identifier(s) for the entity or entities.\n\
      \  descriptions: list[str] - Descriptions to be summarized.\n\nReturns:\n  str\
      \ - The summarized descriptions as a string.\n\nRaises:\n  Exception - If the\
      \ underlying LLM call fails or processing encounters an error."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n    ) -> SummarizationResult"
    docstring: "Asynchronously process the given descriptions for the specified id\
      \ and return the summarization result.\n\nArgs:\n  id: str | tuple[str, str]\
      \ - The identifier for the summarization target. It can be a string or a tuple\
      \ of two strings.\n  descriptions: list[str] - The list of description strings\
      \ to summarize. If empty, the resulting description will be an empty string;\
      \ if a single element, that element is returned; otherwise a summarization is\
      \ performed.\n\nReturns:\n  SummarizationResult - An object containing the id\
      \ and the resulting description (description is a string, possibly empty)."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
      \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
    docstring: "Initialize a SummarizeExtractor with the given model invoker and configuration.\n\
      \nArgs:\n    model_invoker (ChatModel): The model invoker used to run prompts.\n\
      \    max_summary_length (int): Maximum length of the summary to produce.\n \
      \   max_input_tokens (int): Maximum number of input tokens to consider for summarization.\n\
      \    summarization_prompt (str | None): Custom prompt to use for summarization.\
      \ If None, defaults to SUMMARIZE_PROMPT.\n    on_error (ErrorHandlerFn | None):\
      \ Optional error handler. If None, a no-op is used.\n\nReturns:\n    None: The\
      \ constructor does not return a value."
  classes:
  - class_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor
    name: SummarizeExtractor
    docstring: "SummarizeExtractor orchestrates the summarization of a list of description\
      \ strings into a single concise description for a target entity by invoking\
      \ a chat-based language model with a summarization prompt. The class is initialized\
      \ with a model invoker and configuration and is intended to be called to process\
      \ descriptions for a given identifier (or pair of identifiers).\n\nThis class\
      \ does not return a value from initialization. __init__ initializes the instance\
      \ and returns None implicitly.\n\nArgs\n    model_invoker: The ChatModel used\
      \ to run prompts.\n    max_summary_length: Maximum length of the summary to\
      \ produce.\n    max_input_tokens: Maximum number of input tokens to consider\
      \ for summarization.\n    summarization_prompt: Optional custom prompt to use\
      \ for summarization.\n    on_error: Optional error handler invoked on errors.\n\
      \nReturns\n    None\n\nRaises\n    Exception: If initialization fails or the\
      \ underlying LLM call fails or processing encounters an error.\n\nAttributes\n\
      \    model_invoker: The ChatModel used to run prompts.\n    max_summary_length:\
      \ Maximum length of the summary to produce.\n    max_input_tokens: Maximum number\
      \ of input tokens to consider for summarization.\n    summarization_prompt:\
      \ Optional custom prompt to use for summarization.\n    on_error: Optional error\
      \ handler invoked on errors.\n\nNotes\n    The class relies on the top-level\
      \ constants ENTITY_NAME_KEY, DESCRIPTION_LIST_KEY, and MAX_LENGTH_KEY to structure\
      \ input payloads for the summarization process. These keys influence how the\
      \ input data is organized before being sent to the language model but are not\
      \ modified by SummarizeExtractor."
    methods:
    - name: _summarize_descriptions_with_llm
      signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
        \ str] | list[str], descriptions: list[str]\n    )"
    - name: _summarize_descriptions
      signature: "def _summarize_descriptions(\n        self, id: str | tuple[str,\
        \ str], descriptions: list[str]\n    ) -> str"
    - name: __call__
      signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n\
        \        descriptions: list[str],\n    ) -> SummarizationResult"
    - name: __init__
      signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
        \        max_summary_length: int,\n        max_input_tokens: int,\n      \
        \  summarization_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
        \ | None = None,\n    )"
- file: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py
  functions:
  - node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_summarize_descriptions
    name: run_summarize_descriptions
    signature: "def run_summarize_descriptions(\n    model: ChatModel,\n    id: str\
      \ | tuple[str, str],\n    descriptions: list[str],\n    args: StrategyConfig,\n\
      ) -> SummarizedDescriptionResult"
    docstring: "Run the entity extraction chain to summarize descriptions for graph\
      \ intelligence.\n\nArgs:\n    model: ChatModel\n        The chat model instance\
      \ used to invoke summarization.\n    id: str | tuple[str, str]\n        Identifier\
      \ for the target item; could be a string or a pair of strings.\n    descriptions:\
      \ list[str]\n        The descriptions to summarize.\n    args: StrategyConfig\n\
      \        Strategy configuration, including max_input_tokens, max_summary_length,\
      \ and optional summarize_prompt.\n\nReturns:\n    SummarizedDescriptionResult\n\
      \        The summarized description along with its identifier.\n\nRaises:\n\
      \    Exception\n        If the underlying extraction process raises an exception\
      \ during processing."
  - node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_graph_intelligence
    name: run_graph_intelligence
    signature: "def run_graph_intelligence(\n    id: str | tuple[str, str],\n    descriptions:\
      \ list[str],\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> SummarizedDescriptionResult"
    docstring: "Run the graph intelligence entity extraction strategy using a language\
      \ model to summarize the provided descriptions.\n\nArgs:\n    id: str | tuple[str,\
      \ str]\n        Identifier for the target item; could be a string or a pair\
      \ of strings.\n    descriptions: list[str]\n        The descriptions to summarize.\n\
      \    cache: PipelineCache\n        Cache to use for the language model and computations.\n\
      \    args: StrategyConfig\n        Strategy configuration, including llm settings\
      \ and summarization prompts.\n\nReturns:\n    SummarizedDescriptionResult\n\
      \        The summarized description result containing the id and generated description."
  classes: []
- file: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  functions:
  - node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::load_strategy
    name: load_strategy
    signature: 'def load_strategy(strategy_type: SummarizeStrategyType) -> SummarizationStrategy'
    docstring: "Load the summarization strategy callable for the given strategy_type.\n\
      \nArgs:\n    strategy_type (SummarizeStrategyType): The strategy type used to\
      \ determine which summarization strategy to load.\n\nReturns:\n    SummarizationStrategy:\
      \ The loaded strategy callable corresponding to the provided strategy_type.\n\
      \nRaises:\n    ValueError: If an unknown strategy_type is provided."
  - node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::do_summarize_descriptions
    name: do_summarize_descriptions
    signature: "def do_summarize_descriptions(\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n        ticker: ProgressTicker,\n       \
      \ semaphore: asyncio.Semaphore,\n    )"
    docstring: "Run a summarization strategy on the provided descriptions for a given\
      \ id or pair of ids.\n\nArgs:\n  id: Identifier for the descriptions to summarize;\
      \ can be a string or a tuple of two strings.\n  descriptions: Descriptions to\
      \ summarize.\n  ticker: ProgressTicker used to report progress.\n  semaphore:\
      \ Semaphore controlling concurrency for the operation.\n\nReturns:\n  The results\
      \ produced by strategy_exec for the given id and descriptions."
  - node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
    name: get_summarized
    signature: "def get_summarized(\n        nodes: pd.DataFrame, edges: pd.DataFrame,\
      \ semaphore: asyncio.Semaphore\n    )"
    docstring: "Summarize descriptions for nodes and edges and return summary dataframes.\n\
      \nArgs:\n    nodes: pd.DataFrame\n        DataFrame containing node information\
      \ with at least a title and a description per node.\n    edges: pd.DataFrame\n\
      \        DataFrame containing edge information with at least source, target,\
      \ and a description per edge.\n    semaphore: asyncio.Semaphore\n        Semaphore\
      \ used to limit concurrent summarization operations.\n\nReturns:\n    tuple[pd.DataFrame,\
      \ pd.DataFrame]\n        A tuple containing:\n        - entity_descriptions:\
      \ DataFrame with columns 'title' and 'description' summarizing each node.\n\
      \        - relationship_descriptions: DataFrame with columns 'source', 'target',\
      \ and 'description' summarizing each edge.\n\nRaises:\n    Exceptions propagated\
      \ from the underlying asynchronous summarization processes and tasks."
  - node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
    name: summarize_descriptions
    signature: "def summarize_descriptions(\n    entities_df: pd.DataFrame,\n    relationships_df:\
      \ pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    strategy: dict[str, Any] | None = None,\n    num_threads: int = 4,\n) ->\
      \ tuple[pd.DataFrame, pd.DataFrame]"
    docstring: "Summarize entity and relationship descriptions from an entity graph,\
      \ using a language model.\n\nArgs:\n    entities_df: DataFrame containing entity\
      \ nodes with at least a title and a description per node.\n    relationships_df:\
      \ DataFrame containing edge information with at least source, target, and a\
      \ description per edge.\n    callbacks: WorkflowCallbacks providing progress\
      \ reporting hooks for long-running operations.\n    cache: PipelineCache used\
      \ to cache results from strategy execution.\n    strategy: dict[str, Any] |\
      \ None: Strategy configuration for the summarization strategy. If None, defaults\
      \ are applied.\n    num_threads: int: Number of concurrent workers to use for\
      \ summarization.\n\nReturns:\n    tuple[pd.DataFrame, pd.DataFrame]: A tuple\
      \ containing the entity descriptions DataFrame and the relationship descriptions\
      \ DataFrame.\n\nRaises:\n    ValueError: If an unknown strategy type is provided."
  classes: []
- file: graphrag/index/operations/summarize_descriptions/typing.py
  functions:
  - node_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this SummarizeStrategyType enum member.\n\
      \nArgs:\n    self: SummarizeStrategyType, the enum member to represent as a\
      \ string.\n\nReturns:\n    str: The enum member's value enclosed in double quotes."
  classes:
  - class_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType
    name: SummarizeStrategyType
    docstring: "Enumeration of strategies used to summarize descriptions within the\
      \ graphrag project.\n\nArgs:\n    There are no initialization parameters for\
      \ this enum.\n\nReturns:\n    SummarizeStrategyType: The enum member representing\
      \ a specific summarization strategy.\n\nRaises:\n    This class does not raise\
      \ exceptions during normal usage.\nNote:\n    The __repr__ method returns the\
      \ enum member's value enclosed in double quotes."
    methods:
    - name: __repr__
      signature: def __repr__(self)
- file: graphrag/index/run/run_pipeline.py
  functions:
  - node_id: graphrag/index/run/run_pipeline.py::_dump_json
    name: _dump_json
    signature: 'def _dump_json(context: PipelineRunContext) -> None'
    docstring: "Dump the stats and context state to the storage.\n\nArgs:\n    context:\
      \ PipelineRunContext\n        The pipeline run context containing stats, state,\
      \ and output storage used for persistence.\n\nReturns:\n    None\n        The\
      \ function completes without returning a value.\n\nRaises:\n    Exception\n\
      \        If storage operations fail or JSON serialization fails."
  - node_id: graphrag/index/run/run_pipeline.py::_copy_previous_output
    name: _copy_previous_output
    signature: "def _copy_previous_output(\n    storage: PipelineStorage,\n    copy_storage:\
      \ PipelineStorage,\n)"
    docstring: "Copy parquet outputs from the source storage to the copy storage asynchronously.\n\
      \nThis async function locates all parquet files in the source storage (matching\
      \ the pattern \".parquet\" at the end of the name), derives a base name by removing\
      \ the \".parquet\" extension from the path, loads the corresponding table from\
      \ the source storage, and writes it to the copy storage under the same base\
      \ name.\n\nArgs:\n    storage (PipelineStorage): The storage backend to read\
      \ parquet files from.\n    copy_storage (PipelineStorage): The storage backend\
      \ to which parquet files will be written.\n\nReturns:\n    None\n\nRaises:\n\
      \    ValueError: If a required parquet file cannot be found or if base name\
      \ extraction yields an invalid name. The underlying load operation may raise\
      \ ValueError.\n    Exception: Exceptions raised by the storage backend or parquet\
      \ reader/writer during the load or write operations may propagate to the caller.\n\
      \nNotes:\n    - base_name is derived by removing the \".parquet\" extension\
      \ from the discovered file path. If a file name contains multiple occurrences\
      \ of \".parquet\", all occurrences will be removed, which may lead to an unexpected\
      \ base name in rare cases.\n    - This function processes all parquet files\
      \ found in storage; if multiple files share the same base name, later files\
      \ may overwrite earlier ones in copy_storage."
  - node_id: graphrag/index/run/run_pipeline.py::_run_pipeline
    name: _run_pipeline
    signature: "def _run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> AsyncIterable[PipelineRunResult]"
    docstring: "Execute the provided pipeline asynchronously and yield results for\
      \ each workflow as it completes.\n\nArgs:\n    pipeline: Pipeline - The pipeline\
      \ to run\n    config: GraphRagConfig - Configuration for the run\n    context:\
      \ PipelineRunContext - Runtime context, including storage, callbacks, and state\n\
      \nReturns:\n    AsyncIterable[PipelineRunResult] - An async iterable that yields\
      \ a PipelineRunResult for each workflow as it runs, and yields a final result\
      \ with errors if an exception occurs.\n\nRaises:\n    None - This function handles\
      \ exceptions internally and does not propagate them to the caller."
  - node_id: graphrag/index/run/run_pipeline.py::run_pipeline
    name: run_pipeline
    signature: "def run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
      \    callbacks: WorkflowCallbacks,\n    is_update_run: bool = False,\n    additional_context:\
      \ dict[str, Any] | None = None,\n    input_documents: pd.DataFrame | None =\
      \ None,\n) -> AsyncIterable[PipelineRunResult]"
    docstring: "Run all workflows using a simplified pipeline.\n\nArgs:\n    pipeline:\
      \ Pipeline\n        The pipeline to run.\n    config: GraphRagConfig\n     \
      \   The GraphRag configuration to use for the run.\n    callbacks: WorkflowCallbacks\n\
      \        The callbacks to invoke during workflow execution.\n    is_update_run:\
      \ bool\n        Whether this run should perform an incremental update (default:\
      \ False).\n    additional_context: dict[str, Any] | None\n        Optional additional\
      \ context to merge into the run state.\n    input_documents: pd.DataFrame |\
      \ None\n        Optional input documents. If provided, they will be written\
      \ directly to storage\n        (skipping the usual load/parse steps) before\
      \ running the pipeline.\n\nReturns:\n    AsyncIterable[PipelineRunResult]\n\
      \        An asynchronous iterable that yields a PipelineRunResult for each workflow\
      \ as it runs.\n\nRaises:\n    ValueError: If the storage type is not registered.\n\
      \    Exception: May raise any exception raised by storage backends during read/write\
      \ operations or by the pipeline execution."
  classes: []
- file: graphrag/index/run/utils.py
  functions:
  - node_id: graphrag/index/run/utils.py::create_callback_chain
    name: create_callback_chain
    signature: "def create_callback_chain(\n    callbacks: list[WorkflowCallbacks]\
      \ | None,\n) -> WorkflowCallbacks"
    docstring: "Create a callback manager that encompasses multiple callbacks.\n\n\
      Args:\n    callbacks: list[WorkflowCallbacks] | None. The callbacks to register\
      \ on the manager. If None, an empty list is used.\n\nReturns:\n    WorkflowCallbacks:\
      \ A manager that aggregates the provided callbacks."
  - node_id: graphrag/index/run/utils.py::create_run_context
    name: create_run_context
    signature: "def create_run_context(\n    input_storage: PipelineStorage | None\
      \ = None,\n    output_storage: PipelineStorage | None = None,\n    previous_storage:\
      \ PipelineStorage | None = None,\n    cache: PipelineCache | None = None,\n\
      \    callbacks: WorkflowCallbacks | None = None,\n    stats: PipelineRunStats\
      \ | None = None,\n    state: PipelineState | None = None,\n) -> PipelineRunContext"
    docstring: "\"\"\"Create the run context for the pipeline.\n\nArgs:\n    input_storage:\
      \ PipelineStorage | None\n        The input storage to use for the run.\n  \
      \  output_storage: PipelineStorage | None\n        The output storage to use\
      \ for the run.\n    previous_storage: PipelineStorage | None\n        The previous\
      \ storage to use for the run.\n    cache: PipelineCache | None\n        The\
      \ cache to use for the run.\n    callbacks: WorkflowCallbacks | None\n     \
      \   The workflow callbacks to use during the run.\n    stats: PipelineRunStats\
      \ | None\n        The statistics collector for the run.\n    state: PipelineState\
      \ | None\n        Initial state for the run.\n\nReturns:\n    PipelineRunContext:\
      \ The configured run context.\n\n\"\"\""
  - node_id: graphrag/index/run/utils.py::get_update_storages
    name: get_update_storages
    signature: "def get_update_storages(\n    config: GraphRagConfig, timestamp: str\n\
      ) -> tuple[PipelineStorage, PipelineStorage, PipelineStorage]"
    docstring: "Get storage objects for the update index run.\n\nThe function creates\
      \ storage objects from the provided config:\n- output_storage: created from\
      \ config.output\n- update_storage: created from config.update_index_output\n\
      - timestamped_storage: derived by applying the given timestamp to update_storage\n\
      - delta_storage: timestamped_storage.child(\"delta\")\n- previous_storage: timestamped_storage.child(\"\
      previous\")\n\nReturns:\n    tuple[PipelineStorage, PipelineStorage, PipelineStorage]:\
      \ A tuple containing\n    output_storage, previous_storage, delta_storage respectively.\n\
      \nArgs:\n    config: GraphRagConfig The configuration containing storage settings\
      \ to use for output and update index outputs.\n    timestamp: str The timestamp\
      \ used to namespace the update index storage.\n\nRaises:\n    ValueError: If\
      \ the storage type is not registered when creating a storage from config."
  classes: []
- file: graphrag/index/text_splitting/check_token_limit.py
  functions:
  - node_id: graphrag/index/text_splitting/check_token_limit.py::check_token_limit
    name: check_token_limit
    signature: def check_token_limit(text, max_token)
    docstring: "\"\"\"Check whether the input text fits within the specified token\
      \ limit.\n\nArgs:\n    text (str): The input text to check against the token\
      \ limit.\n    max_token (int): The maximum number of tokens allowed for a single\
      \ chunk.\n\nReturns:\n    int: 1 if the text can be represented as a single\
      \ chunk under the limit, 0 otherwise.\n\nRaises:\n    Exception: If an error\
      \ occurs during tokenization/splitting using TokenTextSplitter.\n\"\"\""
  classes: []
- file: graphrag/index/text_splitting/text_splitting.py
  functions:
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.num_tokens
    name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
    docstring: "Return the number of tokens in a string.\n\nArgs:\n    text: The input\
      \ string to count tokens in.\n\nReturns:\n    int: The number of tokens in text."
  - node_id: graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
    name: split_multiple_texts_on_tokens
    signature: "def split_multiple_texts_on_tokens(\n    texts: list[str], tokenizer:\
      \ TokenChunkerOptions, tick: ProgressTicker\n) -> list[TextChunk]"
    docstring: "Split multiple texts and return chunks with metadata using the tokenizer.\n\
      \nArgs:\n    texts: list[str] The texts to split into chunks.\n    tokenizer:\
      \ TokenChunkerOptions The tokenizer configuration used to encode texts into\
      \ tokens and decode chunks.\n    tick: ProgressTicker A callback function to\
      \ track progress. If provided, tick(1) is called for each processed text.\n\n\
      Returns:\n    list[TextChunk] A list of TextChunk objects. Each TextChunk contains\
      \ the chunk_text, the indices of source documents contributing to the chunk,\
      \ and the number of tokens in the chunk."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    docstring: "Split text into chunks according to the concrete implementation.\n\
      \nArgs:\n    text: str | list[str]\n        The input text to split. Can be\
      \ a single string or a list of strings.\n\nReturns:\n    Iterable[str]\n   \
      \     An iterable of text chunks produced by the split operation."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
      \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
      \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n      \
      \  length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
      \        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n\
      \    )"
    docstring: "Init method for TextSplitter.\n\nInitialize the text splitter with\
      \ the given configuration.\n\nArgs:\n    chunk_size (int): Maximum length of\
      \ a chunk as measured by length_function. This follows the OpenAI embedding\
      \ model's max input buffer length guidance.\n    chunk_overlap (int): Overlap\
      \ between consecutive chunks, measured using length_function.\n    length_function\
      \ (LengthFn): Function to compute the length of text; defaults to len.\n   \
      \ keep_separator (bool): If True, keep separators when splitting text.\n   \
      \ add_start_index (bool): If True, add the starting index to chunks.\n    strip_whitespace\
      \ (bool): If True, strip leading and trailing whitespace from text.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
    name: split_single_text_on_tokens
    signature: 'def split_single_text_on_tokens(text: str, tokenizer: TokenChunkerOptions)
      -> list[str]'
    docstring: "Split a single text into chunks using the provided tokenizer.\n\n\
      Args:\n    text: str The input text to split into chunks.\n    tokenizer: TokenChunkerOptions\
      \ The tokenizer configuration used to encode the text into tokens and decode\
      \ chunks. It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\
      \nReturns:\n    list[str] The list of chunked text strings produced.\n\nRaises:\n\
      \    Exception: If the underlying tokenizer raises an error during encoding\
      \ or decoding operations."
  - node_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    docstring: "Split text into chunks.\n\nArgs:\n    text: str | list[str]\n    \
      \    The input text to split. A single string or a list of strings.\n\nReturns:\n\
      \    Iterable[str]\n        An iterable of text chunks. If a string is provided,\
      \ returns a single-element list containing the string; if a list of strings\
      \ is provided, returns that list as-is.\n\nRaises:\n    None"
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> list[str]'
    docstring: "Split text into chunks using a token-based tokenizer.\n\nIf text is\
      \ a list of strings, it is joined with spaces to form a single string before\
      \ splitting. If the input is NaN or an empty string, an empty list is returned.\n\
      \nArgs:\n    text: str | list[str]\n        The input text to split. If a list\
      \ of strings is provided, they are joined with spaces prior to splitting.\n\n\
      Returns:\n    list[str]\n        The list of chunked text strings produced.\n\
      \nRaises:\n    TypeError: If a non-string value is encountered during processing."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None\
      \ = None,\n        **kwargs: Any,\n    )"
    docstring: "Init method for TokenTextSplitter with an optional tokenizer.\n\n\
      This initializer sets the tokenizer to use for tokenization. If no tokenizer\
      \ is provided,\na default tokenizer is obtained via get_tokenizer(). Any additional\
      \ keyword arguments are\nforwarded to the base class initializer via super().__init__(**kwargs).\n\
      \nArgs:\n    tokenizer (Tokenizer | None): Tokenizer to use for tokenization.\
      \ If None, a default tokenizer\n        is obtained via get_tokenizer().\n \
      \   **kwargs (Any): Additional keyword arguments forwarded to the base class\
      \ initializer.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
    name: TokenTextSplitter
    docstring: "TokenTextSplitter splits input text into chunks using a token-based\
      \ tokenizer.\n\nArgs:\n    tokenizer: Tokenizer | None\n        Tokenizer to\
      \ use for tokenization. If None, a default tokenizer is obtained via get_tokenizer().\n\
      \    **kwargs: Any\n        Additional keyword arguments forwarded to the base\
      \ class initializer via super().__init__(**kwargs).\n\nAttributes:\n    tokenizer:\
      \ Tokenizer | None\n        Tokenizer used for tokenization. If None, a default\
      \ tokenizer is obtained via get_tokenizer().\n\nReturns:\n    None\n       \
      \ The constructor does not return a value; it initializes the instance.\n\n\
      Raises:\n    Exception: If get_tokenizer() or the base class initializer raise\
      \ an exception."
    methods:
    - name: num_tokens
      signature: 'def num_tokens(self, text: str) -> int'
    - name: split_text
      signature: 'def split_text(self, text: str | list[str]) -> list[str]'
    - name: __init__
      signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None\
        \ = None,\n        **kwargs: Any,\n    )"
  - class_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter
    name: TextSplitter
    docstring: "Abstract base class for splitting input text into chunks for downstream\
      \ processing (e.g., embeddings).\n\nThe class provides shared configuration\
      \ for text chunking and defines the abstract interface split_text, which concrete\
      \ subclasses must implement to perform the actual splitting.\n\nArgs:\n    chunk_size\
      \ (int): Maximum length of a chunk as measured by length_function. This follows\
      \ the OpenAI embedding model's max input buffer length guidance. Default: 8191.\n\
      \    chunk_overlap (int): Overlap between consecutive chunks, measured using\
      \ length_function. Default: 100.\n    length_function (LengthFn): Function to\
      \ compute the length of a string. Default: len.\n    keep_separator (bool):\
      \ Whether to keep the separator between chunks. Default: False.\n    add_start_index\
      \ (bool): Whether to prefix each chunk with its start index. Default: False.\n\
      \    strip_whitespace (bool): Whether to strip leading/trailing whitespace when\
      \ forming chunks. Default: True.\n\nAttributes:\n    chunk_size\n    chunk_overlap\n\
      \    length_function\n    keep_separator\n    add_start_index\n    strip_whitespace\n\
      \nMethods:\n    split_text(text: str | list[str]) -> Iterable[str]\n       \
      \ Abstract method. Split text into chunks according to the concrete implementation.\
      \ Accepts a single string or a list of strings as input and yields an iterable\
      \ of text chunks.\n\nNotes:\n    This is an abstract base class. split_text\
      \ is not implemented here and must be provided by subclasses."
    methods:
    - name: split_text
      signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    - name: __init__
      signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
        \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
        \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n    \
        \    length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
        \        add_start_index: bool = False,\n        strip_whitespace: bool =\
        \ True,\n    )"
  - class_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter
    name: NoopTextSplitter
    docstring: "NoopTextSplitter is a no-op text splitter that returns the input text\
      \ unchanged as a sequence of strings. It is stateless and serves as a minimal\
      \ default splitter when no actual splitting is required.\n\nsplit_text(self,\
      \ text: str | list[str]) -> Iterable[str]:\n    Split text into chunks without\
      \ modification. If text is a string, returns a single-element list containing\
      \ that string; if text is a list of strings, returns that list as-is.\n\nArgs:\n\
      \    text: The input text to split. A single string or a list of strings.\n\n\
      Returns:\n    Iterable[str]: The input text as an iterable of strings. Behavior\
      \ is:\n    - string input -> [string]\n    - list[str] input -> that same list\n\
      \nRaises:\n    None"
    methods:
    - name: split_text
      signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
- file: graphrag/index/typing/pipeline.py
  functions:
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.remove
    name: remove
    signature: 'def remove(self, name: str) -> None'
    docstring: "Remove all workflows from the pipeline that have the given name.\n\
      \nThis method mutates the Pipeline's internal state by removing every workflow\n\
      whose first element (the name) equals the provided value. All matching workflows\n\
      are removed; not just the first match.\n\nTime complexity: O(n), where n is\
      \ the number of workflows in the pipeline.\n\nIf no workflows match, the pipeline\
      \ remains unchanged and no exception is raised.\n\nArgs:\n    name: The name\
      \ of the workflows to remove.\n\nReturns:\n    None"
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.__init__
    name: __init__
    signature: 'def __init__(self, workflows: list[Workflow])'
    docstring: "Initializes the Pipeline with the provided workflows.\n\nArgs:\n \
      \   workflows: list[Workflow] The workflows to include in the pipeline.\n\n\
      Returns:\n    None"
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.run
    name: run
    signature: def run(self) -> Generator[Workflow]
    docstring: "\"\"\"Yield a generator of (name, workflow) pairs from the pipeline.\n\
      \nThe items yielded come from self.workflows and are tuples of (name, Workflow),\n\
      i.e., each yield is a pair containing the workflow's name (str) and the\ncorresponding\
      \ Workflow object.\n\nArgs:\n    self: The Pipeline instance.\n\nReturns:\n\
      \    Generator[tuple[str, Workflow]]: A generator that yields (name, workflow)\
      \ pairs in the pipeline.\n\"\"\""
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.names
    name: names
    signature: def names(self) -> list[str]
    docstring: "Return the names of the workflows in the pipeline.\n\nArgs:\n    self:\
      \ The Pipeline instance.\n\nReturns:\n    list[str]: The names of the workflows\
      \ in the pipeline, extracted from the first element\n        of each workflow\
      \ in self.workflows."
  classes:
  - class_id: graphrag/index/typing/pipeline.py::Pipeline
    name: Pipeline
    docstring: "Pipeline stores and manages an ordered sequence of named Workflow\
      \ objects, exposing operations to inspect and mutate the pipeline.\n\nPurpose\n\
      - Provide a lightweight container for (name, Workflow) pairs that preserves\
      \ order.\n- Enable removal of all entries with a given name, iteration over\
      \ the current entries, and retrieval of names.\n\nAttributes\n- workflows: list[tuple[str,\
      \ Workflow]]\n  Internal storage of the pipeline as (name, Workflow) pairs.\n\
      \nMethods\n- __init__(self, workflows: list[tuple[str, Workflow]])\n  Initialize\
      \ the Pipeline with the provided (name, Workflow) pairs. Returns None.\n\n-\
      \ remove(self, name: str) -> None\n  Remove all workflows from the pipeline\
      \ that have the given name. This mutates the internal state\n  by removing every\
      \ workflow whose first element (the name) equals the provided value. All\n \
      \ matching entries are removed; not just the first match.\n  Complexity: O(n)\
      \ where n is the number of entries. If no entries match, the pipeline remains\
      \ unchanged.\n\n- run(self) -> Generator[tuple[str, Workflow], None, None]\n\
      \  Yield a generator of (name, workflow) pairs from the pipeline. The items\
      \ yielded come from\n  self.workflows and are tuples of (name, Workflow), i.e.,\
      \ each yield is a pair containing the\n  workflow's name (str) and the corresponding\
      \ Workflow object.\n\n- names(self) -> list[str]\n  Return the names of the\
      \ workflows in the pipeline, in the same order as stored in\n  self.workflows.\n\
      \nNotes\n- Iteration reflects the current state of the pipeline; modifications\
      \ after obtaining a generator\n  may not affect items that have already been\
      \ yielded.\n- Edge cases: multiple entries with the same name are all removed\
      \ by remove."
    methods:
    - name: remove
      signature: 'def remove(self, name: str) -> None'
    - name: __init__
      signature: 'def __init__(self, workflows: list[Workflow])'
    - name: run
      signature: def run(self) -> Generator[Workflow]
    - name: names
      signature: def names(self) -> list[str]
- file: graphrag/index/update/communities.py
  functions:
  - node_id: graphrag/index/update/communities.py::_update_and_merge_communities
    name: _update_and_merge_communities
    signature: "def _update_and_merge_communities(\n    old_communities: pd.DataFrame,\n\
      \    delta_communities: pd.DataFrame,\n) -> tuple[pd.DataFrame, dict]"
    docstring: "Update and merge old and delta communities.\n\nThis function mutates\
      \ the provided DataFrames to ensure required structure, remaps delta\ncommunity\
      \ IDs to avoid collisions with old data, and merges them into a single DataFrame\n\
      aligned to COMMUNITIES_FINAL_COLUMNS. It also returns the mapping from original\
      \ delta\ncommunity IDs to the new IDs assigned during the merge.\n\nArgs:\n\
      \    old_communities (pd.DataFrame): The existing/old communities. If 'size'\
      \ or 'period' columns\n        are missing, they will be added with missing\
      \ values. Must contain an integer or numeric\n        'community' column.\n\
      \    delta_communities (pd.DataFrame): The delta/new communities to merge into\
      \ the old data.\n        If 'size' or 'period' columns are missing, they will\
      \ be added with missing values. Must contain\n        a numeric 'community'\
      \ column and a 'parent' column that will be remapped using the computed\n  \
      \      ID mapping.\n\nReturns:\n    tuple[pd.DataFrame, dict]:\n        - The\
      \ updated communities DataFrame, aligned to COMMUNITIES_FINAL_COLUMNS, with\
      \ a new 'title'\n          and 'human_readable_id' based on the remapped 'community'\
      \ IDs.\n        - A dictionary mapping from original delta_communities IDs to\
      \ the new IDs assigned during the merge.\n\nRaises:\n    KeyError: If required\
      \ columns (for example, 'community' in either input DataFrame, or 'parent' in\
      \ delta_communities)\n        are missing.\n\nNotes:\n    - The function mutates\
      \ old_communities and delta_communities in place by adding missing columns and\n\
      \      remapping IDs. Downstream code should be aware of input mutations.\n\
      \    - The internal mapping also includes a sentinel mapping {-1: -1} to preserve\
      \ a special value."
  - node_id: graphrag/index/update/communities.py::_update_and_merge_community_reports
    name: _update_and_merge_community_reports
    signature: "def _update_and_merge_community_reports(\n    old_community_reports:\
      \ pd.DataFrame,\n    delta_community_reports: pd.DataFrame,\n    community_id_mapping:\
      \ dict,\n) -> pd.DataFrame"
    docstring: "Update and merge old and delta community reports into a single DataFrame\
      \ aligned to the final columns.\n\nArgs:\n    old_community_reports: The old\
      \ community reports.\n    delta_community_reports: The delta community reports.\n\
      \    community_id_mapping: The mapping from original delta community IDs to\
      \ final IDs.\n\nReturns:\n    pd.DataFrame: The updated community reports aligned\
      \ to COMMUNITY_REPORTS_FINAL_COLUMNS.\n\nRaises:\n    KeyError: If required\
      \ columns such as 'community' or 'parent' are missing from input DataFrames.\n\
      \    ValueError: If a column intended to be numeric cannot be cast to int when\
      \ applying the mapping.\n    TypeError: If inputs are not DataFrames or the\
      \ mapping is not a dict-like."
  classes: []
- file: graphrag/index/update/entities.py
  functions:
  - node_id: graphrag/index/update/entities.py::_group_and_resolve_entities
    name: _group_and_resolve_entities
    signature: "def _group_and_resolve_entities(\n    old_entities_df: pd.DataFrame,\
      \ delta_entities_df: pd.DataFrame\n) -> tuple[pd.DataFrame, dict]"
    docstring: "Group old and delta entity data, resolve conflicts by title, and return\
      \ a merged entities dataframe along with a mapping from delta to existing entity\
      \ IDs.\n\nThis function merges the existing entities with a delta of new or\
      \ updated entities, constructs a mapping from delta entity IDs to existing entity\
      \ IDs for overlapping titles, and returns a resolved dataframe with a consistent\
      \ column order.\n\nParameters\n----------\nold_entities_df : pd.DataFrame\n\
      \    The existing entities dataframe containing current entities.\ndelta_entities_df\
      \ : pd.DataFrame\n    The delta dataframe containing new or updated entities\
      \ to be merged.\n\nReturns\n-------\ntuple[pd.DataFrame, dict]\n    A pair consisting\
      \ of:\n    - The resolved entities dataframe, with columns ordered according\
      \ to ENTITIES_FINAL_COLUMNS.\n    - id_mapping: A mapping from delta (B) entity\
      \ ids to existing (A) entity ids, in the form {delta_id: existing_id}. The mapping\
      \ only includes titles that exist in both dataframes. If a delta id would produce\
      \ duplicate keys in the mapping (due to duplicate delta ids for the same title),\
      \ a ValueError may be raised because dict construction is done with strict=True.\n\
      \nRaises\n------\nValueError\n    If id_mapping cannot be constructed due to\
      \ duplicate delta ids (id_B) which would produce duplicate keys when building\
      \ the mapping (strict key enforcement).\n\nNotes\n-----\n- For overlapping titles,\
      \ id_mapping records the mapping from the delta entity id (B) to the existing\
      \ entity id (A).\n- human_readable_id in delta_entities_df is incremented to\
      \ continue from the maximum value present in old_entities_df to ensure unique\
      \ identifiers.\n- The old and delta entities are concatenated and grouped by\
      \ title to resolve conflicts; for each title, the first occurrence of fields\
      \ (id, type, human_readable_id, x, y) is kept, while description is collected\
      \ as a list of strings and text_unit_ids are flattened into a single list.\n\
      - Frequency is recomputed as the length of the text_unit_ids list to reflect\
      \ added text units.\n- The final dataframe is explicitly ordered to ENTITIES_FINAL_COLUMNS\
      \ for consistency."
  classes: []
- file: graphrag/index/update/incremental_index.py
  functions:
  - node_id: graphrag/index/update/incremental_index.py::get_delta_docs
    name: get_delta_docs
    signature: "def get_delta_docs(\n    input_dataset: pd.DataFrame, storage: PipelineStorage\n\
      ) -> InputDelta"
    docstring: "Compute the delta between the input dataset and the final documents\
      \ stored in the pipeline storage.\n\nThis asynchronous function compares the\
      \ input_dataset against the documents currently stored in storage and returns\
      \ the delta as an InputDelta with new_inputs and deleted_inputs.\n\nNotes\n\
      \    - new_inputs corresponds to documents in input_dataset whose titles are\
      \ not present in the stored final documents.\n    - deleted_inputs corresponds\
      \ to documents present in the stored final documents but not present in input_dataset.\n\
      \nParameters\n    input_dataset (pd.DataFrame): The input dataset containing\
      \ documents to be indexed.\n    storage (PipelineStorage): The Pipeline storage\
      \ where final documents are stored.\n\nReturns\n    InputDelta\n        The\
      \ input delta containing:\n        new_inputs (pd.DataFrame): The new documents\
      \ to add (rows from input_dataset not present in storage).\n        deleted_inputs\
      \ (pd.DataFrame): The documents to remove (rows from storage not present in\
      \ input_dataset).\n\nRaises\n    Exception\n        Exceptions raised by the\
      \ storage backend or parquet reader during the load operation."
  - node_id: graphrag/index/update/incremental_index.py::concat_dataframes
    name: concat_dataframes
    signature: "def concat_dataframes(\n    name: str,\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> pd.DataFrame"
    docstring: "Concatenate dataframes.\n\nConcatenate old and delta documents: load\
      \ from previous_storage and delta_storage, append delta to old after assigning\
      \ sequential human_readable_id values to delta rows, and write the final dataframe\
      \ to output_storage.\n\nParameters\nname: str\n    Base name for the parquet\
      \ file to load and to which the final dataframe will be written as {name}.parquet.\n\
      previous_storage: PipelineStorage\n    Storage backend containing the existing\
      \ (old) documents.\ndelta_storage: PipelineStorage\n    Storage backend containing\
      \ the delta (new) documents.\noutput_storage: PipelineStorage\n    Storage backend\
      \ where the final concatenated dataframe will be written.\n\nReturns\npd.DataFrame\n\
      \    The concatenated DataFrame containing old and delta documents.\n\nRaises\n\
      ValueError\n    Could not find {name}.parquet in storage!\nException\n    Exceptions\
      \ raised by the storage backend or parquet reader during the load or write operations\
      \ may propagate."
  classes: []
- file: graphrag/index/update/relationships.py
  functions:
  - node_id: graphrag/index/update/relationships.py::_update_and_merge_relationships
    name: _update_and_merge_relationships
    signature: "def _update_and_merge_relationships(\n    old_relationships: pd.DataFrame,\
      \ delta_relationships: pd.DataFrame\n) -> pd.DataFrame"
    docstring: "Update and merge relationships.\n\nArgs:\n    old_relationships: pd.DataFrame\
      \ The old relationships.\n    delta_relationships: pd.DataFrame The delta relationships.\n\
      \nReturns:\n    pd.DataFrame The updated relationships, containing the final\
      \ columns as defined by RELATIONSHIPS_FINAL_COLUMNS.\n\nRaises:\n    KeyError:\
      \ If required columns are missing from the input DataFrames.\n    TypeError:\
      \ If the inputs are not pandas DataFrames."
  classes: []
- file: graphrag/index/utils/dataframes.py
  functions:
  - node_id: graphrag/index/utils/dataframes.py::union
    name: union
    signature: 'def union(*frames: pd.DataFrame) -> pd.DataFrame'
    docstring: "Perform a union operation on the given set of dataframes.\n\nArgs:\n\
      \    frames: A variable number of pandas.DataFrame objects to union.\n\nReturns:\n\
      \    pd.DataFrame: The concatenated DataFrame containing the union of all input\
      \ frames.\n\nRaises:\n    ValueError: If no frames are provided."
  - node_id: graphrag/index/utils/dataframes.py::select
    name: select
    signature: 'def select(df: pd.DataFrame, *columns: str) -> pd.DataFrame'
    docstring: "Select columns from a DataFrame.\n\nArgs:\n    df: The DataFrame to\
      \ select columns from.\n    columns: The names of the columns to select from\
      \ df.\n\nReturns:\n    pd.DataFrame: A DataFrame containing only the specified\
      \ columns, in the order provided.\n\nRaises:\n    KeyError: If any of the provided\
      \ column names do not exist in df."
  - node_id: graphrag/index/utils/dataframes.py::drop_columns
    name: drop_columns
    signature: 'def drop_columns(df: pd.DataFrame, *column: str) -> pd.DataFrame'
    docstring: "Drop specified columns from a DataFrame.\n\nArgs:\n    df (pd.DataFrame):\
      \ The DataFrame from which to drop columns.\n    column (str): One or more column\
      \ names to drop from the DataFrame.\n\nReturns:\n    pd.DataFrame: The DataFrame\
      \ with the specified columns dropped.\n\nRaises:\n    KeyError: If any of the\
      \ specified column names do not exist in df."
  - node_id: graphrag/index/utils/dataframes.py::join
    name: join
    signature: "def join(\n    left: pd.DataFrame, right: pd.DataFrame, key: str,\
      \ strategy: MergeHow = \"left\"\n) -> pd.DataFrame"
    docstring: "Perform a table join.\n\nArgs:\n    left: The left DataFrame.\n  \
      \  right: The right DataFrame.\n    key: The column name to join on.\n    strategy:\
      \ The merge strategy to use (how parameter for pandas merge). Defaults to left.\n\
      \nReturns:\n    pd.DataFrame: The joined DataFrame resulting from left.merge(right,\
      \ on=key, how=strategy)."
  - node_id: graphrag/index/utils/dataframes.py::transform_series
    name: transform_series
    signature: 'def transform_series(series: pd.Series, fn: Callable[[Any], Any])
      -> pd.Series'
    docstring: "Apply a transformation function to a Pandas Series.\n\nArgs:\n   \
      \ series: The input Pandas Series to transform.\n    fn: A callable that takes\
      \ a single value and returns a transformed value.\n\nReturns:\n    pd.Series:\
      \ A new Series with the transformed values.\n\nRaises:\n    Exception: Any exception\
      \ raised by fn will be propagated to the caller."
  - node_id: graphrag/index/utils/dataframes.py::antijoin
    name: antijoin
    signature: 'def antijoin(df: pd.DataFrame, exclude: pd.DataFrame, column: str)
      -> pd.DataFrame'
    docstring: "Return an anti-joined dataframe.\n\nArgs:\n    df (pd.DataFrame):\
      \ The DataFrame to apply the exclusion to.\n    exclude (pd.DataFrame): The\
      \ DataFrame containing rows to remove.\n    column (str): The join-on column.\n\
      \nReturns:\n    pd.DataFrame: The rows from df whose value in column is not\
      \ present in exclude[column]."
  - node_id: graphrag/index/utils/dataframes.py::where_column_equals
    name: where_column_equals
    signature: 'def where_column_equals(df: pd.DataFrame, column: str, value: Any)
      -> pd.DataFrame'
    docstring: "Return a filtered DataFrame where a column equals a value.\n\nArgs:\n\
      \    df (pd.DataFrame): The DataFrame to filter.\n    column (str): The column\
      \ name to compare.\n    value (Any): The value to compare against.\n\nReturns:\n\
      \    pd.DataFrame: A DataFrame containing only rows where df[column] == value.\n\
      \nRaises:\n    KeyError: If the specified column is not in df."
  classes: []
- file: graphrag/index/utils/derive_from_rows.py
  functions:
  - node_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError.__init__
    name: __init__
    signature: 'def __init__(self, num_errors: int, example: str | None = None)'
    docstring: "\"\"\"Initialize a ParallelizationError with details about errors\
      \ during parallel transformation.\n\nArgs:\n    num_errors: The number of errors\
      \ that occurred while running parallel transformation.\n    example: Optional\
      \ example error string to include in the message.\n\nReturns:\n    None\n\"\"\
      \""
  - node_id: graphrag/index/utils/derive_from_rows.py::execute_task
    name: execute_task
    signature: 'def execute_task(task: Coroutine) -> ItemType | None'
    docstring: 'Execute a coroutine task under a concurrency-limiting semaphore and
      return the awaited result.


      Args:

      - task: Coroutine. The coroutine to be awaited to obtain a thread-like awaitable,
      which is then awaited to produce the final ItemType result.


      Returns:

      - ItemType | None: The result produced by awaiting the retrieved thread from
      the task. May be None if the thread yields no value.


      Raises:

      - Propagates any exception raised by awaiting the input task or the retrieved
      thread (no error handling is performed here).'
  - node_id: graphrag/index/utils/derive_from_rows.py::execute
    name: execute
    signature: 'def execute(row: tuple[Any, pd.Series]) -> ItemType | None'
    docstring: "Apply the provided transform to the row data and await if necessary.\n\
      \nArgs:\n    row: tuple[Any, pd.Series] - A row, where row[1] is the pd.Series\
      \ passed to the transform.\n\nReturns:\n    ItemType | None - The transformed\
      \ value cast to ItemType, or None if an error occurred during transformation.\n\
      \nRaises:\n    None"
  - node_id: graphrag/index/utils/derive_from_rows.py::execute_row_protected
    name: execute_row_protected
    signature: "def execute_row_protected(\n            row: tuple[Hashable, pd.Series],\n\
      \        ) -> ItemType | None"
    docstring: "Execute the provided row transformation in a protected asynchronous\
      \ context using a shared semaphore.\n\nThis wrapper acquires the shared semaphore\
      \ before invoking the underlying transform (execute) on the given row and returns\
      \ its result. It does not perform any casting; the return value is whatever\
      \ execute(row) returns (which may be None).\n\nArgs:\n    row (tuple[Hashable,\
      \ pd.Series]): A row, where row[1] is the pd.Series passed to the transform.\n\
      \nReturns:\n    ItemType | None: The result of execute(row); may be None if\
      \ the underlying transform returns None.\n\nRaises:\n    Propagates any exception\
      \ raised by execute(row); exceptions may bubble up to upstream callers."
  - node_id: graphrag/index/utils/derive_from_rows.py::gather
    name: gather
    signature: 'def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]'
    docstring: "Gather results by applying the given execute function to each row\
      \ of the input DataFrame and returning the results as a list.\n\nArgs:\n   \
      \ execute: ExecuteFn[ItemType] - A function that accepts a tuple[Hashable, pd.Series]\
      \ representing a DataFrame row and returns an ItemType or None. This may be\
      \ an awaitable.\n\nReturns:\n    list[ItemType | None]: A list of results corresponding\
      \ to each input row. Each element is either an ItemType or None.\n\nRaises:\n\
      \    Exception: If the underlying execute raises an exception, it will propagate\
      \ to the caller."
  - node_id: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
    name: _derive_from_rows_base
    signature: "def _derive_from_rows_base(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    gather: GatherFn[ItemType],\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]"
    docstring: "Derive from rows asynchronously.\n\nThis is useful for IO bound operations.\n\
      \nArgs:\n    input: pd.DataFrame\n        The input data to process, where each\
      \ row will be transformed.\n    transform: Callable[[pd.Series], Awaitable[ItemType]]\n\
      \        Async function applied to a row's Series to produce an ItemType.\n\
      \    callbacks: WorkflowCallbacks\n        Callbacks used to report progress\
      \ and handle workflow events.\n    gather: GatherFn[ItemType]\n        Function\
      \ that gathers results by applying the given execute function to each row.\n\
      \    progress_msg: str\n        Optional description to accompany progress updates.\n\
      \nReturns:\n    list[ItemType | None]\n        A list of results corresponding\
      \ to each input row; elements may be ItemType or None if an error occurred for\
      \ that row.\n\nRaises:\n    ParallelizationError\n        If any errors were\
      \ encountered during processing."
  - node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
    name: derive_from_rows_asyncio_threads
    signature: "def derive_from_rows_asyncio_threads(\n    input: pd.DataFrame,\n\
      \    transform: Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks:\
      \ WorkflowCallbacks,\n    num_threads: int | None = 4,\n    progress_msg: str\
      \ = \"\",\n) -> list[ItemType | None]"
    docstring: "Threaded (IO-bound) variant: derive results from DataFrame rows by\
      \ dispatching per-row work to a thread pool via asyncio.to_thread. The transform\
      \ is expected to be an asynchronous function that operates on the row's pandas\
      \ Series and returns an ItemType (or None). Internally, rows are passed to the\
      \ executor as a tuple (index, row); the transform should use only the Series,\
      \ not the full tuple.\n\nThis function constrains concurrency with a semaphore\
      \ using num_threads (default 4).\n\nArgs:\n  input: pandas.DataFrame\n     \
      \ The input data to process, where each row will be transformed.\n  transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]]\n      Async function applied to\
      \ the row's Series to produce an ItemType. The function should\n      operate\
      \ on the Series alone and not expect the full (index, Series) tuple.\n  callbacks:\
      \ WorkflowCallbacks\n      Callbacks used to report progress and handle workflow\
      \ lifecycle events.\n  num_threads: int | None\n      Maximum number of concurrent\
      \ threads to use; if None, a default of 4 is used.\n  progress_msg: str\n  \
      \    Optional message to display for progress tracking.\n\nReturns:\n  list[ItemType\
      \ | None]\n      A list containing the result for each input row. Each element\
      \ is either the ItemType\n      produced by transform or None if the row could\
      \ not be transformed.\n\nRaises:\n  Propagates exceptions raised by the per-row\
      \ transform or by callbacks. If multiple errors\n  occur during parallel execution,\
      \ a summary exception may be raised by the internal error\n  aggregator.\n\n\
      Notes:\n  This function is the threaded variant of derive_from_rows and relies\
      \ on asyncio.to_thread to\n  execute transforms in a thread pool while an asyncio\
      \ event loop drives orchestration."
  - node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
    name: derive_from_rows_asyncio
    signature: "def derive_from_rows_asyncio(\n    input: pd.DataFrame,\n    transform:\
      \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
      \    num_threads: int = 4,\n    progress_msg: str = \"\",\n) -> list[ItemType\
      \ | None]"
    docstring: "Derive from rows asynchronously.\n\nThis is useful for IO bound operations.\n\
      \nArgs:\n    input: pd.DataFrame\n        The input data to process, where each\
      \ row will be transformed.\n    transform: Callable[[pd.Series], Awaitable[ItemType]]\n\
      \        Async function applied to a row's Series to produce an ItemType.\n\
      \    callbacks: WorkflowCallbacks\n        Callbacks used to report progress\
      \ and handle workflow events.\n    num_threads: int\n        Number of concurrent\
      \ workers to use. This limits the degree of parallelism; default is 4.\n   \
      \ progress_msg: str\n        Description shown in the progress ticker during\
      \ processing.\n\nReturns:\n    list[ItemType | None]\n        A list of results\
      \ corresponding to each input row; each element is either an ItemType or None\
      \ if a row could not be processed."
  - node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows
    name: derive_from_rows
    signature: "def derive_from_rows(\n    input: pd.DataFrame,\n    transform: Callable[[pd.Series],\
      \ Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks | None = None,\n \
      \   num_threads: int = 4,\n    async_type: AsyncType = AsyncType.AsyncIO,\n\
      \    progress_msg: str = \"\",\n) -> list[ItemType | None]"
    docstring: "Apply a generic transform function to each row in the input DataFrame.\n\
      \n Any errors raised by the transform will be reported and thrown.\n\n Args:\n\
      \   input: The input DataFrame to process; each row will be transformed.\n \
      \  transform: Async function applied to a row's Series to produce an ItemType.\n\
      \   callbacks: Workflow callbacks; used to report progress and handle events.\
      \ If None, NoopWorkflowCallbacks is used.\n   num_threads: Number of concurrent\
      \ workers to use.\n   async_type: Scheduling type to use for execution (AsyncIO\
      \ or Threaded). Defaults to AsyncType.AsyncIO.\n   progress_msg: Optional progress\
      \ message to display during processing.\n\n Returns:\n   list[ItemType | None]:\
      \ A list containing the transformed results, one per input row; entries may\
      \ be None.\n\n Raises:\n   ValueError: If an unsupported scheduling type is\
      \ provided."
  classes:
  - class_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError
    name: ParallelizationError
    docstring: 'ParallelizationError stores information about errors that occurred
      during parallel transformation.


      Attributes:

      - num_errors (int): The number of errors that occurred during the parallel transformation.

      - example (str | None): Optional example error string to include in messages
      or logs. Defaults to None.


      Initialization:

      __init__(self, num_errors: int, example: str | None = None)


      Initializes the instance with the given error details and stores them on the
      object for later access (e.g., for error reporting or messaging).'
    methods:
    - name: __init__
      signature: 'def __init__(self, num_errors: int, example: str | None = None)'
- file: graphrag/index/utils/dicts.py
  functions:
  - node_id: graphrag/index/utils/dicts.py::dict_has_keys_with_types
    name: dict_has_keys_with_types
    signature: "def dict_has_keys_with_types(\n    data: dict, expected_fields: list[tuple[str,\
      \ type]], inplace: bool = False\n) -> bool"
    docstring: "\"\"\"Check that a dictionary contains the specified keys and that\
      \ their values can be cast to the provided types.\n\nArgs:\n    data: The dictionary\
      \ to inspect and (optionally) mutate.\n    expected_fields: A list of (key,\
      \ type) pairs describing the required keys and the types their values must be\
      \ cast to.\n    inplace: If True, casted values are written back into the dictionary\
      \ for the corresponding keys.\n\nReturns:\n    bool: True if all specified keys\
      \ exist in the dictionary and their values can be cast to the given types; otherwise\
      \ False.\n\"\"\""
  classes: []
- file: graphrag/index/utils/graphs.py
  functions:
  - node_id: graphrag/index/utils/graphs.py::get_upper_threshold_by_std
    name: get_upper_threshold_by_std
    signature: 'def get_upper_threshold_by_std(data: list[float] | list[int], std_trim:
      float) -> float'
    docstring: "Get upper threshold by standard deviation.\n\nArgs:\n    data: list[float]\
      \ | list[int], a list of numeric values used to compute the threshold.\n   \
      \ std_trim: float, multiplier for the standard deviation to offset the mean.\n\
      \nReturns:\n    float: The upper threshold computed as mean + std_trim * std\
      \ of the data."
  - node_id: graphrag/index/utils/graphs.py::calculate_root_modularity
    name: calculate_root_modularity
    signature: "def calculate_root_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
    docstring: "Compute the modularity of the graph's root clusters.\n\nThis function\
      \ applies Hierarchical Leiden to the input graph to generate a hierarchical\
      \ clustering and then uses the first_level_hierarchical_clustering (root level)\
      \ to define the root clusters. It returns the modularity of the graph with respect\
      \ to these root clusters.\n\nArgs:\n    graph (nx.Graph): The input graph.\n\
      \    max_cluster_size (int): Maximum cluster size for the root-level clustering\
      \ produced by Hierarchical Leiden.\n    random_seed (int): Seed for the randomized\
      \ algorithm to ensure reproducibility.\n\nReturns:\n    float: The modularity\
      \ score of the graph computed against its root-level clusters.\n\nNotes:\n-\
      \ The computation operates on the entire graph and does not compare to any target\
      \ modularity.\n- Root clusters are obtained via first_level_hierarchical_clustering;\
      \ internal steps involve Hierarchical Leiden."
  - node_id: graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
    name: calculate_pmi_edge_weights
    signature: "def calculate_pmi_edge_weights(\n    nodes_df: pd.DataFrame,\n   \
      \ edges_df: pd.DataFrame,\n    node_name_col: str = \"title\",\n    node_freq_col:\
      \ str = \"frequency\",\n    edge_weight_col: str = \"weight\",\n    edge_source_col:\
      \ str = \"source\",\n    edge_target_col: str = \"target\",\n) -> pd.DataFrame"
    docstring: "Calculate pointwise mutual information (PMI) edge weights for a graph.\n\
      \nArgs:\n    nodes_df (pd.DataFrame): DataFrame containing node information\
      \ with at least the columns\n        specified by node_name_col and node_freq_col.\n\
      \    edges_df (pd.DataFrame): DataFrame containing edge information with at\
      \ least the columns\n        specified by edge_weight_col, edge_source_col,\
      \ and edge_target_col.\n    node_name_col (str): Column in nodes_df that identifies\
      \ the node name.\n    node_freq_col (str): Column in nodes_df that contains\
      \ the frequency/count for each node.\n    edge_weight_col (str): Column in edges_df\
      \ that contains the raw edge weights.\n    edge_source_col (str): Column in\
      \ edges_df that identifies the source node.\n    edge_target_col (str): Column\
      \ in edges_df that identifies the target node.\n\nReturns:\n    pd.DataFrame:\
      \ A DataFrame with PMI-weighted edge weights computed as:\n        pmi(x,y)\
      \ = p(x,y) * log2(p(x,y) / (p(x) * p(y)))\n        where p(x,y) = edge_weight(x,y)\
      \ / total_edge_weights and\n        p(x) = freq(x) / total_freq_occurrences.\
      \ The result is the input edges_df\n        with the edge weights updated to\
      \ the PMI value, and intermediate temporary\n        columns removed (prop_weight,\
      \ source_prop, target_prop).\n\nRaises:\n    KeyError: If any of the required\
      \ columns are missing from the input DataFrames."
  - node_id: graphrag/index/utils/graphs.py::calculate_leaf_modularity
    name: calculate_leaf_modularity
    signature: "def calculate_leaf_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
    docstring: "Compute the modularity score of the graph using the leaf-level partition\
      \ produced by hierarchical Leiden. The function applies hierarchical Leiden\
      \ to the input graph, derives the leaf-level clustering via final_level_hierarchical_clustering,\
      \ and returns the modularity of the graph with respect to that partition.\n\n\
      Args:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int): Maximum\
      \ size of clusters considered during hierarchical Leiden.\n    random_seed (int):\
      \ Seed for random number generation to ensure reproducibility.\n\nReturns:\n\
      \    float: The modularity score of the graph computed using the leaf-cluster\
      \ partition.\n\nRaises:\n    None"
  - node_id: graphrag/index/utils/graphs.py::calculate_rrf_edge_weights
    name: calculate_rrf_edge_weights
    signature: "def calculate_rrf_edge_weights(\n    nodes_df: pd.DataFrame,\n   \
      \ edges_df: pd.DataFrame,\n    node_name_col=\"title\",\n    node_freq_col=\"\
      freq\",\n    edge_weight_col=\"weight\",\n    edge_source_col=\"source\",\n\
      \    edge_target_col=\"target\",\n    rrf_smoothing_factor: int = 60,\n) ->\
      \ pd.DataFrame"
    docstring: "Calculate reciprocal rank fusion (RRF) edge weights as a combination\
      \ of PMI weight and combined freq of source and target.\n\nArgs:\n    nodes_df\
      \ (pd.DataFrame): DataFrame containing node information with at least the columns\
      \ specified by node_name_col and node_freq_col.\n    edges_df (pd.DataFrame):\
      \ DataFrame containing edge information with at least the columns specified\
      \ by edge_weight_col, edge_source_col, and edge_target_col.\n    node_name_col\
      \ (str): Column in nodes_df that identifies the node name. Default \"title\"\
      .\n    node_freq_col (str): Column in nodes_df that indicates node frequency.\
      \ Default \"freq\".\n    edge_weight_col (str): Column in edges_df that holds\
      \ edge weights (PMI) before RRF adjustment. Default \"weight\".\n    edge_source_col\
      \ (str): Column in edges_df that indicates the source node. Default \"source\"\
      .\n    edge_target_col (str): Column in edges_df that indicates the target node.\
      \ Default \"target\".\n    rrf_smoothing_factor (int): Smoothing factor used\
      \ in reciprocal rank fusion. Default 60.\n\nReturns:\n    pd.DataFrame: Edge\
      \ dataframe with edge weights updated using the RRF formula. It first computes\
      \ PMI-based weights, ranks them, and then combines the ranks to produce the\
      \ new weight. The resulting DataFrame retains edge information and the updated\
      \ weight column.\n\nRaises:\n    Not documented in the provided data."
  - node_id: graphrag/index/utils/graphs.py::calculate_graph_modularity
    name: calculate_graph_modularity
    signature: "def calculate_graph_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float"
    docstring: "Calculate modularity of the whole graph.\n\nArgs:\n    graph (nx.Graph):\
      \ The input graph.\n    max_cluster_size (int): Maximum cluster size for the\
      \ root-level clustering produced by Hierarchical Leiden.\n    random_seed (int):\
      \ Seed for random number generation.\n    use_root_modularity (bool): If True,\
      \ compute modularity using root-level clustering; otherwise compute using leaf-level\
      \ clustering.\n\nReturns:\n    float: The modularity score for the graph with\
      \ respect to the chosen clustering."
  - node_id: graphrag/index/utils/graphs.py::calculate_lcc_modularity
    name: calculate_lcc_modularity
    signature: "def calculate_lcc_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n) -> float"
    docstring: "Calculate modularity of the largest connected component of the graph.\n\
      \nArgs:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int):\
      \ Maximum cluster size for the root/leaf hierarchical clustering used to compute\
      \ modularity.\n    random_seed (int): Seed for random number generation.\n \
      \   use_root_modularity (bool): If True, compute modularity using root-level\
      \ clustering; otherwise compute using leaf-level clustering.\n\nReturns:\n \
      \   float: The modularity value of the largest connected component of the input\
      \ graph."
  - node_id: graphrag/index/utils/graphs.py::calculate_weighted_modularity
    name: calculate_weighted_modularity
    signature: "def calculate_weighted_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    min_connected_component_size:\
      \ int = 10,\n    use_root_modularity: bool = True,\n) -> float"
    docstring: "Calculate weighted modularity of all connected components with size\
      \ greater than min_connected_component_size.\n\nModularity = sum(component_modularity\
      \ * component_size) / total_nodes.\nModularity for the overall calculation is\
      \ obtained by weighting each component's modularity by its size and normalizing\
      \ by the total number of nodes in all considered components.\n\nArgs:\n  graph\
      \ (nx.Graph): The input graph.\n  max_cluster_size (int): Maximum cluster size\
      \ for the modularity computations per component.\n  random_seed (int): Seed\
      \ for random number generation.\n  min_connected_component_size (int): Components\
      \ with size less than or equal to this value are ignored; if no components pass\
      \ this threshold, the entire graph is used.\n  use_root_modularity (bool): If\
      \ True, compute modularity using root-level clustering; otherwise compute leaf-level\
      \ clustering.\n\nReturns:\n  float: The weighted modularity value."
  - node_id: graphrag/index/utils/graphs.py::calculate_modularity
    name: calculate_modularity
    signature: "def calculate_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
      \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
      \ = True,\n    modularity_metric: ModularityMetric = ModularityMetric.WeightedComponents,\n\
      ) -> float"
    docstring: "Calculate modularity of the graph based on the modularity metric type.\n\
      \nArgs:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int):\
      \ Maximum cluster size for the root-level clustering produced by Hierarchical\
      \ Leiden.\n    random_seed (int): Seed for random number generation.\n    use_root_modularity\
      \ (bool): If True, compute modularity using root-level clustering; otherwise\
      \ compute using leaf-level clustering.\n    modularity_metric (ModularityMetric):\
      \ The modularity metric to use (Graph, LCC, or WeightedComponents).\n\nReturns:\n\
      \    float: The modularity value for the graph with respect to the chosen clustering.\n\
      \nRaises:\n    ValueError: If an unknown modularity metric type is provided."
  classes: []
- file: graphrag/index/utils/hashing.py
  functions:
  - node_id: graphrag/index/utils/hashing.py::gen_sha512_hash
    name: gen_sha512_hash
    signature: 'def gen_sha512_hash(item: dict[str, Any], hashcode: Iterable[str])'
    docstring: "Generate a SHA512 hash from the concatenation of string representations\
      \ of selected fields of an item.\n\nArgs:\n  item: input dictionary containing\
      \ values to hash.\n  hashcode: keys whose corresponding values are used for\
      \ the hash, in order.\n\nReturns:\n  str: Hexadecimal SHA512 digest string.\n\
      \nRaises:\n  KeyError: if a key from hashcode is not present in item."
  classes: []
- file: graphrag/index/utils/is_null.py
  functions:
  - node_id: graphrag/index/utils/is_null.py::is_nan
    name: is_nan
    signature: def is_nan() -> bool
    docstring: "Check if value is NaN.\n\nReturns:\n    bool: True if value is a float\
      \ and NaN, otherwise False."
  - node_id: graphrag/index/utils/is_null.py::is_none
    name: is_none
    signature: def is_none() -> bool
    docstring: "\"\"\"Check if the input value is None or NaN.\n\nArgs:\n    value\
      \ (Any): The value to check.\n\nReturns:\n    bool: True if value is None or\
      \ NaN (NaN is recognized only for floating-point values); otherwise False.\n\
      \"\"\""
  - node_id: graphrag/index/utils/is_null.py::is_null
    name: is_null
    signature: 'def is_null(value: Any) -> bool'
    docstring: "\"\"\"Check if value is None or NaN.\n\nArgs:\n    value (Any): The\
      \ value to check.\n\nReturns:\n    bool: True if value is None or NaN (NaN is\
      \ recognized only for floating-point values); otherwise False.\n\"\"\""
  classes: []
- file: graphrag/index/utils/stable_lcc.py
  functions:
  - node_id: graphrag/index/utils/stable_lcc.py::_sort_source_target
    name: _sort_source_target
    signature: def _sort_source_target(edge)
    docstring: "Sorts a graph edge so that the source and target are in a stable,\
      \ canonical order.\n\nArgs:\n    edge: A 3-tuple (source, target, edge_data)\
      \ representing an edge from a graph.\n\nReturns:\n    A 3-tuple (source, target,\
      \ edge_data) with source and target ordered such that source <= target.\n\n\
      Raises:\n    ValueError: If edge does not contain exactly three elements.\n\
      \    TypeError: If edge elements do not support comparison."
  - node_id: graphrag/index/utils/stable_lcc.py::_get_edge_key
    name: _get_edge_key
    signature: 'def _get_edge_key(source: Any, target: Any) -> str'
    docstring: "Return a string key for the edge in the format 'source -> target'.\n\
      \nArgs:\n    source (Any): The source node of the edge.\n    target (Any): The\
      \ target node of the edge.\n\nReturns:\n    str: The edge key as a string in\
      \ the format 'source -> target'."
  - node_id: graphrag/index/utils/stable_lcc.py::normalize_node_names
    name: normalize_node_names
    signature: 'def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph
      | nx.DiGraph'
    docstring: "Normalize node names for a graph by applying HTML unescaping, converting\
      \ to uppercase, and trimming whitespace on each node label.\n\nArgs:\n  graph\
      \ (nx.Graph | nx.DiGraph): Input graph whose node names will be normalized.\n\
      \nReturns:\n  nx.Graph | nx.DiGraph: The input graph with node names normalized\
      \ (uppercased, stripped of whitespace, and HTML entities unescaped)."
  - node_id: graphrag/index/utils/stable_lcc.py::_stabilize_graph
    name: _stabilize_graph
    signature: 'def _stabilize_graph(graph: nx.Graph) -> nx.Graph'
    docstring: "\"\"\"Ensure an undirected graph with the same relationships will\
      \ always be read the same way.\n\nArgs:\n    graph: nx.Graph The input graph.\
      \ May be directed or undirected; the function will preserve the directedness\
      \ and return a new graph with deterministic ordering of nodes and edges.\n\n\
      Returns:\n    nx.Graph The stabilized graph. If the input graph is directed,\
      \ the returned graph is a nx.DiGraph; otherwise, a nx.Graph.\n\"\"\""
  - node_id: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
    name: stable_largest_connected_component
    signature: 'def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph'
    docstring: "Return the largest connected component of the graph, with nodes and\
      \ edges sorted in a stable way.\n\nArgs:\n    graph (nx.Graph): Input graph\
      \ from which to compute the stable largest connected component.\n\nReturns:\n\
      \    nx.Graph: The stabilized largest connected component graph with deterministic\
      \ ordering of nodes and edges."
  classes: []
- file: graphrag/index/utils/string.py
  functions:
  - node_id: graphrag/index/utils/string.py::clean_str
    name: clean_str
    signature: 'def clean_str(input: Any) -> str'
    docstring: "Clean an input string by removing HTML escapes, control characters,\
      \ and other unwanted characters.\n\nArgs:\n    input: Any\n        The value\
      \ to sanitize. If the value is not a string, it is returned unchanged.\n\nReturns:\n\
      \    str\n        The sanitized string if the input is a string; otherwise,\
      \ the original value is returned unchanged."
  classes: []
- file: graphrag/index/utils/uuid.py
  functions:
  - node_id: graphrag/index/utils/uuid.py::gen_uuid
    name: gen_uuid
    signature: 'def gen_uuid(rd: Random | None = None)'
    docstring: "Generate a random UUID v4 and return its hex representation.\n\nArgs:\n\
      \    rd: Random | None. Optional random number generator to use. If None, randomness\
      \ is sourced from the default RNG.\n\nReturns:\n    str: Hexadecimal string\
      \ representation of the generated UUID v4."
  classes: []
- file: graphrag/index/validate_config.py
  functions:
  - node_id: graphrag/index/validate_config.py::validate_config_names
    name: validate_config_names
    signature: 'def validate_config_names(parameters: GraphRagConfig) -> None'
    docstring: "Validate config file for model deployment name typos, by running a\
      \ quick test message for each.\n\nArgs:\n  parameters: GraphRagConfig containing\
      \ models to validate.\n\nReturns:\n  None\n\nRaises:\n  SystemExit: If validation\
      \ fails for any model; the process exits with status 1."
  classes: []
- file: graphrag/index/workflows/create_base_text_units.py
  functions:
  - node_id: graphrag/index/workflows/create_base_text_units.py::chunker
    name: chunker
    signature: 'def chunker(row: pd.Series) -> Any'
    docstring: "Chunk a row into text chunks, optionally prepending metadata to each\
      \ chunk.\n\nArgs:\n    row (pd.Series): The input row containing the data to\
      \ be chunked. It is expected to have the 'texts' column, and may include 'metadata'.\
      \ This function also relies on outer-scope configuration such as prepend_metadata,\
      \ size, overlap, encoding_model, strategy, and callbacks.\n\nReturns:\n    pd.Series:\
      \ The input row augmented with a 'chunks' field containing the list of text\
      \ chunks (with metadata prepended if configured).\n\nRaises:\n    ValueError:\
      \ Metadata tokens exceed the maximum tokens per chunk. Please increase the tokens\
      \ per chunk."
  - node_id: graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
    name: chunker_with_logging
    signature: 'def chunker_with_logging(row: pd.Series, row_index: int) -> Any'
    docstring: "Log chunker progress for a row during chunking.\n\nExecutes the chunker\
      \ on the given row and logs progress using total_rows from the surrounding scope.\n\
      \nArgs:\n    row (pd.Series): The input row to be chunked.\n    row_index (int):\
      \ The index of the row being processed (0-based).\n\nReturns:\n    Any: The\
      \ result of the chunker applied to the row.\n\nRaises:\n    Exception: Propagates\
      \ any exception raised by chunker(row)."
  - node_id: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
    name: create_base_text_units
    signature: "def create_base_text_units(\n    documents: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    group_by_columns: list[str],\n    size: int,\n   \
      \ overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
      \    prepend_metadata: bool = False,\n    chunk_size_includes_metadata: bool\
      \ = False,\n) -> pd.DataFrame"
    docstring: "\"\"\"Converts input documents into base text units by grouping, chunking,\
      \ and optional metadata preprocessing.\n\nArgs:\n    documents (pd.DataFrame):\
      \ Input table containing documents. Expected to contain at least the columns\
      \ \"id\" and \"text\". May also include optional \"metadata\".\n    callbacks\
      \ (WorkflowCallbacks): Callbacks used during the chunking process.\n    group_by_columns\
      \ (list[str]): Columns to group documents by before text chunking. If empty,\
      \ all documents are treated as a single group.\n    size (int): Maximum number\
      \ of tokens per chunk (excluding any metadata unless chunk_size_includes_metadata\
      \ is True).\n    overlap (int): Number of tokens to overlap between consecutive\
      \ chunks.\n    encoding_model (str): Encoding model name used to compute token\
      \ lengths for chunking.\n    strategy (ChunkStrategyType): Strategy used by\
      \ the underlying chunk_text operation.\n    prepend_metadata (bool): If True,\
      \ prepend the document metadata to each generated chunk.\n    chunk_size_includes_metadata\
      \ (bool): If True, metadata is counted towards the per-chunk size. When True,\
      \ metadata tokens are subtracted from size and may raise ValueError if they\
      \ exceed the per-chunk limit.\n\nReturns:\n    pd.DataFrame: A dataframe containing\
      \ one row per chunk. Columns include the grouping keys from group_by_columns,\
      \ \"id\" (SHA-512 hash of the chunk), \"text\" (the chunk text), \"document_ids\"\
      \ (list of document ids contributing to the chunk), and \"n_tokens\" (token\
      \ length of the chunk). The exact set of columns may also include the original\
      \ grouping keys.\n\nRaises:\n    ValueError: If prepend_metadata is enabled\
      \ and chunk_size_includes_metadata is True and the computed metadata token length\
      \ exceeds the per-chunk size.\n\nNotes:\n    The function logs progress during\
      \ processing. It sorts documents by id, aggregates text with ids, chunks large\
      \ text units into smaller chunks, optionally prepends metadata to chunks, and\
      \ computes a stable hash id for each chunk.\n\"\"\""
  - node_id: graphrag/index/workflows/create_base_text_units.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the base text units workflow to transform documents into base\
      \ text units by loading documents from storage, chunking them, and writing the\
      \ resulting text units back to storage.\n\nArgs:\n  config (GraphRagConfig):\
      \ GraphRag configuration for the workflow, including chunking settings such\
      \ as group_by_columns, size, overlap, encoding_model, strategy, prepend_metadata,\
      \ and chunk_size_includes_metadata.\n  context (PipelineRunContext): Pipeline\
      \ run context containing the output storage and callbacks used for loading input\
      \ and writing output.\n\nReturns:\n  WorkflowFunctionOutput: The workflow output\
      \ containing the produced text_units DataFrame.\n\nRaises:\n  ValueError: Could\
      \ not find documents.parquet in storage!\n  Exception: Exceptions raised by\
      \ the storage backend or parquet reader during the load or write operations\
      \ may propagate."
  classes: []
- file: graphrag/index/workflows/create_communities.py
  functions:
  - node_id: graphrag/index/workflows/create_communities.py::create_communities
    name: create_communities
    signature: "def create_communities(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int\
      \ | None = None,\n) -> pd.DataFrame"
    docstring: "Create final communities from entities and relationships using graph-based\
      \ clustering and metadata enrichment.\n\nThis function builds a graph from the\
      \ provided relationships, performs Leiden-based clustering to group entities\
      \ into hierarchical communities, and then aggregates related entities and relationships\
      \ into a final, metadata-rich DataFrame. The result is aligned to the column\
      \ schema defined by COMMUNITIES_FINAL_COLUMNS and is suitable for storage and\
      \ downstream processing.\n\nArgs:\n    entities (pd.DataFrame): DataFrame containing\
      \ entities. Must include at least:\n        - title (str): The display title\
      \ used to link to relationships\n        - id (str): The unique identifier for\
      \ the entity\n    relationships (pd.DataFrame): DataFrame containing relationships.\
      \ Must include:\n        - source (str): Title of the source entity\n      \
      \  - target (str): Title of the target entity\n        - id (str): Unique identifier\
      \ for the relationship\n        - text_unit_ids (list): Identifiers of the text\
      \ units associated with the relationship\n        - weight (float, optional):\
      \ Edge weight used when constructing the graph\n    max_cluster_size (int):\
      \ Maximum allowed size for a cluster produced by the clustering step.\n    use_lcc\
      \ (bool): If True, operate on the largest connected component of the input graph.\n\
      \    seed (int | None, optional): Random seed for reproducibility of the clustering\
      \ process.\n\nReturns:\n    pd.DataFrame: Final communities DataFrame containing\
      \ metadata and ready for storage. The exact\n    columns are defined by COMMUNITIES_FINAL_COLUMNS\
      \ and include fields such as identifiers (id,\n    human_readable_id, title),\
      \ hierarchical and grouping fields (community, level, parent, children),\n \
      \   membership aggregations (entity_ids, relationship_ids, text_unit_ids), and\
      \ update-tracking fields\n    (period, size).\n\nRaises:\n    ValueError: If\
      \ required input columns are missing or inputs have invalid types.\n    KeyError:\
      \ If expected keys are missing during processing (indicative of unexpected input\
      \ shape)."
  - node_id: graphrag/index/workflows/create_communities.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the create_communities workflow to generate final communities\
      \ from input entities and relationships.\n\nArgs:\n  config: GraphRagConfig\
      \ containing cluster_graph settings used by the workflow.\n  context: PipelineRunContext\
      \ providing storage context for input and output data.\n\nReturns:\n  WorkflowFunctionOutput:\
      \ The workflow output which includes the resulting communities.\n\nRaises:\n\
      \  ValueError: Could not find the required parquet files in storage when loading\
      \ inputs (entities or relationships).\n  Exception: Exceptions raised by the\
      \ storage backend or by downstream processing may propagate."
  classes: []
- file: graphrag/index/workflows/create_community_reports.py
  functions:
  - node_id: graphrag/index/workflows/create_community_reports.py::_prep_claims
    name: _prep_claims
    signature: 'def _prep_claims(input: pd.DataFrame) -> pd.DataFrame'
    docstring: "Prepare claims data by filling missing descriptions and constructing\
      \ the CLAIM_DETAILS field.\n\nArgs:\n    input: The input DataFrame containing\
      \ claims data. Missing DESCRIPTION values are filled with \"No Description\"\
      \ and a new CLAIM_DETAILS column is created from SHORT_ID, CLAIM_SUBJECT, TYPE,\
      \ CLAIM_STATUS, and DESCRIPTION.\n\nReturns:\n    pd.DataFrame: The input DataFrame\
      \ augmented with a CLAIM_DETAILS column and with missing DESCRIPTION filled\
      \ as \"No Description\"."
  - node_id: graphrag/index/workflows/create_community_reports.py::_prep_edges
    name: _prep_edges
    signature: 'def _prep_edges(input: pd.DataFrame) -> pd.DataFrame'
    docstring: "Prepare edges data by filling missing descriptions and constructing\
      \ the EDGE_DETAILS field.\n\nArgs:\n    input: pd.DataFrame. The input DataFrame\
      \ containing edge data. Missing DESCRIPTION values are filled with \"No Description\"\
      \ and a new EDGE_DETAILS column is created from SHORT_ID, EDGE_SOURCE, EDGE_TARGET,\
      \ DESCRIPTION, and EDGE_DEGREE.\n\nReturns:\n    pd.DataFrame: The input DataFrame\
      \ augmented with an EDGE_DETAILS column. The function mutates the input in place.\n\
      \nRaises:\n    KeyError: If input is missing any of the required columns: SHORT_ID,\
      \ EDGE_SOURCE, EDGE_TARGET, DESCRIPTION, or EDGE_DEGREE."
  - node_id: graphrag/index/workflows/create_community_reports.py::_prep_nodes
    name: _prep_nodes
    signature: 'def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame'
    docstring: "Populate node descriptions and create NODE_DETAILS without filtering.\n\
      \nThis function fills missing descriptions with \"No Description\" and creates\
      \ a new\nNODE_DETAILS column by aggregating SHORT_ID, TITLE, DESCRIPTION, and\
      \ NODE_DEGREE\nfor each node. No rows are filtered; the operation mutates the\
      \ input DataFrame in\nplace and returns the same object.\n\nArgs:\n    input\
      \ (pd.DataFrame): Input nodes DataFrame. Must contain the columns\n        DESCRIPTION,\
      \ SHORT_ID, TITLE, and NODE_DEGREE (as defined by the data model).\n\nReturns:\n\
      \    pd.DataFrame: The same input DataFrame, mutated in place with the new NODE_DETAILS\
      \ column.\n\nRaises:\n    KeyError: If any required column (DESCRIPTION, SHORT_ID,\
      \ TITLE, NODE_DEGREE) is\n        missing from the input DataFrame."
  - node_id: graphrag/index/workflows/create_community_reports.py::create_community_reports
    name: create_community_reports
    signature: "def create_community_reports(\n    edges_input: pd.DataFrame,\n  \
      \  entities: pd.DataFrame,\n    communities: pd.DataFrame,\n    claims_input:\
      \ pd.DataFrame | None,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    summarization_strategy: dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
      \    num_threads: int = 4,\n) -> pd.DataFrame"
    docstring: "Asynchronously transforms input data into finalized community reports\
      \ by preparing nodes and edges, constructing local contexts, performing level-based\
      \ summarization, and finalizing results. The function orchestrates multiple\
      \ preprocessing steps, builds prompts and tokenizer settings, and runs the summarization\
      \ asynchronously before returning the finalized reports.\n\nHigh-level steps:\n\
      - Node and edge preparation: explode_communities, _prep_nodes, and _prep_edges;\
      \ if claims_input is provided, _prep_claims is applied to incorporate claims\
      \ into the context.\n- Context setup: configure extraction_prompt from graph_prompt,\
      \ initialize LanguageModelConfig and tokenizer, determine max_input_length,\
      \ and build local contexts via build_local_context.\n- Summarization: asynchronously\
      \ summarize across community levels with summarize_communities using the provided\
      \ callbacks, cache, and strategy, honoring async_mode and num_threads.\n- Finalization:\
      \ merge and enrich the summarized reports with community metadata via finalize_community_reports.\n\
      \nNotes:\n- claims_input is optional; when provided, claims data are incorporated\
      \ into local contexts.\n- The preparation steps mutate their input DataFrames\
      \ in place.\n- This function executes asynchronously and may involve network\
      \ or model API calls; latency and runtime errors may occur.\n- Callbacks are\
      \ used for progress reporting during processing.\n\nArgs:\n  edges_input: pd.DataFrame\
      \ - Edges data to process.\n  entities: pd.DataFrame - Entity data used to explode\
      \ communities.\n  communities: pd.DataFrame - Community definitions and hierarchy.\n\
      \  claims_input: pd.DataFrame | None - Optional claims data; if provided, used\
      \ to augment context.\n  callbacks: WorkflowCallbacks - Callbacks for progress\
      \ reporting during processing.\n  cache: PipelineCache - Cache for intermediate\
      \ results during summarization.\n  summarization_strategy: dict - Settings for\
      \ summarization, including llm config and prompts.\n  async_mode: AsyncType\
      \ - Async execution mode for the summarization step.\n  num_threads: int - Number\
      \ of worker threads for parallel summarization.\n\nReturns:\n  pd.DataFrame:\
      \ The finalized community reports.\n\nRaises:\n  OSError: If an I/O error occurs\
      \ (e.g., storage or network interactions).\n  ValueError: If input data or strategy\
      \ configurations are invalid or missing required fields.\n  KeyError: If expected\
      \ keys are absent from dictionaries during setup.\n  Exception: Other exceptions\
      \ may be raised by underlying components (e.g., external services or the language\
      \ model) during processing."
  - node_id: graphrag/index/workflows/create_community_reports.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Runs the create_community_reports workflow to transform community\
      \ reports and persist the results.\n\nArgs:\n  config: GraphRagConfig containing\
      \ settings used by the workflow, including language model and data extraction\
      \ configurations.\n  context: PipelineRunContext providing access to output_storage,\
      \ callbacks, and cache used during the workflow.\n\nReturns:\n  WorkflowFunctionOutput:\
      \ The output of the workflow, which wraps the resulting community_reports DataFrame.\n\
      \nRaises:\n  ValueError: Could not find required parquet file in storage during\
      \ loading.\n  Exception: Exceptions raised by the storage backend or parquet\
      \ reader during load or write operations."
  classes: []
- file: graphrag/index/workflows/create_community_reports_text.py
  functions:
  - node_id: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
    name: create_community_reports_text
    signature: "def create_community_reports_text(\n    entities: pd.DataFrame,\n\
      \    communities: pd.DataFrame,\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n    num_threads: int\
      \ = 4,\n) -> pd.DataFrame"
    docstring: "Transforms input data into finalized community reports by building\
      \ local contexts and summarizing communities.\n\nArgs:\n    entities: DataFrame\
      \ containing entities data used to explode communities into nodes.\n    communities:\
      \ DataFrame containing community definitions and metadata.\n    text_units:\
      \ DataFrame containing text unit data.\n    callbacks: WorkflowCallbacks instance\
      \ for workflow callbacks.\n    cache: PipelineCache instance used for caching\
      \ results.\n    summarization_strategy: dict configuring the summarization process\
      \ (e.g., prompts and model settings).\n    async_mode: AsyncType indicating\
      \ the asynchronous backend to use.\n    num_threads: int number of worker threads\
      \ to use.\n\nReturns:\n    pd.DataFrame: Finalized community reports.\n\nRaises:\n\
      \    Propagates exceptions raised by underlying helper functions and data processing\
      \ steps."
  - node_id: graphrag/index/workflows/create_community_reports_text.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Runs the workflow to transform community reports text and persists\
      \ the results to storage.\n\nThis workflow loads input data from storage (entities,\
      \ communities, and text_units), configures language-model and summarization\
      \ settings, builds contextual prompts, generates the community reports text,\
      \ and writes the resulting table to storage as \"community_reports\". The operation\
      \ persists the final output to storage and relies on the provided callbacks\
      \ and cache through the PipelineRunContext.\n\nArgs:\n  config: GraphRagConfig\
      \ containing settings used by the workflow, including language model, data extraction\
      \ configurations, and summarization strategy.\n  context: PipelineRunContext\
      \ providing access to output_storage, callbacks, and cache used during the workflow.\n\
      \nReturns:\n  WorkflowFunctionOutput: The output of the workflow, containing\
      \ the generated community_reports DataFrame as the result.\n\nRaises:\n  FileNotFoundError:\
      \ If required parquet inputs (entities.parquet, communities.parquet, or text_units.parquet)\
      \ cannot be found in storage.\n  Exception: Exceptions raised by the storage\
      \ backend during load or write operations may propagate."
  classes: []
- file: graphrag/index/workflows/create_final_documents.py
  functions:
  - node_id: graphrag/index/workflows/create_final_documents.py::create_final_documents
    name: create_final_documents
    signature: "def create_final_documents(\n    documents: pd.DataFrame, text_units:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    docstring: "Transforms input documents and text units into final documents.\n\n\
      Args:\n    documents: pd.DataFrame\n        Input documents data frame. Expected\
      \ to contain at least the columns referenced by DOCUMENTS_FINAL_COLUMNS.\n \
      \   text_units: pd.DataFrame\n        Input text units data frame. Expected\
      \ to contain an 'document_ids' column indicating related document ids.\n\nReturns:\n\
      \    pd.DataFrame\n        Final documents data frame with columns defined by\
      \ DOCUMENTS_FINAL_COLUMNS. The function ensures a metadata column exists and\
      \ assigns a human_readable_id based on the row index.\n\nRaises:\n    Exception:\
      \ Propagates exceptions raised by pandas operations or data frame manipulations\
      \ if inputs are invalid."
  - node_id: graphrag/index/workflows/create_final_documents.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    _config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Runs the final documents transformation workflow.\n\nThis async function\
      \ loads the documents and text_units tables from storage via the given context,\
      \ creates the final documents with create_final_documents, writes the resulting\
      \ documents table back to storage, and returns a WorkflowFunctionOutput containing\
      \ the produced DataFrame.\n\nArgs:\n  _config: GraphRagConfig\n      GraphRag\
      \ configuration used by the workflow.\n  context: PipelineRunContext\n     \
      \ Runtime context including the output storage handle.\n\nReturns:\n  WorkflowFunctionOutput\n\
      \      The workflow output whose result is the final documents DataFrame.\n\n\
      Raises:\n  Exception\n      Exceptions raised by the storage backend or by the\
      \ create_final_documents\n      operation may propagate to the caller."
  classes: []
- file: graphrag/index/workflows/create_final_text_units.py
  functions:
  - node_id: graphrag/index/workflows/create_final_text_units.py::_covariates
    name: _covariates
    signature: 'def _covariates(df: pd.DataFrame) -> pd.DataFrame'
    docstring: "Compute covariate IDs for each text unit from the input DataFrame.\n\
      \nArgs:\n    df: Input DataFrame containing the columns \"id\" and \"text_unit_id\"\
      .\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\" and \"covariate_ids\"\
      ; for each text_unit_id, covariate_ids is the list of unique ids associated\
      \ with that text unit.\n\nRaises:\n    KeyError: If the required columns \"\
      id\" or \"text_unit_id\" are missing from df."
  - node_id: graphrag/index/workflows/create_final_text_units.py::_entities
    name: _entities
    signature: 'def _entities(df: pd.DataFrame) -> pd.DataFrame'
    docstring: "\"\"\"Compute mapping of text units to the entity IDs that reference\
      \ them.\n\nArgs:\n    df: pd.DataFrame containing the columns \"id\" and \"\
      text_unit_ids\".\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\"\
      \ and \"entity_ids\"; for each text_unit_id, entity_ids is the list of unique\
      \ ids referencing that text unit.\n\nRaises:\n    KeyError: If the required\
      \ columns \"id\" or \"text_unit_ids\" are missing from df.\n\"\"\""
  - node_id: graphrag/index/workflows/create_final_text_units.py::_join
    name: _join
    signature: def _join(left, right)
    docstring: "Join two DataFrames on the id column using a left merge.\n\nArgs:\n\
      \    left: pd.DataFrame\n        Left DataFrame to join on id.\n    right: pd.DataFrame\n\
      \        Right DataFrame to join on id.\n\nReturns:\n    pd.DataFrame\n    \
      \    The result of merging left and right on 'id' with a left join, applying\
      \ suffixes '_1' and '_2' to overlapping columns.\n\nRaises:\n    Exception:\
      \ Propagates exceptions raised by pandas DataFrame.merge during the join operation."
  - node_id: graphrag/index/workflows/create_final_text_units.py::_relationships
    name: _relationships
    signature: 'def _relationships(df: pd.DataFrame) -> pd.DataFrame'
    docstring: "Compute mapping of text units to the relationship IDs that reference\
      \ them.\n\nArgs:\n    df: pd.DataFrame containing the columns \"id\" and \"\
      text_unit_ids\".\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\"\
      \ and \"relationship_ids\"; for each text_unit_id, relationship_ids is the list\
      \ of unique ids referencing that text unit.\n\nRaises:\n    KeyError: If the\
      \ required columns \"id\" or \"text_unit_ids\" are missing from df."
  - node_id: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
    name: create_final_text_units
    signature: "def create_final_text_units(\n    text_units: pd.DataFrame,\n    final_entities:\
      \ pd.DataFrame,\n    final_relationships: pd.DataFrame,\n    final_covariates:\
      \ pd.DataFrame | None,\n) -> pd.DataFrame"
    docstring: "Transfroms input text units and their associated entities, relationships,\
      \ and optional covariates into the final text units DataFrame.\n\nArgs:\n  \
      \  text_units (pd.DataFrame): Input text units. Expected to contain at least\
      \ the columns:\n        - id\n        - text\n        - document_ids\n     \
      \   - n_tokens\n    final_entities (pd.DataFrame): Mapping of entities to text\
      \ units. Must contain:\n        - id (entity_id)\n        - text_unit_ids (list-like\
      \ of text_unit_ids)\n    final_relationships (pd.DataFrame): Mapping of relationships\
      \ to text units. Must contain:\n        - id (relationship_id)\n        - text_unit_ids\
      \ (list-like of text_unit_ids)\n    final_covariates (pd.DataFrame | None):\
      \ Optional covariates mapping. If provided, must contain:\n        - id (covariate_id)\n\
      \        - text_unit_id (text_unit_id to which the covariate applies)\n\nReturns:\n\
      \    pd.DataFrame: Final text units data frame with columns defined by TEXT_UNITS_FINAL_COLUMNS.\n\
      \nRaises:\n    KeyError: If required columns are missing from any input DataFrame:\n\
      \        - text_units must include id, text, document_ids, and n_tokens\n  \
      \      - final_entities must include id and text_unit_ids\n        - final_relationships\
      \ must include id and text_unit_ids\n        - final_covariates (if not None)\
      \ must include id and text_unit_id\n\nProcessing details:\n    1) Select core\
      \ fields from text_units (id, text, document_ids, n_tokens) and add a human_readable_id\
      \ derived from the index.\n    2) Build text-unit-to-entity and text-unit-to-relationship\
      \ mappings via _entities and _relationships.\n    3) Join the selected text\
      \ units with the entity mapping, then with the relationship mapping to propagate\
      \ IDs.\n    4) If final_covariates is provided, build the covariate mapping\
      \ via _covariates and join; otherwise initialize covariate_ids with empty lists.\n\
      \    5) Group by text unit id and take the first row per id to collapse duplicates.\n\
      \    6) Return only the columns defined by TEXT_UNITS_FINAL_COLUMNS.\n\nNotes:\n\
      \    - When final_covariates is None, covariate_ids are set to empty lists for\
      \ every row.\n    - The exact output columns depend on TEXT_UNITS_FINAL_COLUMNS\
      \ and may vary with configuration."
  - node_id: graphrag/index/workflows/create_final_text_units.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the final text units transformation workflow by loading input\
      \ tables from storage, constructing final text units using entities, relationships,\
      \ and optional covariates, and writing the output back to storage.\n\nArgs:\n\
      \  config (GraphRagConfig): GraphRag configuration for the workflow\n  context\
      \ (PipelineRunContext): Pipeline run context containing the output storage used\
      \ for load/write operations\n\nReturns:\n  WorkflowFunctionOutput: The output\
      \ object containing the final text units DataFrame in its result attribute\n\
      \nRaises:\n  Exception: Exceptions raised by storage backends during load or\
      \ write operations may propagate."
  classes: []
- file: graphrag/index/workflows/extract_covariates.py
  functions:
  - node_id: graphrag/index/workflows/extract_covariates.py::extract_covariates
    name: extract_covariates
    signature: "def extract_covariates(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    covariate_type: str,\n\
      \    extraction_strategy: dict[str, Any] | None,\n    async_mode: AsyncType\
      \ = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n    num_threads:\
      \ int = 4,\n) -> pd.DataFrame"
    docstring: 'All the steps to extract and format covariates.


      Args:

      - text_units (pd.DataFrame): Input text units to process. Must contain at least
      the columns "id" and "text". This function mutates text_units in place by adding
      a temporary text_unit_id column equal to id, and then removes it before returning.

      - callbacks (WorkflowCallbacks): Callbacks used during the extraction workflow.

      - cache (PipelineCache): Cache for the extraction process.

      - covariate_type (str): Covariate type to extract (for example, "claim").

      - extraction_strategy (dict[str, Any] | None): Configuration for the extraction
      strategy or None.

      - async_mode (AsyncType): Asynchronous execution mode to use.

      - entity_types (list[str] | None): Entity types to consider; None to include
      all.

      - num_threads (int): Number of threads for the extraction step.


      Returns:

      pd.DataFrame: A covariates dataframe containing the final columns defined by
      COVARIATES_FINAL_COLUMNS. Each row represents an extracted covariate and is
      augmented with a unique id and a human_readable_id corresponding to the dataframe
      index.


      Side effects:

      - The input text_units DataFrame is mutated in place by adding a temporary text_unit_id
      column equal to the original id, which is dropped before returning.


      Raises:

      - KeyError if required columns (e.g., "id" or "text") are missing from text_units.

      - ValueError, TypeError, or other exceptions raised by the underlying extractor
      if inputs are invalid.


      Example:

      Suppose text_units is a DataFrame with columns ["id", "text"] and two rows.
      After calling extract_covariates with appropriate callbacks, cache, covariate_type,
      and strategy, the function returns a covariates DataFrame containing the final
      covariate columns as defined by COVARIATES_FINAL_COLUMNS, with additional id
      (uuid4) and human_readable_id (index) columns. The original text_units is restored
      after processing aside from the transient in-place mutation during execution.'
  - node_id: graphrag/index/workflows/extract_covariates.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the covariates extraction workflow.\n\nThis asynchronous workflow\
      \ performs the following steps:\n- If extraction of claims is enabled in the\
      \ provided config:\n  - Load text_units from storage using the context's output\
      \ storage.\n  - Get language model settings for the configured claims model\
      \ and resolve the extraction strategy.\n  - Execute extract_covariates with\
      \ the prepared inputs and write the resulting covariates to storage under the\
      \ name covariates.\n- If extraction is disabled:\n  - Skip loading, extraction,\
      \ and writing; output remains None.\n\nThe function returns a WorkflowFunctionOutput\
      \ whose result is the covariates DataFrame when extraction occurred, or None\
      \ when extraction was skipped.\n\nArgs:\n  config (GraphRagConfig): GraphRag\
      \ configuration for the covariates extraction workflow.\n  context (PipelineRunContext):\
      \ Pipeline run context containing the output storage, callbacks, and cache used\
      \ by the workflow.\n\nReturns:\n  WorkflowFunctionOutput: The workflow output\
      \ wrapper containing the covariates DataFrame, or None if extraction was skipped.\n\
      \nRaises:\n  ValueError: Could not find required parquet files in storage, or\
      \ other storage load errors.\n  Exception: Exceptions raised by the storage\
      \ backend or the covariate extraction process may propagate during the workflow.\n\
      \nNote:\n  The function may propagate exceptions from storage or extraction;\
      \ these are not caught within the workflow."
  classes: []
- file: graphrag/index/workflows/extract_graph.py
  functions:
  - node_id: graphrag/index/workflows/extract_graph.py::_validate_data
    name: _validate_data
    signature: 'def _validate_data(df: pd.DataFrame) -> bool'
    docstring: "Validate that the dataframe has data.\n\nArgs:\n    df (pd.DataFrame):\
      \ DataFrame to validate.\n\nReturns:\n    bool: True if the DataFrame contains\
      \ at least one row, False otherwise."
  - node_id: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
    name: get_summarized_entities_relationships
    signature: "def get_summarized_entities_relationships(\n    extracted_entities:\
      \ pd.DataFrame,\n    extracted_relationships: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
      \ dict[str, Any] | None = None,\n    summarization_num_threads: int = 4,\n)\
      \ -> tuple[pd.DataFrame, pd.DataFrame]"
    docstring: "Summarize the entities and relationships using the provided summarization\
      \ strategy.\n\nArgs:\n    extracted_entities: DataFrame containing extracted\
      \ entity nodes to be summarized.\n    extracted_relationships: DataFrame containing\
      \ extracted relationships to be summarized.\n    callbacks: WorkflowCallbacks\
      \ providing progress reporting hooks for long-running operations.\n    cache:\
      \ PipelineCache used to cache results from the summarization strategy.\n   \
      \ summarization_strategy: dictionary configuring the summarization approach;\
      \ may be None to use defaults.\n    summarization_num_threads: number of threads\
      \ to use for summarization.\n\nReturns:\n    tuple[pd.DataFrame, pd.DataFrame]:\
      \ A tuple containing:\n        - entities: DataFrame with entity summaries merged\
      \ on \"title\" (after dropping the original \"description\" column).\n     \
      \   - relationships: DataFrame with relationship summaries merged on [\"source\"\
      , \"target\"] (after dropping the original \"description\" column).\n\nRaises:\n\
      \    Exception: If summarization or subsequent DataFrame operations fail, propagating\
      \ exceptions from summarize_descriptions or pandas.\"}"
  - node_id: graphrag/index/workflows/extract_graph.py::extract_graph
    name: extract_graph
    signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks:\
      \ WorkflowCallbacks,\n    cache: PipelineCache,\n    extraction_strategy: dict[str,\
      \ Any] | None = None,\n    extraction_num_threads: int = 4,\n    extraction_async_mode:\
      \ AsyncType = AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n\
      \    summarization_strategy: dict[str, Any] | None = None,\n    summarization_num_threads:\
      \ int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]"
    docstring: "All steps to create the base entity graph.\n\nThis asynchronous function\
      \ processes the input text units to extract entities and relationships, validates\n\
      the extraction results, preserves raw extraction outputs, and summarizes the\
      \ data to produce the final\nentities and relationships DataFrames. It returns\
      \ a tuple of (entities, relationships, raw_entities, raw_relationships).\n\n\
      Args:\n    text_units: DataFrame containing the text units to process.\n   \
      \ callbacks: Callbacks to report progress during the workflow.\n    cache: Cache\
      \ to store/retrieve intermediate results.\n    extraction_strategy: Strategy\
      \ configuration for entity extraction.\n    extraction_num_threads: Number of\
      \ threads to use for extraction.\n    extraction_async_mode: Async mode for\
      \ extraction (e.g., AsyncIO).\n    entity_types: Optional list of entity types\
      \ to constrain extraction.\n    summarization_strategy: Strategy configuration\
      \ for summarization.\n    summarization_num_threads: Number of threads to use\
      \ for summarization.\n\nReturns:\n    tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame,\
      \ pd.DataFrame]:\n        A tuple of four DataFrames:\n        - entities: summarized\
      \ entities\n        - relationships: summarized relationships\n        - raw_entities:\
      \ entities extracted before summarization\n        - raw_relationships: relationships\
      \ extracted before summarization\n\nRaises:\n    ValueError: If no entities\
      \ detected during extraction or no relationships detected during extraction."
  - node_id: graphrag/index/workflows/extract_graph.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: '"""Asynchronously run the extract_graph workflow to build the base
      entity graph and persist results to storage."""'
  classes: []
- file: graphrag/index/workflows/extract_graph_nlp.py
  functions:
  - node_id: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
    name: extract_graph_nlp
    signature: "def extract_graph_nlp(\n    text_units: pd.DataFrame,\n    cache:\
      \ PipelineCache,\n    extraction_config: ExtractGraphNLPConfig,\n) -> tuple[pd.DataFrame,\
      \ pd.DataFrame]"
    docstring: "Asynchronously extract the base entity graph (nodes and edges) from\
      \ the given text units.\n\nArgs:\n    text_units: pd.DataFrame: Input text units\
      \ used to extract noun phrases for graph construction.\n    cache: PipelineCache:\
      \ Cache used during extraction and graph construction.\n    extraction_config:\
      \ ExtractGraphNLPConfig: Configuration for extraction settings, including text_analyzer,\
      \ normalize_edge_weights, concurrent_requests, and async_mode.\n\nReturns:\n\
      \    tuple[pd.DataFrame, pd.DataFrame]: A tuple containing extracted_nodes and\
      \ extracted_edges. extracted_nodes has an added 'type' column with value 'NOUN\
      \ PHRASE' and an added 'description' column (empty string); extracted_edges\
      \ has an added 'description' column (empty string).\n\nRaises:\n    Exception:\
      \ Propagates exceptions raised by the underlying noun-phrase extractor creation\
      \ or by build_noun_graph."
  - node_id: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the extract_graph_nlp workflow to build the base entity graph\
      \ and persist results to storage.\n\nThis coroutine orchestrates the extraction\
      \ of noun-phrase based graph components by loading text units from storage,\
      \ invoking extract_graph_nlp to produce entities and relationships, writing\
      \ the resulting tables back to storage, and returning a WorkflowFunctionOutput\
      \ containing the produced data.\n\nArgs:\n  config: GraphRagConfig\n      Configuration\
      \ for the extract_graph_nlp workflow, including extraction_config parameters.\n\
      \  context: PipelineRunContext\n      The runtime context containing storage\
      \ handles and cache used by the workflow.\n\nReturns:\n  WorkflowFunctionOutput\n\
      \      The output with a result dictionary containing:\n        entities: DataFrame\
      \ of extracted entities\n        relationships: DataFrame of extracted relationships\n\
      \nRaises:\n  ValueError\n      Could not find the required text_units.parquet\
      \ in storage (text_units).\n  Exception\n      Exceptions raised by the storage\
      \ backend or parquet reader/writer during load or write operations, or errors\
      \ raised by extract_graph_nlp."
  classes: []
- file: graphrag/index/workflows/factory.py
  functions:
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register
    name: register
    signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
    docstring: "Register a custom workflow function.\n\nArgs:\n    cls: The class\
      \ that provides access to the registry (PipelineFactory).\n    name: The name\
      \ under which the workflow will be registered.\n    workflow: The workflow function\
      \ to register for the given name.\n\nReturns:\n    None"
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_all
    name: register_all
    signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
    docstring: "Register a dict of custom workflow functions.\n\nArgs:\n    cls: The\
      \ class that provides access to the registry (PipelineFactory).\n    workflows:\
      \ A dictionary mapping workflow names to workflow functions.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.create_pipeline
    name: create_pipeline
    signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
      \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline"
    docstring: "Create a pipeline for executing a sequence of workflows.\n\nArgs:\n\
      \    cls: The class reference (provided automatically for classmethod)\n   \
      \ config: GraphRagConfig\n    method: The indexing method or key to select a\
      \ predefined pipeline. Defaults to IndexingMethod.Standard.\n\nReturns:\n  \
      \  Pipeline: The constructed Pipeline object.\n\nRaises:\n    KeyError: If any\
      \ workflow name in the selected workflows is not registered in the class-level\
      \ workflows registry."
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_pipeline
    name: register_pipeline
    signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
    docstring: "Register a new pipeline method as a list of workflow names.\n\nArgs:\n\
      \    name: The name of the pipeline to register.\n    workflows: A list of workflow\
      \ names that constitute the pipeline.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/index/workflows/factory.py::PipelineFactory
    name: PipelineFactory
    docstring: "PipelineFactory coordinates registration and construction of pipelines\
      \ composed of workflow functions. It maintains a class-level registry of named\
      \ WorkflowFunction callables and can assemble these into reusable Pipeline objects\
      \ for GraphRag-based workflows. A Pipeline is a sequence of WorkflowFunction\
      \ objects executed in order to process GraphRag data.\n\nAttributes:\n  registry:\
      \ ClassVar[dict[str, WorkflowFunction]] - class-level mapping of names to workflow\
      \ callables used to build pipelines and validate references.\n\nMethods:\n \
      \ register(cls, name: str, workflow: WorkflowFunction)\n    Register a custom\
      \ workflow function.\n    Args:\n      cls: The class that provides access to\
      \ the registry (PipelineFactory).\n      name: The name under which the workflow\
      \ will be registered.\n      workflow: The workflow function to register for\
      \ the given name.\n    Returns:\n      None\n    Raises:\n      TypeError: If\
      \ the provided name or workflow are of incorrect types.\n\n  register_all(cls,\
      \ workflows: dict[str, WorkflowFunction])\n    Register a dict of custom workflow\
      \ functions.\n    Args:\n      cls: The class that provides access to the registry\
      \ (PipelineFactory).\n      workflows: A dictionary mapping workflow names to\
      \ workflow functions.\n    Returns:\n      None\n    Raises:\n      TypeError:\
      \ If the mapping is not of the expected type or contains invalid entries.\n\n\
      \  create_pipeline(\n        cls,\n        config: GraphRagConfig,\n       \
      \ method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline\n\
      \    Create a pipeline for executing a sequence of workflows.\n    Args:\n \
      \     cls: The class reference (provided automatically for classmethod).\n \
      \     config: GraphRagConfig describing the graph/rag indexing setup.\n    \
      \  method: The indexing method or key to select a predefined pipeline. Defaults\
      \ to IndexingMethod.Standard.\n    Returns:\n      Pipeline: The constructed\
      \ Pipeline object.\n    Raises:\n      KeyError: If any workflow name in the\
      \ selected workflows is not registered.\n      TypeError: If the provided config\
      \ or method have invalid types.\n      ValueError: If the resolved workflow\
      \ list is empty or otherwise invalid.\n\n  register_pipeline(cls, name: str,\
      \ workflows: list[str])\n    Register a new pipeline method as a list of workflow\
      \ names.\n    Args:\n      cls: The class reference (PipelineFactory).\n   \
      \   name: The name of the pipeline to register.\n      workflows: A list of\
      \ workflow names that constitute the pipeline.\n    Returns:\n      None\n \
      \   Raises:\n      TypeError: If inputs have incorrect types.\n      KeyError:\
      \ If any referenced workflow name is not registered.\n      ValueError: If the\
      \ workflows list is empty."
    methods:
    - name: register
      signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
    - name: register_all
      signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
    - name: create_pipeline
      signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
        \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) ->\
        \ Pipeline"
    - name: register_pipeline
      signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
- file: graphrag/index/workflows/finalize_graph.py
  functions:
  - node_id: graphrag/index/workflows/finalize_graph.py::finalize_graph
    name: finalize_graph
    signature: "def finalize_graph(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
      \ bool = False,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
    docstring: "Finalize the entity and relationship formats by applying the finalization\
      \ steps.\n\nArgs:\n    entities (pd.DataFrame): Input entities to be transformed\
      \ into final records.\n    relationships (pd.DataFrame): DataFrame containing\
      \ edge information used to finalize relationships.\n    embed_config (EmbedGraphConfig\
      \ | None): Optional configuration for embedding graphs; passed to finalization.\n\
      \    layout_enabled (bool): If True, enables applying a layout during finalization.\n\
      \nReturns:\n    tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the final_entities\
      \ DataFrame and final_relationships DataFrame.\n\nRaises:\n    Propagates exceptions\
      \ raised by finalize_entities or finalize_relationships."
  - node_id: graphrag/index/workflows/finalize_graph.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: Run the finalize_graph workflow to finalize the entity and relationship
      data, persist updates to storage, and optionally snapshot GraphML graphs.
  classes: []
- file: graphrag/index/workflows/generate_text_embeddings.py
  functions:
  - node_id: graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
    name: _run_embeddings
    signature: "def _run_embeddings(\n    name: str,\n    data: pd.DataFrame,\n  \
      \  embed_column: str,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
      \    text_embed_config: dict,\n) -> pd.DataFrame"
    docstring: "All steps to generate a single embedding.\n\nArgs:\n  name: The name\
      \ of the embedding, used as the embedding_name when calling embed_text.\n  data:\
      \ DataFrame containing input data; the function adds an embedding column and\
      \ returns a DataFrame with only id and embedding.\n  embed_column: The column\
      \ in data to embed; passed to embed_text as the embed_column.\n  callbacks:\
      \ WorkflowCallbacks used to report progress and handle lifecycle events during\
      \ embedding.\n  cache: PipelineCache used by embed_text for caching and resource\
      \ management.\n  text_embed_config: Dictionary with embedding configuration;\
      \ should include a strategy key.\n\nReturns:\n  pd.DataFrame: A DataFrame containing\
      \ the id and embedding columns.\n\nRaises:\n  May raise exceptions from embed_text\
      \ or DataFrame operations."
  - node_id: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
    name: generate_text_embeddings
    signature: "def generate_text_embeddings(\n    documents: pd.DataFrame | None,\n\
      \    relationships: pd.DataFrame | None,\n    text_units: pd.DataFrame | None,\n\
      \    entities: pd.DataFrame | None,\n    community_reports: pd.DataFrame | None,\n\
      \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    text_embed_config:\
      \ dict,\n    embedded_fields: list[str],\n) -> dict[str, pd.DataFrame]"
    docstring: "All the steps to generate all embeddings.\n\nArgs:\n  documents: DataFrame\
      \ or None. Data for document text embeddings; when provided, expected to include\
      \ id and text columns.\n  relationships: DataFrame or None. Data for relationship\
      \ descriptions; when provided, expected to include id and description columns.\n\
      \  text_units: DataFrame or None. Data for text units; when provided, expected\
      \ to include id and text columns.\n  entities: DataFrame or None. Data for entities;\
      \ may include id, title, and description used for embeddings.\n  community_reports:\
      \ DataFrame or None. Data for community reports; used for title, summary, and\
      \ full content embeddings if provided.\n  callbacks: WorkflowCallbacks. Callbacks\
      \ used during embedding processing.\n  cache: PipelineCache. Cache used by the\
      \ embedding routine.\n  text_embed_config: dict. Embedding configuration (e.g.,\
      \ strategy) passed to the embedding function.\n  embedded_fields: list[str].\
      \ The list of embedding fields to generate; each corresponds to a key in the\
      \ embedding_param_map.\n\nReturns:\n  dict[str, pd.DataFrame]. Mapping from\
      \ embedding name to a DataFrame containing columns [\"id\", \"embedding\"] for\
      \ that embedding.\n\nRaises:\n  Exceptions raised by the underlying embedding\
      \ operations (e.g., _run_embeddings or embed_text) may propagate to the caller\
      \ if embedding fails."
  - node_id: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Generates text embeddings for the configured fields and returns the\
      \ results. This workflow loads input data conditionally based on the configured\
      \ embed_text fields, invokes the embedding generation, and optionally persists\
      \ the resulting embeddings as snapshots to storage.\n\nArgs:\n  config: GraphRagConfig\n\
      \      GraphRagConfig containing embed_text and snapshot settings used by the\
      \ workflow.\n  context: PipelineRunContext\n      PipelineRunContext providing\
      \ output_storage, callbacks, and cache.\n\nReturns:\n  WorkflowFunctionOutput\n\
      \      The workflow result containing generated embeddings as a mapping from\
      \ embedding name to DataFrame.\n\nRaises:\n  ValueError\n      Could not find\
      \ {name}.parquet in storage!\n  Exception\n      Exceptions raised by the storage\
      \ backend or parquet reader during the load operation, or by the storage backend\
      \ during the write operation when snapshots are enabled."
  classes: []
- file: graphrag/index/workflows/load_input_documents.py
  functions:
  - node_id: graphrag/index/workflows/load_input_documents.py::load_input_documents
    name: load_input_documents
    signature: "def load_input_documents(\n    config: InputConfig, storage: PipelineStorage\n\
      ) -> pd.DataFrame"
    docstring: "Load and parse input documents into a standard format.\n\nArgs:\n\
      \    config: InputConfig containing input configuration (such as file_type and\
      \ metadata) and storage base_dir information.\n    storage: PipelineStorage\
      \ used to access the input data.\n\nReturns:\n    pandas.DataFrame: The loaded\
      \ input data as a DataFrame."
  - node_id: graphrag/index/workflows/load_input_documents.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Load input documents, write them to storage, and return the result\
      \ as a WorkflowFunctionOutput.\n\nArgs:\n    config: GraphRagConfig containing\
      \ input configuration and related settings.\n    context: PipelineRunContext\
      \ providing access to input_storage, output_storage, and runtime statistics.\n\
      \nReturns:\n    WorkflowFunctionOutput: The output containing the loaded input\
      \ documents as a pandas DataFrame in the result field.\n\nRaises:\n    Exception:\
      \ Exceptions raised by the storage backend during the write operation may propagate."
  classes: []
- file: graphrag/index/workflows/load_update_documents.py
  functions:
  - node_id: graphrag/index/workflows/load_update_documents.py::load_update_documents
    name: load_update_documents
    signature: "def load_update_documents(\n    config: InputConfig,\n    input_storage:\
      \ PipelineStorage,\n    previous_storage: PipelineStorage,\n) -> pd.DataFrame"
    docstring: "Load and parse update-only input documents into a standard format.\n\
      \nThis asynchronous function loads input documents using create_input(config,\
      \ input_storage), computes the delta against previously stored documents using\
      \ get_delta_docs(input_documents, previous_storage), and returns the new inputs\
      \ as a DataFrame.\n\nArgs:\n    config: InputConfig containing input configuration\
      \ (such as file_type and metadata) and storage base_dir information.\n    input_storage:\
      \ PipelineStorage used to access the input data.\n    previous_storage: PipelineStorage\
      \ containing the previously stored final documents to diff against.\n\nReturns:\n\
      \    pandas.DataFrame: The new/update input documents as determined by the delta\
      \ computation (delta_documents.new_inputs).\n\nRaises:\n    Exceptions raised\
      \ by create_input and get_delta_docs as encountered."
  - node_id: graphrag/index/workflows/load_update_documents.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Run the update-document loading workflow: load update-only input documents,\
      \ write them to storage, and return the result.\n\nArgs:\n  config: GraphRagConfig\
      \ containing input configuration and related settings.\n  context: PipelineRunContext\
      \ providing access to input_storage, output_storage, and runtime statistics.\n\
      \nReturns:\n  WorkflowFunctionOutput: The output containing the loaded update\
      \ documents as a DataFrame, or a stop signal if no update documents were found.\n\
      \nRaises:\n  Exception: Exceptions raised by the input loading or storage backends\
      \ may propagate."
  classes: []
- file: graphrag/index/workflows/prune_graph.py
  functions:
  - node_id: graphrag/index/workflows/prune_graph.py::prune_graph
    name: prune_graph
    signature: "def prune_graph(\n    entities: pd.DataFrame,\n    relationships:\
      \ pd.DataFrame,\n    pruning_config: PruneGraphConfig,\n) -> tuple[pd.DataFrame,\
      \ pd.DataFrame]"
    docstring: "Prune a full graph based on graph statistics.\n\nArgs:\n    entities\
      \ (pd.DataFrame): DataFrame of entity nodes used to construct the graph. Must\
      \ include a 'title' column to identify nodes.\n    relationships (pd.DataFrame):\
      \ DataFrame of relationships/edges. Must include 'source' and 'target' columns.\
      \ May include a 'weight' column used as an edge attribute during pruning.\n\
      \    pruning_config (PruneGraphConfig): Configuration object containing pruning\
      \ parameters such as min_node_freq, max_node_freq_std, min_node_degree, max_node_degree_std,\
      \ min_edge_weight_pct, remove_ego_nodes, and lcc_only.\n\nReturns:\n    tuple[pd.DataFrame,\
      \ pd.DataFrame]: A tuple containing the pruned entities and pruned relationships\
      \ as DataFrames. These DataFrames are subsets of the input DataFrames corresponding\
      \ to the pruned graph.\n\nRaises:\n    Propagates exceptions from the underlying\
      \ graph construction and pruning operations (e.g., due to invalid input data\
      \ or missing required columns)."
  - node_id: graphrag/index/workflows/prune_graph.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Executes the prune-graph workflow in three steps: load entities and\
      \ relationships from storage, prune using the provided pruning configuration,\
      \ and write the pruned entities and relationships back to storage. Returns the\
      \ pruned data as part of the WorkflowFunctionOutput.\n\nArgs:\n  config (GraphRagConfig):\
      \ Configuration for pruning, including parameters exposed under prune_graph\
      \ to control pruning behavior.\n  context (PipelineRunContext): Execution context\
      \ containing the storage backend and runtime information used for reading and\
      \ writing tables.\n\nReturns:\n  WorkflowFunctionOutput: Output with a result\
      \ dictionary containing:\n  - \"entities\": pruned entities DataFrame\n  - \"\
      relationships\": pruned relationships DataFrame\n\nRaises:\n  ValueError: If\
      \ required input tables (entities or relationships) are not found in storage.\n\
      \  Exception: Exceptions raised by the storage backend during load or write\
      \ operations may propagate."
  classes: []
- file: graphrag/index/workflows/update_clean_state.py
  functions:
  - node_id: graphrag/index/workflows/update_clean_state.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(  # noqa: RUF029\n    _config: GraphRagConfig,\n\
      \    context: PipelineRunContext,\n) -> WorkflowFunctionOutput"
    docstring: "Clean the state after the update.\n\nArgs:\n    _config (GraphRagConfig):\
      \ GraphRag configuration.\n    context (PipelineRunContext): Runtime context\
      \ for the workflow execution.\n\nReturns:\n    WorkflowFunctionOutput: Output\
      \ object for the workflow function; result is None."
  classes: []
- file: graphrag/index/workflows/update_communities.py
  functions:
  - node_id: graphrag/index/workflows/update_communities.py::_update_communities
    name: _update_communities
    signature: "def _update_communities(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> dict"
    docstring: "Update the communities output.\n\nArgs:\n  previous_storage: PipelineStorage\n\
      \      Storage containing the existing/previous communities.\n  delta_storage:\
      \ PipelineStorage\n      Storage containing the delta (updated) communities.\n\
      \  output_storage: PipelineStorage\n      Storage to write the merged communities\
      \ to.\n\nReturns:\n  dict\n      Mapping from original delta community IDs to\
      \ the new IDs assigned during the merge.\n\nRaises:\n  ValueError\n      Could\
      \ not find {name}.parquet in storage!\n  Exception\n      Exceptions raised\
      \ by the storage backend or parquet reader during the load operation.\n  Exception\n\
      \      Exceptions raised by the storage backend during the write operation may\
      \ propagate."
  - node_id: graphrag/index/workflows/update_communities.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the communities from an incremental index run.\n\nArgs:\n \
      \   config (GraphRagConfig): GraphRagConfig configuration for the workflow.\n\
      \    context (PipelineRunContext): PipelineRunContext carrying the state for\
      \ the run, including update_timestamp.\n\nReturns:\n    WorkflowFunctionOutput:\
      \ The output of the workflow function. The result is None.\n\nNotes:\n    During\
      \ execution, context.state[\"incremental_update_community_id_mapping\"] is set\
      \ to the\n    mapping produced by updating and merging the communities.\n\n\
      Raises:\n    Exception: Propagates exceptions raised by storage backends or\
      \ related IO operations (e.g.,\n        storage IO errors, network issues, or\
      \ data serialization problems)."
  classes: []
- file: graphrag/index/workflows/update_community_reports.py
  functions:
  - node_id: graphrag/index/workflows/update_community_reports.py::_update_community_reports
    name: _update_community_reports
    signature: "def _update_community_reports(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    community_id_mapping: dict,\n) -> pd.DataFrame"
    docstring: "Update the community reports output by merging old and delta reports\
      \ and writing the result to storage.\n\nArgs:\n    previous_storage: PipelineStorage\n\
      \        Storage containing the existing/previous community reports.\n    delta_storage:\
      \ PipelineStorage\n        Storage containing the delta (updated) community\
      \ reports.\n    output_storage: PipelineStorage\n        Storage to write the\
      \ merged community reports to.\n    community_id_mapping: dict\n        Mapping\
      \ from original delta community IDs to final IDs.\n\nReturns:\n    pd.DataFrame\n\
      \        The updated community reports aligned to COMMUNITY_REPORTS_FINAL_COLUMNS.\n\
      \nRaises:\n    ValueError\n        Could not find community_reports.parquet\
      \ in storage.\n    Exception\n        Exceptions raised by the storage backend\
      \ or parquet reader during the load operation.\n    Exception\n        Exceptions\
      \ raised by the storage backend during the write operation may propagate.\n\
      \    KeyError\n        If required columns such as 'community' or 'parent' are\
      \ missing from the input data when merging."
  - node_id: graphrag/index/workflows/update_community_reports.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the community reports from a incremental index run.\n\nArgs:\n\
      \    config: GraphRagConfig\n        GraphRagConfig configuration for the workflow.\n\
      \    context: PipelineRunContext\n        PipelineRunContext carrying the state\
      \ for the run, including update_timestamp and incremental_update_community_id_mapping.\n\
      \nReturns:\n    WorkflowFunctionOutput\n        The output of the workflow function.\
      \ The result is None.\n\nRaises:\n    KeyError\n        If 'update_timestamp'\
      \ is not present in context.state."
  classes: []
- file: graphrag/index/workflows/update_covariates.py
  functions:
  - node_id: graphrag/index/workflows/update_covariates.py::_merge_covariates
    name: _merge_covariates
    signature: "def _merge_covariates(\n    old_covariates: pd.DataFrame, delta_covariates:\
      \ pd.DataFrame\n) -> pd.DataFrame"
    docstring: "Merge the covariates.\n\nThis function merges the existing old covariates\
      \ with the delta covariates. The delta covariates are mutated in-place to assign\
      \ new human_readable_id values that are consecutive and start from max(old_covariates.human_readable_id)\
      \ + 1. The function then concatenates the old covariates and the mutated delta\
      \ covariates using ignore_index=True and returns the resulting DataFrame.\n\n\
      Args:\n    old_covariates (pd.DataFrame): The old covariates.\n    delta_covariates\
      \ (pd.DataFrame): The delta covariates to be merged into the old covariates.\
      \ This DataFrame is mutated in-place to assign new IDs.\n\nReturns:\n    pd.DataFrame:\
      \ The merged covariates, with old_covariates preceding delta_covariates and\
      \ a reset index (ignore_index=True)."
  - node_id: graphrag/index/workflows/update_covariates.py::_update_covariates
    name: _update_covariates
    signature: "def _update_covariates(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      ) -> None"
    docstring: "Update the covariates output by merging existing covariates with the\
      \ delta covariates and writing the result to storage.\n\nArgs:\n    previous_storage:\
      \ The storage containing the previous covariates.\n    delta_storage: The storage\
      \ containing the delta covariates to apply.\n    output_storage: The storage\
      \ to write the merged covariates to.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
      \ Could not find covariates.parquet in storage!\n    Exception: Exceptions raised\
      \ by the storage backend or parquet reader during load or write operations."
  - node_id: graphrag/index/workflows/update_covariates.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the covariates from a incremental index run.\n\nArgs:\n   \
      \ config: GraphRagConfig configuration for the workflow.\n    context: PipelineRunContext\
      \ containing state for the run, including update_timestamp.\n\nReturns:\n  \
      \  WorkflowFunctionOutput: The output of the workflow function. The result is\
      \ None.\n\nRaises:\n    Exception: Propagates exceptions raised by storage backends\
      \ or related IO operations."
  classes: []
- file: graphrag/index/workflows/update_entities_relationships.py
  functions:
  - node_id: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
    name: _update_entities_and_relationships
    signature: "def _update_entities_and_relationships(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    config: GraphRagConfig,\n    cache: PipelineCache,\n    callbacks: WorkflowCallbacks,\n\
      ) -> tuple[pd.DataFrame, pd.DataFrame, dict]"
    docstring: "Update Final Entities and Relationships output.\n\nThis function merges\
      \ the existing (previous) entities with the delta of new/updated entities, updates\
      \ and merges relationships, applies summarization to the merged entities and\
      \ relationships, and writes the results to the provided output storage.\n\n\
      Parameters:\n  previous_storage (PipelineStorage): The storage containing the\
      \ previous state data.\n  delta_storage (PipelineStorage): The storage containing\
      \ delta (new/updated) data.\n  output_storage (PipelineStorage): The storage\
      \ to write updated entities and relationships to.\n  config (GraphRagConfig):\
      \ GraphRag configuration used for summarization and merging.\n  cache (PipelineCache):\
      \ Cache used by the summarization routine.\n  callbacks (WorkflowCallbacks):\
      \ Callbacks for progress reporting during the workflow.\n\nReturns:\n  tuple[pd.DataFrame,\
      \ pd.DataFrame, dict]: The updated entities DataFrame, the updated relationships\
      \ DataFrame, and the entity_id_mapping dictionary.\n\nRaises:\n  ValueError:\
      \ If required data is missing from storage or a storage read error occurs.\n\
      \  KeyError: If required columns are missing from the input DataFrames used\
      \ by _update_and_merge_relationships.\n  TypeError: If inputs to _update_and_merge_relationships\
      \ are not pandas DataFrames.\n  Exception: General exceptions raised by the\
      \ storage backends during read/write operations or by the summarization step."
  - node_id: graphrag/index/workflows/update_entities_relationships.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the entities and relationships from a incremental index run.\n\
      \nArgs:\n    config: GraphRagConfig\n        GraphRagConfig containing configuration\
      \ for the workflow.\n    context: PipelineRunContext\n        PipelineRunContext\
      \ carrying the state for the run, including update_timestamp.\n\nReturns:\n\
      \    WorkflowFunctionOutput\n        The output of the workflow function.\n\n\
      Raises:\n    KeyError\n        If 'update_timestamp' is not present in context.state."
  classes: []
- file: graphrag/index/workflows/update_final_documents.py
  functions:
  - node_id: graphrag/index/workflows/update_final_documents.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the final documents from an incremental index run.\n\nArgs:\n\
      \    config: GraphRagConfig\n        GraphRagConfig containing configuration\
      \ for the workflow.\n\n    context: PipelineRunContext\n        PipelineRunContext\
      \ carrying the state for the run.\n\nReturns:\n    WorkflowFunctionOutput\n\
      \        A WorkflowFunctionOutput with result=None.\n\nRaises:\n    KeyError\n\
      \        If 'update_timestamp' is not present in context.state.\n\nSide effects:\n\
      \    Updates context.state['incremental_update_final_documents'] with the final\n\
      \    documents dataframe produced by concatenating previous and delta\n    documents\
      \ into the output storage."
  classes: []
- file: graphrag/index/workflows/update_text_embeddings.py
  functions:
  - node_id: graphrag/index/workflows/update_text_embeddings.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the text embeddings from an incremental index run.\n\nArgs:\n\
      \  config: GraphRagConfig containing configuration for embedding and storage\
      \ behavior.\n  context: PipelineRunContext carrying the state for the run, including\
      \ update_timestamp and incremental update data. The function reads the following\
      \ keys from context.state: 'update_timestamp', 'incremental_update_final_documents',\
      \ 'incremental_update_merged_relationships', 'incremental_update_merged_text_units',\
      \ 'incremental_update_merged_entities', and 'incremental_update_merged_community_reports'.\
      \ It also uses context.callbacks and context.cache.\n\nReturns:\n  WorkflowFunctionOutput:\
      \ A WorkflowFunctionOutput with result=None.\n\nRaises:\n  KeyError: If required\
      \ keys are missing from context.state (for example, update_timestamp or any\
      \ incremental_update_* keys).\n\nNotes:\n  The function calls get_update_storages(config,\
      \ context.state['update_timestamp']) to obtain storage backends, and then generate_text_embeddings\
      \ with the incremental update data to produce embeddings.\n  If config.snapshots.embeddings\
      \ is True, the resulting embedding tables are written to storage using write_table_to_storage\
      \ with names embeddings.<name>.\n\nState requirements:\n  context.state must\
      \ include:\n    update_timestamp (str)\n    incremental_update_final_documents\n\
      \    incremental_update_merged_relationships\n    incremental_update_merged_text_units\n\
      \    incremental_update_merged_entities\n    incremental_update_merged_community_reports"
  classes: []
- file: graphrag/index/workflows/update_text_units.py
  functions:
  - node_id: graphrag/index/workflows/update_text_units.py::_update_and_merge_text_units
    name: _update_and_merge_text_units
    signature: "def _update_and_merge_text_units(\n    old_text_units: pd.DataFrame,\n\
      \    delta_text_units: pd.DataFrame,\n    entity_id_mapping: dict,\n) -> pd.DataFrame"
    docstring: "Update and merge text units.\n\nArgs:\n  old_text_units: pd.DataFrame\n\
      \      The old text units.\n  delta_text_units: pd.DataFrame\n      The delta\
      \ text units.\n  entity_id_mapping: dict\n      The entity id mapping.\n\nReturns:\n\
      \  pd.DataFrame\n  The updated text units.\n\nRaises:\n  KeyError: If required\
      \ columns are missing from the input dataframes (e.g., 'entity_ids' or 'human_readable_id')."
  - node_id: graphrag/index/workflows/update_text_units.py::_update_text_units
    name: _update_text_units
    signature: "def _update_text_units(\n    previous_storage: PipelineStorage,\n\
      \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n\
      \    entity_id_mapping: dict,\n) -> pd.DataFrame"
    docstring: "Asynchronously update and merge text units from storage and write\
      \ the result to the output storage.\n\nArgs:\n    previous_storage: PipelineStorage\n\
      \        The storage containing the old text units.\n    delta_storage: PipelineStorage\n\
      \        The storage containing the delta text units to apply.\n    output_storage:\
      \ PipelineStorage\n        The storage where the merged text units will be written.\n\
      \    entity_id_mapping: dict\n        Mapping from old entity ids to new ids\
      \ to apply to delta_text_units.\n\nReturns:\n    pd.DataFrame\n        The updated\
      \ text units.\n\nRaises:\n    ValueError\n        Could not find text_units.parquet\
      \ in storage.\n    Exception\n        Exceptions raised by the storage backend\
      \ or parquet reader during the load or write operations.\n    KeyError\n   \
      \     If required columns are missing from the input dataframes (e.g., 'entity_ids'\
      \ or 'human_readable_id') during the update/merge process."
  - node_id: graphrag/index/workflows/update_text_units.py::run_workflow
    name: run_workflow
    signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
      ) -> WorkflowFunctionOutput"
    docstring: "Update the text units from a incremental index run.\n\nArgs:\n   \
      \ config: GraphRagConfig containing configuration for the workflow.\n    context:\
      \ PipelineRunContext carrying the state for the run.\n\nReturns:\n    WorkflowFunctionOutput:\
      \ A WorkflowFunctionOutput with result=None.\n\nRaises:\n    KeyError: If 'update_timestamp'\
      \ is not present in context.state."
  classes: []
- file: graphrag/language_model/events/base.py
  functions:
  - node_id: graphrag/language_model/events/base.py::ModelEventHandler.on_error
    name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    docstring: "Handle an model error.\n\nArgs:\n    error: BaseException | None:\
      \ The error that occurred, or None if no error is provided.\n    traceback:\
      \ str | None: The traceback string associated with the error, or None if not\
      \ available.\n    arguments: dict[str, Any] | None: Additional contextual arguments\
      \ related to the error, or None.\nReturns:\n    None: The function does not\
      \ return a value."
  classes:
  - class_id: graphrag/language_model/events/base.py::ModelEventHandler
    name: ModelEventHandler
    docstring: 'ModelEventHandler Protocol for handling model-related events, with
      a focus on error handling within the language model system.


      Purpose:

      Define the contract that concrete event handlers must follow to process and
      respond to errors raised by model operations.


      Key attributes:

      - on_error: The error-handling contract that implementations must provide.


      on_error signature:

      def on_error(self, error: BaseException | None, traceback: str | None = None,
      arguments: dict[str, Any] | None = None) -> None


      Notes:

      - The Args described below correspond to the on_error method parameters; there
      are no separate class-level parameters.

      - All parameters are optional to allow graceful handling when error information
      is incomplete.


      Args:

      - error: The error that occurred, or None if no error is provided.

      - traceback: The traceback string associated with the error, or None if not
      available.

      - arguments: Additional contextual arguments related to the error, or None.


      Returns:

      - None: The function does not return a value.


      Raises:

      - None'
    methods:
    - name: on_error
      signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
        \        traceback: str | None = None,\n        arguments: dict[str, Any]\
        \ | None = None,\n    ) -> None"
- file: graphrag/language_model/factory.py
  functions:
  - node_id: graphrag/language_model/factory.py::ModelFactory.register_embedding
    name: register_embedding
    signature: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
      \ EmbeddingModel]\n    ) -> None"
    docstring: "Register an EmbeddingModel implementation.\n\nStores the given creator\
      \ in the internal _embedding_registry mapping for the specified model_type.\n\
      \nArgs:\n    model_type: The type identifier for the EmbeddingModel to register.\n\
      \    creator: A callable that returns an EmbeddingModel instance when invoked.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/factory.py::ModelFactory.get_embedding_models
    name: get_embedding_models
    signature: def get_embedding_models(cls) -> list[str]
    docstring: "Get the registered EmbeddingModel implementations.\n\nArgs:\n    cls:\
      \ The class that maintains the _embedding_registry mapping of EmbeddingModel\
      \ implementations.\n\nReturns:\n    list[str]: A list of the registered EmbeddingModel\
      \ implementation names."
  - node_id: graphrag/language_model/factory.py::ModelFactory.create_chat_model
    name: create_chat_model
    signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
    docstring: "Create a ChatModel instance from a registered implementation.\n\n\
      Args:\n    model_type: The type of ChatModel to create.\n    **kwargs: Additional\
      \ keyword arguments for the ChatModel constructor.\n\nReturns:\n    A ChatModel\
      \ instance.\n\nRaises:\n    ValueError: If the provided model_type is not registered."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_model
    name: is_supported_model
    signature: 'def is_supported_model(cls, model_type: str) -> bool'
    docstring: "Determine whether the provided model_type is supported by any registered\
      \ model backends (chat or embedding).\n\nArgs:\n    model_type: The type of\
      \ model to check.\n\nReturns:\n    bool: True if the model_type is registered\
      \ as either a chat model or an embedding model; otherwise False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_chat_model
    name: is_supported_chat_model
    signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
    docstring: "Check if the given chat model type is supported by registered chat\
      \ model implementations.\n\nArgs:\n    cls: type The class reference (classmethod\
      \ parameter).\n    model_type: str The type identifier for the chat model to\
      \ check.\n\nReturns:\n    bool: True if model_type is registered as a chat model,\
      \ otherwise False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.create_embedding_model
    name: create_embedding_model
    signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any) ->
      EmbeddingModel'
    docstring: "Create an EmbeddingModel instance.\n\nArgs:\n    model_type: str The\
      \ type of EmbeddingModel to create.\n    **kwargs: Any Additional keyword arguments\
      \ for the EmbeddingModel constructor.\n\nReturns:\n    EmbeddingModel: The EmbeddingModel\
      \ instance.\n\nRaises:\n    ValueError: If the provided model_type is not registered."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_embedding_model
    name: is_supported_embedding_model
    signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
    docstring: "Check if the given embedding model type is supported.\n\nArgs:\n \
      \   cls: type The class reference (classmethod parameter).\n    model_type:\
      \ str The type identifier for the embedding model to check.\n\nReturns:\n  \
      \  bool: True if model_type is registered in the embedding registry, otherwise\
      \ False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.get_chat_models
    name: get_chat_models
    signature: def get_chat_models(cls) -> list[str]
    docstring: "Get the registered ChatModel implementations.\n\nArgs:\n    cls: The\
      \ class that maintains the _chat_registry mapping of ChatModel implementations.\n\
      \nReturns:\n    list[str]: A list of the registered ChatModel implementation\
      \ names."
  - node_id: graphrag/language_model/factory.py::ModelFactory.register_chat
    name: register_chat
    signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
      -> None'
    docstring: "Register a ChatModel implementation in the registry.\n\nRegisters\
      \ a ChatModel implementation in the internal _chat_registry mapping for the\
      \ specified model_type.\n\nArgs:\n    cls: The ModelFactory class.\n    model_type:\
      \ str\n        The unique identifier for the ChatModel implementation to register.\n\
      \    creator: Callable[..., ChatModel]\n        A callable that returns a ChatModel\
      \ instance when invoked.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/language_model/factory.py::ModelFactory
    name: ModelFactory
    docstring: 'ModelFactory is a registry-based factory that creates chat and embedding
      language model backends.


      Purpose:

      - Maintain registries for embedding and chat model implementations.

      - Provide a uniform API to register model backends and instantiate models by
      type.

      - Offer utilities to query supported model types.


      Attributes:

      - _embedding_registry: ClassVar mapping[str, Callable[..., EmbeddingModel]]
      of model_type to a constructor for EmbeddingModel.

      - _chat_registry: ClassVar mapping[str, Callable[..., ChatModel]] of model_type
      to a constructor for ChatModel.


      Summary:

      - Coordinates model backends from different providers (e.g., FNLLM, Litellm)
      by model type.


      Returns:

      - None


      Raises:

      - ValueError: If attempting to create a model for an unregistered model_type
      via create_chat_model or create_embedding_model.'
    methods:
    - name: register_embedding
      signature: "def register_embedding(\n        cls, model_type: str, creator:\
        \ Callable[..., EmbeddingModel]\n    ) -> None"
    - name: get_embedding_models
      signature: def get_embedding_models(cls) -> list[str]
    - name: create_chat_model
      signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
    - name: is_supported_model
      signature: 'def is_supported_model(cls, model_type: str) -> bool'
    - name: is_supported_chat_model
      signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
    - name: create_embedding_model
      signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any)
        -> EmbeddingModel'
    - name: is_supported_embedding_model
      signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
    - name: get_chat_models
      signature: def get_chat_models(cls) -> list[str]
    - name: register_chat
      signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
        -> None'
- file: graphrag/language_model/manager.py
  functions:
  - node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_chat_model
    name: get_or_create_chat_model
    signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
      \ str, **chat_kwargs: Any\n    ) -> ChatModel"
    docstring: "Get or create the ChatLLM instance registered under the given name.\n\
      \nIf the ChatLLM does not exist, it is created and registered.\n\nArgs:\n  \
      \  name: Unique identifier for the ChatLLM instance.\n    model_type: Key for\
      \ the ChatModel implementation in LLMFactory.\n    chat_kwargs: Additional keyword\
      \ arguments for instantiation.\n\nReturns:\n    ChatModel: The ChatLLM instance\
      \ associated with the given name.\n\nRaises:\n    Exception: Any error raised\
      \ during creation via register_chat or the underlying factory."
  - node_id: graphrag/language_model/manager.py::ModelManager.list_chat_models
    name: list_chat_models
    signature: def list_chat_models(self) -> dict[str, ChatModel]
    docstring: "Return a copy of all registered ChatModel instances.\n\nReturns:\n\
      \    dict[str, ChatModel]: A dictionary mapping model names to ChatModel instances."
  - node_id: graphrag/language_model/manager.py::ModelManager.remove_chat
    name: remove_chat
    signature: 'def remove_chat(self, name: str) -> None'
    docstring: "Remove the ChatLLM instance registered under the given name.\n\nArgs:\n\
      \    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n    None"
  - node_id: graphrag/language_model/manager.py::ModelManager.list_embedding_models
    name: list_embedding_models
    signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
    docstring: "Return a shallow copy of all registered EmbeddingModel instances.\n\
      \nThis returns a shallow copy of the internal mapping of embedding models keyed\
      \ by\ntheir registration name. Modifications to the returned dictionary do not\
      \ affect\nthe internal registry.\n\nReturns:\n    dict[str, EmbeddingModel]:\
      \ A shallow copy mapping model names to EmbeddingModel\n    instances."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_chat_model
    name: get_chat_model
    signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
    docstring: "Retrieve the ChatLLM instance registered under the given name.\n\n\
      Args:\n    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n \
      \   ChatModel: The ChatLLM instance registered under the given name.\n\nRaises:\n\
      \    ValueError: If no ChatLLM is registered under the name."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_embedding_model
    name: get_or_create_embedding_model
    signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
      \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    docstring: "Retrieve the EmbeddingsLLM instance registered under the given name.\n\
      \nIf the EmbeddingsLLM does not exist, it is created and registered.\n\nArgs:\n\
      \    name: Unique identifier for the EmbeddingsLLM instance.\n    model_type:\
      \ Key for the EmbeddingsLLM implementation in LLMFactory.\n    **embedding_kwargs:\
      \ Additional parameters for instantiation.\n\nReturns:\n    EmbeddingModel:\
      \ The EmbeddingModel instance associated with the given name."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_instance
    name: get_instance
    signature: def get_instance(cls) -> ModelManager
    docstring: "Return the singleton instance of ModelManager.\n\nThis is a classmethod\
      \ that returns the existing ModelManager singleton by delegating to the class's\
      \ __new__ method. No additional parameters are required beyond cls.\n\nArgs:\n\
      \    cls: The ModelManager class used to access the singleton instance.\n\n\
      Returns:\n    ModelManager: The singleton ModelManager instance."
  - node_id: graphrag/language_model/manager.py::ModelManager.register_embedding
    name: register_embedding
    signature: "def register_embedding(\n        self, name: str, model_type: str,\
      \ **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    docstring: "Register an EmbeddingsLLM instance under a unique name.\n\nRegisters\
      \ a new EmbeddingModel in self.embedding_models using the specified model_type\
      \ and the provided keyword arguments.\n\nArgs:\n    name (str): Unique identifier\
      \ for the EmbeddingsLLM instance.\n    model_type (str): Key for the EmbeddingsLLM\
      \ implementation in the factory (ModelFactory).\n    **embedding_kwargs: Additional\
      \ keyword arguments for instantiation, passed to the model factory.\n\nReturns:\n\
      \    EmbeddingModel: The EmbeddingModel instance registered under the given\
      \ name.\n\nRaises:\n    ValueError: If the provided model_type is invalid or\
      \ if the underlying factory encounters an error constructing the model.\n\n\
      Notes:\n    The function assigns the given name into embedding_kwargs before\
      \ creation, and stores the resulting model in self.embedding_models under the\
      \ provided name."
  - node_id: graphrag/language_model/manager.py::ModelManager.__new__
    name: __new__
    signature: def __new__(cls) -> Self
    docstring: "Create a new singleton instance of ModelManager if it does not exist.\n\
      \nArgs:\n    cls: Type[ModelManager] The ModelManager class used to access the\
      \ singleton instance.\n\nReturns:\n    Self: The singleton ModelManager instance.\n\
      \nRaises:\n    None: This method does not raise any exceptions."
  - node_id: graphrag/language_model/manager.py::ModelManager.register_chat
    name: register_chat
    signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
      \ Any\n    ) -> ChatModel"
    docstring: "Register a ChatModel instance under a unique name.\n\nThis method\
      \ injects the provided name into the chat_kwargs before instantiation,\ncreates\
      \ the ChatModel using ModelFactory.create_chat_model, and registers it in\n\
      self.chat_models under the given name.\n\nArgs:\n    name (str): Unique identifier\
      \ for the ChatLLM/ChatModel instance.\n    model_type (str): Key for the ChatLLM\
      \ implementation in LLMFactory.\n    chat_kwargs (dict[str, Any]): Additional\
      \ keyword arguments for instantiation.\n        The dictionary will have the\
      \ key 'name' added prior to the factory call.\n\nReturns:\n    ChatModel: The\
      \ ChatModel instance registered under the given name.\n\nRaises:\n    Exception\
      \ types raised by the underlying factory call (ModelFactory.create_chat_model)\n\
      \    or input/validation errors may propagate to the caller."
  - node_id: graphrag/language_model/manager.py::ModelManager.__init__
    name: __init__
    signature: def __init__(self) -> None
    docstring: "\"\"\"Initialize the singleton LLM manager's internal state on first\
      \ instantiation.\n\nArgs:\n    self: The instance being initialized. Sets up\
      \ internal dictionaries for chat_models and embedding_models and marks the instance\
      \ as initialized to avoid reinitialization.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    None: This method does not raise\
      \ any exceptions.\n\"\"\""
  - node_id: graphrag/language_model/manager.py::ModelManager.remove_embedding
    name: remove_embedding
    signature: 'def remove_embedding(self, name: str) -> None'
    docstring: "Remove the EmbeddingsLLM instance registered under the given name.\n\
      \nArgs:\n    name: str \u2014 Unique identifier for the EmbeddingsLLM instance.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/manager.py::ModelManager.get_embedding_model
    name: get_embedding_model
    signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
    docstring: "\"\"\"\nRetrieve the EmbeddingsLLM instance registered under the given\
      \ name.\n\nArgs:\n    name (str): Unique identifier for the EmbeddingsLLM instance.\n\
      \nReturns:\n    EmbeddingModel: The EmbeddingModel instance registered under\
      \ the name.\n\nRaises:\n    ValueError: If no EmbeddingsLLM is registered under\
      \ the name.\n\"\"\""
  classes:
  - class_id: graphrag/language_model/manager.py::ModelManager
    name: ModelManager
    docstring: "Singleton manager for chat and embedding language models.\n\nOverview:\n\
      ModelManager is a singleton responsible for creating, registering, retrieving,\
      \ and listing ChatModel and EmbeddingModel instances. It delegates on-demand\
      \ instantiation to ModelFactory and stores instances in internal registries\
      \ for reuse. Access to the singleton is provided via __new__ or get_instance.\n\
      \nAttributes:\n    chat_models (dict[str, ChatModel]): Registry of registered\
      \ chat models keyed by name.\n    embedding_models (dict[str, EmbeddingModel]):\
      \ Registry of registered embedding models keyed by name.\n    _initialized (bool):\
      \ Initialization flag to prevent reinitialization.\n\nRaises:\n    ValueError:\
      \ If attempting to retrieve a non-registered chat or embedding model."
    methods:
    - name: get_or_create_chat_model
      signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
        \ str, **chat_kwargs: Any\n    ) -> ChatModel"
    - name: list_chat_models
      signature: def list_chat_models(self) -> dict[str, ChatModel]
    - name: remove_chat
      signature: 'def remove_chat(self, name: str) -> None'
    - name: list_embedding_models
      signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
    - name: get_chat_model
      signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
    - name: get_or_create_embedding_model
      signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
        \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    - name: get_instance
      signature: def get_instance(cls) -> ModelManager
    - name: register_embedding
      signature: "def register_embedding(\n        self, name: str, model_type: str,\
        \ **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    - name: __new__
      signature: def __new__(cls) -> Self
    - name: register_chat
      signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
        \ Any\n    ) -> ChatModel"
    - name: __init__
      signature: def __init__(self) -> None
    - name: remove_embedding
      signature: 'def remove_embedding(self, name: str) -> None'
    - name: get_embedding_model
      signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
- file: graphrag/language_model/protocol/base.py
  functions:
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    docstring: "Generate a response for the given text.\n\nArgs:\n    prompt: The\
      \ text to generate a response for.\n    history: The conversation history.\n\
      \    **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    ModelResponse: The response for the given text."
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Asynchronously generate embedding vectors for the given list of strings.\n\
      \nArgs:\n    text_list: The list of strings to generate embeddings for.\n  \
      \  **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns\n\
      -------\n    list[list[float]]: A list of embedding vectors for each input item\
      \ in the batch.\n\nRaises:\n    Exception: If an error occurs during embedding\
      \ generation."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.chat
    name: chat
    signature: "def chat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    docstring: "Generate a response for the given text.\n\nArgs:\n    prompt (str):\
      \ The text to generate a response for.\n    history (list | None): The conversation\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    ModelResponse: The response for the given text.\n\nRaises:\n\
      \    Exception: If an error occurs during generation."
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text:\
      \ The text to generate an embedding for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding vector.\n\
      \nRaises:\n    Exception: If an error occurs during embedding generation."
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "\"\"\"\nGenerate embedding vectors for the given list of strings.\n\
      \nArgs:\n    text_list: The list of strings to generate embeddings for.\n  \
      \  **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    list[list[float]]: A list of embedding vectors for each input item in the\
      \ batch.\n\nRaises:\n    Exception: If an error occurs during embedding generation.\n\
      \"\"\""
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text\
      \ (str): The text to generate an embedding for.\n    **kwargs: Additional keyword\
      \ arguments (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding\
      \ vector."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    docstring: "Generate a response for the given text using a streaming interface.\n\
      \nArgs:\n    prompt: str \u2014 The text to generate a response for.\n    history:\
      \ list | None \u2014 The conversation history.\n    **kwargs: Any \u2014 Additional\
      \ keyword arguments (e.g., model parameters).\n\nReturns:\n    Generator[str,\
      \ None] \u2014 The generator that yields strings representing the response."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream the given prompt via an asynchronous streaming interface. This\
      \ generator yields partial results over time as the model streams its response.\n\
      \nArgs:\n  prompt: The text to generate a response for.\n  history: The conversation\
      \ history. Optional prior messages that may influence generation.\n  **kwargs:\
      \ Additional keyword arguments (e.g., model parameters, streaming controls).\n\
      \nReturns:\n  AsyncGenerator[str, None]: An asynchronous generator that yields\
      \ strings representing portions of the response as they become available.\n\n\
      Raises:\n  Propagates exceptions raised by the underlying model call or streaming\
      \ backend (e.g., network errors, timeouts, invalid parameters).\n\nUsage:\n\
      \  async for chunk in model.achat_stream(\"Hello, world!\", history=None):\n\
      \      print(chunk)"
  classes:
  - class_id: graphrag/language_model/protocol/base.py::ChatModel
    name: ChatModel
    docstring: "ChatModel protocol for chat-based language model interfaces.\n\nPurpose\n\
      \    Abstract protocol that defines how chat-based language models should generate\
      \ responses\n    from prompts, with optional history, and support both synchronous\
      \ and streaming interfaces\n    via four methods: achat, chat, chat_stream,\
      \ and achat_stream.\n\nAttributes\n    config: LanguageModelConfig\n       \
      \ The configuration for the language model, including model name and generation\
      \ options\n        that implementations may use to control behavior.\n\nMethods\n\
      \    achat(self, prompt: str, history: list | None = None, **kwargs: Any) ->\
      \ ModelResponse\n        Generate a synchronous, non-streaming response for\
      \ the given prompt.\n        Args:\n            prompt: The text to generate\
      \ a response for.\n            history: Optional list of prior messages in the\
      \ conversation.\n            **kwargs: Additional keyword arguments (e.g., model\
      \ parameters, generation controls).\n        Returns:\n            ModelResponse:\
      \ The generated response wrapped in a ModelResponse.\n        Raises:\n    \
      \        Exception: If an error occurs during generation.\n\n    chat(self,\
      \ prompt: str, history: list | None = None, **kwargs: Any) -> ModelResponse\n\
      \        Generate a synchronous, non-streaming response using the standard chat\
      \ interface.\n        Args:\n            prompt: The text to generate a response\
      \ for.\n            history: Optional list of prior messages in the conversation.\n\
      \            **kwargs: Additional keyword arguments (e.g., model parameters,\
      \ generation controls).\n        Returns:\n            ModelResponse: The generated\
      \ response.\n        Raises:\n            Exception: If an error occurs during\
      \ generation.\n\n    chat_stream(self, prompt: str, history: list | None = None,\
      \ **kwargs: Any) -> Generator[str, None]\n        Streaming interface that yields\
      \ partial strings as the model generates a response.\n        Args:\n      \
      \      prompt: The text to generate a response for.\n            history: Optional\
      \ list of prior messages in the conversation.\n            **kwargs: Additional\
      \ keyword arguments (e.g., model parameters, streaming controls).\n        Returns:\n\
      \            Generator[str, None]: A generator that yields strings composing\
      \ the final response.\n        Raises:\n            Exception: If an error occurs\
      \ during generation.\n\n    achat_stream(self, prompt: str, history: list |\
      \ None = None, **kwargs: Any) -> AsyncGenerator[str, None]\n        Asynchronous\
      \ streaming interface that yields partial strings over time as the model progresses.\n\
      \        Args:\n            prompt: The text to generate a response for.\n \
      \           history: Optional list of prior messages in the conversation.\n\
      \            **kwargs: Additional keyword arguments (e.g., model parameters,\
      \ streaming controls).\n        Returns:\n            AsyncGenerator[str, None]:\
      \ An async generator yielding strings for the final response.\n        Raises:\n\
      \            Exception: If an error occurs during generation."
    methods:
    - name: achat
      signature: "def achat(\n        self, prompt: str, history: list | None = None,\
        \ **kwargs: Any\n    ) -> ModelResponse"
    - name: chat
      signature: "def chat(\n        self, prompt: str, history: list | None = None,\
        \ **kwargs: Any\n    ) -> ModelResponse"
    - name: chat_stream
      signature: "def chat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    - name: achat_stream
      signature: "def achat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
  - class_id: graphrag/language_model/protocol/base.py::EmbeddingModel
    name: EmbeddingModel
    docstring: "EmbeddingModel protocol for generating text embeddings.\n\nPurpose:\n\
      \    Defines the interface for producing embedding vectors from text, including\
      \ asynchronous and synchronous single-item and batch operations.\n\nArgs:\n\
      \    text_list (list[str]): The list of strings to generate embeddings for.\
      \ Used by aembed_batch and embed_batch.\n    text (str): The text to generate\
      \ an embedding for. Used by aembed and embed.\n    kwargs (Any): Additional\
      \ keyword arguments (e.g., model parameters) supplied to embedding methods.\n\
      \nReturns:\n    For aembed_batch and embed_batch: list[list[float]] (a batch\
      \ of embedding vectors).\n    For aembed and embed: list[float] (a single embedding\
      \ vector).\n\nRaises:\n    Exception: If an error occurs during embedding generation."
    methods:
    - name: aembed_batch
      signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs:\
        \ Any\n    ) -> list[list[float]]"
    - name: embed
      signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
- file: graphrag/language_model/providers/fnllm/cache.py
  functions:
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear the cache.\n\nClears all entries from the underlying cache managed\
      \ by this provider.\n\nReturns:\n    None\nRaises:\n    Exception: if the underlying\
      \ cache clear operation fails."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.get
    name: get
    signature: 'def get(self, key: str) -> Any | None'
    docstring: "Asynchronous retrieval of a value from the underlying cache.\n\nThis\
      \ method is part of the FNLLMCacheProvider and delegates to the underlying cache\
      \ (self._cache).\n\nArgs:\n    key: The key to retrieve from the cache.\n\n\
      Returns:\n    Any | None: The value associated with the key, or None if not\
      \ present.\n\nNotes:\n    Exceptions may propagate from the underlying cache."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously check if the cache has a value for the given key.\n\
      \nArgs:\n    key: The cache key to check.\n\nReturns:\n    bool: True if a value\
      \ exists for the key in the cache, otherwise False."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.remove
    name: remove
    signature: 'def remove(self, key: str) -> None'
    docstring: "Remove a value from the cache.\n\nArgs:\n    key (str): The key to\
      \ remove from the cache.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    Exceptions may propagate from the underlying cache."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.__init__
    name: __init__
    signature: 'def __init__(self, cache: PipelineCache)'
    docstring: "Initialize FNLLMCacheProvider with a PipelineCache.\n\nArgs:\n  cache:\
      \ The underlying PipelineCache instance used by this provider.\n\nReturns:\n\
      \  None"
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.set
    name: set
    signature: "def set(\n        self, key: str, value: Any, metadata: dict[str,\
      \ Any] | None = None\n    ) -> None"
    docstring: "Write a value into the cache.\n\nArgs:\n    key: str \u2014 The key\
      \ under which to store the value.\n    value: Any \u2014 The value to store\
      \ in the cache.\n    metadata: dict[str, Any] | None \u2014 Optional metadata\
      \ associated with the value.\n\nReturns:\n    None \u2014 The method does not\
      \ return a value.\n\nRaises:\n    Exceptions raised by the underlying cache\
      \ operation are propagated to the caller."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.child
    name: child
    signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
    docstring: "Create a child cache.\n\nArgs:\n    key: The key used to create the\
      \ child cache.\n\nReturns:\n    FNLLMCacheProvider: A new FNLLMCacheProvider\
      \ wrapping the child cache created for the given key."
  classes:
  - class_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider
    name: FNLLMCacheProvider
    docstring: "FNLLMCacheProvider adapts a PipelineCache to the FNLLM cache interface\
      \ by delegating all cache operations to the underlying cache instance.\n\nAttributes:\n\
      - _cache: The underlying cache instance used by this provider to perform cache\
      \ operations.\n\nArgs:\n  cache: The underlying PipelineCache instance used\
      \ by this provider.\n\nReturns:\n  None\n\nRaises:\n  Exceptions may propagate\
      \ from the underlying cache operations."
    methods:
    - name: clear
      signature: def clear(self) -> None
    - name: get
      signature: 'def get(self, key: str) -> Any | None'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: remove
      signature: 'def remove(self, key: str) -> None'
    - name: __init__
      signature: 'def __init__(self, cache: PipelineCache)'
    - name: set
      signature: "def set(\n        self, key: str, value: Any, metadata: dict[str,\
        \ Any] | None = None\n    ) -> None"
    - name: child
      signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
- file: graphrag/language_model/providers/fnllm/events.py
  functions:
  - node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.__init__
    name: __init__
    signature: 'def __init__(self, on_error: ErrorHandlerFn)'
    docstring: "\"\"\"Initialize FNLLMEvents with an error handler to be called on\
      \ errors.\n\nArgs:\n    on_error: ErrorHandlerFn to be invoked on errors.\n\n\
      Returns:\n    None\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.on_error
    name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    docstring: "Handle an fnllm error.\n\nArgs:\n    error (BaseException | None):\
      \ The error to handle, or None if no error is available.\n    traceback (str\
      \ | None): The traceback string, or None if not provided.\n    arguments (dict[str,\
      \ Any] | None): Additional arguments related to the error, or None.\n\nReturns:\n\
      \    None\n\nRaises:\n    Exception: If the configured error handler raises\
      \ an exception."
  classes:
  - class_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
    name: FNLLMEvents
    docstring: "FNLLMEvents handles FNLLM-specific events and delegates error processing\
      \ to a provided error handler.\n\nArgs:\n    on_error: ErrorHandlerFn to be\
      \ invoked on errors.\n\nReturns:\n    None\n\nRaises:\n    Exception: If the\
      \ configured error handler raises an exception."
    methods:
    - name: __init__
      signature: 'def __init__(self, on_error: ErrorHandlerFn)'
    - name: on_error
      signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
        \        traceback: str | None = None,\n        arguments: dict[str, Any]\
        \ | None = None,\n    ) -> None"
- file: graphrag/language_model/providers/fnllm/models.py
  functions:
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed_batch
    name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "Embed the given texts using the Model.\n\nArgs:\n    text_list: The\
      \ texts to embed.\n    kwargs: Additional arguments to pass to the LLM.\n\n\
      Returns:\n    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n\
      \    ValueError: If no embeddings are found in the response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    docstring: "\"\"\"\nStream Chat with the Model asynchronously using the given\
      \ prompt.\n\nThis is an asynchronous generator that streams chunks of the model's\
      \ response as they become available. Each yielded value is a non-None string\
      \ from response.output.content.\n\nArgs:\n    prompt (str): The prompt to chat\
      \ with.\n    history (list[str] | None): Optional history to pass to the Model.\
      \ If provided, the model will consider this history when generating streamed\
      \ output.\n    kwargs: Additional keyword arguments to pass to the Model. (type:\
      \ dict[str, Any])\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous\
      \ generator yielding the streamed response chunks as non-None strings.\n\"\"\
      \""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "Embed the given text using the Model.\n\nArgs:\n    text: The text\
      \ to embed.\n    kwargs: Additional arguments to pass to the Model.\n\nReturns:\n\
      \    The embeddings of the text.\n\nRaises:\n    ValueError: If no embeddings\
      \ are found in the response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The chat history to include in the\
      \ chat, or None for no history.\n    kwargs: Additional arguments to pass to\
      \ the Model.\n\nReturns:\n    ModelResponse: The response from the Model.\n\n\
      Raises:\n    Exception: Exceptions raised by the underlying model call are propagated."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    docstring: "Chat with the Model using the given prompt.\n\nThis method supports\
      \ an optional conversation history. If history is None, the\nmodel is called\
      \ with the prompt and any provided kwargs. If history is provided, it\nis sent\
      \ to the model along with the prompt.\n\nArgs:\n    prompt (str): The prompt\
      \ to chat with.\n    history (list | None): The conversation history to include\
      \ in the chat, or None for no history.\n    kwargs (dict[str, Any]): Additional\
      \ keyword arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The\
      \ response from the Model.\n\nRaises:\n    Exception: Exceptions raised by the\
      \ underlying model call may propagate."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
      \ Additional arguments to pass to the Model.\n\nReturns:\n    An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying model call or streaming\
      \ response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed_batch
    name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list:\
      \ The texts to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
      \nReturns:\n    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n\
      \    ValueError: If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    docstring: "\"\"\"\nStream Chat with the Model using the given prompt.\n\nArgs:\n\
      \    prompt (str): The prompt to chat with.\n    history (list[str] | None):\
      \ Optional history to pass to the Model. If provided, the model will consider\
      \ it.\n    kwargs (dict[str, Any], optional): Additional keyword arguments to\
      \ pass to the Model.\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying model call or streaming\
      \ response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"Embed the given text using the Model.\n\nArgs:\n    text: The\
      \ text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\n\
      Returns:\n    The embeddings of the text.\n\nRaises:\n    ValueError: If no\
      \ embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
      \ Additional arguments to pass to the Model.\n\nReturns:\n    Generator[str,\
      \ None]: A generator that yields strings representing the response.\n\nRaises:\n\
      \    NotImplementedError: chat_stream is not supported for synchronous execution."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    docstring: "Chat with the Model using the given prompt.\n\nParameters:\n    prompt\
      \ (str): The prompt to chat with.\n    history (list | None): The conversation\
      \ history to include in the chat, or None for no history.\n    kwargs: Additional\
      \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
      \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
      \ model call are propagated to the caller."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
      \ (list[str]): The texts to embed.\n    kwargs (dict[str, Any]): Additional\
      \ arguments to pass to the LLM.\n\nReturns:\n    list[list[float]]: The embeddings\
      \ for the input texts.\n\nRaises:\n    ValueError: If no embeddings are found\
      \ in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
      \ The text to embed.\n    kwargs (dict[str, Any]): Additional arguments to pass\
      \ to the Model.\n\nReturns:\n    list[float]: The embeddings of the text.\n\n\
      Raises:\n    ValueError: If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt (str):\
      \ The prompt to chat with.\n    history (list | None): The conversation history\
      \ to include in the chat, or None for no history.\n    kwargs: Additional keyword\
      \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
      \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
      \ model call are propagated."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
      \ (list[str]): The texts to embed.\n    kwargs: Additional arguments to pass\
      \ to the Model.\n\nReturns:\n    list[list[float]]: The embeddings for the input\
      \ texts.\n\nRaises:\n    ValueError: If no embeddings are found in the response.\n\
      \"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
      \ The text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
      \nReturns:\n    list[float]: The embeddings of the text.\n\nRaises:\n    ValueError:\
      \ If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an OpenAI Chat FNLLM provider using the given configuration\
      \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
      \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
      \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
      \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided,\
      \ an error handler will be created and used.\n    cache: PipelineCache | None\n\
      \        Optional PipelineCache to back the model cache provider.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an OpenAI Embedding FNLLM provider using the given configuration\
      \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
      \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
      \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
      \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided,\
      \ an error handler will be created to log errors.\n    cache: PipelineCache\
      \ | None\n        The pipeline cache to wrap. If None, no cache is used.\n\n\
      Returns:\n    None\n        This initializer does not return a value.\n\nRaises:\n\
      \    Exception\n        Propagates exceptions raised by the underlying helper\
      \ functions and OpenAI client initialization."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an Azure OpenAI Chat LLM provider instance.\n\nArgs:\n\
      \    name: str\n        The name to assign to the internal cache provider and\
      \ model instance.\n    config: LanguageModelConfig\n        The configuration\
      \ used to derive the OpenAI configuration.\n    callbacks: WorkflowCallbacks\
      \ | None\n        Optional WorkflowCallbacks; if provided, an error handler\
      \ will be created to log issues.\n    cache: PipelineCache | None\n        Optional\
      \ cache to wrap for the underlying FNLLM cache provider; if None, no caching\
      \ is used.\n\nReturns:\n    None\n\nRaises:\n    Exception: If initialization\
      \ of the OpenAI config, client, or FNLLM components fails due to underlying\
      \ library errors."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an Azure OpenAI Embedding FNLLM provider using the given\
      \ configuration and optional components.\n\nArgs:\n    name (str): The name\
      \ to assign to the internal cache provider and model instance.\n    config (LanguageModelConfig):\
      \ The configuration used to derive the OpenAI configuration.\n    callbacks\
      \ (WorkflowCallbacks | None): Optional WorkflowCallbacks; if provided, an error\
      \ handler will be created to log issues.\n    cache (PipelineCache | None):\
      \ Optional PipelineCache to back the embedding cache.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM
    name: OpenAIEmbeddingFNLLM
    docstring: "OpenAI Embedding FNLLM provider that generates text embeddings using\
      \ the FNLLM OpenAI embeddings LLM.\n\nArgs:\n  name: The name to assign to the\
      \ internal cache provider and model instance.\n  config: LanguageModelConfig\
      \ used to derive the OpenAI configuration.\n  callbacks: Optional WorkflowCallbacks;\
      \ if provided, used for workflow event handling.\n  cache: Optional PipelineCache;\
      \ if provided, used to cache results.\n\nReturns:\n  OpenAIEmbeddingFNLLM: An\
      \ initialized instance of the provider.\n\nRaises:\n  Exception: If initialization\
      \ fails due to configuration issues or internal errors."
    methods:
    - name: aembed_batch
      signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    - name: embed
      signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        name: str,\n \
        \       config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks\
        \ | None = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  - class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM
    name: OpenAIChatFNLLM
    docstring: "OpenAIChatFNLLM provider that integrates FNLLM's OpenAI chat LLM with\
      \ Graphrag's language-model framework to support chat-based interactions within\
      \ Graphrag's LLM workflows. It wires FNLLM's OpenAI chat client to Graphrag's\
      \ API surface, deriving its configuration from a LanguageModelConfig and optionally\
      \ enabling event callbacks and caching.\n\nArgs:\n  name (str): The name assigned\
      \ to this provider instance, used for internal identification and cache naming.\n\
      \  config (LanguageModelConfig): The configuration used to derive the OpenAI\
      \ client and related settings (e.g., model, prompts, etc.).\n  callbacks (WorkflowCallbacks\
      \ | None): Optional WorkflowCallbacks for propagating lifecycle events and errors\
      \ through Graphrag.\n  cache (PipelineCache | None): Optional cache to persist\
      \ responses; when provided, the provider reads from and writes to this cache\
      \ as part of operation.\n\nReturns:\n  None\n\nRaises:\n  Exception: Exceptions\
      \ raised by the underlying FNLLM/OpenAI integrations are propagated to the caller.\n\
      \nAttributes:\n  name: Identifier for the provider instance.\n  config: The\
      \ LanguageModelConfig used to configure the OpenAI components.\n  callbacks:\
      \ Optional WorkflowCallbacks for event propagation.\n  cache: Optional PipelineCache\
      \ used for caching model outputs.\n\nNotes:\n  If a cache is provided, responses\
      \ may be cached and reused for repeated prompts; the cache lifecycle and eviction\
      \ are governed by the PipelineCache implementation."
    methods:
    - name: chat_stream
      signature: "def chat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs\n    ) -> Generator[str, None]"
    - name: achat
      signature: "def achat(\n        self, prompt: str, history: list | None = None,\
        \ **kwargs\n    ) -> ModelResponse"
    - name: achat_stream
      signature: "def achat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    - name: chat
      signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
        -> ModelResponse'
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        name: str,\n \
        \       config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks\
        \ | None = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  - class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM
    name: AzureOpenAIEmbeddingFNLLM
    docstring: "Azure OpenAI Embedding FNLLM provider for generating text embeddings.\n\
      \nThis class provides embedding capabilities using an Azure OpenAI embedding\
      \ model via FNLLM and exposes methods to embed single texts and batches, with\
      \ both asynchronous and synchronous variants. It is initialized with a name,\
      \ a LanguageModelConfig used to derive the OpenAI configuration, and optional\
      \ components such as callbacks and a cache.\n\nArgs:\n  name: The name to assign\
      \ to the internal cache provider and model instance.\n  config: The LanguageModelConfig\
      \ used to derive the OpenAI configuration.\n  callbacks: Optional WorkflowCallbacks;\
      \ if provided, an error handler will be used.\n  cache: Optional PipelineCache.\n\
      \nReturns:\n  None\n\nRaises:\n  ValueError: If no embeddings are found in the\
      \ response."
    methods:
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    - name: aembed_batch
      signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    - name: embed
      signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        name: str,\n \
        \       config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks\
        \ | None = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  - class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM
    name: AzureOpenAIChatFNLLM
    docstring: "Azure OpenAI Chat LLM provider using FNLLM.\n\nResponsible for interfacing\
      \ with Azure OpenAI's chat endpoints through FNLLM wrappers, exposing both synchronous\
      \ and streaming chat interfaces, supporting optional conversation history, and\
      \ integrating caching and error handling via optional callbacks.\n\nArgs:\n\
      \    name (str): The name to assign to the internal cache provider and model\
      \ instance.\n    config (LanguageModelConfig): The configuration used to derive\
      \ the OpenAI configuration.\n    callbacks (WorkflowCallbacks | None): Optional\
      \ WorkflowCallbacks; if provided, an error handler will be created to log issues.\n\
      \    cache (PipelineCache | None): Optional PipelineCache used for caching responses\
      \ and model state.\n\nReturns:\n    None\n\nRaises:\n    Exception: Exceptions\
      \ raised by the underlying model initialization or by auxiliary utilities may\
      \ propagate."
    methods:
    - name: achat
      signature: "def achat(\n        self, prompt: str, history: list | None = None,\
        \ **kwargs\n    ) -> ModelResponse"
    - name: achat_stream
      signature: "def achat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    - name: chat_stream
      signature: "def chat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs\n    ) -> Generator[str, None]"
    - name: chat
      signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
        -> ModelResponse'
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        name: str,\n \
        \       config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks\
        \ | None = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
- file: graphrag/language_model/providers/fnllm/utils.py
  functions:
  - node_id: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
    name: _create_error_handler
    signature: 'def _create_error_handler(callbacks: WorkflowCallbacks) -> ErrorHandlerFn'
    docstring: "Create an error handler for LLM invocation errors.\n\nInternal helper\
      \ that returns an ErrorHandlerFn which logs errors using the\nconfigured logger.\
      \ The returned handler does not invoke any callbacks.\n\nArgs:\n    callbacks:\
      \ WorkflowCallbacks\n        A container of optional callbacks. This parameter\
      \ is currently unused by\n        the error handler.\n\nReturns:\n    ErrorHandlerFn\n\
      \        A function that accepts error: BaseException | None, stack: str | None,\
      \ and\n        details: dict | None, and logs an error with the message \"Error\
      \ Invoking LLM\",\n        including the exception information and any extra\
      \ context.\n\nRaises:\n    None"
  - node_id: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
    name: run_coroutine_sync
    signature: 'def run_coroutine_sync(coroutine: Coroutine[Any, Any, T]) -> T'
    docstring: "Run a coroutine synchronously.\n\nArgs:\n    coroutine: Coroutine[Any,\
      \ Any, T] The coroutine to run.\n\nReturns:\n    T: The result of the coroutine.\n\
      \nRaises:\n    Exception: If the coroutine raises an exception, it will be propagated\
      \ to the caller."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::is_reasoning_model
    name: is_reasoning_model
    signature: 'def is_reasoning_model(model: str) -> bool'
    docstring: "Check if a model name is a known OpenAI reasoning model.\n\nArgs:\n\
      \    model: The name of the model to check.\n\nReturns:\n    bool: True if the\
      \ model is one of the known OpenAI reasoning models (o1, o1-mini, o3-mini);\
      \ otherwise False."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::_create_cache
    name: _create_cache
    signature: 'def _create_cache(cache: PipelineCache | None, name: str) -> FNLLMCacheProvider
      | None'
    docstring: "Create an FNLLM cache provider from a pipeline cache.\n\nArgs:\n \
      \   cache: PipelineCache | None - The pipeline cache to wrap. If None, returns\
      \ None.\n    name: str - The name to assign to the child cache provider.\n\n\
      Returns:\n    FNLLMCacheProvider | None - The created cache provider, or None\
      \ if cache is None."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::on_error
    name: on_error
    signature: "def on_error(\n        error: BaseException | None = None,\n     \
      \   stack: str | None = None,\n        details: dict | None = None,\n    ) ->\
      \ None"
    docstring: "Log an error that occurred while invoking the LLM.\n\nArgs:\n    error\
      \ (BaseException | None): The exception to log; passed to exc_info in logger.error.\n\
      \    stack (str | None): Optional stack trace or contextual information; included\
      \ in the log's extra under \"stack\".\n    details (dict | None): Optional additional\
      \ details; included in the log's extra under \"details\".\n\nReturns:\n    None:\
      \ This function does not return a value.\n\nRaises:\n    None: This function\
      \ does not raise exceptions."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
    name: get_openai_model_parameters_from_dict
    signature: 'def get_openai_model_parameters_from_dict(config: dict[str, Any])
      -> dict[str, Any]'
    docstring: "Get the OpenAI API parameters from a configuration dictionary, adjusting\
      \ for reasoning model differences.\n\nArgs:\n    config: dict[str, Any] - Configuration\
      \ dictionary containing model and related parameter fields used to derive OpenAI\
      \ API parameters.\n\nReturns:\n    dict[str, Any] - Dictionary of OpenAI API\
      \ parameters derived from the input config. Includes 'n' and either:\n     \
      \   - For reasoning models: 'max_completion_tokens' and 'reasoning_effort'\n\
      \        - For non-reasoning models: 'max_tokens', 'temperature', 'frequency_penalty',\
      \ 'presence_penalty', 'top_p'\n      If 'response_format' is provided in config,\
      \ it is included as 'response_format' in the result.\n\nRaises:\n    KeyError\
      \ - If required keys (such as 'model') are missing from config."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
    name: get_openai_model_parameters_from_config
    signature: "def get_openai_model_parameters_from_config(\n    config: LanguageModelConfig,\n\
      ) -> dict[str, Any]"
    docstring: "Get the OpenAI API parameters for a given language model config.\n\
      \nArgs:\n    config (LanguageModelConfig): The language model configuration.\
      \ This is converted to a dictionary via model_dump() and then used to derive\
      \ OpenAI API parameters, with adjustments for reasoning models handled by get_openai_model_parameters_from_dict.\n\
      \nReturns:\n    dict[str, Any]: A dictionary of OpenAI API parameters derived\
      \ from the input config, including 'n' and model-specific fields such as 'max_tokens',\
      \ 'temperature', 'max_completion_tokens', etc., depending on whether the model\
      \ is a reasoning model."
  - node_id: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
    name: _create_openai_config
    signature: 'def _create_openai_config(config: LanguageModelConfig, azure: bool)
      -> OpenAIConfig'
    docstring: "Create an OpenAIConfig from a LanguageModelConfig.\n\nArgs:\n    config:\
      \ LanguageModelConfig. The configuration used to derive the OpenAI parameters,\
      \ including encoding_model, model_supports_json, and other fields; a chat_parameters\
      \ object is built from get_openai_model_parameters_from_config(config).\n  \
      \  azure: bool. If True, construct an AzureOpenAIConfig; otherwise construct\
      \ a PublicOpenAIConfig.\n\nReturns:\n    OpenAIConfig. The constructed OpenAI\
      \ configuration (AzureOpenAIConfig when azure is True, PublicOpenAIConfig otherwise).\n\
      \nRaises:\n    ValueError: Azure OpenAI Chat LLM requires an API base when azure\
      \ is True."
  classes: []
- file: graphrag/language_model/providers/litellm/chat_model.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel._get_kwargs
    name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    docstring: "Get model arguments supported by litellm.\n\nArgs:\n  kwargs (dict[str,\
      \ Any]): Arbitrary keyword arguments. Only keys in the following set will be\
      \ included in the returned dictionary: \"name\", \"modalities\", \"prediction\"\
      , \"audio\", \"logit_bias\", \"metadata\", \"user\", \"response_format\", \"\
      seed\", \"tools\", \"tool_choice\", \"logprobs\", \"top_logprobs\", \"parallel_tool_calls\"\
      , \"web_search_options\", \"extra_headers\", \"functions\", \"function_call\"\
      , \"thinking\".\n\nReturns:\n  dict[str, Any]: A dictionary containing the subset\
      \ of keyword arguments that litellm supports. If a 'json' keyword argument is\
      \ provided, response_format is set to {\"type\": \"json_object\"}. If a 'json_model'\
      \ keyword argument is provided and it is a subclass of pydantic.BaseModel, response_format\
      \ is set to that model."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    docstring: "Generate a response for the given prompt and history.\n\nArgs:\n \
      \   prompt: The prompt to generate a response for.\n    history: Optional chat\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    AsyncGenerator[str, None]: The generated response as a stream\
      \ of strings."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs:
      Any) -> "MR"'
    docstring: "Generate a response for the given prompt and history.\n\nArgs\n----\n\
      \    prompt: The prompt to generate a response for.\n    history: Optional chat\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns\n-------\n    LitellmModelResponse: The generated model response."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    docstring: "Generate a response for the given prompt and history.\n\nArgs:\n \
      \   prompt (str): The prompt to generate a response for.\n    history (list[dict[str,\
      \ str]] | None): Optional chat history represented as a list of messages. Each\
      \ message is a dict with keys such as \"role\" and \"content\".\n    kwargs\
      \ (Any): Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    Generator[str, None]: The generated response as a stream of strings.\n\n\
      Raises:\n    Exception: Exceptions raised by the underlying streaming mechanism\
      \ or model client may propagate to the caller."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_completion
    name: _base_completion
    signature: 'def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
    docstring: "Merge base arguments with provided keyword arguments and invoke the\
      \ litellm completion.\n\nArgs:\n  kwargs: Any\n      Additional keyword arguments\
      \ to merge with base_args and pass to completion after removing the \"name\"\
      \ key if present.\n\nReturns:\n  ModelResponse | CustomStreamWrapper\n     \
      \ The result from the underlying completion call.\n\nRaises:\n  Exception\n\
      \      Exceptions raised by the underlying completion call."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> \"MR\""
    docstring: "Asynchronously generate a response for the given prompt and history.\n\
      \nArgs:\n    prompt: The prompt to generate a response for.\n    history: Optional\
      \ chat history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    MR: The generated model response."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_acompletion
    name: _base_acompletion
    signature: 'def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
    docstring: "Merge base_args with provided keyword arguments and invoke the asynchronous\
      \ litellm acompletion.\n\nArgs:\n  kwargs: Any\n      Additional keyword arguments\
      \ to merge with base_args and pass to acompletion after removing the \"name\"\
      \ key if present.\n\nReturns:\n  ModelResponse | CustomStreamWrapper\n     \
      \ The result from the underlying acompletion call.\n\nRaises:\n  Exception\n\
      \      Exceptions raised by the underlying acompletion."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_base_completions
    name: _create_base_completions
    signature: "def _create_base_completions(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelCompletion, AFixedModelCompletion]"
    docstring: "Wrap the base litellm completion function with the model configuration.\n\
      \nArgs:\n    model_config: The configuration for the language model.\n\nReturns:\n\
      \    A tuple containing the synchronous and asynchronous completion functions.\n\
      \nRaises:\n    ValueError"
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
    name: _create_completions
    signature: "def _create_completions(\n    model_config: \"LanguageModelConfig\"\
      ,\n    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelCompletion,\
      \ AFixedModelCompletion]"
    docstring: "Wrap the base litellm completion function with the model configuration\
      \ and additional features.\n\nWrap the base litellm completion function with\
      \ instance variables based on the model configuration.\nThen wrap additional\
      \ features such as rate limiting, retries, and caching, if enabled.\n\nFinal\
      \ function composition order:\n- Logging(Cache(Retries(RateLimiter(ModelCompletion()))))\n\
      \nArgs:\n    model_config: The configuration for the language model.\n    cache:\
      \ Optional cache for storing responses.\n    cache_key_prefix: Prefix for cache\
      \ keys.\n\nReturns:\n    A tuple containing the synchronous and asynchronous\
      \ completion functions."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    docstring: "Initialize the Litellm chat model with the provided name and configuration.\n\
      \nArgs:\n    name: str\n        The name of the model instance.\n    config:\
      \ LanguageModelConfig\n        The configuration for the language model.\n \
      \   cache: PipelineCache | None\n        Optional cache to use for responses.\
      \ If provided, a child cache scoped to this model's name is created.\n    **kwargs:\
      \ Any\n        Additional keyword arguments.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel
    name: LitellmChatModel
    docstring: "LitellmChatModel is a Graphrag wrapper around a Litellm chat model\
      \ with streaming, caching, and resilience features.\n\nArgs:\n  name: The name\
      \ of the model instance.\n  config: LanguageModelConfig containing configuration\
      \ for the language model.\n  cache: Optional PipelineCache to use for responses;\
      \ if provided, a child cache scoped to this model's name is created.\n  kwargs:\
      \ Arbitrary keyword arguments forwarded to the underlying Litellm client.\n\n\
      Returns:\n  LitellmChatModel: The initialized model instance.\n\nRaises:\n \
      \ May raise exceptions from underlying libraries (e.g., authentication or network\
      \ errors) during initialization or operation."
    methods:
    - name: _get_kwargs
      signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    - name: achat_stream
      signature: "def achat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    - name: chat
      signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs:
        Any) -> "MR"'
    - name: chat_stream
      signature: "def chat_stream(\n        self, prompt: str, history: list | None\
        \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    - name: achat
      signature: "def achat(\n        self, prompt: str, history: list | None = None,\
        \ **kwargs: Any\n    ) -> \"MR\""
    - name: __init__
      signature: "def __init__(\n        self,\n        name: str,\n        config:\
        \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
        \        **kwargs: Any,\n    )"
- file: graphrag/language_model/providers/litellm/embedding_model.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel._get_kwargs
    name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    docstring: "Get model arguments supported by litellm.\n\nArgs:\n    kwargs: Arbitrary\
      \ keyword arguments. Only those keys in [\"name\", \"dimensions\", \"encoding_format\"\
      , \"timeout\", \"user\"] will be included in the returned dictionary.\n\nReturns:\n\
      \    dict[str, Any]: A dictionary containing the subset of keyword arguments\
      \ that litellm supports."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
      \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    A list of embeddings, where each\
      \ embedding is a list of floats."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
      \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    list[list[float]]: A batch of embeddings."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_aembedding
    name: _base_aembedding
    signature: 'def _base_aembedding(**kwargs: Any) -> EmbeddingResponse'
    docstring: "Base asynchronous embedding wrapper that forwards to litellm.aembedding\
      \ with merged base arguments.\n\nArgs\n    kwargs: Any\n        Additional keyword\
      \ arguments to pass to the underlying aembedding call. The keys are merged with\
      \ base_args, and the key \"name\" is removed from the resulting arguments if\
      \ present before invocation.\n\nReturns\n    EmbeddingResponse\n        The\
      \ embedding response produced by aembedding, obtained by awaiting the underlying\
      \ call with the merged arguments.\n\nRaises\n    Exception: Exceptions raised\
      \ by aembedding may be propagated."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_embedding
    name: _base_embedding
    signature: 'def _base_embedding(**kwargs: Any) -> EmbeddingResponse'
    docstring: "Base synchronous embedding wrapper that forwards to litellm.embedding\
      \ with merged base arguments.\n\nArgs\n    kwargs: Any\n        Additional keyword\
      \ arguments to pass to the underlying embedding function. The keys are merged\
      \ with base_args, and if the resulting dictionary contains the key \"name\"\
      , it will be removed before invocation.\n\nReturns\n    EmbeddingResponse\n\
      \        The embedding response produced by the underlying embedding call.\n\
      \nRaises\n    Exception\n        Propagates exceptions raised by the underlying\
      \ embedding function."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Async embed.\n\nArgs:\n    text: The text to generate an embedding\
      \ for.\n    kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    list[float]: The embedding."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_base_embeddings
    name: _create_base_embeddings
    signature: "def _create_base_embeddings(\n    model_config: \"LanguageModelConfig\"\
      ,\n) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]"
    docstring: "Wrap the base litellm embedding function with the model configuration.\n\
      \nArgs\n----\n    model_config: LanguageModelConfig\n        The configuration\
      \ for the language model.\n\nReturns\n-------\n    tuple[FixedModelEmbedding,\
      \ AFixedModelEmbedding]\n        A tuple containing the synchronous and asynchronous\
      \ embedding callables\n        produced by wrapping the base embedding with\
      \ the provided model\n        configuration.\n\nRaises\n------\n    ValueError\n\
      \        Azure Managed Identity authentication is only supported for Azure models\
      \ when\n        model_provider is not \"azure\"; in that case authentication\
      \ setup is rejected with\n        the corresponding error message.\n\nNotes\n\
      -----\n    Azure authentication branch:\n    - Triggered when model_config.auth_type\
      \ == AuthType.AzureManagedIdentity.\n    - If model_config.model_provider ==\
      \ \"azure\":\n      - azure_scope is set from the audience value (audience is\
      \ moved to azure_scope).\n      - azure_ad_token_provider is added, constructed\
      \ via get_bearer_token_provider(\n        DefaultAzureCredential(), model_config.audience\
      \ or COGNITIVE_SERVICES_AUDIENCE).\n    - If model_config.model_provider !=\
      \ \"azure\": a ValueError is raised as described above."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Embed a single text input.\n\nArgs:\n    text: The text to generate\
      \ an embedding for.\n    **kwargs: Additional keyword arguments passed to the\
      \ embedding model. These are forwarded to the underlying embedding request and\
      \ may influence the resulting embedding.\n\nReturns:\n    list[float]: The embedding\
      \ for the input text as a list of floating-point numbers. If no embedding is\
      \ produced or the response contains no data, returns [].\n\nNotes:\n    This\
      \ function does not raise an exception on its own. If the underlying embedding\
      \ call fails, the exception will propagate to the caller."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
    name: _create_embeddings
    signature: "def _create_embeddings(\n    model_config: \"LanguageModelConfig\"\
      ,\n    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelEmbedding,\
      \ AFixedModelEmbedding]"
    docstring: "Wrap the base litellm embedding function with the model configuration\
      \ and additional features.\n\nWrap the base litellm embedding function with\
      \ instance variables based on the model configuration. Then wrap additional\
      \ features such as rate limiting, retries, and caching, if enabled.\n\nFinal\
      \ function composition order:\n- Logging(Cache(Retries(RateLimiter(ModelEmbedding()))))\n\
      \nArgs:\n  model_config: LanguageModelConfig. The configuration for the language\
      \ model.\n  cache: PipelineCache | None. Optional cache for storing responses.\n\
      \  cache_key_prefix: str. Prefix for cache keys.\n\nReturns:\n  tuple[FixedModelEmbedding,\
      \ AFixedModelEmbedding]. A tuple containing the synchronous and asynchronous\
      \ embedding functions.\n\nRaises:\n  ValueError: Azure Managed Identity authentication\
      \ is only supported for Azure models."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    docstring: "Initialize the Litellm embedding model with the given name and configuration.\n\
      \nArgs:\n    name: str\n        The name of the model instance.\n    config:\
      \ LanguageModelConfig\n        The configuration for the language model.\n \
      \   cache: PipelineCache | None\n        Optional cache to use for embeddings.\
      \ If provided, a child cache scoped to this model's name is created.\n    **kwargs:\
      \ Any\n        Additional keyword arguments accepted for compatibility. They\
      \ are not used to configure the embedding model at initialization and will be\
      \ ignored here; some kwargs may be processed by the embedding methods via _get_kwargs.\n\
      \nReturns:\n    None\n\nRaises:\n    Exception\n        If the underlying embedding\
      \ initialization fails (e.g., invalid configuration or cache-related errors)."
  classes:
  - class_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel
    name: LitellmEmbeddingModel
    docstring: LitellmEmbeddingModel wraps Litellm's embedding endpoints to generate
      vector representations for text inputs, with support for batch and single-input
      embeddings, and optional request-handling wrappers for caching, logging, rate
      limiting, and retries.
    methods:
    - name: _get_kwargs
      signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    - name: aembed_batch
      signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs:\
        \ Any\n    ) -> list[list[float]]"
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    - name: embed
      signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    - name: __init__
      signature: "def __init__(\n        self,\n        name: str,\n        config:\
        \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
        \        **kwargs: Any,\n    )"
- file: graphrag/language_model/providers/litellm/get_cache_key.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_get_parameters
    name: _get_parameters
    signature: "def _get_parameters(\n    model_config: \"LanguageModelConfig\",\n\
      \    **kwargs: Any,\n) -> dict[str, Any]"
    docstring: "Pluck out the parameters that define a cache key.\n\nUse the same\
      \ parameters as fnllm except request timeout.\n- embeddings: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/embeddings/parameters.py#L12\n\
      - chat: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/chat/parameters.py#L25\n\
      \nArgs:\n    model_config: The configuration of the language model.\n    kwargs:\
      \ Additional model input parameters.\n\nReturns:\n    dict[str, Any]: A dictionary\
      \ of parameters that define the cache key."
  - node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_hash
    name: _hash
    signature: 'def _hash(input: str) -> str'
    docstring: "Generate a SHA-256 hash for the input string.\n\nArgs:\n    input:\
      \ str - the input string to hash\n\nReturns:\n    str - hexadecimal digest of\
      \ the SHA-256 hash of the input\n\nRaises:\n    AttributeError - if the input\
      \ object does not support the encode() method"
  - node_id: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
    name: get_cache_key
    signature: "def get_cache_key(\n    model_config: \"LanguageModelConfig\",\n \
      \   prefix: str,\n    messages: str | None = None,\n    input: str | None =\
      \ None,\n    **kwargs: Any,\n) -> str"
    docstring: "Generate a cache key based on the model configuration, input arguments,\
      \ and optional name.\n\nModeled after the fnllm cache key generation.\nhttps://github.com/microsoft/essex-toolkit/blob/23d3077b65c0e8f1d89c397a2968fe570a25f790/python/fnllm/fnllm/caching/base.py#L50\n\
      \nArgs:\n    model_config: The configuration of the language model.\n    prefix:\
      \ A prefix for the cache key.\n    messages: Optional messages input for the\
      \ cache key.\n    input: Optional single input for the cache key.\n    **kwargs:\
      \ Additional model input parameters. May include 'name', which, if present,\
      \ is appended to the prefix after hashing.\n\nReturns:\n    str: The generated\
      \ cache key in the form '{prefix}_{data_hash}_v{version}'. Note that the provided\
      \ 'name' (via kwargs) is appended to the prefix after computing the data hash,\
      \ not before.\n\nRaises:\n    ValueError: If both 'messages' and 'input' are\
      \ provided. The exact message is: \"Only one of 'messages' or 'input' should\
      \ be provided.\"\n    ValueError: If neither 'messages' nor 'input' is provided.\
      \ The exact message is: \"Either 'messages' or 'input' must be provided.\""
  classes: []
- file: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
    name: with_cache
    signature: "def with_cache(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n   \
      \ cache: \"PipelineCache\",\n    request_type: Literal[\"chat\", \"embedding\"\
      ],\n    cache_key_prefix: str,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    docstring: "\"\"\"Cache wrapper for Litellm request functions.\n\nArgs\n----\n\
      \    sync_fn: The synchronous chat/embedding request function to wrap.\n   \
      \ async_fn: The asynchronous chat/embedding request function to wrap.\n    model_config:\
      \ The configuration for the language model.\n    cache: The cache to use for\
      \ storing responses.\n    request_type: The type of request being made, either\
      \ \"chat\" or \"embedding\".\n    cache_key_prefix: The prefix to use for cache\
      \ keys.\n\nReturns\n-------\n    A tuple containing the wrapped synchronous\
      \ and asynchronous chat/embedding request functions.\n\nRaises\n------\n   \
      \ Exceptions raised by the underlying sync_fn/async_fn or by the cache operations.\n\
      \"\"\""
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache
    name: _wrapped_with_cache
    signature: 'def _wrapped_with_cache(**kwargs: Any) -> Any'
    docstring: "Synchronous cache wrapper for Litellm requests.\n\nArgs\n----\nkwargs:\
      \ Any\n    The keyword arguments forwarded to the wrapped synchronous function.\
      \ This\n    includes the streaming flag ('stream') and other inputs used to\
      \ build the\n    cache key. When streaming is requested, caching is bypassed\
      \ and the\n    underlying function is called directly.\n\nReturns\n-------\n\
      Any\n    The response produced by the wrapped function. If a valid cached response\
      \ is\n    found, a corresponding ModelResponse or EmbeddingResponse is returned\n\
      \    instead of calling the wrapped function.\n\nRaises\n------\nPropagates\
      \ exceptions raised by the wrapped function or by the cache\noperations."
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache_async
    name: _wrapped_with_cache_async
    signature: "def _wrapped_with_cache_async(\n        **kwargs: Any,\n    ) -> Any"
    docstring: "Asynchronous cache wrapper for Litellm requests.\n\nArgs:\n    kwargs:\
      \ Any\n        The keyword arguments forwarded to the wrapped asynchronous function.\
      \ This includes the streaming flag ('stream') and other inputs used to build\
      \ the cache key. When streaming is requested, caching is bypassed and the underlying\
      \ function is called directly.\n\nReturns:\n    Any\n        The response produced\
      \ by the wrapper. This may be a ModelResponse or EmbeddingResponse constructed\
      \ from a cached entry, or the result of the wrapped asynchronous function. When\
      \ streaming is requested, the underlying function is called directly and no\
      \ caching is performed.\n\nRaises:\n    Exception\n        Exceptions raised\
      \ by the underlying asynchronous function or by cache operations."
  classes: []
- file: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging
    name: _wrapped_with_logging
    signature: 'def _wrapped_with_logging(**kwargs: Any) -> Any'
    docstring: "\"\"\"Wraps the synchronous request function with logging.\n\nArgs:\n\
      \    kwargs: Keyword arguments passed to the underlying synchronous request\
      \ function.\n\nReturns:\n    Any: The value returned by the underlying sync_fn\
      \ when called with the provided kwargs.\n\nRaises:\n    Exception: Re-raised\
      \ after logging the exception encountered during the call.\n\"\"\""
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging_async
    name: _wrapped_with_logging_async
    signature: "def _wrapped_with_logging_async(\n        **kwargs: Any,\n    ) ->\
      \ Any"
    docstring: "Wraps the asynchronous request function with logging.\n\nArgs:\n \
      \   kwargs: Keyword arguments passed to the underlying asynchronous request\
      \ function.\n\nReturns:\n    Any: The value returned by the underlying async_fn\
      \ when called with the provided kwargs.\n\nRaises:\n    Exception: Re-raised\
      \ after logging the exception encountered during the call."
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
    name: with_logging
    signature: "def with_logging(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    docstring: "Wrap the provided synchronous and asynchronous Litellm request functions\
      \ with logging for exceptions.\n\nArgs\n----\n    sync_fn: LitellmRequestFunc\n\
      \        The synchronous chat/embedding request function to wrap.\n    async_fn:\
      \ AsyncLitellmRequestFunc\n        The asynchronous chat/embedding request function\
      \ to wrap.\n\nReturns\n-------\ntuple[LitellmRequestFunc, AsyncLitellmRequestFunc]\n\
      \        A tuple containing the wrapped synchronous and asynchronous chat/embedding\
      \ request functions.\n\nRaises\n------\nException\n        If either wrapped\
      \ function raises an exception, the exception is logged via logger.exception\
      \ and re-raised."
  classes: []
- file: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter_async
    name: _wrapped_with_rate_limiter_async
    signature: "def _wrapped_with_rate_limiter_async(\n        **kwargs: Any,\n  \
      \  ) -> Any"
    docstring: "Asynchronous wrapper that applies rate limiting to a request function.\n\
      \nArgs:\n    kwargs: Any\n        Arbitrary keyword arguments forwarded to the\
      \ wrapped asynchronous function.\n        The wrapper computes the rate-limiting\
      \ token count from max_tokens plus\n        a token count derived from the provided\
      \ 'messages' or 'input' in kwargs.\n\nReturns:\n    Any\n        The result\
      \ of the wrapped asynchronous function after acquiring the rate\n        limiter.\n\
      \nRaises:\n    Propagates exceptions raised by the rate limiter or by the wrapped\
      \ asynchronous\n    function."
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
    name: with_rate_limiter
    signature: "def with_rate_limiter(\n    *,\n    sync_fn: LitellmRequestFunc,\n\
      \    async_fn: AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\"\
      ,\n    rpm: int | None = None,\n    tpm: int | None = None,\n) -> tuple[LitellmRequestFunc,\
      \ AsyncLitellmRequestFunc]"
    docstring: "\"\"\"\nWrap the synchronous and asynchronous Litellm request functions\
      \ with rate limiting.\n\nArgs\n----\nsync_fn: The synchronous chat/embedding\
      \ request function to wrap.\nasync_fn: The asynchronous chat/embedding request\
      \ function to wrap.\nmodel_config: LanguageModelConfig containing rate_limit_strategy\
      \ and related model parameters.\nrpm: Optional rate limit in requests per minute.\
      \ If None or 0, the RPM limit is disabled.\ntpm: Optional rate limit in tokens\
      \ per minute. If None or 0, the TPM limit is disabled.\n\nReturns\n-------\n\
      tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]\n    The wrapped synchronous\
      \ and asynchronous request functions.\n\nRaises\n------\nValueError\n    If\
      \ the rate limiter strategy in model_config is None or not registered with the\
      \ RateLimiterFactory.\n\"\"\""
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter
    name: _wrapped_with_rate_limiter
    signature: 'def _wrapped_with_rate_limiter(**kwargs: Any) -> Any'
    docstring: "Wrapped synchronous request function with rate limiting.\n\nArgs:\n\
      \    kwargs: Any\n        Arbitrary keyword arguments forwarded to the wrapped\
      \ synchronous function.\n        The wrapper computes the rate-limiting token\
      \ count from max_tokens plus\n        a token count derived from the provided\
      \ 'messages' or 'input' in kwargs.\n\nReturns:\n    Any\n        The result\
      \ of the wrapped synchronous request function.\n\nRaises:\n    Exception\n \
      \       Propagates exceptions raised by the rate limiter or by the wrapped function."
  classes: []
- file: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries_async
    name: _wrapped_with_retries_async
    signature: "def _wrapped_with_retries_async(\n        **kwargs: Any,\n    ) ->\
      \ Any"
    docstring: "Wrap the asynchronous request function with retries using the configured\
      \ retry service.\n\nArgs:\n    kwargs: Keyword arguments passed to the underlying\
      \ asynchronous request function.\n\nReturns:\n    Any: The value returned by\
      \ the underlying asynchronous request function when called with the provided\
      \ kwargs.\n\nRaises:\n    Exception: Propagated from the underlying asynchronous\
      \ function or the retry service."
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries
    name: _wrapped_with_retries
    signature: 'def _wrapped_with_retries(**kwargs: Any) -> Any'
    docstring: "Wrap the synchronous request function with retries using the configured\
      \ retry service.\n\nArgs:\n    kwargs: Keyword arguments passed to the underlying\
      \ synchronous request function. (type: Any)\n\nReturns:\n    Any: The value\
      \ returned by the underlying synchronous request function when called with the\
      \ provided kwargs.\n\nRaises:\n    Exception: Propagated from the underlying\
      \ synchronous function or the retry service."
  - node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
    name: with_retries
    signature: "def with_retries(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
      \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n) ->\
      \ tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
    docstring: "Wrap the synchronous and asynchronous request functions with retries.\n\
      \nThis function constructs a retry service from model_config and returns two\
      \ wrappers:\n- a synchronous wrapper that uses retry to invoke the provided\
      \ sync_fn\n- an asynchronous wrapper that uses aretry to invoke the provided\
      \ async_fn\n\nRetry configuration is driven by model_config fields: retry_strategy,\
      \ max_retries, and max_retry_wait.\n\nNotes:\n- The asynchronous wrapper uses\
      \ aretry on the async_fn, while the synchronous wrapper uses retry on the sync_fn.\n\
      \nArgs:\n    sync_fn (LitellmRequestFunc): The synchronous chat/embedding request\
      \ function to wrap.\n    async_fn (AsyncLitellmRequestFunc): The asynchronous\
      \ chat/embedding request function to wrap.\n    model_config (LanguageModelConfig):\
      \ Configuration for the language model, including retry parameters (retry_strategy,\
      \ max_retries, max_retry_wait).\n\nReturns:\n    tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]:\
      \ A tuple containing the wrapped synchronous and\n        asynchronous chat/embedding\
      \ request functions.\n\nRaises:\n    Propagates exceptions from the retry mechanism\
      \ or the wrapped functions when retries are exhausted or an unrecoverable error\
      \ occurs."
  classes: []
- file: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n\
      \    ) -> None"
    docstring: "Abstract initializer for rate limiters. Subclasses must implement\
      \ their own initialization logic; this method should not perform concrete initialization.\n\
      \nArgs:\n    kwargs: Additional keyword arguments passed to initialization.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.acquire
    name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    docstring: "\"\"\"Acquire Rate Limiter.\n\nParameters\n    token_count (int):\
      \ The estimated number of tokens for the current request.\n\nReturns\n    Iterator[None]:\
      \ A context manager that yields None and does not return any value.\n\nRaises\n\
      \    NotImplementedError: RateLimiter subclasses must implement the acquire\
      \ method.\n\"\"\""
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter
    name: RateLimiter
    docstring: "RateLimiter is an abstract base class that defines the interface for\
      \ rate limiting strategies. Concrete subclasses must implement their own initialization\
      \ logic and provide a concrete acquire method that returns a context manager\
      \ guarding a request.\n\nArgs:\n  kwargs: Additional keyword arguments passed\
      \ to initialization. Subclasses may use them to configure the limiter; the base\
      \ class does not perform concrete initialization.\nReturns:\n  None\nNotes:\n\
      \  This class is abstract and cannot be instantiated. Subclasses must implement\
      \ acquire to provide a concrete rate-limiting context manager.\n\nAcquire:\n\
      \  token_count (int): The estimated number of tokens for the current request.\n\
      \  Returns:\n    ContextManager[None]: A context manager that yields None and\
      \ guards the request."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n\
        \    ) -> None"
    - name: acquire
      signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
- file: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        rpm: int | None\
      \ = None,\n        tpm: int | None = None,\n        default_stagger: float =\
      \ 0.0,\n        period_in_seconds: int = 60,\n        **kwargs: Any,\n    )"
    docstring: "Initialize the static rate limiter with optional RPM/TPM limits and\
      \ configuration.\n\nArgs:\n    rpm: int | None\n        RPM limit; positive\
      \ integer or None to disable.\n    tpm: int | None\n        TPM limit; positive\
      \ integer or None to disable.\n    default_stagger: float\n        Default stagger\
      \ between requests; must be >= 0.\n    period_in_seconds: int\n        Length\
      \ of the period in seconds; must be a positive integer.\n    kwargs: Any\n \
      \       Additional keyword arguments (ignored).\n\nReturns:\n    None\n    \
      \    This initializer does not return a value.\n\nRaises:\n    ValueError\n\
      \        If both rpm and tpm are None (disabled), or if rpm/tpm are non-positive,\
      \ or if default_stagger is negative, or if period_in_seconds is not positive."
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.acquire
    name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    docstring: "Acquire Rate Limiter.\n\nArgs:\n    token_count: The estimated number\
      \ of tokens for the current request.\n\nReturns:\n    None: This context manager\
      \ yields None and does not return any value."
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter
    name: StaticRateLimiter
    docstring: 'StaticRateLimiter enforces fixed-per-period limits on requests per
      minute (RPM) and tokens per minute (TPM) with an optional default stagger between
      requests. It tracks usage within a configurable period and resets counts at
      period boundaries. Limits can be disabled individually by setting rpm or tpm
      to None. The acquire() method is a context manager used to guard a block of
      code; pass token_count as the estimated number of tokens for the current request.
      The context manager yields None and blocks as needed to ensure the limits are
      not exceeded. If both RPM and TPM are disabled, acquire() yields immediately.
      A non-negative default_stagger is applied between requests when appropriate.


      Key attributes

      - rpm: RPM limit; positive integer or None to disable.

      - tpm: TPM limit; positive integer or None to disable.

      - default_stagger: Non-negative delay between requests, in seconds.

      - period_in_seconds: Length of the monitoring period in seconds; must be > 0.


      Constructor and usage notes

      - rpm and tpm must be None or a positive integer; invalid values raise ValueError.

      - default_stagger must be >= 0; invalid values raise ValueError.

      - period_in_seconds must be > 0; invalid values raise ValueError.

      - token_count should be a positive integer; misuse may raise ValueError.


      Thread-safety

      - The limiter is designed for multi-threaded use; internal state is synchronized
      to support concurrent acquire() calls.


      Behavior overview

      - Limits are enforced within each period window and reset when a new period
      begins. The limiter blocks (and optionally staggers) to ensure that usage never
      exceeds the configured RPM/TPM within a period.'
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        rpm: int | None\
        \ = None,\n        tpm: int | None = None,\n        default_stagger: float\
        \ = 0.0,\n        period_in_seconds: int = 60,\n        **kwargs: Any,\n \
        \   )"
    - name: acquire
      signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
- file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n   \
      \     **kwargs: Any,\n    )"
    docstring: "Initialize a LiteLLM Exponential Retry Service with retry configuration.\n\
      \nArgs:\n    max_retries: The maximum number of retry attempts (int). Must be\
      \ greater than 0.\n    base_delay: The base delay between retries in seconds\
      \ (float). Must be greater than 1.0.\n    jitter: Whether to apply jitter to\
      \ the delay (bool).\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n\
      \    None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n\
      \    ValueError: base_delay must be greater than 1.0."
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function using exponential backoff.\n\nRetries\
      \ the provided function on failure up to the configured max_retries with an\
      \ exponential backoff delay. The initial delay is 1.0 second and increases by\
      \ the base_delay factor; if jitter is enabled, a small random amount is added.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs) and its return value will be returned on success.\n    kwargs:\
      \ Any - Keyword arguments to pass to func when calling it.\n\nReturns:\n   \
      \ Any: The value returned by func when it succeeds.\n\nRaises:\n    Exception:\
      \ The last exception raised by func when the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function.\n\nArgs:\n  func: The asynchronous\
      \ function to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword\
      \ arguments to pass to the function. (Any)\n\nReturns:\n  Any: The result of\
      \ the awaited function.\n\nRaises:\n  Exception: If the wrapped function keeps\
      \ raising and the maximum number of retries is exceeded."
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry
    name: ExponentialRetry
    docstring: "ExponentialRetry provides exponential backoff retry logic for both\
      \ synchronous and asynchronous callables, with optional jitter.\n\nArgs:\n \
      \ max_retries: int. Maximum number of retry attempts. Must be greater than 0.\
      \ Default: 5.\n  base_delay: float. Base delay between retries in seconds. Must\
      \ be greater than 0.0. Default: 2.0.\n  jitter: bool. If True, apply a small\
      \ random jitter to each delay. Default: True.\n  kwargs: Any. Additional keyword\
      \ arguments accepted for forward compatibility; not used by the retry logic.\n\
      \nAttributes:\n  max_retries: int\n  base_delay: float\n  jitter: bool\n  _logger:\
      \ logging.Logger (internal). Diagnostic logger for retry events.\n\nNotes:\n\
      \  - Jitter, when enabled, adds randomness to delays to reduce thundering herd\
      \ problems and applies to both retry and aretry paths.\n  - The retry behavior\
      \ is determined by the configured max_retries, base_delay, and jitter; delays\
      \ are computed according to an exponential backoff scheme.\n  - This class is\
      \ intended to be instantiated with the given configuration and used to invoke\
      \ retry on synchronous functions or aretry on asynchronous functions.\n\nRaises:\n\
      \  ValueError: If max_retries <= 0 or base_delay <= 0.0."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
        \ = 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n \
        \       **kwargs: Any,\n    )"
    - name: retry
      signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    - name: aretry
      signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
        \        **kwargs: Any,\n    ) -> Any"
- file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    docstring: "Initialize an Incremental Wait Retry instance with retry configuration.\n\
      \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
      \    max_retries: The maximum number of retry attempts (int). Must be greater\
      \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n   \
      \ None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n   \
      \ ValueError: max_retry_wait must be greater than 0."
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function with incremental delay until it succeeds\
      \ or the maximum number of retries is reached.\n\nArgs:\n  func: The asynchronous\
      \ function to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword\
      \ arguments to pass to the function. (Any)\n\nReturns:\n  Any: The result of\
      \ the awaited function.\n\nRaises:\n  Exception: If the wrapped function keeps\
      \ raising and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function.\n\nRetries the provided function until\
      \ it succeeds or the maximum number of retries is reached, applying an incremental\
      \ delay between retries.\n\nArgs:\n    func: Callable[..., Any] - The function\
      \ to invoke. It will be called as func(**kwargs) and its result will be returned\
      \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
      \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
      \    Exception - The last exception raised by func when the maximum number of\
      \ retries is exceeded."
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry
    name: IncrementalWaitRetry
    docstring: "IncrementalWaitRetry provides a retry strategy that inserts incremental\
      \ delays between attempts for both asynchronous and synchronous callables.\n\
      \nAttributes:\n  max_retry_wait (float): The maximum delay between retries in\
      \ seconds.\n  max_retries (int): The maximum number of retry attempts.\n  base_delay\
      \ (float, optional): Optional initial delay used in the incremental computation.\n\
      \  delay_increment (float, optional): Optional per-retry increment for the incremental\
      \ delay.\n\nDelay calculation:\n  The delay before the nth retry is computed\
      \ as:\n  delay_n = min(max_retry_wait, base_delay + (n - 1) * delay_increment)\n\
      \  where n starts at 1 for the first retry. If base_delay or delay_increment\
      \ are not provided, the implementation uses sensible defaults.\n\naretry:\n\
      \  Retry an asynchronous callable until success or the maximum number of retries\
      \ is reached. The function is invoked as await func(**kwargs). The computed\
      \ incremental delay is applied between attempts. Returns the result of the awaited\
      \ function on success; if all retries fail, the last raised exception is propagated.\n\
      \nretry:\n  Retry a synchronous callable until success or the maximum number\
      \ of retries is reached. The function is invoked as func(**kwargs). The computed\
      \ incremental delay is applied between attempts. Returns the value returned\
      \ by the wrapped function on success; if all retries fail, the last raised exception\
      \ is propagated.\n\nRaises:\n  ValueError: max_retries must be greater than\
      \ 0.\n  ValueError: max_retry_wait must be greater than 0."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
        \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    - name: aretry
      signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
        \        **kwargs: Any,\n    ) -> Any"
    - name: retry
      signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
- file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function until it succeeds or the maximum number\
      \ of retries is reached.\n\nArgs:\n    func: The asynchronous function to retry.\n\
      \    kwargs: Additional keyword arguments to pass to the function.\n\nReturns:\n\
      \    Any: The result of the awaited function.\n\nRaises:\n    Exception: If\
      \ the wrapped function keeps raising and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function until it succeeds or max_retries is reached.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs) and its result will be returned on success.\n    kwargs:\
      \ Any - Keyword arguments to pass to func.\n\nReturns:\n    Any - The value\
      \ returned by func on a successful invocation.\n\nRaises:\n    Exception - The\
      \ last exception raised by func after exhausting max_retries."
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        **kwargs: Any,\n    )"
    docstring: "Initialize NativeRetry with retry configuration.\n\nArgs:\n  max_retries:\
      \ The maximum number of retry attempts (int). Must be greater than 0.\n  kwargs:\
      \ Additional keyword arguments (Any).\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
      \ max_retries must be greater than 0."
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry
    name: NativeRetry
    docstring: "NativeRetry provides retry logic for both asynchronous and synchronous\
      \ callables with a configurable maximum number of retries.\n\nArgs:\n    max_retries:\
      \ The maximum number of retry attempts. Must be greater than 0.\n    kwargs:\
      \ Additional keyword arguments accepted by the initializer.\n\nAttributes:\n\
      \    max_retries: The maximum number of retry attempts.\n\nSummary:\n    The\
      \ class is initialized with max_retries (default 5) and exposes two methods:\n\
      \    aretry(func, **kwargs): Retry an asynchronous function until it succeeds\
      \ or the maximum number of retries is reached. Returns the result of the awaited\
      \ function.\n    retry(func, **kwargs): Retry a synchronous function until it\
      \ succeeds or the maximum number of retries is reached. Returns the value returned\
      \ by the function on success.\n\nNotes:\n    Both aretry and retry return the\
      \ wrapped function's result (not None) when successful. They retry on exceptions\
      \ raised by the wrapped function. If the maximum number of retries is exceeded,\
      \ the last exception raised by the wrapped function is propagated.\n\nReturns:\n\
      \    None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n\
      \    Exception: If the wrapped function keeps raising and the maximum number\
      \ of retries is exceeded."
    methods:
    - name: aretry
      signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
        \        **kwargs: Any,\n    ) -> Any"
    - name: retry
      signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
        \ = 5,\n        **kwargs: Any,\n    )"
- file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    docstring: "\"\"\"Initialize a RandomWaitRetry instance with retry configuration.\n\
      \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
      \    max_retries: The maximum number of retry attempts (int). Must be greater\
      \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n   \
      \ None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n   \
      \ ValueError: max_retry_wait must be greater than 0.\n\"\"\""
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function with a random delay between retries\
      \ until it succeeds or the maximum number of retries is reached.\n\nArgs:\n\
      \    func: The asynchronous function to retry.\n    kwargs: Additional keyword\
      \ arguments to pass to the function.\n\nReturns:\n    The result of the awaited\
      \ function.\n\nRaises:\n    Exception: If the wrapped function keeps raising\
      \ and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function until it succeeds or the maximum number\
      \ of retries is reached.\n\nArgs:\n    func: Callable[..., Any] - The function\
      \ to invoke. It will be called as func(**kwargs) and its result will be returned\
      \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
      \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
      \    Exception - The last exception raised by func if the maximum number of\
      \ retries is exceeded."
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry
    name: RandomWaitRetry
    docstring: "Retry policy that retries both asynchronous and synchronous functions\
      \ using a random delay between attempts, up to a configurable maximum number\
      \ of retries.\n\nArgs:\n    max_retry_wait: The maximum delay, in seconds, between\
      \ retries. The actual delay is drawn uniformly from [0, max_retry_wait].\n \
      \   max_retries: The maximum number of retry attempts. Must be greater than\
      \ 0.\n    kwargs: Additional keyword arguments accepted by the constructor (for\
      \ extensibility).\n\nReturns:\n    None\n\nRaises:\n    ValueError: If max_retries\
      \ <= 0 or max_retry_wait <= 0.\n\nAttributes:\n    max_retry_wait: float - maximum\
      \ delay (seconds) between retries.\n    max_retries: int - maximum number of\
      \ retry attempts.\n    logger: logging.Logger - internal logger for debug messages\
      \ (if initialized).\n\nSummary:\n    - aretry retries an asynchronous function\
      \ by awaiting func(**kwargs) until it succeeds or max_retries is reached; returns\
      \ the result on success.\n    - retry retries a synchronous function by invoking\
      \ func(**kwargs) until it succeeds or max_retries is reached; returns the result\
      \ on success.\n    - The delay before each retry is a random value drawn uniformly\
      \ from [0, max_retry_wait]. For the async path this is implemented via await\
      \ asyncio.sleep(delay); for the sync path this uses time.sleep(delay).\n   \
      \ - If all attempts fail, the last raised exception is propagated to the caller."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
        \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    - name: aretry
      signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
        \        **kwargs: Any,\n    ) -> Any"
    - name: retry
      signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
- file: graphrag/language_model/providers/litellm/services/retry/retry.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function.\n\nAbstract method that subclasses must\
      \ implement to retry the provided function using their configured retry strategy.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs).\n    kwargs: Any - Additional keyword arguments to pass\
      \ to the function being retried or to control the retry behavior.\n\nReturns:\n\
      \    Any - The result of the retried function invocation.\n\nRaises:\n    NotImplementedError\
      \ - Subclasses must implement this method."
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.__init__
    name: __init__
    signature: 'def __init__(self, /, **kwargs: Any)'
    docstring: "Initialize a Retry subclass.\n\nArgs:\n  kwargs (Any): Arbitrary keyword\
      \ arguments for subclass initialization.\n\nReturns:\n  None\n\nRaises:\n  NotImplementedError:\
      \ If subclass does not implement __init__."
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function.\n\nArgs:\n  func (Callable[..., Awaitable[Any]]):\
      \ The asynchronous function to retry.\n  kwargs (Any): Additional keyword arguments\
      \ to pass to the function.\n\nReturns:\n  Any: The result of the awaited function.\n\
      \nRaises:\n  NotImplementedError: Subclasses must implement this method"
  classes:
  - class_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry
    name: Retry
    docstring: "Retry is an abstract base class that defines the interface for applying\
      \ a configurable retry policy to operations.\n\nPurpose\n  Provide a pluggable\
      \ retry mechanism by specifying concrete strategies for retrying both synchronous\
      \ and asynchronous callables.\n\nAttributes\n  This base class does not define\
      \ concrete state. Subclasses may store configuration (e.g., max_retries, backoff,\
      \ or jitter) in their own __init__.\n\nAbstract interface\n  Concrete subclasses\
      \ must implement:\n    - retry(func: Callable[..., Any], **kwargs: Any) -> Any\n\
      \      Retry a synchronous function and return the final result of the successful\
      \ invocation.\n    - aretry(func: Callable[..., Awaitable[Any]], **kwargs: Any)\
      \ -> Any\n      Retry an asynchronous function and return the final awaited\
      \ result.\n\nInitialization\n  __init__(self, /, **kwargs: Any)\n    Initialize\
      \ a Retry subclass with arbitrary keyword arguments used to configure the strategy.\n\
      \nNotes\n  This class is abstract. Attempting to instantiate it directly or\
      \ instantiate a subclass that leaves abstract methods unimplemented will raise\
      \ TypeError (not NotImplementedError).\n\nArgs\n  kwargs: Arbitrary keyword\
      \ arguments for subclass initialization.\n\nRaises\n  TypeError: If attempting\
      \ to instantiate this abstract class or a subclass with unimplemented abstract\
      \ methods."
    methods:
    - name: retry
      signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    - name: __init__
      signature: 'def __init__(self, /, **kwargs: Any)'
    - name: aretry
      signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
        \        **kwargs: Any,\n    ) -> Any"
- file: graphrag/language_model/providers/litellm/types.py
  functions:
  - node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
      \ params: see https://platform.openai.com/docs/api-reference/chat/create\n \
      \       messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
      \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
      \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
      \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
      \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
      \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n \
      \       logit_bias: dict | None = None,  # type: ignore\n        user: str |\
      \ None = None,\n        # openai v1.0+ new params\n        response_format:\
      \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int |\
      \ None = None,\n        tools: list | None = None,  # type: ignore\n       \
      \ tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
      \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    docstring: "Asynchronous chat completion function for litellm integration; calls\
      \ the OpenAI-compatible chat completion API and supports streaming responses.\n\
      \nArgs:\n    messages: list\n        Chat messages to include in the request.\n\
      \    stream: bool | None\n        If True, stream partial responses as they\
      \ arrive.\n    stream_options: dict | None\n        Options for streaming.\n\
      \    stop: Any\n        Stop sequences for the generation.\n    max_completion_tokens:\
      \ int | None\n        Maximum tokens for the completion.\n    max_tokens: int\
      \ | None\n        Maximum tokens to generate.\n    modalities: list[ChatCompletionModality]\
      \ | None\n        Modality configuration.\n    prediction: ChatCompletionPredictionContentParam\
      \ | None\n        Prediction content.\n    audio: ChatCompletionAudioParam |\
      \ None\n        Audio parameters.\n    logit_bias: dict | None\n        Logit\
      \ bias overrides.\n    user: str | None\n        User identifier.\n    response_format:\
      \ dict | type[BaseModel] | None\n        Response format or model.\n    seed:\
      \ int | None\n        Random seed.\n    tools: list | None\n        Tools to\
      \ use.\n    tool_choice: str | dict | None\n        Tool selection.\n    logprobs:\
      \ bool | None\n        Include log probabilities.\n    top_logprobs: int | None\n\
      \        Top logprobs to return.\n    parallel_tool_calls: bool | None\n   \
      \     Run tool calls in parallel.\n    web_search_options: OpenAIWebSearchOptions\
      \ | None\n        Web search options.\n    deployment_id: str | None\n     \
      \   Optional deployment identifier.\n    extra_headers: dict | None\n      \
      \  Extra HTTP headers.\n    functions: list | None\n        Deprecated OpenAI\
      \ functions.\n    function_call: str | None\n        Function call.\n    thinking:\
      \ AnthropicThinkingParam | None\n        LiteLLM thinking parameter.\n    kwargs:\
      \ Any\n        Additional keyword arguments accepted.\n\nReturns:\n    ModelResponse\
      \ | CustomStreamWrapper\n\nRaises:\n    Exception types raised by the underlying\
      \ API client or streaming wrapper (e.g., OpenAI API errors, network errors,\
      \ or litellm runtime errors)."
  - node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    docstring: 'Embedding function.


      Args:

      - request_id: Optional request identifier

      - input: List input to embed

      - dimensions: Optional embedding dimensions

      - encoding_format: Optional encoding format

      - timeout: Timeout in seconds for the request (default 600)

      - api_base: Optional API base

      - api_version: Optional API version

      - api_key: Optional API key

      - api_type: Optional API type

      - caching: Whether to enable caching

      - user: Optional user identifier

      - kwargs: Additional keyword arguments that will be forwarded to the underlying
      request


      Returns:

      - EmbeddingResponse: The embedding result


      Raises:

      - None'
  - node_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        messages: list =\
      \ [],  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n \
      \       stream_options: dict | None = None,  # type: ignore\n        stop=None,\
      \  # type: ignore\n        max_completion_tokens: int | None = None,\n     \
      \   max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
      \ | None = None,\n        prediction: ChatCompletionPredictionContentParam |\
      \ None = None,\n        audio: ChatCompletionAudioParam | None = None,\n   \
      \     logit_bias: dict | None = None,  # type: ignore\n        user: str | None\
      \ = None,\n        # openai v1.0+ new params\n        response_format: dict\
      \ | type[BaseModel] | None = None,  # type: ignore\n        seed: int | None\
      \ = None,\n        tools: list | None = None,  # type: ignore\n        tool_choice:\
      \ str | dict | None = None,  # type: ignore\n        logprobs: bool | None =\
      \ None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    docstring: "Compute a chat completion using a language model and return the model's\
      \ response (or a streaming wrapper).\n\nArgs:\n    messages: list = []\n   \
      \     List of ChatCompletion messages to include in the request.\n    stream:\
      \ bool | None\n        If True, stream partial responses as they arrive.\n \
      \   stream_options: dict | None\n        Options for streaming.\n    stop: Any\n\
      \        Stop sequence or token.\n    max_completion_tokens: int | None\n  \
      \      Maximum number of tokens for the completion.\n    max_tokens: int | None\n\
      \        Maximum number of tokens to generate.\n    modalities: list[ChatCompletionModality]\
      \ | None\n        Modalities for the chat completion.\n    prediction: ChatCompletionPredictionContentParam\
      \ | None\n        Prediction content parameter for the chat completion.\n  \
      \  audio: ChatCompletionAudioParam | None\n        Audio parameters for the\
      \ chat completion.\n    logit_bias: dict | None\n        Biases to apply to\
      \ token logits.\n    user: str | None\n        User identifier.\n    response_format:\
      \ dict | type[BaseModel] | None\n        Response format specification.\n  \
      \  seed: int | None\n        Random seed for deterministic sampling.\n    tools:\
      \ list | None\n        Tools to use during the chat completion.\n    tool_choice:\
      \ str | dict | None\n        Tool selection to use for the request.\n    logprobs:\
      \ bool | None\n        Include log probabilities in the response.\n    top_logprobs:\
      \ int | None\n        Number of top log probabilities to return.\n    parallel_tool_calls:\
      \ bool | None\n        Enable parallel tool calls during processing.\n    web_search_options:\
      \ OpenAIWebSearchOptions | None\n        Web search options used during retrieval.\n\
      \    deployment_id: Any\n        Deployment identifier.\n    extra_headers:\
      \ dict | None\n        Extra HTTP headers to include in the request.\n    functions:\
      \ list | None\n        Deprecated OpenAI functions parameter.\n    function_call:\
      \ str | None\n        Function call specification.\n    thinking: AnthropicThinkingParam\
      \ | None\n        Optional liteLLM thinking parameter.\n    kwargs: Any\n  \
      \      Additional keyword arguments.\n\nReturns:\n    ModelResponse | CustomStreamWrapper\n\
      \        The model response object or a streaming wrapper for streaming responses.\n\
      \nRaises:\n    NotImplementedError\n        If invoked on the base protocol\
      \ without an implementing class.\n\nExample:\n    # Synchronous usage\n    response\
      \ = model(messages=[{'role': 'user', 'content': 'Hello'}], max_tokens=50)\n\n\
      \    # Streaming usage\n    stream = model(messages=[{'role': 'user', 'content':\
      \ 'Explain this concept'}], stream=True)\n    for chunk in stream:\n       \
      \ pass  # handle streaming chunks"
  - node_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc.__call__
    name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    docstring: "Asynchronous request function.\n\nRepresents an asynchronous call\
      \ to either a chat completion or embedding function. The implementation forwards\
      \ all provided keyword arguments to the underlying request function, enabling\
      \ flexible use with different backends.\n\nArgs:\n    kwargs: Arbitrary keyword\
      \ arguments forwarded to the underlying request function. Specific accepted\
      \ keys depend on the concrete implementation (e.g., chat completion or embedding).\n\
      \nReturns:\n    Any: The result produced by the underlying request function.\
      \ The exact type depends on the concrete function being invoked (e.g., a chat\
      \ completion response or an embedding).\n\nRaises:\n    Exception: Exceptions\
      \ raised by the underlying request function are propagated to the caller (e.g.,\
      \ API or network errors)."
  - node_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    docstring: "Embedding function for a model configured elsewhere.\n\nCompute embeddings\
      \ for a batch of inputs using a pre-configured model (no model parameter is\
      \ required). This synchronous embedding function mirrors litellm.embedding but\
      \ omits the model argument, relying on the model configuration.\n\nArgs:\n \
      \ request_id (str | None): Optional request identifier.\n  input (list): List\
      \ of inputs to embed.\n  dimensions (int | None): Optional embedding dimensions\
      \ to request. If None, the model's default is used.\n  encoding_format (str\
      \ | None): Optional encoding format to return embeddings in. If None, the default\
      \ format is used.\n  timeout (int): Timeout in seconds for the request (default\
      \ 600).\n  api_base (str | None): Optional API base URL.\n  api_version (str\
      \ | None): Optional API version.\n  api_key (str | None): Optional API key for\
      \ authentication.\n  api_type (str | None): Optional API type.\n  caching (bool):\
      \ Enable or disable caching of embeddings. Default is False.\n  user (str |\
      \ None): Optional user identifier for the request.\n  kwargs (Any): Additional\
      \ keyword arguments passed to the underlying embedding call.\n\nReturns:\n \
      \ EmbeddingResponse: The embedding response object containing the embeddings\
      \ and related metadata.\n\nRaises:\n  ValueError: If input is not a list.\n\
      \  TimeoutError: If the embedding request times out.\n  Exception: If an error\
      \ occurs during the embedding request."
  - node_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc.__call__
    name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    docstring: "Synchronous request function.\n\nRepresents either a chat completion\
      \ or embedding function. The implementation forwards all provided keyword arguments\
      \ to the underlying request function, enabling flexible use with different backends.\n\
      \nArgs:\n    kwargs: Arbitrary keyword arguments forwarded to the underlying\
      \ request function. Specific accepted keys depend on the concrete impl...\n\n\
      Returns:\n    Any: The result of the underlying request function.\n\nRaises:\n\
      \    Exception: If the underlying request function raises an exception, it will\
      \ propagate to the caller."
  classes:
  - class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion
    name: AFixedModelCompletion
    docstring: "Async fixed-model chat completion interface for litellm integration.\
      \ This class exposes a callable, asynchronous surface to perform chat completions\
      \ against an OpenAI compatible API using a fixed model, with optional streaming\
      \ support via litellm. It is designed for straightforward, open-ended chat interactions\
      \ and can be combined with function calling and tools when provided.\n\nArgs:\n\
      \  messages: list. Chat messages to include in the request. Defaults to an empty\
      \ list. Optional.\n  stream: bool | None. If True, stream partial responses\
      \ as they arrive. Defaults to None (non-streaming).\n  stream_options: dict\
      \ | None. Options controlling streaming behavior. Defaults to None.\n  stop:\
      \ Any. Stop sequence or sequences. Optional.\n  max_completion_tokens: int |\
      \ None. Maximum tokens allowed in the completion. Optional.\n  max_tokens: int\
      \ | None. Maximum tokens allowed for the response. Optional.\n  modalities:\
      \ list of ChatCompletionModality | None. Modality hints to apply. Optional.\n\
      \  prediction: ChatCompletionPredictionContentParam | None. Prediction content\
      \ to request. Optional.\n  audio: ChatCompletionAudioParam | None. Audio-related\
      \ parameters for the request. Optional.\n  logit_bias: dict | None. Biases for\
      \ logits of the model. Optional.\n  user: str | None. User identifier for the\
      \ request. Optional.\n  response_format: dict | type BaseModel | None. Response\
      \ format specification. Optional.\n  seed: int | None. Random seed for reproducibility.\
      \ Optional.\n  tools: list | None. Tools to be used in processing the request.\
      \ Optional.\n  tool_choice: str | dict | None. Tool selection information. Optional.\n\
      \  logprobs: bool | None. Whether to include log probabilities in the result.\
      \ Optional.\n  top_logprobs: int | None. Number of top log probabilities to\
      \ return. Optional.\n  parallel_tool_calls: bool | None. Allow parallel tool\
      \ invocations. Optional.\n  web_search_options: OpenAIWebSearchOptions | None.\
      \ Options for web search augmentation. Optional.\n  deployment_id: any. Deployment\
      \ identifier. Optional.\n  extra_headers: dict | None. Additional HTTP headers\
      \ to include. Optional.\n  functions: list | None. Deprecated parameter. Use\
      \ function_call instead. Optional and deprecated.\n  function_call: str | None.\
      \ Behavior for function calling (eg, auto, none, or function name). Optional.\n\
      \  thinking: AnthropicThinkingParam | None. Thinking constraints for the underlying\
      \ model. Optional.\n  kwargs: Any. Additional keyword arguments forwarded to\
      \ the API. Optional.\n\nReturns:\n  ModelResponse | CustomStreamWrapper. The\
      \ chat completion result or a streaming wrapper for partial results.\n\nRaises:\n\
      \  Exceptions raised by the underlying litellm/OpenAI API clients during request\
      \ handling.\n\nExample:\n  # Non-streaming call\n  ac = AFixedModelCompletion(...)\n\
      \  result = await ac(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n\
      \n  # Streaming call\n  stream = ac(messages=[{\"role\": \"user\", \"content\"\
      : \"Tell me a story.\"}], stream=True)\n  async for chunk in stream:\n     \
      \ # process streaming chunks\n      pass"
    methods:
    - name: __call__
      signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
        \ params: see https://platform.openai.com/docs/api-reference/chat/create\n\
        \        messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
        \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
        \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
        \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
        \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
        \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n\
        \        logit_bias: dict | None = None,  # type: ignore\n        user: str\
        \ | None = None,\n        # openai v1.0+ new params\n        response_format:\
        \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int\
        \ | None = None,\n        tools: list | None = None,  # type: ignore\n   \
        \     tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
        \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
        \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions\
        \ | None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
        \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
        \ by OpenAI\n        functions: list | None = None,  # type: ignore\n    \
        \    function_call: str | None = None,\n        # Optional liteLLM function\
        \ params\n        thinking: AnthropicThinkingParam | None = None,\n      \
        \  **kwargs: Any,\n    ) -> ModelResponse | CustomStreamWrapper"
  - class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding
    name: AFixedModelEmbedding
    docstring: "Callable interface to obtain embeddings via a fixed Litellm model.\n\
      \nThis class stores the configuration for a pre-selected model and exposes a\
      \ callable interface (__call__) that returns an EmbeddingResponse from the underlying\
      \ Litellm/OpenAI service. Key configuration attributes include the selected\
      \ model parameters and per-request settings (timeouts, API credentials, and\
      \ caching) used when requesting embeddings.\n\nArgs:\n  request_id: str | None\
      \ (default: None)\n      Optional request identifier.\n  input: list (default:\
      \ [])\n      List input to embed.\n  dimensions: int | None (default: None)\n\
      \      Optional embedding dimensions.\n  encoding_format: str | None (default:\
      \ None)\n      Optional encoding format.\n  timeout: int (default: 600)\n  \
      \    Timeout in seconds for the request.\n  api_base: str | None (default: None)\n\
      \      Optional API base.\n  api_version: str | None (default: None)\n     \
      \ Optional API version.\n  api_key: str | None (default: None)\n      Optional\
      \ API key.\n  api_type: str | None (default: None)\n      Optional API type.\n\
      \  caching: bool (default: False)\n      Whether to enable caching.\n  user:\
      \ str | None (default: None)\n      Optional user.\n  **kwargs: Any\n      Additional\
      \ keyword arguments forwarded to the embedding service.\n\nReturns:\n  EmbeddingResponse\n\
      \      The embedding response as produced by the underlying service (type alias\
      \ to CreateEmbeddingResponse in litellm).\n\nRaises:\n  Exception\n      Exceptions\
      \ raised by the embedding service or underlying libraries."
    methods:
    - name: __call__
      signature: "def __call__(\n        self,\n        *,\n        request_id: str\
        \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
        \        # Optional params\n        dimensions: int | None = None,\n     \
        \   encoding_format: str | None = None,\n        timeout: int = 600,  # default\
        \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
        \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
        \ str | None = None,\n        api_type: str | None = None,\n        caching:\
        \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
        \    ) -> EmbeddingResponse"
  - class_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion
    name: FixedModelCompletion
    docstring: "FixedModelCompletion is a fixed-model chat completion provider backed\
      \ by Litellm. It exposes a callable interface that forwards a set of chat completion\
      \ parameters to the underlying service and returns either the model's final\
      \ response or a streaming wrapper when streaming is requested.\n\nPurpose\n\
      - Provide a lightweight, fixed-model completion integration built on Litellm\
      \ for generating chat completions against a predetermined model.\n- Offer a\
      \ simple __call__ interface that mirrors the underlying client while surfacing\
      \ the fixed-model semantics.\n\nKey behavior\n- Returns either a ModelResponse\
      \ when streaming is not enabled, or a CustomStreamWrapper when stream is True,\
      \ allowing incremental consumption of results.\n- Delegates all additional keyword\
      \ arguments to the underlying client via kwargs, enabling access to the full\
      \ range of Litellm/OpenAI options.\n\nArgs\n- messages: List of ChatCompletion\
      \ messages to include in the request. Defaults to an empty list.\n- stream:\
      \ If True, stream partial responses as they arrive. Defaults to None (non-streaming).\n\
      - stream_options: Options for streaming.\n- stop: Stop sequence or token.\n\
      - max_completion_tokens: Maximum number of tokens in the completion.\n- max_tokens:\
      \ Maximum tokens for the response.\n- modalities: Modality to use for the completion.\n\
      - prediction: Prediction content parameter.\n- audio: Audio parameter for audio-enabled\
      \ modes.\n- logit_bias: Token bias mapping.\n- user: User identifier for the\
      \ request.\n- response_format: OpenAI v1.0+ response format parameter; may be\
      \ a dict or a model type.\n- seed: Random seed for deterministic behavior.\n\
      - tools: Tools to be used during the completion.\n- tool_choice: Specific tool\
      \ choice or mapping.\n- logprobs: Whether to return log probabilities.\n- top_logprobs:\
      \ Number of top logprobs to return.\n- parallel_tool_calls: Whether to perform\
      \ tool calls in parallel.\n- web_search_options: Web search options for information\
      \ retrieval.\n- deployment_id: Deployment identifier for the model.\n- extra_headers:\
      \ Additional headers to include in the request.\n- functions: OpenAI function\
      \ definitions (legacy).\n- function_call: Function call type or directive.\n\
      - thinking: AnthropicThinkingParam to influence model thinking behavior.\n-\
      \ kwargs: Additional keyword arguments passed through to the underlying client.\n\
      \nReturns\n- ModelResponse: The model's response when streaming is not enabled.\n\
      - CustomStreamWrapper: A streaming wrapper that yields results when streaming\
      \ is enabled.\n\nRaises\n- Propagates exceptions raised by the underlying Litellm/OpenAI\
      \ clients (e.g., network errors, validation errors) to surface service failures.\n\
      \nExamples\n- Non-streaming use:\n  FixedModelCompletion(...)(messages=[...])\n\
      - Streaming use:\n  stream = FixedModelCompletion(...)(messages=[...], stream=True)\n\
      \  for chunk in stream:\n      ..."
    methods:
    - name: __call__
      signature: "def __call__(\n        self,\n        *,\n        messages: list\
        \ = [],  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n\
        \        stream_options: dict | None = None,  # type: ignore\n        stop=None,\
        \  # type: ignore\n        max_completion_tokens: int | None = None,\n   \
        \     max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
        \ | None = None,\n        prediction: ChatCompletionPredictionContentParam\
        \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n\
        \        logit_bias: dict | None = None,  # type: ignore\n        user: str\
        \ | None = None,\n        # openai v1.0+ new params\n        response_format:\
        \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int\
        \ | None = None,\n        tools: list | None = None,  # type: ignore\n   \
        \     tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
        \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
        \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions\
        \ | None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
        \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
        \ by OpenAI\n        functions: list | None = None,  # type: ignore\n    \
        \    function_call: str | None = None,\n        # Optional liteLLM function\
        \ params\n        thinking: AnthropicThinkingParam | None = None,\n      \
        \  **kwargs: Any,\n    ) -> ModelResponse | CustomStreamWrapper"
  - class_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc
    name: AsyncLitellmRequestFunc
    docstring: "AsyncLitellmRequestFunc is a callable wrapper around an asynchronous\
      \ Litellm request function used for chat completions or embeddings. It forwards\
      \ all provided keyword arguments to the underlying request function, enabling\
      \ flexible use with different backends.\n\nArgs:\n    kwargs: Arbitrary keyword\
      \ arguments forwarded to the underlying request function. Specific accepted\
      \ keys depend on the concrete implementation.\n\nReturns:\n    The result produced\
      \ by the underlying request function.\n\nRaises:\n    Exception: Exceptions\
      \ raised by the underlying request function."
    methods:
    - name: __call__
      signature: 'def __call__(self, /, **kwargs: Any) -> Any'
  - class_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding
    name: FixedModelEmbedding
    docstring: "FixedModelEmbedding: Synchronous embedding function for a pre-configured\
      \ model.\n\nThis class provides an embedding interface that uses a pre-configured\
      \ model. It computes embeddings for a batch of inputs using the configured model\
      \ and does not require a model parameter. It mirrors litellm.embedding but omits\
      \ the model argument, relying on the model configuration.\n\nArgs:\n  request_id:\
      \ Optional request identifier.\n  input: List of inputs to embed.\n  dimensions:\
      \ Embedding dimensionality to request (optional).\n  encoding_format: Encoding\
      \ format to return (optional).\n  timeout: Timeout for the embedding request\
      \ in seconds (default to 600, i.e., 10 minutes).\n  api_base: API base URL override\
      \ (optional).\n  api_version: API version override (optional).\n  api_key: API\
      \ key override (optional).\n  api_type: API type override (optional).\n  caching:\
      \ Enable/disable caching.\n  user: User identifier (optional).\n  **kwargs:\
      \ Additional keyword arguments.\n\nReturns:\n  EmbeddingResponse: The embedding\
      \ response containing the computed embeddings.\n\nRaises:\n  Exception: If an\
      \ error occurs during embedding computation or API calls (propagates from underlying\
      \ libraries)."
    methods:
    - name: __call__
      signature: "def __call__(\n        self,\n        *,\n        request_id: str\
        \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
        \        # Optional params\n        dimensions: int | None = None,\n     \
        \   encoding_format: str | None = None,\n        timeout: int = 600,  # default\
        \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
        \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
        \ str | None = None,\n        api_type: str | None = None,\n        caching:\
        \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
        \    ) -> EmbeddingResponse"
  - class_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc
    name: LitellmRequestFunc
    docstring: "Synchronous request function for Litellm that forwards calls to the\
      \ underlying request function, capable of handling either chat completion or\
      \ embedding.\n\nArgs:\n    kwargs: Arbitrary keyword arguments forwarded to\
      \ the underlying request function. Specific accepted keys depend on the concrete\
      \ implementation.\n\nReturns:\n    Any: The result of the underlying request\
      \ function.\n\nRaises:\n    Exception: Exceptions raised by the underlying request\
      \ function."
    methods:
    - name: __call__
      signature: 'def __call__(self, /, **kwargs: Any) -> Any'
- file: graphrag/language_model/response/base.py
  functions:
  - node_id: graphrag/language_model/response/base.py::ModelResponse.output
    name: output
    signature: def output(self) -> ModelOutput
    docstring: "Return the output of the response. This is a property-like member\
      \ of the response object (no parentheses needed).\n\nReturns:\n    ModelOutput:\
      \ The output associated with this response. The returned object provides:\n\
      \        content: str - The textual content of the output.\n        full_response:\
      \ dict[str, Any] | None - The complete JSON response from the LLM provider,\
      \ or None if not available.\n\nNotes:\n    - The ModelOutput is always produced;\
      \ accessing this property does not raise exceptions in normal operation.\n \
      \   - If content is empty, the content field may be an empty string."
  - node_id: graphrag/language_model/response/base.py::ModelResponse.history
    name: history
    signature: def history(self) -> list
    docstring: "Return the history of the response.\n\nReturns:\n    list[Any]: The\
      \ history of the response."
  - node_id: graphrag/language_model/response/base.py::ModelResponse.parsed_response
    name: parsed_response
    signature: def parsed_response(self) -> T | None
    docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The\
      \ instance of the implementing class providing the parsed_response property.\n\
      \nReturns:\n    T | None: The parsed response, or None if not available.\n\n\
      Raises:\n    None..."
  - node_id: graphrag/language_model/response/base.py::ModelOutput.content
    name: content
    signature: def content(self) -> str
    docstring: "Return the textual content of the output.\n\nReturns:\n    str: The\
      \ textual content of the output."
  - node_id: graphrag/language_model/response/base.py::ModelOutput.full_response
    name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
    docstring: "\"\"\"Return the complete JSON response returned by the model.\n\n\
      Args:\n    self: The instance from which the full_response is accessed.\n\n\
      Returns:\n    dict[str, Any] | None: The complete JSON response returned by\
      \ the model.\n\n\"\"\""
  classes:
  - class_id: graphrag/language_model/response/base.py::ModelResponse
    name: ModelResponse
    docstring: "ModelResponse is a generic container for responses from an LLM provider.\
      \ It encapsulates the textual output, the provider's full JSON response, and\
      \ an optional parsed model instance along with a history of responses. The class\
      \ is parameterized by T, a subclass of BaseModel, representing a typed interpretation\
      \ of the response.\n\nAttributes:\n  output (ModelOutput): The output associated\
      \ with this response. Access the textual content via output.content and the\
      \ complete provider JSON via output.full_response (dict[str, Any] or None).\n\
      \  history (list[Any]): The history of this response as a list of entries.\n\
      \  parsed_response (T | None): The parsed model instance of type T, or None\
      \ if not available.\n\nNotes:\n- ModelOutput is a separate type that wraps the\
      \ textual content and the full provider JSON.\n- This docstring describes class-level\
      \ behavior and fields; initialization parameters, if any, are defined in the\
      \ implementation."
    methods:
    - name: output
      signature: def output(self) -> ModelOutput
    - name: history
      signature: def history(self) -> list
    - name: parsed_response
      signature: def parsed_response(self) -> T | None
  - class_id: graphrag/language_model/response/base.py::ModelOutput
    name: ModelOutput
    docstring: "ModelOutput encapsulates the textual content of a language model's\
      \ output along with the complete raw JSON response returned by the LLM provider.\n\
      \nPurpose:\n    Provide convenient access to both the human-readable content\
      \ and the full provider response for debugging and downstream processing.\n\n\
      Returns:\n    content() -> str: The textual content of the output.\n    full_response()\
      \ -> dict[str, Any] | None: The complete JSON response returned by the model."
    methods:
    - name: content
      signature: def content(self) -> str
    - name: full_response
      signature: def full_response(self) -> dict[str, Any] | None
- file: graphrag/language_model/response/base.pyi
  functions:
  - node_id: graphrag/language_model/response/base.pyi::ModelOutput.full_response
    name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
    docstring: "Return the full response payload as a dictionary, or None if not available.\n\
      \nArgs:\n    self: The instance of the class.\n\nReturns:\n    dict[str, Any]\
      \ | None: The full response payload as a dictionary, or None if not present."
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.parsed_response
    name: parsed_response
    signature: def parsed_response(self) -> _T | None
    docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The\
      \ instance of the implementing class providing the parsed_response property.\n\
      \nReturns:\n    _T | None: The parsed response, or None if not available.\n\n\
      Raises:\n    None"
  - node_id: graphrag/language_model/response/base.pyi::ModelOutput.content
    name: content
    signature: def content(self) -> str
    docstring: "Content of the model output as a string.\n\nReturns:\n    str: The\
      \ content of the model output as a string."
  - node_id: graphrag/language_model/response/base.pyi::BaseModelOutput.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
      \ dict[str, Any] | None = None,\n    ) -> None"
    docstring: "BaseModelOutput initialization.\n\nInitializes a BaseModelOutput with\
      \ the given content and optional full_response.\n\nArgs:\n    content: The output\
      \ content as a string.\n    full_response: Optional dict[str, Any] representing\
      \ the full response; defaults to None.\n\nReturns:\n    None"
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.history
    name: history
    signature: def history(self) -> list[Any]
    docstring: "History of the model response.\n\nReturns\n    list[Any]: The history\
      \ as a list of items."
  - node_id: graphrag/language_model/response/base.pyi::BaseModelResponse.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n \
      \       parsed_response: _T | None = None,\n        history: list[Any] = ...,\
      \  # default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
      \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
      \ bool | None = None,\n    ) -> None"
    docstring: "Initializes a BaseModelResponse with the given output, parsed_response,\
      \ and optional metadata.\n\nArgs:\n    output: BaseModelOutput\n        BaseModelOutput\
      \ instance containing the content and full_response.\n    parsed_response: _T\
      \ | None\n        The parsed response of type _T, or None.\n    history: list[Any]\n\
      \        History list; default provided by Pydantic.\n    tool_calls: list[Any]\n\
      \        Tool calls list; default provided by Pydantic.\n    metrics: Any |\
      \ None\n        Metrics associated with the response; may be None.\n    cache_hit:\
      \ bool | None\n        Indicates whether a cache hit occurred; may be None.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.output
    name: output
    signature: def output(self) -> ModelOutput
    docstring: "Return the ModelOutput for this response.\n\nArgs:\n    self: The\
      \ response object.\n\nReturns:\n    ModelOutput: The output associated with\
      \ this response. The returned object provides:\n        content: str - The textual\
      \ content of the output.\n        full_response: dict[str, Any] | None - The\
      \ complete JSON response from the LLM provider, or None if not available."
  classes:
  - class_id: graphrag/language_model/response/base.pyi::ModelOutput
    name: ModelOutput
    docstring: "ModelOutput: Represents the outcome produced by a language model,\
      \ providing access to the textual content and the complete raw payload when\
      \ available.\n\nPurpose:\n  Encapsulates model output data and provides convenient\
      \ accessors to the core content and the full payload.\n\nMethods:\n  content()\
      \ -> str\n    Returns the textual content of the model output as a string.\n\
      \n  full_response() -> dict[str, Any] | None\n    Returns the full raw response\
      \ payload as a dictionary, or None if available.\n\nNotes:\n  These are methods\
      \ (not attributes). Access data by calling content() and full_response() on\
      \ the instance."
    methods:
    - name: full_response
      signature: def full_response(self) -> dict[str, Any] | None
    - name: content
      signature: def content(self) -> str
  - class_id: graphrag/language_model/response/base.pyi::ModelResponse
    name: ModelResponse
    docstring: "Protocol describing a model response produced by the GraphRAG language\
      \ model integration. This Protocol exposes three properties: parsed_response,\
      \ history, and output, and is generic over _T, the type of the parsed response.\n\
      \nType parameters:\n- _T: The type of the parsed_response value.\n\nAttributes:\n\
      - parsed_response: _T | None \u2014 The parsed response, or None if not available.\n\
      - history: list[Any] \u2014 The history of the model responses as a list of\
      \ items.\n- output: ModelOutput \u2014 The structured output for this response.\
      \ ModelOutput is defined elsewhere and typically exposes:\n  - content: str\
      \ \u2014 The textual content of the output.\n  - full_response: dict[str, Any]\
      \ | None \u2014 The full JSON response from the LLM provider, or None if not\
      \ available."
    methods:
    - name: parsed_response
      signature: def parsed_response(self) -> _T | None
    - name: history
      signature: def history(self) -> list[Any]
    - name: output
      signature: def output(self) -> ModelOutput
  - class_id: graphrag/language_model/response/base.pyi::BaseModelOutput
    name: BaseModelOutput
    docstring: "BaseModelOutput stores the result produced by a language model, including\
      \ the main content and an optional full_response payload. Key attributes: content,\
      \ full_response.\n\nArgs:\n    content: The output content as a string.\n  \
      \  full_response: Optional dict[str, Any] representing the full response; defaults\
      \ to None.\n\nReturns:\n    None"
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
        \ dict[str, Any] | None = None,\n    ) -> None"
  - class_id: graphrag/language_model/response/base.pyi::BaseModelResponse
    name: BaseModelResponse
    docstring: "BaseModelResponse is a generic container for the response produced\
      \ by a base language model. It pairs the raw model output with optional parsed\
      \ content and related metadata, and is parameterized by the type _T of the parsed\
      \ response. The actual raw content is held in output (BaseModelOutput), while\
      \ parsed_response holds a typed interpretation if available.\n\nArgs:\n    output:\
      \ BaseModelOutput\n        BaseModelOutput instance containing the content and\
      \ full_response produced by the base model.\n    parsed_response: _T | None\n\
      \        The parsed response of type _T, or None if no parsing was performed.\n\
      \    history: list[Any]\n        History list related to the interaction; defaults\
      \ are provided by the Pydantic framework.\n    tool_calls: list[Any]\n     \
      \   Tool calls associated with the response; defaults are provided by the Pydantic\
      \ framework.\n    metrics: Any | None\n        Optional metrics about the response\
      \ (e.g., latency, resource usage).\n    cache_hit: bool | None\n        Indicates\
      \ whether a cached response was used, if applicable.\n\nAttributes:\n    output:\
      \ BaseModelOutput\n        Raw model output content and full_response.\n   \
      \ parsed_response: _T | None\n        Parsed content of type _T, if available.\n\
      \    history: list[Any]\n        Interaction history;\n    tool_calls: list[Any]\n\
      \        Tool calls associated with the response.\n    metrics: Any | None\n\
      \        Optional metrics about the response.\n    cache_hit: bool | None\n\
      \        Whether a cached response was used.\n\nReturns:\n    None. The constructor\
      \ is provided by the Pydantic BaseModel superclass; this class does not implement\
      \ a custom __init__.\n\nRaises:\n    None"
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n\
        \        parsed_response: _T | None = None,\n        history: list[Any] =\
        \ ...,  # default provided by Pydantic\n        tool_calls: list[Any] = ...,\
        \  # default provided by Pydantic\n        metrics: Any | None = None,\n \
        \       cache_hit: bool | None = None,\n    ) -> None"
- file: graphrag/logger/blob_workflow_logger.py
  functions:
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._write_log
    name: _write_log
    signature: 'def _write_log(self, log: dict[str, Any])'
    docstring: "Write log data to blob storage.\n\nThis method appends the provided\
      \ log data as a JSON-formatted line to the blob, and reinitializes the internal\
      \ client when the accumulated block count reaches the configured maximum.\n\n\
      Args:\n    log: dict[str, Any] Log data to serialize as JSON and append as a\
      \ line in the blob.\n\nReturns:\n    None\n\nRaises:\n    OSError: If an I/O\
      \ error occurs during blob operations or during client reinitialization.\n \
      \   ValueError: If the log data cannot be serialized to JSON."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._get_log_type
    name: _get_log_type
    signature: 'def _get_log_type(self, level: int) -> str'
    docstring: "Get log type string for a given numeric log level.\n\nArgs:\n    level:\
      \ int - The numeric log level (e.g., logging.INFO, logging.WARNING, logging.ERROR).\n\
      \nReturns:\n    str - The log type: \"error\" if level >= logging.ERROR, \"\
      warning\" if level >= logging.WARNING, otherwise \"log\"."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
      \        container_name: str | None,\n        blob_name: str = \"\",\n     \
      \   base_dir: str | None = None,\n        storage_account_blob_url: str | None\
      \ = None,\n        level: int = logging.NOTSET,\n    )"
    docstring: "Create a new instance of the BlobWorkflowLogger class.\n\nArgs:\n\
      \  connection_string: Connection string for the blob storage, or None\n  container_name:\
      \ Name of the blob container\n  blob_name: Name of the blob to create; if empty,\
      \ a timestamped default will be used\n  base_dir: Base directory to prepend\
      \ to the blob name, or None\n  storage_account_blob_url: URL of the storage\
      \ account blob service, or None\n  level: Logging level\n\nReturns:\n  None\n\
      \nRaises:\n  ValueError: No container name provided for blob storage.\n  ValueError:\
      \ No storage account blob url provided for blob storage.\n  ValueError: Either\
      \ connection_string or storage_account_blob_url must be provided."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.emit
    name: emit
    signature: def emit(self, record) -> None
    docstring: "Emit a log record to blob storage.\n\nCreates a JSON structure from\
      \ the given log record, including type (\"log\", \"warning\", or \"error\")\
      \ based on the record level, and the main message. Optional fields such as details,\
      \ cause (from exc_info), and stack (if present) are added if they exist. The\
      \ resulting payload is passed to _write_log for persistence in Azure Blob storage.\
      \ If writing fails with OSError or ValueError, those exceptions are not propagated;\
      \ they are handled by self.handleError(record). This method is part of the BlobWorkflowLogger\
      \ class and interacts with _write_log and the logic that reinitializes the blob\
      \ client when the block counter reaches the maximum.\n\nArgs:\n    record: logging.LogRecord\
      \ The log record to emit to blob storage.\n\nReturns:\n    None\n\nRaises:\n\
      \    OSError: Not raised; errors are caught and delegated to self.handleError(record)\
      \ instead.\n    ValueError: Not raised; errors are caught and delegated to self.handleError(record)\
      \ instead."
  classes:
  - class_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger
    name: BlobWorkflowLogger
    docstring: "Blob-based workflow logger that persists log records to Azure Blob\
      \ storage as JSON lines.\n\nSummary:\nThe BlobWorkflowLogger is a logging handler\
      \ that formats records into JSON payloads and appends them as lines to a blob\
      \ in an Azure Storage container. It supports categorizing log entries by type\
      \ (log, warning, error) and reinitializes its internal client when the accumulation\
      \ reaches a configured maximum.\n\nArgs:\n    connection_string: Connection\
      \ string for the blob storage, or None\n    container_name: Name of the blob\
      \ container\n    blob_name: Name of the blob to create; if empty, a timestamped\
      \ default will be used\n    base_dir: Base directory to prepend to the blob\
      \ name, or None\n    storage_account_blob_url: URL of the storage account blob\
      \ service, or None\n    level: Logging level threshold (default NOTSET)\n\n\
      Returns:\n    None\n\nRaises:\n    OSError: If an I/O error occurs during blob\
      \ operations or persistence"
    methods:
    - name: _write_log
      signature: 'def _write_log(self, log: dict[str, Any])'
    - name: _get_log_type
      signature: 'def _get_log_type(self, level: int) -> str'
    - name: __init__
      signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
        \        container_name: str | None,\n        blob_name: str = \"\",\n   \
        \     base_dir: str | None = None,\n        storage_account_blob_url: str\
        \ | None = None,\n        level: int = logging.NOTSET,\n    )"
    - name: emit
      signature: def emit(self, record) -> None
- file: graphrag/logger/factory.py
  functions:
  - node_id: graphrag/logger/factory.py::LoggerFactory.create_logger
    name: create_logger
    signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
    docstring: "Create a logger handler for the requested type using the built-in\
      \ registry.\n\nThis method looks up the given reporting_type in the internal\
      \ registry and invokes the registered\ncreator with the provided kwargs to create\
      \ and return a logging.Handler instance.\n\nArgs:\n    reporting_type (str):\
      \ The type identifier of the logger/handler to create.\n    kwargs (dict): Keyword\
      \ arguments forwarded to the registered creator to configure the handler.\n\n\
      Returns:\n    logging.Handler: The configured handler instance for the requested\
      \ type.\n\nRaises:\n    ValueError: If the reporting_type is not registered\
      \ in the registry."
  - node_id: graphrag/logger/factory.py::LoggerFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
    docstring: "Check if the given logger type is supported.\n\nArgs:\n    cls: The\
      \ class reference (classmethod parameter).\n    reporting_type (str): The type\
      \ identifier for the logger.\n\nReturns:\n    bool: True if the reporting type\
      \ is registered in the registry, False otherwise."
  - node_id: graphrag/logger/factory.py::LoggerFactory.register
    name: register
    signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
      \ logging.Handler]\n    ) -> None"
    docstring: "Register a custom logger implementation.\n\nThis is a classmethod\
      \ on LoggerFactory. It updates the internal registry (cls._registry) by storing\
      \ a mapping from the provided reporting_type to the given creator callable.\
      \ The registry is consulted by create_logger to instantiate loggers for the\
      \ requested type.\n\nArgs:\n    reporting_type: The type identifier for the\
      \ logger.\n    creator: A class or callable that initializes and returns a logging.Handler\
      \ instance.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/logger/factory.py::create_file_logger
    name: create_file_logger
    signature: def create_file_logger(**kwargs) -> logging.Handler
    docstring: "Create a file-based logger handler.\n\nArgs:\n    root_dir: The root\
      \ directory under which logs are stored.\n    base_dir: The base directory under\
      \ root_dir where logs are written.\n    filename: The log filename to use for\
      \ the log file.\n\nReturns:\n    logging.Handler: A configured handler writing\
      \ to the specified log file.\n\nRaises:\n    KeyError: If required keys (root_dir,\
      \ base_dir, filename) are missing in kwargs.\n    OSError: If the log directory\
      \ cannot be created or the log file cannot be opened."
  - node_id: graphrag/logger/factory.py::LoggerFactory.get_logger_types
    name: get_logger_types
    signature: def get_logger_types(cls) -> list[str]
    docstring: "\"\"\"Get the registered logger implementations.\n\nArgs:\n    cls:\
      \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
      \ The list of registered logger implementation names.\n\"\"\""
  - node_id: graphrag/logger/factory.py::create_blob_logger
    name: create_blob_logger
    signature: def create_blob_logger(**kwargs) -> logging.Handler
    docstring: "Create a blob storage-based logger.\n\nArgs:\n    kwargs: The keyword\
      \ arguments for configuring the blob logger.\n        connection_string: The\
      \ Azure Blob Storage connection string.\n        container_name: The name of\
      \ the blob container.\n        base_dir: The base directory inside the container\
      \ where logs should be stored.\n        storage_account_blob_url: The URL of\
      \ the blob storage account used by the logger.\n\nReturns:\n    logging.Handler:\
      \ A configured BlobWorkflowLogger instance.\n\nRaises:\n    KeyError: If required\
      \ keys (connection_string, container_name, base_dir, storage_account_blob_url)\
      \ are missing from kwargs."
  classes:
  - class_id: graphrag/logger/factory.py::LoggerFactory
    name: LoggerFactory
    docstring: "LoggerFactory is a registry-based factory for creating logging.Handler\
      \ instances for various reporting types.\n\nPurpose:\n- Maintain an internal\
      \ registry mapping reporting_type identifiers to creator callables.\n- Provide\
      \ a classmethod-based interface to register new loggers, check supported types,\
      \ create loggers for a given type, and retrieve the set of available types.\n\
      \nAttributes:\n- _registry: ClassVar[dict[str, Callable[..., logging.Handler]]]\n\
      \    Internal registry that maps a reporting_type string to a creator callable\
      \ that returns a logging.Handler instance when invoked with appropriate keyword\
      \ arguments.\n\nSummary:\nThe class acts as a centralized factory and registry\
      \ for logger handlers. Client code can register new logger implementations,\
      \ query supported types, and request a handler for a specific reporting type.\
      \ All operations are performed at the class level."
    methods:
    - name: create_logger
      signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
    - name: is_supported_type
      signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
    - name: register
      signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
        \ logging.Handler]\n    ) -> None"
    - name: get_logger_types
      signature: def get_logger_types(cls) -> list[str]
- file: graphrag/logger/progress.py
  functions:
  - node_id: graphrag/logger/progress.py::ProgressTicker.__call__
    name: __call__
    signature: 'def __call__(self, num_ticks: int = 1) -> None'
    docstring: "Emit progress.\n\nArgs:\n    num_ticks (int): Number of ticks to advance\
      \ the progress.\n\nReturns:\n    None: This method updates internal counters\
      \ and, if a callback is set, notifies it with a Progress object."
  - node_id: graphrag/logger/progress.py::progress_ticker
    name: progress_ticker
    signature: "def progress_ticker(\n    callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n) -> ProgressTicker"
    docstring: "Create a progress ticker.\n\nArgs:\n    callback: ProgressHandler\
      \ | None\n        Optional callback to be invoked with Progress updates.\n \
      \   num_total: int\n        Total number of items to track progress for.\n \
      \   description: str\n        Optional description to accompany the progress\
      \ updates.\n\nReturns:\n    ProgressTicker\n        A ProgressTicker instance\
      \ configured with the provided callback, total, and description."
  - node_id: graphrag/logger/progress.py::ProgressTicker.__init__
    name: __init__
    signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n    )"
    docstring: "Initialize a ProgressTicker with the provided callback, total items,\
      \ and optional description.\n\nArgs:\n    callback: ProgressHandler | None\n\
      \        A function to handle progress reports, or None to disable updates.\n\
      \    num_total: int\n        Total number of items to track.\n    description:\
      \ str\n        Optional description for the progress updates.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/logger/progress.py::ProgressTicker.done
    name: done
    signature: def done(self) -> None
    docstring: "Mark the progress as done.\n\nIf a callback was provided (self._callback\
      \ is not None), invoke it with a Progress object whose total_items equals the\
      \ configured total (self._num_total) and whose completed_items equals the same\
      \ value, indicating completion. The description from initialization (self._description)\
      \ is preserved.\n\nThis is a bound method; the self parameter is implicit and\
      \ not documented.\n\nReturns:\n    None"
  - node_id: graphrag/logger/progress.py::progress_iterable
    name: progress_iterable
    signature: "def progress_iterable(\n    iterable: Iterable[T],\n    progress:\
      \ ProgressHandler | None,\n    num_total: int | None = None,\n    description:\
      \ str = \"\",\n) -> Iterable[T]"
    docstring: "Wrap an iterable with a progress reporter. After each item is yielded,\
      \ the progress callback will be called with a Progress object describing the\
      \ total number of items and how many have been completed so far. If the callback\
      \ is None, no updates will be emitted.\n\nThe Progress object provides:\n- total_items:\
      \ total number of items (as given by num_total)\n- completed_items: number of\
      \ items yielded so far\n- description: optional description included with updates\n\
      \nNote: If num_total is None, the total will be inferred by consuming the iterable\
      \ (via list(iterable)), which may exhaust inputs that cannot be re-iterated.\
      \ To avoid this, pass a known num_total or ensure the iterable can be iterated\
      \ multiple times.\n\nArgs:\n    iterable (Iterable[T]): The input iterable to\
      \ wrap. Each item yielded is unchanged.\n    progress (ProgressHandler | None):\
      \ Callback invoked with a Progress instance after each item is yielded. If None,\
      \ updates are suppressed.\n    num_total (int | None): Total number of items.\
      \ If None, inferred by consuming the iterable (may exhaust it).\n    description\
      \ (str): Optional description to attach to progress updates.\n\nReturns:\n \
      \   Iterable[T]: A generator that yields the same items as the input iterable,\
      \ while updating progress after each item is yielded."
  classes:
  - class_id: graphrag/logger/progress.py::ProgressTicker
    name: ProgressTicker
    docstring: 'ProgressTicker tracks progress toward a known total and optionally
      notifies a callback with progress updates.


      This class maintains an internal counter of completed items out of a known total
      and, on demand, emits progress events to an optional callback. A progress event
      is represented as an object with at least total_items and completed_items fields,
      and may also include the description provided at construction.


      Args:

      - callback: ProgressHandler | None - A function to handle progress reports,
      or None to disable updates. Default: None. The callback receives a progress-like
      object describing current progress.

      - num_total: int - Total number of items to track.

      - description: str - Optional description for the progress updates. Default:
      "".


      Returns:

      - None


      Raises:

      - None


      Attributes:

      - _callback: ProgressHandler | None - The function to call with progress updates,
      or None to suppress updates.

      - _num_total: int - Total number of items to track.

      - _description: str - Optional description for display.

      - _completed_items: int - Count of completed ticks.


      Behavior:

      - __call__(self, num_ticks: int = 1) - Advances the internal counter by num_ticks.
      If a callback is set, invokes it with a progress-like object containing total_items
      (equal to _num_total) and completed_items (updated value).

      - done(self) - Marks the progress as done. If a callback is provided, invokes
      it with a progress-like object whose total_items and completed_items both equal
      _num_total, preserving the description from initialization.


      Example:

      A minimal usage example:

      Define a callback function that prints the progress, create a ProgressTicker
      with a total, and advance it by calling the instance. Call done() to report
      completion.'
    methods:
    - name: __call__
      signature: 'def __call__(self, num_ticks: int = 1) -> None'
    - name: __init__
      signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
        \ int, description: str = \"\"\n    )"
    - name: done
      signature: def done(self) -> None
- file: graphrag/logger/standard_logging.py
  functions:
  - node_id: graphrag/logger/standard_logging.py::init_loggers
    name: init_loggers
    signature: "def init_loggers(\n    config: GraphRagConfig,\n    verbose: bool\
      \ = False,\n    filename: str = DEFAULT_LOG_FILENAME,\n) -> None"
    docstring: "Initialize logging for graphrag based on configuration.\n\nConfigures\
      \ the top-level 'graphrag' logger with a handler derived from the provided GraphRagConfig.\
      \ It sets the log level to DEBUG when verbose is True, otherwise INFO. Before\
      \ attaching the new handler, all existing handlers on the logger are removed;\
      \ any FileHandler instances are closed to avoid resource leaks and duplicate\
      \ logs.\n\nArgs:\n    config (GraphRagConfig): The GraphRagConfig instance providing\
      \ logging settings (including reporting and root_dir).\n    verbose (bool):\
      \ If True, enable DEBUG logging; otherwise INFO.\n    filename (str): The log\
      \ filename on disk. If not provided, defaults to DEFAULT_LOG_FILENAME.\n\nReturns:\n\
      \    None\n\nRaises:\n    Propagates exceptions from internal components (for\
      \ example, LoggerFactory.create_logger) if encountered."
  classes: []
- file: graphrag/prompt_tune/generator/community_report_rating.py
  functions:
  - node_id: graphrag/prompt_tune/generator/community_report_rating.py::generate_community_report_rating
    name: generate_community_report_rating
    signature: "def generate_community_report_rating(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str"
    docstring: "Generate a community report rating description using a language model.\n\
      \nArgs:\n    model (ChatModel): The LLM to use for generation\n    domain (str):\
      \ The domain to generate a rating for\n    persona (str): The persona to generate\
      \ a rating for\n    docs (str | list[str]): Documents used to contextualize\
      \ the rating\n\nReturns:\n    str: The generated rating description prompt response.\n\
      \nRaises:\n    Exception: If the underlying chat model call fails."
  classes: []
- file: graphrag/prompt_tune/generator/community_report_summarization.py
  functions:
  - node_id: graphrag/prompt_tune/generator/community_report_summarization.py::create_community_summarization_prompt
    name: create_community_summarization_prompt
    signature: "def create_community_summarization_prompt(\n    persona: str,\n  \
      \  role: str,\n    report_rating_description: str,\n    language: str,\n   \
      \ output_path: Path | None = None,\n) -> str"
    docstring: "Create a prompt for community summarization. If output_path is provided,\
      \ write the prompt to a file.\n\nArgs:\n    persona (str): The persona to use\
      \ for the community summarization prompt.\n    role (str): The role to use for\
      \ the community summarization prompt.\n    report_rating_description (str):\
      \ Description of the report rating to incorporate into the prompt.\n    language\
      \ (str): The language to use for the community summarization prompt.\n    output_path\
      \ (Path | None): The path to write the prompt to. If None, the prompt is not\
      \ written to a file. Defaults to None.\nReturns:\n    str: The community summarization\
      \ prompt."
  classes: []
- file: graphrag/prompt_tune/generator/community_reporter_role.py
  functions:
  - node_id: graphrag/prompt_tune/generator/community_reporter_role.py::generate_community_reporter_role
    name: generate_community_reporter_role
    signature: "def generate_community_reporter_role(\n    model: ChatModel, domain:\
      \ str, persona: str, docs: str | list[str]\n) -> str"
    docstring: "\"\"\"Generate a community reporter role for GraphRAG prompts.\n\n\
      Args:\n    model (ChatModel): The LLM to use for generation\n    domain (str):\
      \ The domain to generate a persona for\n    persona (str): The persona to generate\
      \ a role for\n    docs (str | list[str]): Documents to contextualize the persona;\
      \ if a list, these will be joined into a single string\n\nReturns:\n    str:\
      \ The generated domain prompt response content.\n\nRaises:\n    Exception: If\
      \ the underlying model call fails\n\"\"\""
  classes: []
- file: graphrag/prompt_tune/generator/domain.py
  functions:
  - node_id: graphrag/prompt_tune/generator/domain.py::generate_domain
    name: generate_domain
    signature: 'def generate_domain(model: ChatModel, docs: str | list[str]) -> str'
    docstring: "\"\"\"Generate an LLM persona to use for GraphRAG prompts.\n\nArgs:\n\
      \    model (ChatModel): The LLM to use for generation\n    docs (str | list[str]):\
      \ The domain to generate a persona for\n\nReturns:\n    str: The generated domain\
      \ prompt response.\n\nRaises:\n    Exception: If the underlying model call fails.\n\
      \"\"\""
  classes: []
- file: graphrag/prompt_tune/generator/entity_relationship.py
  functions:
  - node_id: graphrag/prompt_tune/generator/entity_relationship.py::generate_entity_relationship_examples
    name: generate_entity_relationship_examples
    signature: "def generate_entity_relationship_examples(\n    model: ChatModel,\n\
      \    persona: str,\n    entity_types: str | list[str] | None,\n    docs: str\
      \ | list[str],\n    language: str,\n    json_mode: bool = False,\n) -> list[str]"
    docstring: "Generate a list of entity/relationships examples for use in generating\
      \ an entity configuration.\n\nArgs:\n    model: ChatModel to use for generating\
      \ responses.\n    persona: Persona content used to seed the system history for\
      \ the chat.\n    entity_types: Optional entity types to guide generation. Can\
      \ be a string or a list of strings; if None, untyped prompts are generated.\n\
      \    docs: Documentation text to base the examples on. Can be a string or a\
      \ list of strings.\n    language: Target language for the prompts.\n    json_mode:\
      \ Whether to format the output as JSON (True) or as a tuple_delimiter format\
      \ (False).\n\nReturns:\n    list[str]: The generated examples as strings. If\
      \ json_mode is True, each string is a JSON-formatted example; otherwise the\
      \ examples are in tuple_delimiter format. Up to MAX_EXAMPLES items (5).\n\n\
      Raises:\n    Exceptions raised by the underlying model interactions (e.g., model.achat)\
      \ may propagate to the caller."
  classes: []
- file: graphrag/prompt_tune/generator/entity_summarization_prompt.py
  functions:
  - node_id: graphrag/prompt_tune/generator/entity_summarization_prompt.py::create_entity_summarization_prompt
    name: create_entity_summarization_prompt
    signature: "def create_entity_summarization_prompt(\n    persona: str,\n    language:\
      \ str,\n    output_path: Path | None = None,\n) -> str"
    docstring: "\"\"\"\nCreate a prompt for entity summarization.\n\nThe generated\
      \ prompt is created by formatting ENTITY_SUMMARIZATION_PROMPT with the provided\
      \ persona and language. If output_path is provided, the prompt is written to\
      \ a file named summarize_descriptions.txt within output_path, creating directories\
      \ as needed.\n\nArgs:\n    persona (str): The persona to use for the entity\
      \ summarization prompt\n    language (str): The language to use for the entity\
      \ summarization prompt\n    output_path (Path | None): The path to write the\
      \ prompt to. Default is None.\n\nReturns:\n    str: The generated prompt.\n\n\
      Raises:\n    OSError: If the prompt cannot be written to output_path.\n\"\"\""
  classes: []
- file: graphrag/prompt_tune/generator/entity_types.py
  functions:
  - node_id: graphrag/prompt_tune/generator/entity_types.py::generate_entity_types
    name: generate_entity_types
    signature: "def generate_entity_types(\n    model: ChatModel,\n    domain: str,\n\
      \    persona: str,\n    docs: str | list[str],\n    task: str = DEFAULT_TASK,\n\
      \    json_mode: bool = False,\n) -> str | list[str]"
    docstring: "Generate entity type categories from a given set of documents.\n\n\
      Args:\n    model: ChatModel. The chat model to use for generation.\n    domain:\
      \ str. The domain context to tailor prompts.\n    persona: str. The system persona\
      \ content used as the initial system prompt.\n    docs: str | list[str]. A single\
      \ string or a list of strings containing the documents to extract entity types\
      \ from.\n    task: str. Task specification to format with domain; defaults to\
      \ DEFAULT_TASK.\n    json_mode: bool. If True, parse the response as JSON using\
      \ EntityTypesResponse and return a list of entity types; otherwise return the\
      \ raw text output.\n\nReturns:\n    str | list[str]. When json_mode is True,\
      \ returns a list of entity types extracted from the documents (or empty list\
      \ on failure). When json_mode is False, returns the raw string output from the\
      \ model.\n\nRaises:\n    Exception. If the underlying model call fails or returns\
      \ an unexpected structure."
  classes: []
- file: graphrag/prompt_tune/generator/extract_graph_prompt.py
  functions:
  - node_id: graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
    name: create_extract_graph_prompt
    signature: "def create_extract_graph_prompt(\n    entity_types: str | list[str]\
      \ | None,\n    docs: list[str],\n    examples: list[str],\n    language: str,\n\
      \    max_token_count: int,\n    tokenizer: Tokenizer | None = None,\n    json_mode:\
      \ bool = False,\n    output_path: Path | None = None,\n    min_examples_required:\
      \ int = 2,\n) -> str"
    docstring: "\"\"\"\nCreate a prompt for entity extraction.\n\nArgs:\n    entity_types\
      \ (str | list[str] | None): The entity types to extract.\n    docs (list[str]):\
      \ The list of documents to extract entities from.\n    examples (list[str]):\
      \ The list of examples to use for entity extraction.\n    language (str): The\
      \ language of the inputs and outputs.\n    max_token_count (int): The maximum\
      \ number of tokens to use for the prompt.\n    tokenizer (Tokenizer | None):\
      \ The tokenizer to use for encoding and decoding text. If None, a default tokenizer\
      \ will be used.\n    json_mode (bool): Whether to use JSON mode for the prompt.\
      \ Default is False.\n    output_path (Path | None): The path to write the prompt\
      \ to. Default is None.\n    min_examples_required (int): The minimum number\
      \ of examples required. Default is 2.\n\nReturns:\n    str: The entity extraction\
      \ prompt.\n\"\"\""
  classes: []
- file: graphrag/prompt_tune/generator/language.py
  functions:
  - node_id: graphrag/prompt_tune/generator/language.py::detect_language
    name: detect_language
    signature: 'def detect_language(model: ChatModel, docs: str | list[str]) -> str'
    docstring: "Detect input language to use for GraphRAG prompts.\n\nParameters\n\
      \    model (ChatModel): The language model to use for language detection\n \
      \   docs (str | list[str]): The docs to detect language from\n\nReturns\n  \
      \  str: The detected language.\n\nRaises\n    Exception: If the underlying model\
      \ API raises an error during language detection."
  classes: []
- file: graphrag/prompt_tune/generator/persona.py
  functions:
  - node_id: graphrag/prompt_tune/generator/persona.py::generate_persona
    name: generate_persona
    signature: "def generate_persona(\n    model: ChatModel, domain: str, task: str\
      \ = DEFAULT_TASK\n) -> str"
    docstring: "\"\"\"Generate an LLM persona to use for GraphRAG prompts.\n\nArgs:\n\
      \    model (ChatModel): The LLM to use for generation\n    domain (str): The\
      \ domain to generate a persona for\n    task (str): The task to generate a persona\
      \ for. Default is DEFAULT_TASK\n\nReturns:\n    str: The generated persona string\n\
      \nRaises:\n    Exception: If the underlying model call fails\n\"\"\""
  classes: []
- file: graphrag/prompt_tune/loader/input.py
  functions:
  - node_id: graphrag/prompt_tune/loader/input.py::_sample_chunks_from_embeddings
    name: _sample_chunks_from_embeddings
    signature: "def _sample_chunks_from_embeddings(\n    text_chunks: pd.DataFrame,\n\
      \    embeddings: np.ndarray[float, np.dtype[np.float_]],\n    k: int = K,\n\
      ) -> pd.DataFrame"
    docstring: "Sample k text chunks whose embeddings are closest to the center of\
      \ the embedding set.\n\nArgs:\n  text_chunks: DataFrame containing text chunks\
      \ to sample from.\n  embeddings: Array of embedding vectors corresponding to\
      \ the text chunks.\n  k: Number of chunks to sample (default K).\n\nReturns:\n\
      \  DataFrame containing the sampled text chunks.\n  The rows correspond to the\
      \ k chunks with embeddings closest to the mean embedding.\n\nRaises:\n  None"
  - node_id: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
    name: load_docs_in_chunks
    signature: "def load_docs_in_chunks(\n    config: GraphRagConfig,\n    select_method:\
      \ DocSelectionType,\n    limit: int,\n    logger: logging.Logger,\n    chunk_size:\
      \ int,\n    overlap: int,\n    n_subset_max: int = N_SUBSET_MAX,\n    k: int\
      \ = K,\n) -> list[str]"
    docstring: "Load documents into chunks for generating prompts.\n\nLoad documents\
      \ from the configured input, convert them into base text units according\nto\
      \ the chunking configuration, and optionally sample or embed chunks to meet\
      \ the\nrequested selection method. The function returns a list of chunk texts,\
      \ with braces\nescaped to avoid issues with Python's str.format when parsing\
      \ LaTeX in markdown.\n\nArgs:\n    config: GraphRagConfig The overall configuration\
      \ for the graph-based RAG pipeline, including input sources and chunking options.\n\
      \    select_method: DocSelectionType The strategy to select chunks: TOP, RANDOM,\
      \ or AUTO.\n    limit: int Maximum number of chunks to return. If out of range,\
      \ a default is used.\n    logger: logging.Logger Logger used to emit warnings\
      \ and information during processing.\n    chunk_size: int The size of each chunk.\n\
      \    overlap: int The amount of overlap between consecutive chunks.\n    n_subset_max:\
      \ int Maximum number of chunks to sample when using AUTO selection (default\
      \ N_SUBSET_MAX).\n    k: int Number of chunks to select when using AUTO (must\
      \ be > 0 when AUTO is chosen).\n\nReturns:\n    list[str] A list containing\
      \ the chunk texts. Each text has braces escaped by doubling\n    braces to prevent\
      \ the str.format parser from interpreting LaTeX or other content.\n\nRaises:\n\
      \    ValueError: If select_method is DocSelectionType.AUTO and k is not a positive\
      \ integer."
  classes: []
- file: graphrag/prompt_tune/types.py
  functions:
  - node_id: graphrag/prompt_tune/types.py::DocSelectionType.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "Return the string representation of the enum value.\n\nArgs:\n   \
      \ self: The enum member.\n\nReturns:\n    str: The string representation of\
      \ the enum value."
  classes:
  - class_id: graphrag/prompt_tune/types.py::DocSelectionType
    name: DocSelectionType
    docstring: "DocSelectionType is an enumeration of strategies for selecting documents\
      \ in the prompt tuning workflow.\n\nIt defines four strategies, each associated\
      \ with a string value:\n- ALL -> \"all\"\n- RANDOM -> \"random\"\n- TOP -> \"\
      top\"\n- AUTO -> \"auto\"\n\nAttributes:\n    ALL (str): The \"all\" selection\
      \ strategy.\n    RANDOM (str): The \"random\" selection strategy.\n    TOP (str):\
      \ The \"top\" selection strategy.\n    AUTO (str): The \"auto\" selection strategy.\n\
      \nMethods:\n    __str__(self) -> str:\n        Returns: str\n            The\
      \ string representation of the enum value."
    methods:
    - name: __str__
      signature: def __str__(self)
- file: graphrag/query/context_builder/builders.py
  functions:
  - node_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
      \    ) -> tuple[pd.DataFrame, dict[str, int]]"
    docstring: "Build the context used to prime subsequent search actions for the\
      \ given query.\n\nThis asynchronous method constructs a DataFrame of contextual\
      \ items and a metrics dictionary\nthat can be used to warm up or seed downstream\
      \ DRIFT search processes.\n\nArgs:\n    self: The instance of the class.\n \
      \   query (str): The search query for which to build the context.\n    **kwargs:\
      \ Additional keyword arguments to customize context construction.\n\nReturns:\n\
      \    tuple[pd.DataFrame, dict[str, int]]: A pair where the first element is\
      \ a pandas DataFrame containing\n    the contextual items to be used for downstream\
      \ search (columns and contents vary by\n    implementation but typically include\
      \ the text and related metadata), and the second element is\n    a mapping from\
      \ metric names (strings) to integers representing context-related counters or\
      \ scores\n    produced during construction.\n\nRaises:\n    Exception: Implementation-specific\
      \ errors may be raised during context construction. Callers should\n       \
      \        handle broad exceptions and consider fallback or retry as appropriate."
  - node_id: graphrag/query/context_builder/builders.py::BasicContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "Build the context for the basic search mode.\n\nArgs:\n    query:\
      \ The user query to build context for.\n    conversation_history: Optional conversation\
      \ history to consider while constructing the context.\n    **kwargs: Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \    ContextBuilderResult: The result containing the built context for the basic\
      \ search mode."
  - node_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "Build the context for the global search mode.\n\nArgs:\n  query: The\
      \ user query to build context for.\n  conversation_history: Optional conversation\
      \ history to consider while constructing the context.\n  **kwargs: Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \  ContextBuilderResult: The result containing the built context."
  - node_id: graphrag/query/context_builder/builders.py::LocalContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "\"\"\"Build the context for the local search mode.\n\nArgs:\n  query\
      \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
      \ | None): Optional conversation history to consider while constructing the\
      \ context.\n  **kwargs: Additional keyword arguments that may influence how\
      \ the context is built.\n\nReturns:\n  ContextBuilderResult: The result containing\
      \ the built context for the local search mode.\n\nRaises:\n  NotImplementedError:\
      \ If invoked on the abstract base class.\n\"\"\""
  classes:
  - class_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder
    name: DRIFTContextBuilder
    docstring: "DRIFTContextBuilder is an abstract base class that defines the contract\
      \ for constructing the DRIFT context used to prime subsequent search actions\
      \ for a given query. It specifies an asynchronous interface to build the context,\
      \ exposed via the build_context(query, **kwargs) method, which returns a tuple\
      \ containing a DataFrame of contextual items and a metrics dictionary used to\
      \ warm up or seed downstream DRIFT search processes.\n\nArgs:\n    self: The\
      \ instance of the class.\n    query (str): The search query for which to build\
      \ the context.\n    **kwargs: Additional keyword arguments to customize or influence\
      \ context construction.\n\nReturns:\n    tuple[pd.DataFrame, dict[str, int]]:\
      \ A pair consisting of a DataFrame of contextual items and a metrics dictionary\
      \ mapping metric names to integer counts.\n\nRaises:\n    NotImplementedError:\
      \ If the subclass does not implement build_context."
    methods:
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ **kwargs,\n    ) -> tuple[pd.DataFrame, dict[str, int]]"
  - class_id: graphrag/query/context_builder/builders.py::BasicContextBuilder
    name: BasicContextBuilder
    docstring: 'BasicContextBuilder is a concrete implementation of a context builder
      that constructs the minimal context required for the basic search mode by combining
      the user query with optional conversation history.


      Args:

      - None: The constructor takes no public parameters.


      Returns:

      - ContextBuilderResult: The result type produced when build_context is called,
      representing the assembled context for a basic search.


      Raises:

      - TypeError: If the provided inputs do not match expected types when building
      the context.

      - ValueError: If inputs are invalid (e.g., non-string query).


      Attributes:

      - No public attributes are declared for this class. Internal state, if any,
      is encapsulated.


      Summary:

      - This class participates in Graphrag''s query context builder system as the
      basic-mode context creator, supplying the minimal context necessary to perform
      basic search.'
    methods:
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        **kwargs,\n\
        \    ) -> ContextBuilderResult"
  - class_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder
    name: GlobalContextBuilder
    docstring: "GlobalContextBuilder builds the context used for the global search\
      \ mode.\n\nPurpose:\nA specialized builder responsible for constructing the\
      \ context used when performing a global search. It considers the user query\
      \ and may incorporate optional conversation history and additional keyword arguments\
      \ to assemble a ContextBuilderResult that downstream components can use to execute\
      \ or facilitate the global search.\n\nAttributes:\n- No explicit instance attributes\
      \ are defined in the provided interface.\n\nMethods:\n- build_context(self,\
      \ query: str, conversation_history: ConversationHistory | None = None, **kwargs)\
      \ -> ContextBuilderResult\n  Build the context for the global search mode.\n\
      \nArgs (for build_context):\n- query: The user query to build context for.\n\
      - conversation_history: Optional conversation history to consider while constructing\
      \ the context.\n- kwargs: Additional keyword arguments that may influence how\
      \ the context is built.\n\nReturns (for build_context):\n- ContextBuilderResult:\
      \ The result containing the built context for the global search operation. The\
      \ exact contents depend on downstream usage and typically include the constructed\
      \ context data and any necessary metadata.\n\nRaises:\n- NotImplementedError:\
      \ This class declares build_context as an abstract method. Concrete subclasses\
      \ must provide an implementation."
    methods:
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        **kwargs,\n\
        \    ) -> ContextBuilderResult"
  - class_id: graphrag/query/context_builder/builders.py::LocalContextBuilder
    name: LocalContextBuilder
    docstring: "Abstract base class for building the local-context used in local search\
      \ mode.\n\nThis builder defines the contract for assembling the user query and\
      \ optional conversation history into a ContextBuilderResult that downstream\
      \ components can use to perform a local (on-device) search.\n\nAttributes:\n\
      \    No explicit data attributes are defined on this base class. Concrete implementations\
      \ may define configuration parameters, caches, or data sources as needed.\n\n\
      Args:\n    query (str): The user query to build context for.\n    conversation_history\
      \ (ConversationHistory | None): Optional conversation history to consider while\
      \ constructing the context.\n    **kwargs: Additional keyword arguments that\
      \ may influence how the context is built. Implementations may interpret or ignore\
      \ these as needed.\n\nReturns:\n    ContextBuilderResult: The result containing\
      \ the built context for downstream processing.\n\nRaises:\n    NotImplementedError:\
      \ If invoked on the abstract base class.\n    ValueError: If inputs are invalid\
      \ or inconsistent (implementation-specific)."
    methods:
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        **kwargs,\n\
        \    ) -> ContextBuilderResult"
- file: graphrag/query/context_builder/community_context.py
  functions:
  - node_id: graphrag/query/context_builder/community_context.py::_report_context_text
    name: _report_context_text
    signature: "def _report_context_text(\n        report: CommunityReport, attributes:\
      \ list[str]\n    ) -> tuple[str, list[str]]"
    docstring: 'Builds a single-line context text for a CommunityReport using the
      given attributes.


      This helper relies on global flags to determine content and formatting:

      - use_community_summary: if True, include report.summary; otherwise include
      report.full_content.

      - include_community_rank: if True, append the report.rank to the line.

      - column_delimiter: string used to join fields into the line.


      Args:

      - report (CommunityReport): The report to extract data from.

      - attributes (list[str]): Attribute field names to include from report.attributes
      (in order).


      Returns:

      - tuple[str, list[str]]: A pair where the first element is the single-line text
      (with a trailing newline) formed by joining the context fields with column_delimiter,
      and the second element is the raw list of context fields used to build that
      line.


      Notes:

      - report.short_id is included as "" when missing.

      - report.title is included as a string; if it can be None, behavior is undefined.

      - For each field in attributes, the value is str(report.attributes.get(field,
      "")) if report.attributes is not None; otherwise "".

      - If include_community_rank is True, report.rank is appended as a string.'
  - node_id: graphrag/query/context_builder/community_context.py::_rank_report_context
    name: _rank_report_context
    signature: "def _rank_report_context(\n    report_df: pd.DataFrame,\n    weight_column:\
      \ str | None = \"occurrence weight\",\n    rank_column: str | None = \"rank\"\
      ,\n) -> pd.DataFrame"
    docstring: "Sorts the report context by the provided weight and rank columns in\
      \ descending order, in-place.\n\nArgs:\n  report_df (pd.DataFrame): The DataFrame\
      \ containing the report context to sort. The function mutates this DataFrame\
      \ in place by casting the configured columns to float and sorting by them in\
      \ descending order. If neither weight_column nor rank_column is provided, the\
      \ DataFrame is returned unchanged.\n\n  weight_column (str | None): Name of\
      \ the column to use for weighting. If not None, the column is cast to float\
      \ and used for sorting; defaults to \"occurrence weight\". If None, this column\
      \ is ignored.\n\n  rank_column (str | None): Name of the column to use for ranking.\
      \ If not None, the column is cast to float and used for sorting; defaults to\
      \ \"rank\". If None, this column is ignored.\n\nReturns:\n  pd.DataFrame: The\
      \ input DataFrame, sorted by the specified columns in descending order. This\
      \ is the same object that was passed in.\n\nRaises:\n  KeyError: If a provided\
      \ column name does not exist in report_df.\n  ValueError: If a provided column\
      \ cannot be cast to float.\n  TypeError: If an invalid argument type is provided."
  - node_id: graphrag/query/context_builder/community_context.py::_init_batch
    name: _init_batch
    signature: def _init_batch() -> None
    docstring: "Initialize batch state for the current context.\nThis updates nonlocal\
      \ batch_text, batch_tokens, and batch_records by:\n- building the batch_text\
      \ header as \"-----{context_name}-----\" followed by a newline and the header\
      \ row joined by column_delimiter\n- computing batch_tokens from the batch_text\
      \ using tokenizer.num_tokens\n- resetting batch_records to an empty list\nReturns:\n\
      \    None"
  - node_id: graphrag/query/context_builder/community_context.py::_get_header
    name: _get_header
    signature: 'def _get_header(attributes: list[str]) -> list[str]'
    docstring: "Builds the header row for the community context data table based on\
      \ the given attributes and surrounding configuration.\n\nArgs:\n    attributes:\
      \ List[str] - Attributes to include in the header (after removing the default\
      \ id and title duplicates).\n\nReturns:\n    List[str] - The constructed header\
      \ list, starting with \"id\" and \"title\", followed by filtered attributes,\
      \ then either \"summary\" or \"content\" depending on use_community_summary,\
      \ and optionally the rank name if include_community_rank is True. The exact\
      \ behavior may depend on surrounding configuration variables such as include_community_weight,\
      \ community_weight_name, and community_rank_name."
  - node_id: graphrag/query/context_builder/community_context.py::_compute_community_weights
    name: _compute_community_weights
    signature: "def _compute_community_weights(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None,\n    weight_attribute: str = \"occurrence\"\
      ,\n    normalize: bool = True,\n) -> list[CommunityReport]"
    docstring: "Compute weights for communities based on the number of text units\
      \ associated with entities in each community.\n\nArgs:\n  community_reports:\
      \ List[CommunityReport]\n      Reports for communities to assign weights to.\n\
      \  entities: list[Entity] | None\n      Entities that reference community_ids\
      \ and contain text_unit_ids. Text units are aggregated by community_id across\
      \ all entities.\n  weight_attribute: str\n      Name of the attribute stored\
      \ on each CommunityReport's attributes dictionary to hold the computed weight.\
      \ Defaults to \"occurrence\".\n  normalize: bool\n      If True, normalize weights\
      \ by the maximum weight across all reports so weights lie in [0, 1].\n\nReturns:\n\
      \  list[CommunityReport]\n      The updated list of CommunityReport objects\
      \ with the computed weight stored under the specified weight_attribute in report.attributes.\
      \ Weights are normalized when normalize is True."
  - node_id: graphrag/query/context_builder/community_context.py::_is_included
    name: _is_included
    signature: 'def _is_included(report: CommunityReport) -> bool'
    docstring: "Determine whether the given CommunityReport should be included in\
      \ the context based on its rank.\n\nArgs:\n    report (CommunityReport): The\
      \ community report to evaluate for inclusion.\n\nReturns:\n    bool: True if\
      \ report.rank is not None and report.rank >= min_community_rank, otherwise False."
  - node_id: graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
    name: _convert_report_context_to_df
    signature: "def _convert_report_context_to_df(\n    context_records: list[list[str]],\n\
      \    header: list[str],\n    weight_column: str | None = None,\n    rank_column:\
      \ str | None = None,\n) -> pd.DataFrame"
    docstring: "Convert report context records to a pandas DataFrame and sort by weight\
      \ and rank if provided.\n\nArgs:\n    context_records (list[list[str]]): The\
      \ report context records to convert. Each inner list represents a row.\n   \
      \ header (list[str]): Column names for the resulting DataFrame.\n    weight_column\
      \ (str | None): Name of the column to use for weighting. If not None, the column\
      \ will be cast to float and used for sorting in descending order.\n    rank_column\
      \ (str | None): Name of the column to use for ranking. If not None, the column\
      \ will be cast to float and used for sorting in descending order.\n\nReturns:\n\
      \    pd.DataFrame: A DataFrame constructed from the context records with the\
      \ provided header. If context_records is empty, an empty DataFrame is returned.\
      \ The DataFrame is sorted in-place by the specified weight and rank columns\
      \ when provided.\n\nRaises:\n    None"
  - node_id: graphrag/query/context_builder/community_context.py::_cut_batch
    name: _cut_batch
    signature: def _cut_batch() -> None
    docstring: 'Convert the current batch of context records to a DataFrame, convert
      it to CSV, and append it to the aggregated context. This function calls _convert_report_context_to_df
      with context_records=batch_records and header=header, passing weight_column
      as community_weight_name if entities and include_community_weight are truthy,
      otherwise None, and rank_column as community_rank_name if include_community_rank
      is truthy, otherwise None. If the resulting DataFrame is empty, the function
      returns without modification. Otherwise, it converts the DataFrame to CSV with
      index=None and sep=column_delimiter. If not all_context_text and single_batch,
      it prefixes the current context header to the CSV text. Finally, it appends
      the CSV text to all_context_text and the DataFrame to all_context_records. Returns:
      None'
  - node_id: graphrag/query/context_builder/community_context.py::build_community_context
    name: build_community_context
    signature: "def build_community_context(\n    community_reports: list[CommunityReport],\n\
      \    entities: list[Entity] | None = None,\n    tokenizer: Tokenizer | None\
      \ = None,\n    use_community_summary: bool = True,\n    column_delimiter: str\
      \ = \"|\",\n    shuffle_data: bool = True,\n    include_community_rank: bool\
      \ = False,\n    min_community_rank: int = 0,\n    community_rank_name: str =\
      \ \"rank\",\n    include_community_weight: bool = True,\n    community_weight_name:\
      \ str = \"occurrence weight\",\n    normalize_community_weight: bool = True,\n\
      \    max_context_tokens: int = 8000,\n    single_batch: bool = True,\n    context_name:\
      \ str = \"Reports\",\n    random_state: int = 86,\n) -> tuple[str | list[str],\
      \ dict[str, pd.DataFrame]]"
    docstring: 'Build context data from community reports for use in a system prompt.


      If entities are provided, compute per-community weights from the number of text
      units associated with entities within each community. The computed weight is
      added to each CommunityReport''s attributes and included in the context data
      table.


      Args:

      - community_reports (list[CommunityReport]): Reports representing each community
      to be included in the context.

      - entities (list[Entity] | None): Optional entities used to derive weights by
      counting text units linked to each community.

      - tokenizer (Tokenizer | None): Tokenizer to use for estimating token counts.
      If None, a default tokenizer is obtained.

      - use_community_summary (bool): If True, include report.summary in the context
      line; otherwise include report.full_content.

      - column_delimiter (str): Delimiter used to join fields when constructing per-report
      context lines.

      - shuffle_data (bool): If True, shuffle the selected reports before batching.

      - include_community_rank (bool): If True, append a rank value to each line and
      include a rank column in the header.

      - min_community_rank (int): Minimum rank for a report to be included.

      - community_rank_name (str): Header name for the rank column when included.

      - include_community_weight (bool): If True, include a weight column (occurrence
      weight) for each report.

      - community_weight_name (str): Attribute name used to store the weight on each
      report.

      - normalize_community_weight (bool): If True, apply normalization to computed
      weights.

      - max_context_tokens (int): Maximum token budget per batch; when exceeded, a
      new batch is started.

      - single_batch (bool): If True, produce a single batch; otherwise accumulate
      multiple batches up to the token limit.

      - context_name (str): Name used as the batch header for the context section.

      - random_state (int): Seed for deterministic shuffling when shuffle_data is
      True.


      Returns:

      - tuple[str | list[str], dict[str, pd.DataFrame]]: A pair containing the generated
      context text(s) and a mapping from the lower-cased context name to a pandas
      DataFrame with the compiled context records. The first element is either a single
      string or a list of strings representing the context segments produced; the
      second element is a dictionary like {"reports": <DataFrame>} containing the
      concatenated context data.


      Raises:

      - May raise exceptions from the underlying tokenizer, pandas operations, or
      user-provided data models if inputs are invalid or internal processing fails.


      Notes:

      - When no reports pass the inclusion filter, an empty context is returned as
      ([], {}).

      - The function is designed to support batching for large contexts and to prefer
      deterministic outputs when a random_state is provided.'
  classes: []
- file: graphrag/query/context_builder/conversation_history.py
  functions:
  - node_id: graphrag/query/context_builder/conversation_history.py::QATurn.get_answer_text
    name: get_answer_text
    signature: def get_answer_text(self) -> str | None
    docstring: "Return the concatenated text of the assistant answers.\n\nArgs:\n\
      \    self: The QATurn instance containing assistant answers.\n\nReturns:\n \
      \   str | None: The assistant answers contents joined by newline characters,\
      \ or None if there are no assistant answers."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.to_qa_turns
    name: to_qa_turns
    signature: def to_qa_turns(self) -> list[QATurn]
    docstring: "\"\"\"Convert conversation history to a list of QA turns.\n\nReturns:\n\
      \    list[QATurn]: A list of QA turns created from the conversation history,\
      \ where each QA turn contains a user_query from a USER turn and a list of assistant_answers\
      \ collected from subsequent turns until the next USER turn.\n\"\"\""
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.from_list
    name: from_list
    signature: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
      \    ) -> \"ConversationHistory\""
    docstring: "Create a ConversationHistory from a list of conversation turns.\n\n\
      Each turn is a dictionary in the form of {\"role\": \"<conversation_role>\"\
      , \"content\": \"<turn content>\"}.\n\nArgs:\n    cls: The class object, used\
      \ to instantiate a new ConversationHistory.\n    conversation_turns: A list\
      \ of dictionaries representing turns. Each dictionary has keys \"role\" and\
      \ \"content\".\n\nReturns:\n    ConversationHistory: A new ConversationHistory\
      \ containing the parsed turns."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize a ConversationHistory with an empty list of turns.\n\n\
      Returns:\n    None\n        This initializer does not return a value. It initializes\
      \ the turns attribute to an empty list."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the conversation turn.\n\nArgs:\n\
      \    self: The ConversationTurn instance for which to obtain the string representation.\n\
      \nReturns:\n    str: The string representation of the turn in the format \"\
      <role>: <content>\".\n\nRaises:\n    None"
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.add_turn
    name: add_turn
    signature: 'def add_turn(self, role: ConversationRole, content: str)'
    docstring: "Add a new turn to the conversation history.\n\nArgs:\n    role (ConversationRole):\
      \ The role for the new turn.\n    content (str): The content of the turn.\n\n\
      Returns:\n    None"
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the enum value.\n\nArgs:\n    self:\
      \ The enum member.\n\nReturns:\n    str: The string representation of the enum\
      \ value."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.from_string
    name: from_string
    signature: 'def from_string(value: str) -> "ConversationRole"'
    docstring: "\"\"\"Convert string to ConversationRole.\n\nArgs:\n    value: str.\
      \ The role as a string. Expected values are \"system\", \"user\", or \"assistant\"\
      .\n\nReturns:\n    ConversationRole. The corresponding ConversationRole enum\
      \ member.\n\nRaises:\n    ValueError: If value is not one of the supported roles.\n\
      \"\"\""
  - node_id: graphrag/query/context_builder/conversation_history.py::QATurn.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the QA turn.\n\nArgs:\n    self: QATurn\
      \ instance to stringify.\n\nReturns:\n    str: The string representation of\
      \ the QA turn. If there are assistant answers, the string is\n        \"Question:\
      \ <user_query.content>\\nAnswer: <answers>\"; otherwise, it's\n        \"Question:\
      \ <user_query.content>\"."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.get_user_turns
    name: get_user_turns
    signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
    docstring: '"""Get the last user turns in the conversation history.\n\nArgs:\n    max_user_turns:
      Maximum number of user turns to include from history. If None, include all user
      turns. Default is 1.\n\nReturns:\n    list[str]: A list of user turn contents,
      in reverse chronological order (most recent first), up to max_user_turns.\n\nRaises:\n    None\n"""'
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        tokenizer: Tokenizer |\
      \ None = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
      \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
      \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Prepare conversation history as context data for system prompt.\n\n\
      Parameters\n----------\ntokenizer : Tokenizer | None\n    The tokenizer to use.\
      \ If None, a default tokenizer retrieved by get_tokenizer() will be used.\n\
      include_user_turns_only : bool\n    If True, only user queries (not assistant\
      \ responses) will be included in the context. Default is True.\nmax_qa_turns\
      \ : int | None\n    Maximum number of QA turns to include in the context. If\
      \ None, there is no explicit limit. Default is 5.\nmax_context_tokens : int\n\
      \    Maximum number of tokens allowed for the context data. Default is 8000.\n\
      recency_bias : bool\n    If True, reverse the order of the conversation history\
      \ to prioritize the most recent QA turn. Default is True.\ncolumn_delimiter\
      \ : str\n    Delimiter to use for separating columns in the context data. Default\
      \ is \"|\".\ncontext_name : str\n    Name of the context, default is \"Conversation\
      \ History\".\n\nReturns\n-------\ntuple[str, dict[str, pd.DataFrame]]\n    A\
      \ tuple containing:\n    - context_text: the context data text (string) including\
      \ a header line and the QA turns formatted as a table.\n    - context_dict:\
      \ a mapping from the lowercase context name (context_name.lower()) to a DataFrame\
      \ containing the turns included in the context. The key is consistently lowercase\
      \ in both empty and non-empty cases."
  classes:
  - class_id: graphrag/query/context_builder/conversation_history.py::QATurn
    name: QATurn
    docstring: "QATurn is a dataclass that represents a single turn in a question-and-answer\
      \ conversation history, pairing a user query with optional assistant answers\
      \ and providing helpers to derive text for display.\n\nAttributes:\n  user_query:\
      \ Optional[object] - The user query for this turn. The object should have a\
      \ 'content' attribute used for display. May be None.\n  assistant_answers: Optional[List[str]]\
      \ - The assistant's answers for this turn. If None, no answer has been provided\
      \ yet.\n\nArgs:\n  user_query: Optional[object] - The user query component for\
      \ this turn. The object should have a 'content' attribute.\n  assistant_answers:\
      \ Optional[List[str]] - The assistant's answers for this turn. None indicates\
      \ no answer yet.\n\nMethods:\n  get_answer_text(self) -> Optional[str]: Returns\
      \ the assistant answers contents joined by newline characters, or None if there\
      \ are no answers.\n  __str__(self) -> str: Returns a readable representation\
      \ of the turn. If assistant answers exist, the string is \"Question: <user_query.content>\\\
      nAnswer: <concatenated answers>\"; otherwise, it's \"Question: <user_query.content>\"\
      ."
    methods:
    - name: get_answer_text
      signature: def get_answer_text(self) -> str | None
    - name: __str__
      signature: def __str__(self) -> str
  - class_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory
    name: ConversationHistory
    docstring: "ConversationHistory is a helper for storing and manipulating a sequence\
      \ of conversation turns used for prompt construction and QA-turn generation.\n\
      \nThe history is stored as an ordered list of turns, where each turn is represented\
      \ as a dictionary with keys \"role\" and \"content\". Roles typically correspond\
      \ to the constants SYSTEM, USER, and ASSISTANT.\n\nAttributes:\n    turns: list[dict[str,\
      \ str]]\n        The conversation turns in chronological order. Each dictionary\
      \ has keys \"role\" and \"content\".\n\nThis class provides methods to initialize\
      \ from an existing list, add new turns, retrieve user turns, convert the history\
      \ to QA-turns, and build context data for prompts."
    methods:
    - name: to_qa_turns
      signature: def to_qa_turns(self) -> list[QATurn]
    - name: from_list
      signature: "def from_list(\n        cls, conversation_turns: list[dict[str,\
        \ str]]\n    ) -> \"ConversationHistory\""
    - name: __init__
      signature: def __init__(self)
    - name: add_turn
      signature: 'def add_turn(self, role: ConversationRole, content: str)'
    - name: get_user_turns
      signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
    - name: build_context
      signature: "def build_context(\n        self,\n        tokenizer: Tokenizer\
        \ | None = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
        \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
        \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
        \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
  - class_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn
    name: ConversationTurn
    docstring: "Represents a single turn in a conversation history.\n\nArgs:\n   \
      \ role: The role associated with this turn (for example, SYSTEM, USER, or ASSISTANT).\n\
      \    content: The textual content of the turn.\n\nReturns:\n    str: The string\
      \ representation of the turn, provided by __str__, in the format \"<role>: <content>\"\
      .\n\nRaises:\n    None"
    methods:
    - name: __str__
      signature: def __str__(self) -> str
  - class_id: graphrag/query/context_builder/conversation_history.py::ConversationRole
    name: ConversationRole
    docstring: "ConversationRole is an enumeration that represents the role of a participant\
      \ in a conversation (system, user, or assistant). It defines three members:\
      \ SYSTEM, USER, and ASSISTANT, each with an underlying string value: \"system\"\
      , \"user\", and \"assistant\". The underlying string value is accessible via\
      \ the .value attribute. Note that __str__(self) may not return the underlying\
      \ value by default; depending on the implementation, str(role) can yield the\
      \ enum member name (for example, ConversationRole.SYSTEM). If you need the string\
      \ form, use role.value.\n\nFrom_string(value: str) converts a string to the\
      \ corresponding ConversationRole. Args: value: The role as a string. Expected\
      \ values are \"system\", \"user\", or \"assistant\". Returns: The corresponding\
      \ ConversationRole enum member. Raises: ValueError if value is not one of the\
      \ supported roles.\n\nAttributes:\n  SYSTEM: The member representing the system\
      \ role with value \"system\".\n  USER: The member representing the user role\
      \ with value \"user\".\n  ASSISTANT: The member representing the assistant role\
      \ with value \"assistant\".\n\nExamples:\n  role = ConversationRole.from_string(\"\
      system\")\n  assert role is ConversationRole.SYSTEM\n  print(role.value)  #\
      \ \"system\""
    methods:
    - name: __str__
      signature: def __str__(self) -> str
    - name: from_string
      signature: 'def from_string(value: str) -> "ConversationRole"'
- file: graphrag/query/context_builder/dynamic_community_selection.py
  functions:
  - node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
      \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
      \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
      \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
      \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize the DynamicCommunitySelection with the provided data and\
      \ prepare internal state for dynamic community selection.\n\nArgs:\n    community_reports\
      \ (list[CommunityReport]): Reports for communities to consider, mapped by community_id.\n\
      \    communities (list[Community]): Community objects used to build the hierarchy\
      \ and starting points.\n    model (ChatModel): Language model instance used\
      \ to rate relevance of communities.\n    tokenizer (Tokenizer): Tokenizer used\
      \ for text processing.\n    rate_query (str): Query string used for rate prompting.\
      \ Defaults to RATE_QUERY.\n    use_summary (bool): If True, use the summary\
      \ content when rating; otherwise use full_content.\n    threshold (int): Minimum\
      \ rating threshold for a report to be considered relevant.\n    keep_parent\
      \ (bool): Whether to preserve parent relationships during selection.\n    num_repeats\
      \ (int): Number of times to repeat the relevance rating process.\n    max_level\
      \ (int): Maximum depth of levels to explore.\n    concurrent_coroutines (int):\
      \ Maximum number of concurrent asynchronous tasks.\n    model_params (dict[str,\
      \ Any] | None): Optional additional parameters to pass to the model; if None,\
      \ an empty dict is used.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
    name: select
    signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
      Any]]'
    docstring: "Asynchronously select relevant communities with respect to the query.\n\
      \nArgs:\n    query (str): The query to rate against.\n\nReturns:\n    tuple[list[CommunityReport],\
      \ dict[str, Any]]: A tuple containing a list of CommunityReport objects representing\
      \ the relevant communities and a dictionary with additional information including\
      \ llm usage metrics (llm_calls, prompt_tokens, output_tokens) and the ratings\
      \ mapping under the key \"ratings\".\n\nRaises:\n    Exceptions raised by rate_relevancy\
      \ or asyncio.gather may propagate to the caller."
  classes:
  - class_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection
    name: DynamicCommunitySelection
    docstring: "DynamicCommunitySelection orchestrates dynamic selection of relevant\
      \ communities for a given query using a language model and relevancy scoring.\n\
      \nArgs:\n  community_reports (list[CommunityReport]): Reports for communities\
      \ to consider, mapped by community_id.\n  communities (list[Community]): Community\
      \ objects used to build the hierarchy and starting points.\n  model (ChatModel):\
      \ Language model instance used to rate relevancy of communities to the query.\n\
      \  tokenizer (Tokenizer): Tokenizer instance used to prepare text for prompting\
      \ and evaluation.\n  rate_query (str): Rate query string used to guide the relevancy\
      \ assessment. Default is RATE_QUERY.\n  use_summary (bool): If True, summaries\
      \ are incorporated into the evaluation context.\n  threshold (int): Threshold\
      \ determining the cutoff for considering a community relevant.\n  keep_parent\
      \ (bool): If True, keep parent relationships during hierarchical selection.\n\
      \  num_repeats (int): Number of times to repeat the evaluation steps.\n  max_level\
      \ (int): Maximum depth level to traverse in the hierarchy.\n  concurrent_coroutines\
      \ (int): Maximum number of concurrent coroutines for asynchronous processing.\n\
      \  model_params (dict[str, Any] | None): Optional dictionary of additional parameters\
      \ for the model.\n\nReturns:\n  tuple[list[CommunityReport], dict[str, Any]]:\
      \ A tuple containing:\n    - A list of CommunityReport objects representing\
      \ the relevant communities.\n    - A dictionary with additional information,\
      \ including llm usage metrics (llm_calls, prompt_tokens, output_tokens) and\
      \ rating results.\n\nRaises:\n  Exception: If an error occurs during selection\
      \ processing.\n\nAttributes:\n  community_reports: The provided community reports\
      \ used for consideration.\n  communities: The provided Community objects used\
      \ for hierarchy and starting points.\n  model: Language model used for prompting/rating.\n\
      \  tokenizer: Tokenizer used for text processing.\n  rate_query: The rate query\
      \ string.\n  use_summary: Whether summaries are used in evaluation.\n  threshold:\
      \ Selection threshold.\n  keep_parent: Whether to retain parent relationships.\n\
      \  num_repeats: Number of repeats in evaluation.\n  max_level: Maximum hierarchy\
      \ depth.\n  concurrent_coroutines: Concurrency limit for asynchronous tasks.\n\
      \  model_params: Optional model configuration parameters."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
        \        communities: list[Community],\n        model: ChatModel,\n      \
        \  tokenizer: Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary:\
        \ bool = False,\n        threshold: int = 1,\n        keep_parent: bool =\
        \ False,\n        num_repeats: int = 1,\n        max_level: int = 2,\n   \
        \     concurrent_coroutines: int = 8,\n        model_params: dict[str, Any]\
        \ | None = None,\n    )"
    - name: select
      signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
        Any]]'
- file: graphrag/query/context_builder/entity_extraction.py
  functions:
  - node_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey.from_string
    name: from_string
    signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
    docstring: "\"\"\"Convert string to EntityVectorStoreKey.\n\nArgs:\n    value:\
      \ str. The string key to convert. Expected to be \"id\" or \"title\".\n\nReturns:\n\
      \    EntityVectorStoreKey: The corresponding enum member (EntityVectorStoreKey.ID\
      \ for \"id\", EntityVectorStoreKey.TITLE for \"title\").\n\nRaises:\n    ValueError:\
      \ If value is not a valid EntityVectorStoreKey (i.e., not \"id\" or \"title\"\
      ).\n\"\"\""
  - node_id: graphrag/query/context_builder/entity_extraction.py::find_nearest_neighbors_by_entity_rank
    name: find_nearest_neighbors_by_entity_rank
    signature: "def find_nearest_neighbors_by_entity_rank(\n    entity_name: str,\n\
      \    all_entities: list[Entity],\n    all_relationships: list[Relationship],\n\
      \    exclude_entity_names: list[str] | None = None,\n    k: int | None = 10,\n\
      ) -> list[Entity]"
    docstring: "Retrieve entities that have direct connections with the target entity,\
      \ sorted by their rank.\n\nArgs:\n    entity_name: The name of the target entity.\n\
      \    all_entities: The list of all Entity objects to search.\n    all_relationships:\
      \ The list of Relationship objects to consider for connections.\n    exclude_entity_names:\
      \ Optional list of entity titles to exclude from results.\n    k: Optional number\
      \ of neighbors to return; if provided, at most k items are returned; if None,\
      \ all related entities are returned.\n\nReturns:\n    list[Entity]: A list of\
      \ Entity objects representing neighboring entities connected to the target entity,\
      \ sorted by rank in descending order. If k is provided, at most k entities are\
      \ returned; otherwise all related entities are returned."
  - node_id: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
    name: map_query_to_entities
    signature: "def map_query_to_entities(\n    query: str,\n    text_embedding_vectorstore:\
      \ BaseVectorStore,\n    text_embedder: EmbeddingModel,\n    all_entities_dict:\
      \ dict[str, Entity],\n    embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n\
      \    include_entity_names: list[str] | None = None,\n    exclude_entity_names:\
      \ list[str] | None = None,\n    k: int = 10,\n    oversample_scaler: int = 2,\n\
      ) -> list[Entity]"
    docstring: "Extracts entities that match a given query using semantic similarity\
      \ between the query and entity descriptions, with optional explicit inclusion\
      \ or exclusion of entities.\n\nArgs:\n    query: str. The query string to search\
      \ for relevant entities. If empty, top-ranked entities by rank are returned.\n\
      \    text_embedding_vectorstore: BaseVectorStore. The vector store used to perform\
      \ semantic similarity search over entity descriptions.\n    text_embedder: EmbeddingModel.\
      \ The embedding model used to encode text for similarity search.\n    all_entities_dict:\
      \ dict[str, Entity]. Mapping of entity IDs to Entity objects.\n    embedding_vectorstore_key:\
      \ str. Key used to extract the corresponding field from embedding results (e.g.,\
      \ \"id\" or \"title\"). Default EntityVectorStoreKey.ID.\n    include_entity_names:\
      \ list[str] | None. Names of entities to explicitly include. If provided, those\
      \ entities are retrieved and prepended to the final results.\n    exclude_entity_names:\
      \ list[str] | None. Names of entities to exclude from the matched results portion.\n\
      \    k: int. Number of top results to consider when a non-empty query is provided\
      \ (before exclusions/inclusions). When the query is empty, this caps the number\
      \ of top-ranked entities.\n    oversample_scaler: int. Multiplier to oversample\
      \ results to account for potential exclusions.\n\nReturns:\n    list[Entity].\
      \ The selected entities, with explicitly included entities first, followed by\
      \ the matched or top-ranked entities. Note that included entities are not filtered\
      \ by exclude_entity_names, and duplicates between the two groups may occur.\n\
      \nRaises:\n    AttributeError: May be raised by underlying retrieval helpers\
      \ if an expected attribute is missing on an Entity or in a retrieved result.\
      \ This behavior is not guaranteed by the function's signature."
  classes:
  - class_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey
    name: EntityVectorStoreKey
    docstring: "Enum that defines the keys used to locate and identify Entity vectors\
      \ in a vector store.\n\nPurpose:\n    Provides the valid keys for identifying\
      \ an Entity vector, typically by id or by title.\n\nAttributes:\n    ID: The\
      \ enum member representing the string key \"id\".\n    TITLE: The enum member\
      \ representing the string key \"title\".\n\nMethods:\n    from_string(value:\
      \ str) -> EntityVectorStoreKey:\n        Convert a string key to the corresponding\
      \ EntityVectorStoreKey enum member.\n        Returns: The corresponding enum\
      \ member (EntityVectorStoreKey.ID for \"id\",\n                 EntityVectorStoreKey.TITLE\
      \ for \"title\").\n        Raises: ValueError if value is not a valid key (\"\
      id\" or \"title\")."
    methods:
    - name: from_string
      signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
- file: graphrag/query/context_builder/local_context.py
  functions:
  - node_id: graphrag/query/context_builder/local_context.py::build_entity_context
    name: build_entity_context
    signature: "def build_entity_context(\n    selected_entities: list[Entity],\n\
      \    tokenizer: Tokenizer | None = None,\n    max_context_tokens: int = 8000,\n\
      \    include_entity_rank: bool = True,\n    rank_description: str = \"number\
      \ of relationships\",\n    column_delimiter: str = \"|\",\n    context_name=\"\
      Entities\",\n) -> tuple[str, pd.DataFrame]"
    docstring: "Prepare entity data table as context data for system prompt.\n\nArgs:\n\
      \    selected_entities (list[Entity]): Entities to include in the context.\n\
      \    tokenizer (Tokenizer | None): Tokenizer to measure token counts; if None,\
      \ get_tokenizer() is used.\n    max_context_tokens (int): Maximum allowed tokens\
      \ for the generated context text. Parsing stops when adding a new entity would\
      \ exceed this limit.\n    include_entity_rank (bool): Whether to include the\
      \ entity's rank in the context row.\n    rank_description (str): Label for the\
      \ rank column (e.g., \"number of relationships\").\n    column_delimiter (str):\
      \ Delimiter to join fields in the context text.\n    context_name (str): Name\
      \ of the context section used in the header, default \"Entities\".\n\nReturns:\n\
      \    tuple[str, pd.DataFrame]: A tuple containing:\n        - current_context_text:\
      \ The textual context including header and entity rows (up to max_context_tokens).\n\
      \        - record_df: A DataFrame of the context records (excluding the header).\
      \ If no entities were processed, this is an empty DataFrame."
  - node_id: graphrag/query/context_builder/local_context.py::build_covariates_context
    name: build_covariates_context
    signature: "def build_covariates_context(\n    selected_entities: list[Entity],\n\
      \    covariates: list[Covariate],\n    tokenizer: Tokenizer | None = None,\n\
      \    max_context_tokens: int = 8000,\n    column_delimiter: str = \"|\",\n \
      \   context_name: str = \"Covariates\",\n) -> tuple[str, pd.DataFrame]"
    docstring: "Prepare covariate data tables as context data for system prompt.\n\
      \nArgs:\n  selected_entities (list[Entity]): Entities to include in the covariate\
      \ context.\n  covariates (list[Covariate]): Covariates from which to build context\
      \ for the entities.\n  tokenizer (Tokenizer | None): Tokenizer to count tokens;\
      \ if None, get_tokenizer() is used.\n  max_context_tokens (int): Maximum allowed\
      \ tokens for the generated context text. Parsing stops when adding a new covariate\
      \ would exceed this limit.\n  column_delimiter (str): Delimiter used to join\
      \ fields in the context rows (default \"|\").\n  context_name (str): Name used\
      \ to label the context section in the output (default \"Covariates\").\n\nReturns:\n\
      \  tuple[str, pd.DataFrame]: A tuple containing:\n    - current_context_text\
      \ (str): The assembled context text including a header and covariate rows.\n\
      \    - record_df (pd.DataFrame): A DataFrame of the covariate records (excluding\
      \ the header). If no covariates were added, this is an empty DataFrame."
  - node_id: graphrag/query/context_builder/local_context.py::get_candidate_context
    name: get_candidate_context
    signature: "def get_candidate_context(\n    selected_entities: list[Entity],\n\
      \    entities: list[Entity],\n    relationships: list[Relationship],\n    covariates:\
      \ dict[str, list[Covariate]],\n    include_entity_rank: bool = True,\n    entity_rank_description:\
      \ str = \"number of relationships\",\n    include_relationship_weight: bool\
      \ = False,\n) -> dict[str, pd.DataFrame]"
    docstring: "Prepare candidate context data tables (entities, relationships, and\
      \ covariates) to be used in a system prompt.\n\nArgs:\n  selected_entities (list[Entity]):\
      \ The selected entities for which to build candidate context.\n  entities (list[Entity]):\
      \ The pool of entities to consider when constructing the candidate entity DataFrame.\n\
      \  relationships (list[Relationship]): The pool of relationships to filter to\
      \ candidate relationships.\n  covariates (dict[str, list[Covariate]]): Mapping\
      \ from covariate group names to Covariate objects to include in the context.\n\
      \  include_entity_rank (bool): Whether to include a rank column for entities.\
      \ Default is True.\n  entity_rank_description (str): Header name for the rank\
      \ column when include_entity_rank is True. Default is \"number of relationships\"\
      .\n  include_relationship_weight (bool): Whether to include a weight column\
      \ in the relationships DataFrame. Default is False.\n\nReturns:\n  dict[str,\
      \ pd.DataFrame]: A dictionary containing the following DataFrames:\n    - \"\
      relationships\": DataFrame of candidate relationships (optionally including\
      \ weight).\n    - \"entities\": DataFrame of candidate entities (with optional\
      \ rank column).\n    - For each key in covariates, a lowercase-keyed DataFrame\
      \ of the corresponding covariates."
  - node_id: graphrag/query/context_builder/local_context.py::_filter_relationships
    name: _filter_relationships
    signature: "def _filter_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    top_k_relationships: int = 10,\n\
      \    relationship_ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
    docstring: "Filter and sort relationships based on a set of selected entities\
      \ and a ranking attribute.\n\nFirst priority: in-network relationships (i.e.\
      \ relationships between selected entities). Second priority: out-of-network\
      \ relationships (i.e. relationships between selected entities and other entities\
      \ not in the selected set). Within out-of-network relationships, mutual relationships\
      \ (shared with multiple selected entities) are prioritized by counting links\
      \ per out-network entity.\n\nArgs:\n    selected_entities (list[Entity]): The\
      \ selected entities to consider.\n    relationships (list[Relationship]): The\
      \ pool of relationships to search within.\n    top_k_relationships (int): The\
      \ maximum number of out-of-network relationships to include per selected entity\
      \ (default 10).\n    relationship_ranking_attribute (str): The attribute name\
      \ used for ranking; defaults to \"rank\".\n\nReturns:\n    list[Relationship]:\
      \ The filtered and sorted relationships. The result is the concatenation of\
      \ in-network relationships and the top-ranked out-of-network relationships,\
      \ truncated to a budget equal to top_k_relationships * len(selected_entities)."
  - node_id: graphrag/query/context_builder/local_context.py::build_relationship_context
    name: build_relationship_context
    signature: "def build_relationship_context(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    tokenizer: Tokenizer | None = None,\n\
      \    include_relationship_weight: bool = False,\n    max_context_tokens: int\
      \ = 8000,\n    top_k_relationships: int = 10,\n    relationship_ranking_attribute:\
      \ str = \"rank\",\n    column_delimiter: str = \"|\",\n    context_name: str\
      \ = \"Relationships\",\n) -> tuple[str, pd.DataFrame]"
    docstring: "Prepare relationship context data tables as context data for system\
      \ prompt.\n\nArgs:\n  selected_entities (list[Entity]): The selected entities\
      \ for which to build the relationship context.\n  relationships (list[Relationship]):\
      \ The pool of relationships to filter to generate context from.\n  tokenizer\
      \ (Tokenizer | None): Tokenizer to count tokens; if None, get_tokenizer() is\
      \ used.\n  include_relationship_weight (bool): Whether to include the relationship\
      \ weight in the generated context.\n  max_context_tokens (int): Maximum allowed\
      \ tokens for the generated context text. Parsing stops when adding a new relationship\
      \ would exceed this limit.\n  top_k_relationships (int): Number of top relationships\
      \ to consider when building the context.\n  relationship_ranking_attribute (str):\
      \ Attribute name used for ranking relationships.\n  column_delimiter (str):\
      \ Delimiter used to separate columns in the generated text.\n  context_name\
      \ (str): Name used in the header block of the context.\n\nReturns:\n  tuple[str,\
      \ pd.DataFrame]: The generated context text and a DataFrame containing the records\
      \ (excluding the header). If no relevant entities or relationships exist, returns\
      \ an empty string and an empty DataFrame."
  classes: []
- file: graphrag/query/context_builder/rate_relevancy.py
  functions:
  - node_id: graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
    name: rate_relevancy
    signature: "def rate_relevancy(\n    query: str,\n    description: str,\n    model:\
      \ ChatModel,\n    tokenizer: Tokenizer,\n    rate_query: str = RATE_QUERY,\n\
      \    num_repeats: int = 1,\n    semaphore: asyncio.Semaphore | None = None,\n\
      \    **model_params: Any,\n) -> dict[str, Any]"
    docstring: "Rate the relevancy between the query and description on a scale of\
      \ 0 to 10.\n\nArgs:\n    query (str): the query (or question) to rate against\n\
      \    description (str): the community description to rate, it can be the community\
      \ title, summary, or the full content.\n    model (ChatModel): LLM model to\
      \ use for rating\n    tokenizer (Tokenizer): tokenizer\n    rate_query (str):\
      \ prompt template used to format the system message with the given description\
      \ and question\n    num_repeats (int): number of times to repeat the rating\
      \ process for the same community (default: 1)\n    semaphore (asyncio.Semaphore\
      \ | None): asyncio.Semaphore to limit the number of concurrent LLM calls (default:\
      \ None)\n    model_params (dict[str, Any]): additional arguments to pass to\
      \ the LLM model\nReturns:\n    dict[str, Any]: a dictionary containing the final\
      \ rating and related metadata:\n        rating (int): the final chosen rating\n\
      \        ratings (list[int]): list of individual ratings collected\n       \
      \ llm_calls (int): number of LLM calls performed\n        prompt_tokens (int):\
      \ total number of tokens used for prompts\n        output_tokens (int): total\
      \ number of tokens produced by the model\nRaises:\n    Exception: if an error\
      \ occurs during model invocation or response parsing that propagates from the\
      \ underlying LLM API"
  classes: []
- file: graphrag/query/context_builder/source_context.py
  functions:
  - node_id: graphrag/query/context_builder/source_context.py::count_relationships
    name: count_relationships
    signature: "def count_relationships(\n    entity_relationships: list[Relationship],\
      \ text_unit: TextUnit\n) -> int"
    docstring: "Count the number of relationships of the selected entity that are\
      \ associated with the text unit.\n\nArgs:\n    entity_relationships: list[Relationship]\n\
      \        The relationships for the selected entity.\n    text_unit: TextUnit\n\
      \        The text unit for which to count related relationships.\n\nReturns:\n\
      \    int\n        The number of relationships in entity_relationships that are\
      \ associated with the given text_unit. If the text_unit has no relationship_ids,\
      \ this is the count of relationships whose text_unit_ids contain the text_unit's\
      \ id; otherwise it is the count of relationships whose id appears in text_unit.relationship_ids."
  - node_id: graphrag/query/context_builder/source_context.py::build_text_unit_context
    name: build_text_unit_context
    signature: "def build_text_unit_context(\n    text_units: list[TextUnit],\n  \
      \  tokenizer: Tokenizer | None = None,\n    column_delimiter: str = \"|\",\n\
      \    shuffle_data: bool = True,\n    max_context_tokens: int = 8000,\n    context_name:\
      \ str = \"Sources\",\n    random_state: int = 86,\n) -> tuple[str, dict[str,\
      \ pd.DataFrame]]"
    docstring: "Prepare text-unit data table as context data for system prompt.\n\n\
      Args:\n    text_units: list[TextUnit]\n        Text units to include in the\
      \ context.\n    tokenizer: Tokenizer | None\n        Tokenizer used to count\
      \ tokens; if None, a tokenizer is retrieved with get_tokenizer().\n    column_delimiter:\
      \ str\n        Delimiter used to separate fields in the context rows.\n    shuffle_data:\
      \ bool\n        Whether to shuffle text_units before building the context.\n\
      \    max_context_tokens: int\n        Maximum number of tokens allowed for the\
      \ generated context.\n    context_name: str\n        Name of the context section\
      \ included in the header.\n    random_state: int\n        Seed used to shuffle\
      \ when shuffle_data is True.\nReturns:\n    tuple[str, dict[str, pd.DataFrame]]\n\
      \        The generated text context and a dictionary mapping the context name\
      \ (lowercased) to a pandas DataFrame containing the context rows."
  classes: []
- file: graphrag/query/factory.py
  functions:
  - node_id: graphrag/query/factory.py::get_drift_search_engine
    name: get_drift_search_engine
    signature: "def get_drift_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    description_embedding_store: BaseVectorStore,\n\
      \    response_type: str,\n    local_system_prompt: str | None = None,\n    reduce_system_prompt:\
      \ str | None = None,\n    callbacks: list[QueryCallbacks] | None = None,\n)\
      \ -> DRIFTSearch"
    docstring: "\"\"\"Create a local drift search engine based on data + configuration.\n\
      \nArgs:\n    config: GraphRagConfig\n        GraphRag configuration object containing\
      \ drift_search settings used to configure the search engine and models.\n  \
      \  reports: list[CommunityReport]\n        Community reports to be used by the\
      \ search context.\n    text_units: list[TextUnit]\n        Text units to be\
      \ included in the search context.\n    entities: list[Entity]\n        Entities\
      \ to be included in the search context.\n    relationships: list[Relationship]\n\
      \        Relationships to be included in the search context.\n    description_embedding_store:\
      \ BaseVectorStore\n        Vector store of text embeddings for entity descriptions.\n\
      \    response_type: str\n        Type of response to generate.\n    local_system_prompt:\
      \ str | None\n        Optional system prompt to be used locally for prompt construction.\n\
      \    reduce_system_prompt: str | None\n        Optional reduced system prompt\
      \ for shorter prompts.\n    callbacks: list[QueryCallbacks] | None\n       \
      \ Optional query callbacks to handle search events.\n\nReturns:\n    DRIFTSearch\n\
      \        A configured DRIFTSearch instance ready to execute drift-based searches.\n\
      \nRaises:\n    None\n\"\"\""
  - node_id: graphrag/query/factory.py::get_local_search_engine
    name: get_local_search_engine
    signature: "def get_local_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    covariates: dict[str, list[Covariate]],\n\
      \    response_type: str,\n    description_embedding_store: BaseVectorStore,\n\
      \    system_prompt: str | None = None,\n    callbacks: list[QueryCallbacks]\
      \ | None = None,\n) -> LocalSearch"
    docstring: "Create a local search engine based on data + configuration.\n\nArgs:\n\
      \    config: GraphRagConfig\n        GraphRag configuration object containing\
      \ local search settings used to configure the search engine and models.\n  \
      \  reports: list[CommunityReport]\n        Community reports to be used by the\
      \ local search engine context.\n    text_units: list[TextUnit]\n        Text\
      \ units to be included in the search context.\n    entities: list[Entity]\n\
      \        Entities to be considered in the search context.\n    relationships:\
      \ list[Relationship]\n        Relationships to be considered in the search context.\n\
      \    covariates: dict[str, list[Covariate]]\n        Covariates to augment context\
      \ for the local search.\n    response_type: str\n        Type of response to\
      \ return from the search engine.\n    description_embedding_store: BaseVectorStore\n\
      \        Vector store containing description embeddings for entities.\n    system_prompt:\
      \ str | None\n        Optional system prompt to guide the local search model.\n\
      \    callbacks: list[QueryCallbacks] | None\n        Optional list of query\
      \ callbacks to execute during search.\n\nReturns:\n    LocalSearch\n       \
      \ A configured LocalSearch instance ready to execute queries.\n\nRaises:\n \
      \   Exception\n        If an error occurs during model initialization or tokenizer\
      \ setup."
  - node_id: graphrag/query/factory.py::get_global_search_engine
    name: get_global_search_engine
    signature: "def get_global_search_engine(\n    config: GraphRagConfig,\n    reports:\
      \ list[CommunityReport],\n    entities: list[Entity],\n    communities: list[Community],\n\
      \    response_type: str,\n    dynamic_community_selection: bool = False,\n \
      \   map_system_prompt: str | None = None,\n    reduce_system_prompt: str | None\
      \ = None,\n    general_knowledge_inclusion_prompt: str | None = None,\n    callbacks:\
      \ list[QueryCallbacks] | None = None,\n) -> GlobalSearch"
    docstring: "Create a global search engine based on data + configuration.\n\nArgs:\n\
      \    config (GraphRagConfig): GraphRag configuration object containing global\
      \ search settings used to configure the global search engine and models.\n \
      \   reports (list[CommunityReport]): Community reports to be used by the global\
      \ search context.\n    entities (list[Entity]): Entities to be included in the\
      \ global search context.\n    communities (list[Community]): Communities to\
      \ be included in the global search context.\n    response_type (str): Response\
      \ type to be used by the global search engine.\n    dynamic_community_selection\
      \ (bool): Whether to enable dynamic community selection for the global search.\n\
      \    map_system_prompt (str | None): Optional system prompt used for mapping\
      \ in the global search.\n    reduce_system_prompt (str | None): Optional system\
      \ prompt used to reduce content for the global search.\n    general_knowledge_inclusion_prompt\
      \ (str | None): Optional prompt to include general knowledge in the global search.\n\
      \    callbacks (list[QueryCallbacks] | None): Optional callbacks to handle query\
      \ events during global search.\n\nReturns:\n    GlobalSearch: A GlobalSearch\
      \ instance configured with the provided data and settings."
  - node_id: graphrag/query/factory.py::get_basic_search_engine
    name: get_basic_search_engine
    signature: "def get_basic_search_engine(\n    text_units: list[TextUnit],\n  \
      \  text_unit_embeddings: BaseVectorStore,\n    config: GraphRagConfig,\n   \
      \ system_prompt: str | None = None,\n    response_type: str = \"multiple paragraphs\"\
      ,\n    callbacks: list[QueryCallbacks] | None = None,\n) -> BasicSearch"
    docstring: "Create a basic search engine based on data + configuration.\n\nArgs:\n\
      \    text_units (list[TextUnit]): Text units to be included in the search context.\n\
      \    text_unit_embeddings (BaseVectorStore): Vector store for text unit embeddings.\n\
      \    config (GraphRagConfig): GraphRag configuration containing basic_search\
      \ settings.\n    system_prompt (str | None): Optional system prompt to override\
      \ the default prompt.\n    response_type (str): Type of response to generate.\
      \ Default: \"multiple paragraphs\".\n    callbacks (list[QueryCallbacks] | None):\
      \ Optional callbacks for query handling.\n\nReturns:\n    BasicSearch: A configured\
      \ BasicSearch instance."
  classes: []
- file: graphrag/query/indexer_adapters.py
  functions:
  - node_id: graphrag/query/indexer_adapters.py::_filter_under_community_level
    name: _filter_under_community_level
    signature: "def _filter_under_community_level(\n    df: pd.DataFrame, community_level:\
      \ int\n) -> pd.DataFrame"
    docstring: "Filter a DataFrame by community level.\n\nArgs:\n    df: pandas DataFrame\
      \ that must contain a column named level used for filtering.\n    community_level:\
      \ int threshold; keep rows where level <= community_level.\n\nReturns:\n   \
      \ pandas.DataFrame with rows where level <= community_level.\n\nRaises:\n  \
      \  AttributeError: if the input DataFrame does not have a level column."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_report_embeddings
    name: read_indexer_report_embeddings
    signature: "def read_indexer_report_embeddings(\n    community_reports: list[CommunityReport],\n\
      \    embeddings_store: BaseVectorStore,\n)"
    docstring: "Read in the Community Reports from the raw indexing outputs.\n\nArgs:\n\
      \  community_reports: list[CommunityReport] - The community reports to enrich\
      \ with embeddings.\n  embeddings_store: BaseVectorStore - The vector store used\
      \ to fetch embeddings by report id.\n\nReturns:\n  None - This function mutates\
      \ the input CommunityReport objects by setting their full_content_embedding."
  - node_id: graphrag/query/indexer_adapters.py::embed_community_reports
    name: embed_community_reports
    signature: "def embed_community_reports(\n    reports_df: pd.DataFrame,\n    embedder:\
      \ EmbeddingModel,\n    source_col: str = \"full_content\",\n    embedding_col:\
      \ str = \"full_content_embedding\",\n) -> pd.DataFrame"
    docstring: "Embed a source column of the reports dataframe using the given embedder.\n\
      \nArgs:\n    reports_df (pd.DataFrame): The reports dataframe to embed. This\
      \ function may mutate the dataframe in place by adding a new embedding column\
      \ if it does not already exist.\n    embedder (EmbeddingModel): The model used\
      \ to generate embeddings from the content of the source column.\n    source_col\
      \ (str): Name of the column in reports_df that contains the text to embed. Defaults\
      \ to \"full_content\".\n    embedding_col (str): Name of the column to store\
      \ the generated embeddings. If this column does not exist, it will be created\
      \ and populated. Defaults to \"full_content_embedding\".\n\nReturns:\n    pd.DataFrame:\
      \ The input DataFrame, augmented with embedding_col. The same DataFrame object\
      \ is returned (in-place mutation when embedding_col is created).\n\nRaises:\n\
      \    ValueError: If the source_col is missing from reports_df.\n\nExamples:\n\
      \    # Basic usage\n    df = embed_community_reports(reports_df, embedder)"
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_text_units
    name: read_indexer_text_units
    signature: 'def read_indexer_text_units(final_text_units: pd.DataFrame) -> list[TextUnit]'
    docstring: "Read in the Text Units from the raw indexing outputs.\n\nArgs:\n \
      \ final_text_units (pd.DataFrame): The DataFrame containing the final text units\
      \ produced by indexing outputs.\n\nReturns:\n  list[TextUnit]: A list of TextUnit\
      \ objects parsed from the input DataFrame."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_entities
    name: read_indexer_entities
    signature: "def read_indexer_entities(\n    entities: pd.DataFrame,\n    communities:\
      \ pd.DataFrame,\n    community_level: int | None,\n) -> list[Entity]"
    docstring: "Read in the Entities from the raw indexing outputs.\n\nThis function\
      \ joins the entity records with their associated communities, optionally filters\
      \ by a\nspecified community_level, aggregates community memberships per entity,\
      \ and converts the resulting\nrecords into Entity objects using read_entities.\n\
      \nArgs:\n    entities (pd.DataFrame): DataFrame containing entity information\
      \ with at least an \"id\" column.\n    communities (pd.DataFrame): DataFrame\
      \ containing community information including \"entity_ids\",\n        \"community\"\
      , and \"level\".\n    community_level (int | None): If not None, keep entities\
      \ with associated communities at or below this level.\n\nReturns:\n    list[Entity]:\
      \ The list of Entity objects constructed from the input data.\n\nRaises:\n \
      \   AttributeError: If community_level is not None and the intermediate DataFrame\
      \ does not contain a\n        \"level\" column (as expected by _filter_under_community_level)."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_relationships
    name: read_indexer_relationships
    signature: 'def read_indexer_relationships(final_relationships: pd.DataFrame)
      -> list[Relationship]'
    docstring: "Read in the Relationships from the raw indexing outputs.\n\nThis function\
      \ delegates to read_relationships with the following mappings: df = final_relationships,\
      \ short_id_col = \"human_readable_id\", rank_col = \"combined_degree\", description_embedding_col\
      \ = None, attributes_cols = None.\n\nArgs:\n    final_relationships (pd.DataFrame):\
      \ The DataFrame containing raw indexing outputs for relationships.\n\nReturns:\n\
      \    list[Relationship]: A list of Relationship objects constructed from the\
      \ input data."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_covariates
    name: read_indexer_covariates
    signature: 'def read_indexer_covariates(final_covariates: pd.DataFrame) -> list[Covariate]'
    docstring: "Read in the Covariates from the raw indexing outputs.\n\nThis function\
      \ converts the id column to a string and delegates to read_covariates to construct\
      \ Covariate objects using the input DataFrame with id cast to string, short_id_col\
      \ set to human_readable_id, attributes_cols set to object_id, status, start_date,\
      \ end_date, and description, and text_unit_ids_col set to None.\n\nArgs:\n \
      \ final_covariates (pd.DataFrame): DataFrame containing covariate records produced\
      \ by the indexing process. Must include an id column (which will be cast to\
      \ string) and a human_readable_id column used as the short identifier. The covariate\
      \ attributes to export are object_id, status, start_date, end_date, and description.\n\
      \nReturns:\n  list[Covariate]: A list of Covariate objects parsed from the covariates\
      \ DataFrame.\n\nRaises:\n  KeyError: If required columns are missing from final_covariates.\n\
      \  ValueError: If data in columns cannot be coerced to the expected types.\n\
      \  Exception: Propagates exceptions raised by read_covariates."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_reports
    name: read_indexer_reports
    signature: "def read_indexer_reports(\n    final_community_reports: pd.DataFrame,\n\
      \    final_communities: pd.DataFrame,\n    community_level: int | None,\n  \
      \  dynamic_community_selection: bool = False,\n    content_embedding_col: str\
      \ = \"full_content_embedding\",\n    config: GraphRagConfig | None = None,\n\
      ) -> list[CommunityReport]"
    docstring: "Read in the Community Reports from the raw indexing outputs.\n\nIf\
      \ not dynamic_community_selection, then select reports with the max community\
      \ level that an entity belongs to.\n\nArgs:\n    final_community_reports: pd.DataFrame\n\
      \        The DataFrame containing raw community reports produced by the indexer.\n\
      \    final_communities: pd.DataFrame\n        The DataFrame containing final\
      \ communities; entities are exploded from the entity_ids column.\n    community_level:\
      \ int | None\n        Threshold on community level to filter results. If None,\
      \ no filtering is applied.\n    dynamic_community_selection: bool\n        If\
      \ False, perform a community level roll up to the maximum level per title.\n\
      \    content_embedding_col: str\n        Column name for content embeddings.\
      \ Defaults to \"full_content_embedding\".\n    config: GraphRagConfig | None\n\
      \        Optional configuration object for embedding model lookup and generation.\n\
      \nReturns:\n    list[CommunityReport]\n        A list of CommunityReport objects\
      \ read from the processed reports dataframe.\n\nRaises:\n    AttributeError:\
      \ if the input DataFrame does not have a level column...."
  - node_id: graphrag/query/indexer_adapters.py::read_indexer_communities
    name: read_indexer_communities
    signature: "def read_indexer_communities(\n    final_communities: pd.DataFrame,\n\
      \    final_community_reports: pd.DataFrame,\n) -> list[Community]"
    docstring: "Read in the Communities from the raw indexing outputs and reconstruct\
      \ the community hierarchy information.\n\nThe function explodes the entity_ids\
      \ per community to propagate entity membership, validates that the communities\
      \ present in final_communities are covered by final_community_reports, logs\
      \ a warning and filters if any are missing, and finally builds Community objects\
      \ via read_communities with a mapping of columns.\n\nArgs:\n  final_communities\
      \ (pd.DataFrame): The final communities DataFrame produced by the indexer. Expected\
      \ to contain at minimum id, community, title, level, and entity_ids (a list\
      \ of related entity IDs) for proper expansion.\n  final_community_reports (pd.DataFrame):\
      \ The DataFrame containing community reports used to validate and align the\
      \ community structure. Must include a community column used for matching.\n\n\
      Returns:\n  list[Community]: A list of reconstructed Community objects, including\
      \ hierarchy information and sub-community relations, as produced by read_communities.\
      \ The input communities are filtered to align with reported communities when\
      \ necessary.\n\nRaises:\n  ValueError: If required columns are missing from\
      \ inputs (e.g., id, title, level, community, or entity_ids) or if downstream\
      \ read_communities raises due to malformed data.\n  Exceptions propagated from\
      \ read_communities (e.g., KeyError, TypeError) as a result of input validation\
      \ or data shape issues.\n\nNotes:\n  - If there are communities in final_communities\
      \ that do not appear in final_community_reports,\n    a warning is logged and\
      \ those communities (and their exploded entity mappings) are dropped to\n  \
      \  yield results consistent with the available reports.\n  - Internal steps\
      \ include exploding entity_ids to map entities to communities, filtering by\n\
      \    reported communities, and delegating to read_communities with specific\
      \ column mappings:\n      id_col=\"id\", short_id_col=\"community\", title_col=\"\
      title\", level_col=\"level\",\n      parent_col=\"parent\", children_col=\"\
      children\"."
  classes: []
- file: graphrag/query/input/loaders/dfs.py
  functions:
  - node_id: graphrag/query/input/loaders/dfs.py::_prepare_records
    name: _prepare_records
    signature: 'def _prepare_records(df: pd.DataFrame) -> list[dict]'
    docstring: "Reset the index of the DataFrame, rename the reset index column to\
      \ 'Index', and convert the result to a list of dictionaries.\n\nArgs:\n    df:\
      \ The DataFrame to process.\n\nReturns:\n    list[dict]: A list of dictionaries\
      \ representing the DataFrame rows. Each dictionary includes an 'Index' key for\
      \ the original row index."
  - node_id: graphrag/query/input/loaders/dfs.py::read_text_units
    name: read_text_units
    signature: "def read_text_units(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    text_col: str = \"text\",\n    entities_col: str | None = \"entity_ids\"\
      ,\n    relationships_col: str | None = \"relationship_ids\",\n    covariates_col:\
      \ str | None = \"covariate_ids\",\n    tokens_col: str | None = \"n_tokens\"\
      ,\n    document_ids_col: str | None = \"document_ids\",\n    attributes_cols:\
      \ list[str] | None = None,\n) -> list[TextUnit]"
    docstring: "Read text units from a dataframe using pre-converted records.\n\n\
      Args:\n  df (pd.DataFrame): The DataFrame to process.\n  id_col (str): Column\
      \ name for the text unit identifier. Default 'id'.\n  text_col (str): Column\
      \ name containing the text of the unit. Default 'text'.\n  entities_col (str\
      \ | None): Column name for entity_ids, or None to omit. Default 'entity_ids'.\n\
      \  relationships_col (str | None): Column name for relationship_ids, or None.\
      \ Default 'relationship_ids'.\n  covariates_col (str | None): Column name for\
      \ covariate_ids, or None. Default 'covariate_ids'.\n  tokens_col (str | None):\
      \ Column name for the token count. Default 'n_tokens'.\n  document_ids_col (str\
      \ | None): Column name for document_ids, or None. Default 'document_ids'.\n\
      \  attributes_cols (list[str] | None): Additional per-row attributes to include\
      \ in the TextUnit. If None, no extra attributes are captured. Default None.\n\
      \nReturns:\n  list[TextUnit]: A list of TextUnit objects constructed from the\
      \ dataframe rows.\n\nRaises:\n  ValueError: If column_name is None or the column\
      \ is missing from data during value extraction (as enforced by to_str).\n  TypeError:\
      \ If a value in a column cannot be converted to the expected type by the helper\
      \ utilities (as raised by to_optional_list, to_optional_dict, or to_optional_int)."
  - node_id: graphrag/query/input/loaders/dfs.py::read_entities
    name: read_entities
    signature: "def read_entities(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
      ,\n    short_id_col: str | None = \"human_readable_id\",\n    title_col: str\
      \ = \"title\",\n    type_col: str | None = \"type\",\n    description_col: str\
      \ | None = \"description\",\n    name_embedding_col: str | None = \"name_embedding\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    community_col: str | None = \"community_ids\",\n    text_unit_ids_col:\
      \ str | None = \"text_unit_ids\",\n    rank_col: str | None = \"degree\",\n\
      \    attributes_cols: list[str] | None = None,\n) -> list[Entity]"
    docstring: "Read entities from a dataframe using pre-converted records.\n\nThis\
      \ function prepares the input DataFrame by resetting the index and converting\
      \ it to a list of dictionaries, then constructs Entity objects from each record.\n\
      \nArgs:\n  df (pd.DataFrame): The DataFrame to process.\n  id_col (str): Column\
      \ name for the entity identifier. Default id.\n  short_id_col (str | None):\
      \ Column name for a short/human readable id. If None, uses the index value.\
      \ Default human_readable_id.\n  title_col (str): Column name for the entity\
      \ title. Default title.\n  type_col (str | None): Column name for the entity\
      \ type. If None, the type is omitted. Default type.\n  description_col (str\
      \ | None): Column name for the entity description. If None, the description\
      \ is omitted. Default description.\n  name_embedding_col (str | None): Column\
      \ name for the name embedding vector. If None, omitted. Default name_embedding.\n\
      \  description_embedding_col (str | None): Column name for the description embedding\
      \ vector. If None, omitted. Default description_embedding.\n  community_col\
      \ (str | None): Column name for the list of community IDs. If None, omitted.\
      \ Default community_ids.\n  text_unit_ids_col (str | None): Column name for\
      \ the list of text unit IDs. If None, omitted. Default text_unit_ids.\n  rank_col\
      \ (str | None): Column name for the rank/degree. If None, omitted. Default degree.\n\
      \  attributes_cols (list[str] | None): Optional list of additional attribute\
      \ column names to include in the resulting Entity.attributes. If None, no extra\
      \ attributes are included.\n\nReturns:\n  list[Entity]: A list of Entity objects\
      \ created from the dataframe records.\n\nRaises:\n  ValueError, TypeError: If\
      \ required data is missing or has invalid type for the expected columns, as\
      \ raised by the underlying conversion helpers (e.g., to_str, to_optional_str,\
      \ to_optional_list, to_optional_int)."
  - node_id: graphrag/query/input/loaders/dfs.py::read_relationships
    name: read_relationships
    signature: "def read_relationships(\n    df: pd.DataFrame,\n    id_col: str =\
      \ \"id\",\n    short_id_col: str | None = \"human_readable_id\",\n    source_col:\
      \ str = \"source\",\n    target_col: str = \"target\",\n    description_col:\
      \ str | None = \"description\",\n    rank_col: str | None = \"combined_degree\"\
      ,\n    description_embedding_col: str | None = \"description_embedding\",\n\
      \    weight_col: str | None = \"weight\",\n    text_unit_ids_col: str | None\
      \ = \"text_unit_ids\",\n    attributes_cols: list[str] | None = None,\n) ->\
      \ list[Relationship]"
    docstring: "Read relationships from a dataframe using pre-converted records.\n\
      \nArgs:\n    df: The DataFrame to process.\n    id_col: Column name for the\
      \ relationship identifier.\n    short_id_col: Column name for a short/human-readable\
      \ id. If None, uses the index-derived id.\n    source_col: Column name for the\
      \ relationship source.\n    target_col: Column name for the relationship target.\n\
      \    description_col: Optional column with a textual description of the relationship.\n\
      \    rank_col: Optional column with the rank (degree) of the relationship.\n\
      \    description_embedding_col: Optional column containing a description embedding\
      \ as a list of floats.\n    weight_col: Optional column with the weight of the\
      \ relationship.\n    text_unit_ids_col: Optional column with text unit identifiers\
      \ as a list of strings.\n    attributes_cols: Optional list of additional attribute\
      \ column names to include in the attributes dict.\n\nReturns:\n    list[Relationship]:\
      \ A list of Relationship objects constructed from the dataframe records.\n\n\
      Raises:\n    ValueError: If a required column name is None or missing from the\
      \ input data when converting values.\n    TypeError: If a value cannot be converted\
      \ to the expected type (e.g., incorrect list element types or mismatched types\
      \ in conversions)."
  - node_id: graphrag/query/input/loaders/dfs.py::read_covariates
    name: read_covariates
    signature: "def read_covariates(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    short_id_col: str | None = \"human_readable_id\",\n    subject_col:\
      \ str = \"subject_id\",\n    covariate_type_col: str | None = \"type\",\n  \
      \  text_unit_ids_col: str | None = \"text_unit_ids\",\n    attributes_cols:\
      \ list[str] | None = None,\n) -> list[Covariate]"
    docstring: "Read covariates from a dataframe using pre-converted records.\n\n\
      Args:\n    df (pd.DataFrame): The DataFrame to process.\n    id_col (str): Column\
      \ name for the covariate identifier. Default 'id'.\n    short_id_col (str |\
      \ None): Column name for a short/human-readable id. If None, uses the index-derived\
      \ id.\n    subject_col (str): Column name for the subject identifier. Default\
      \ 'subject_id'.\n    covariate_type_col (str | None): Column name for covariate\
      \ type. If None, defaults to 'claim'.\n    text_unit_ids_col (str | None): Column\
      \ name for text_unit_ids. Default 'text_unit_ids'.\n    attributes_cols (list[str]\
      \ | None): Optional list of additional attribute column names to include in\
      \ Covariate.attributes.\n\nReturns:\n    list[Covariate]: A list of Covariate\
      \ objects constructed from the dataframe.\n\nRaises:\n    ValueError: If a required\
      \ column name is None or missing in a row.\n    TypeError: If the value for\
      \ text_unit_ids_col cannot be interpreted as a list (as required by to_optional_list)."
  - node_id: graphrag/query/input/loaders/dfs.py::read_community_reports
    name: read_community_reports
    signature: "def read_community_reports(\n    df: pd.DataFrame,\n    id_col: str\
      \ = \"id\",\n    short_id_col: str | None = \"community\",\n    title_col: str\
      \ = \"title\",\n    community_col: str = \"community\",\n    summary_col: str\
      \ = \"summary\",\n    content_col: str = \"full_content\",\n    rank_col: str\
      \ | None = \"rank\",\n    content_embedding_col: str | None = \"full_content_embedding\"\
      ,\n    attributes_cols: list[str] | None = None,\n) -> list[CommunityReport]"
    docstring: "Read community reports from a dataframe using pre-converted records.\n\
      \nArgs:\n    df: The DataFrame to process.\n    id_col: The column name to use\
      \ for the report id.\n    short_id_col: The column name for the short identifier;\
      \ if None, the code falls back to the index.\n    title_col: The column name\
      \ for the report title.\n    community_col: The column name containing the community\
      \ identifier.\n    summary_col: The column name containing the summary.\n  \
      \  content_col: The column name containing the full content.\n    rank_col:\
      \ The column name containing the rank value, if present.\n    content_embedding_col:\
      \ The column name containing the full content embeddings, if present.\n    attributes_cols:\
      \ Optional list of additional attribute columns to include; if None, no extra\
      \ attributes are added.\n\nReturns:\n    list[CommunityReport]: A list of CommunityReport\
      \ objects constructed from the dataframe rows.\n\nRaises:\n    ValueError: If\
      \ a required column is None or missing from the input data, or if value conversion\
      \ fails in helper utilities.\n    TypeError: If attribute column values are\
      \ not of expected types."
  - node_id: graphrag/query/input/loaders/dfs.py::read_communities
    name: read_communities
    signature: "def read_communities(\n    df: pd.DataFrame,\n    id_col: str = \"\
      id\",\n    short_id_col: str | None = \"community\",\n    title_col: str = \"\
      title\",\n    level_col: str = \"level\",\n    entities_col: str | None = \"\
      entity_ids\",\n    relationships_col: str | None = \"relationship_ids\",\n \
      \   text_units_col: str | None = \"text_unit_ids\",\n    covariates_col: str\
      \ | None = \"covariate_ids\",\n    parent_col: str | None = \"parent\",\n  \
      \  children_col: str | None = \"children\",\n    attributes_cols: list[str]\
      \ | None = None,\n) -> list[Community]"
    docstring: "Read communities from a dataframe using pre-converted records.\n\n\
      Args:\n  df: The DataFrame to process.\n  id_col: Column name for the community\
      \ identifier.\n  short_id_col: Column name for the short identifier; if None,\
      \ uses the index as a fallback.\n  title_col: Title column name.\n  level_col:\
      \ Level column name.\n  entities_col: Column name for the list of entity_ids\
      \ associated with the community.\n  relationships_col: Column name for the list\
      \ of relationship_ids associated with the community.\n  text_units_col: Column\
      \ name for the list of text_unit_ids.\n  covariates_col: Column name for the\
      \ dictionary of covariate_ids.\n  parent_col: Column name for the parent identifier.\n\
      \  children_col: Column name for the children list.\n  attributes_cols: Optional\
      \ list of additional attribute columns to include in the Community attributes;\
      \ None to skip.\n\nReturns:\n  list[Community]: A list of Community objects\
      \ constructed from the dataframe.\n\nRaises:\n  ValueError: If a required column\
      \ is None or missing from a row during conversion.\n  TypeError: If a value\
      \ is not of an expected type."
  classes: []
- file: graphrag/query/input/loaders/utils.py
  functions:
  - node_id: graphrag/query/input/loaders/utils.py::to_optional_float
    name: to_optional_float
    signature: 'def to_optional_float(data: Mapping[str, Any], column_name: str |
      None) -> float | None'
    docstring: "Convert a value to an optional float.\n\nIf the specified column is\
      \ missing or its value is None, returns None. Otherwise, converts the value\
      \ to a float using Python's built-in float().\n\nArgs:\n    data (Mapping[str,\
      \ Any]): Input data mapping containing potential value\n    column_name (str\
      \ | None): Key to retrieve from data; if None or not present, returns None\n\
      \nReturns:\n    float | None: The value converted to a float, or None if the\
      \ column is missing or its value is None\n\nRaises:\n    ValueError: If the\
      \ value cannot be converted to a float (e.g., non-numeric strings).\n    TypeError:\
      \ If the value type is not compatible with float().\n    OverflowError: If the\
      \ numeric value is too large to convert.\n\nNotes:\n    - If column_name is\
      \ None or not present in data, returns None.\n    - If the value is None, returns\
      \ None.\n    - Non-numeric inputs that cannot be parsed as float will raise\
      \ ValueError."
  - node_id: graphrag/query/input/loaders/utils.py::_get_value
    name: _get_value
    signature: "def _get_value(\n    data: Mapping[str, Any], column_name: str | None,\
      \ required: bool = True\n) -> Any"
    docstring: "Retrieve a value for a column from the given data mapping.\n\nIf the\
      \ column is required (required=True), raises ValueError when:\n- column_name\
      \ is None, or\n- column_name is not in data.\n\nFor optional columns (required=False),\
      \ returns None when column_name is None or when the column is missing from data.\n\
      \nArgs:\n    data: Mapping[str, Any]\n        The mapping that contains column\
      \ values.\n    column_name: str | None\n        The name of the column to retrieve,\
      \ or None.\n    required: bool\n        Whether the column must be present in\
      \ data.\n\nReturns:\n    Any\n        The value associated with column_name\
      \ in data if present; otherwise None when\n        the column is optional and\
      \ missing, or when column_name is None and required is False.\n\nRaises:\n \
      \   ValueError\n        If column_name is None and required is True, or if column_name\
      \ is not in data and\n        required is True."
  - node_id: graphrag/query/input/loaders/utils.py::to_optional_int
    name: to_optional_int
    signature: 'def to_optional_int(data: Mapping[str, Any], column_name: str | None)
      -> int | None'
    docstring: "Convert and validate a value to an optional int.\n\nIf the specified\
      \ column is missing from data or is None, returns None. If the value is a float,\
      \ it will be converted to an int by truncation. If the resulting value is not\
      \ an int, a TypeError is raised.\n\nArgs:\n    data (Mapping[str, Any]): Input\
      \ data mapping containing potential value\n    column_name (str | None): Key\
      \ to retrieve from data; if None or not present, returns None\n\nReturns:\n\
      \    int | None: The value converted to int, or None if missing or None\n\n\
      Raises:\n    TypeError: If the value cannot be interpreted as an int after conversion"
  - node_id: graphrag/query/input/loaders/utils.py::to_optional_dict
    name: to_optional_dict
    signature: "def to_optional_dict(\n    data: Mapping[str, Any],\n    column_name:\
      \ str | None,\n    key_type: type | None = None,\n    value_type: type | None\
      \ = None,\n) -> dict | None"
    docstring: "\"\"\"Convert and validate a value to an optional dict.\n\nArgs:\n\
      \    data: Mapping[str, Any]\n        Input data container.\n    column_name:\
      \ str | None\n        The key in data to retrieve. If None or not present, returns\
      \ None.\n    key_type: type | None\n        If provided, require all dict keys\
      \ to be instances of this type.\n    value_type: type | None\n        If provided,\
      \ require all dict values to be instances of this type.\n\nReturns:\n    dict\
      \ | None\n        The extracted dict if present and not None; otherwise None.\n\
      \nRaises:\n    TypeError\n        If the value associated with column_name is\
      \ not a dict, or if any dict key does not\n        match key_type, or if any\
      \ dict value does not match value_type.\n\"\"\""
  - node_id: graphrag/query/input/loaders/utils.py::to_optional_list
    name: to_optional_list
    signature: "def to_optional_list(\n    data: Mapping[str, Any], column_name: str\
      \ | None, item_type: type | None = None\n) -> list | None"
    docstring: "Convert and validate a value to an optional list.\n\nThis function\
      \ retrieves the value for column_name from data. If column_name is None or not\
      \ present in data, or if the value is None, it returns None. If the value is\
      \ a numpy array, it is converted to a Python list. If the value is a string,\
      \ it is wrapped in a single-element list. If the resulting value is not a list,\
      \ a TypeError is raised. If item_type is provided, every element in the list\
      \ must be an instance of item_type; otherwise a TypeError is raised.\n\nArgs:\n\
      \  data: Mapping[str, Any]\n      Input data container.\n  column_name: str\
      \ | None\n      The key in data to retrieve. If None or not present, returns\
      \ None. If present but value is None, also returns None.\n  item_type: type\
      \ | None\n      If provided, require all list items to be instances of this\
      \ type.\n\nReturns:\n  list | None\n      The optional list value, or None if\
      \ the key is missing or its value is None.\n\nRaises:\n  TypeError\n      If\
      \ the retrieved value cannot be converted to a list or if any list item is not\
      \ of the specified item_type."
  - node_id: graphrag/query/input/loaders/utils.py::to_str
    name: to_str
    signature: 'def to_str(data: Mapping[str, Any], column_name: str | None) -> str'
    docstring: "Convert and validate a value to a string.\n\nArgs:\n  data (Mapping[str,\
      \ Any]): The mapping that contains column values.\n  column_name (str | None):\
      \ The name of the column to retrieve, or None.\n\nReturns:\n  str: The string\
      \ representation of the value.\n\nRaises:\n  ValueError: If column_name is None\
      \ or the column is missing from data."
  - node_id: graphrag/query/input/loaders/utils.py::to_optional_str
    name: to_optional_str
    signature: 'def to_optional_str(data: Mapping[str, Any], column_name: str | None)
      -> str | None'
    docstring: "Convert and validate a value to an optional string.\n\nRetrieve the\
      \ value for column_name from data and convert it to a string if it is not None.\
      \ If the value is None, returns None. If column_name is None or not present\
      \ in data, raises ValueError.\n\nArgs:\n    data (Mapping[str, Any]): Input\
      \ data mapping containing column values.\n    column_name (str | None): The\
      \ name of the column to retrieve. If None or not present in data, a ValueError\
      \ will be raised.\n\nReturns:\n    str | None: The string representation of\
      \ the value, or None if the value is None.\n\nRaises:\n    ValueError: If column_name\
      \ is None or column_name is not found in data."
  - node_id: graphrag/query/input/loaders/utils.py::to_list
    name: to_list
    signature: "def to_list(\n    data: Mapping[str, Any], column_name: str | None,\
      \ item_type: type | None = None\n) -> list"
    docstring: "Convert and validate a value to a list.\n\nThis function retrieves\
      \ the value for the given column from data using _get_value with required=True.\
      \ If the value is a numpy.ndarray, it is converted to a Python list before validation.\
      \ If the resulting value is not a list, a TypeError is raised. If item_type\
      \ is provided, all items in the list must be instances of that type; otherwise\
      \ a TypeError is raised.\n\nArgs:\n    data (Mapping[str, Any]): The mapping\
      \ that contains column values.\n    column_name (str | None): The name of the\
      \ column to retrieve, or None. If None or missing, _get_value will raise ValueError.\n\
      \    item_type (type | None): Optional type that each item in the resulting\
      \ list must be.\n\nReturns:\n    list: The converted and validated list.\n\n\
      Raises:\n    TypeError: If the value is not a list, or any list item is not\
      \ of the specified item_type.\n    ValueError: If column_name is None or the\
      \ column is missing from data (via _get_value)."
  - node_id: graphrag/query/input/loaders/utils.py::to_int
    name: to_int
    signature: 'def to_int(data: Mapping[str, Any], column_name: str | None) -> int'
    docstring: "Convert and validate a value to an int. The value is retrieved from\
      \ data via _get_value(data, column_name, required=True). If the retrieved value\
      \ is a Python float, it is truncated to an integer. After conversion, if the\
      \ value is not a Python int, a TypeError is raised.\n\nArgs:\n    data (Mapping[str,\
      \ Any]): The mapping that contains column values.\n    column_name (str | None):\
      \ The name of the column to retrieve from data. If None or the column is missing,\
      \ _get_value will raise a ValueError.\n\nReturns:\n    int: The value converted\
      \ to int.\n\nRaises:\n    ValueError: If column_name is None or the column_name\
      \ is not present in data (as raised by _get_value with required=True).\n   \
      \ TypeError: If the retrieved value cannot be interpreted as an int after any\
      \ necessary conversion. Note that numpy integer types (e.g., np.int64) are not\
      \ treated as ints and will trigger this error."
  - node_id: graphrag/query/input/loaders/utils.py::to_float
    name: to_float
    signature: 'def to_float(data: Mapping[str, Any], column_name: str | None) ->
      float'
    docstring: "Convert and validate a value to a float.\n\nArgs:\n    data: The mapping\
      \ that contains column values.\n    column_name: The name of the column to retrieve,\
      \ or None.\n\nReturns:\n    float: The value as a float.\n\nRaises:\n    TypeError:\
      \ If the retrieved value is not a float."
  - node_id: graphrag/query/input/loaders/utils.py::to_dict
    name: to_dict
    signature: "def to_dict(\n    data: Mapping[str, Any],\n    column_name: str |\
      \ None,\n    key_type: type | None = None,\n    value_type: type | None = None,\n\
      ) -> dict"
    docstring: "Convert and validate a value to a dict.\n\nThis function retrieves\
      \ a value for the given column from data using _get_value with required=True\
      \ and validates that the value is a dictionary. If key_type is provided, all\
      \ keys must be instances of that type. If value_type is provided, all values\
      \ must be instances of that type. Returns the dict if valid.\n\nArgs:\n  data:\
      \ Mapping[str, Any]\n      The mapping that contains column values.\n  column_name:\
      \ str | None\n      The name of the column to retrieve, or None.\n  key_type:\
      \ type | None\n      If provided, require all dict keys to be instances of this\
      \ type.\n  value_type: type | None\n      If provided, require all dict values\
      \ to be instances of this type.\n\nReturns:\n  dict: The validated dictionary\
      \ retrieved from data.\n\nRaises:\n  ValueError: If column_name is None or not\
      \ in data.\n  TypeError: If the value is not a dict, or if any keys/values do\
      \ not match the provided types."
  classes: []
- file: graphrag/query/input/retrieval/community_reports.py
  functions:
  - node_id: graphrag/query/input/retrieval/community_reports.py::to_community_report_dataframe
    name: to_community_report_dataframe
    signature: "def to_community_report_dataframe(\n    reports: list[CommunityReport],\n\
      \    include_community_rank: bool = False,\n    use_community_summary: bool\
      \ = False,\n) -> pd.DataFrame"
    docstring: "Convert a list of CommunityReport objects to a pandas DataFrame.\n\
      \nArgs:\n  reports: list[CommunityReport] - List of CommunityReport objects\
      \ to convert to a DataFrame.\n  include_community_rank: bool - Whether to include\
      \ a rank column in the output.\n  use_community_summary: bool - Whether to include\
      \ a summary column (summary) instead of content.\n\nReturns:\n  pd.DataFrame\
      \ - A DataFrame representing the provided community reports. If the input list\
      \ is empty, an empty DataFrame is returned.\n\nRaises:\n  None - This function\
      \ does not raise exceptions."
  - node_id: graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
    name: get_candidate_communities
    signature: "def get_candidate_communities(\n    selected_entities: list[Entity],\n\
      \    community_reports: list[CommunityReport],\n    include_community_rank:\
      \ bool = False,\n    use_community_summary: bool = False,\n) -> pd.DataFrame"
    docstring: "Get all communities that are related to selected entities.\n\nThis\
      \ function collects all community IDs from the provided selected entities, filters\
      \ the given\ncommunity_reports to those IDs, and returns a DataFrame produced\
      \ by to_community_report_dataframe\nusing the specified options.\n\nArgs:\n\
      \  selected_entities: The selected entities for which to retrieve candidate\
      \ communities.\n  community_reports: The pool of CommunityReport objects to\
      \ search.\n  include_community_rank: Whether to include a rank column in the\
      \ output.\n  use_community_summary: Whether to include the summary field instead\
      \ of full content.\n\nReturns:\n  pd.DataFrame: A DataFrame representing the\
      \ candidate communities related to the selected entities.\n\nRaises:\n  None"
  classes: []
- file: graphrag/query/input/retrieval/covariates.py
  functions:
  - node_id: graphrag/query/input/retrieval/covariates.py::get_candidate_covariates
    name: get_candidate_covariates
    signature: "def get_candidate_covariates(\n    selected_entities: list[Entity],\n\
      \    covariates: list[Covariate],\n) -> list[Covariate]"
    docstring: "Get all covariates that are related to selected entities.\n\nArgs:\n\
      \    selected_entities: The list of Entity objects representing the selected\
      \ entities.\n    covariates: The list of Covariate objects to filter.\n\nReturns:\n\
      \    list[Covariate]: Covariates related to the selected entities."
  - node_id: graphrag/query/input/retrieval/covariates.py::to_covariate_dataframe
    name: to_covariate_dataframe
    signature: 'def to_covariate_dataframe(covariates: list[Covariate]) -> pd.DataFrame'
    docstring: "Convert a list of covariates to a pandas DataFrame.\n\nArgs:\n   \
      \ covariates: list[Covariate] - Covariate objects to convert. Each covariate\
      \ is expected to have short_id, subject_id, and attributes (a dict). The resulting\
      \ DataFrame will have columns: id, entity, and one column per attribute key\
      \ found in the first covariate's attributes (excluding id and entity). Values\
      \ are strings or empty strings when missing.\n\nReturns:\n    pd.DataFrame -\
      \ DataFrame representation of the covariates. If covariates is empty, returns\
      \ an empty DataFrame.\n\nRaises:\n    AttributeError, TypeError: If covariates\
      \ do not conform to the expected Covariate interface or if attribute access\
      \ fails."
  classes: []
- file: graphrag/query/input/retrieval/entities.py
  functions:
  - node_id: graphrag/query/input/retrieval/entities.py::to_entity_dataframe
    name: to_entity_dataframe
    signature: "def to_entity_dataframe(\n    entities: list[Entity],\n    include_entity_rank:\
      \ bool = True,\n    rank_description: str = \"number of relationships\",\n)\
      \ -> pd.DataFrame"
    docstring: "Convert a list of entities to a pandas DataFrame.\n\nArgs:\n    entities:\
      \ list[Entity]\n        The list of Entity objects to convert to a dataframe.\n\
      \    include_entity_rank: bool\n        If True, include a column for the entity\
      \ rank. The header for this column uses rank_description.\n    rank_description:\
      \ str\n        The header name for the rank column when include_entity_rank\
      \ is True.\n\nReturns:\n    pd.DataFrame\n        A dataframe with one row per\
      \ entity. Columns start with \"id\", \"entity\", \"description\", and, if include_entity_rank\
      \ is True, a rank column with the header given by rank_description. Additional\
      \ columns are derived from the keys of the first entity's attributes (excluding\
      \ any header names). Each row contains the corresponding values as strings where\
      \ possible; empty strings are used for missing values.\n\nRaises:\n    None"
  - node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_attribute
    name: get_entity_by_attribute
    signature: "def get_entity_by_attribute(\n    entities: Iterable[Entity], attribute_name:\
      \ str, attribute_value: Any\n) -> list[Entity]"
    docstring: "Get entities by attribute.\n\nArgs:\n    entities: Iterable[Entity],\
      \ the collection of entities to search.\n    attribute_name: str, the name of\
      \ the attribute to match.\n    attribute_value: Any, the value to match for\
      \ the given attribute.\n\nReturns:\n    list[Entity], a list of entities whose\
      \ attributes contain attribute_name with the given attribute_value.\n\nRaises:\n\
      \    TypeError: if entities is not an iterable.\n    AttributeError: if an element\
      \ in entities does not have an attributes attribute."
  - node_id: graphrag/query/input/retrieval/entities.py::is_valid_uuid
    name: is_valid_uuid
    signature: 'def is_valid_uuid(value: str) -> bool'
    docstring: "\"\"\"Determine if a string is a valid UUID.\n\nArgs:\n    value:\
      \ str. The string to validate as a UUID.\n\nReturns:\n    bool: True if the\
      \ string is a valid UUID, False otherwise.\n\"\"\""
  - node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_name
    name: get_entity_by_name
    signature: 'def get_entity_by_name(entities: Iterable[Entity], entity_name: str)
      -> list[Entity]'
    docstring: "Get entities by name.\n\nArgs:\n    entities: Iterable[Entity], the\
      \ collection of entities to search.\n    entity_name: str, the name to match\
      \ against the entity.title attribute.\n\nReturns:\n    list[Entity], a list\
      \ of entities whose title matches the given name.\n\nRaises:\n    AttributeError:\
      \ if any entity in entities does not have a 'title' attribute."
  - node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_id
    name: get_entity_by_id
    signature: 'def get_entity_by_id(entities: dict[str, Entity], value: str) -> Entity
      | None'
    docstring: "Get entity by id.\n\nArgs:\n    entities: dict[str, Entity]. Mapping\
      \ of entity IDs to Entity objects.\n    value: str. The id value to look up.\
      \ If value is a valid UUID, also try the same value with dashes removed.\n\n\
      Returns:\n    Entity | None. The matching Entity if found, otherwise None."
  - node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_key
    name: get_entity_by_key
    signature: "def get_entity_by_key(\n    entities: Iterable[Entity], key: str,\
      \ value: str | int\n) -> Entity | None"
    docstring: "Get an entity by key from a collection.\n\nArgs:\n    entities: Iterable[Entity].\
      \ The collection of Entity objects to search.\n    key: str. The attribute name\
      \ on the Entity to compare.\n    value: str or int. The value to compare against\
      \ the attribute. If value is a string UUID, also compare the same UUID with\
      \ dashes removed.\n\nReturns:\n    Entity | None: The first matching Entity\
      \ if found, otherwise None.\n\nRaises:\n    AttributeError: If any entity in\
      \ the iterable does not have the attribute named by 'key'."
  classes: []
- file: graphrag/query/input/retrieval/relationships.py
  functions:
  - node_id: graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
    name: sort_relationships_by_rank
    signature: "def sort_relationships_by_rank(\n    relationships: list[Relationship],\n\
      \    ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
    docstring: "Sort relationships by a ranking_attribute.\n\nThis function sorts\
      \ the provided list of Relationship objects in descending order\nbased on a\
      \ ranking attribute. If the ranking_attribute exists as a key in a\nRelationship's\
      \ attributes dictionary, its value is used (converted to int)\nfor sorting.\
      \ If not, and ranking_attribute is \"rank\" or \"weight\", the\ncorresponding\
      \ attributes on the Relationship are used (with a default of 0\nor 0.0 when\
      \ missing). The input list is sorted in place and returned. If the\ninput list\
      \ is empty, it is returned unchanged.\n\nArgs:\n    relationships: List of Relationship\
      \ objects to be sorted.\n    ranking_attribute: Attribute name used for ranking.\
      \ May be a key in each\n        Relationship's attributes, or one of \"rank\"\
      \ or \"weight\". Defaults to\n        \"rank\".\n\nReturns:\n    list[Relationship]:\
      \ The same list object, now sorted in descending order by the\n        chosen\
      \ ranking attribute.\n\nRaises:\n    ValueError: If a ranking_attribute value\
      \ exists in a Relationship.attributes\n        dictionary but cannot be converted\
      \ to int when ranking_attribute is present\n        in attribute_names."
  - node_id: graphrag/query/input/retrieval/relationships.py::get_candidate_relationships
    name: get_candidate_relationships
    signature: "def get_candidate_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n) -> list[Relationship]"
    docstring: "Get candidate relationships associated with the selected entities.\n\
      \nArgs:\n    selected_entities: The selected entities for which to retrieve\
      \ candidate relationships.\n    relationships: The pool of relationships to\
      \ filter.\n\nReturns:\n    list[Relationship]: Relationships involving any of\
      \ the selected entities, i.e., where the relationship's source or target matches\
      \ a selected entity's title."
  - node_id: graphrag/query/input/retrieval/relationships.py::to_relationship_dataframe
    name: to_relationship_dataframe
    signature: "def to_relationship_dataframe(\n    relationships: list[Relationship],\
      \ include_relationship_weight: bool = True\n) -> pd.DataFrame"
    docstring: "Convert a list of relationships to a pandas dataframe.\n\nArgs:\n\
      \  relationships: list[Relationship] - List of Relationship objects to convert\
      \ to a pandas DataFrame.\n  include_relationship_weight: bool - Whether to include\
      \ the weight column in the output.\n\nReturns:\n  pd.DataFrame - A DataFrame\
      \ representing the relationships. If the relationships list is empty, an empty\
      \ DataFrame is returned. The DataFrame contains columns: id, source, target,\
      \ description, and optionally weight. It also includes any additional attribute\
      \ columns derived from the first relationship's attributes (excluding any columns\
      \ already present in the header). The rows reflect each relationship's short_id,\
      \ source, target, description, weight, and attribute values as strings where\
      \ available."
  - node_id: graphrag/query/input/retrieval/relationships.py::get_entities_from_relationships
    name: get_entities_from_relationships
    signature: "def get_entities_from_relationships(\n    relationships: list[Relationship],\
      \ entities: list[Entity]\n) -> list[Entity]"
    docstring: "Get all entities that are associated with the selected relationships.\n\
      \nArgs:\n    relationships (list[Relationship]): The relationships to inspect.\n\
      \    entities (list[Entity]): The pool of entities to filter from.\n\nReturns:\n\
      \    list[Entity]: The subset of entities whose title matches the source or\
      \ target of any relationship in relationships."
  - node_id: graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
    name: get_in_network_relationships
    signature: "def get_in_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]"
    docstring: "Get all directed relationships between the selected entities, sorted\
      \ by ranking_attribute.\n\nArgs:\n  selected_entities: The selected entities\
      \ to consider.\n  relationships: The pool of relationships to search within.\n\
      \  ranking_attribute: The attribute name used for sorting; defaults to \"rank\"\
      .\n\nReturns:\n  list[Relationship]: The relationships where both the source\
      \ and target are in the selected entities; if more than one such relationship\
      \ exists, they are returned sorted by ranking_attribute; otherwise the original\
      \ list is returned."
  - node_id: graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
    name: get_out_network_relationships
    signature: "def get_out_network_relationships(\n    selected_entities: list[Entity],\n\
      \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
      ,\n) -> list[Relationship]"
    docstring: "Get relationships that connect the selected entities to other entities,\
      \ considering both directions (outgoing from and incoming to the selected set).\
      \ The other endpoint may lie outside the selected set. The resulting relationships\
      \ are sorted by the specified ranking_attribute.\n\nArgs:\n  selected_entities\
      \ (list[Entity]): The selected entities; used as one end of the relationships\
      \ to consider.\n  relationships (list[Relationship]): The pool of relationships\
      \ to search within.\n  ranking_attribute (str): The attribute name used for\
      \ sorting; defaults to \"rank\".\n\nReturns:\n  list[Relationship]: The relationships\
      \ where either the source is in the selected_entities and the target is outside\
      \ the selected set, or the target is in the selected_entities and the source\
      \ is outside the selected set; i.e., relationships connected to the selected\
      \ entities but with the other endpoint possibly outside the set. The list is\
      \ sorted by ranking_attribute.\n\nNotes:\n  If the input is empty, an empty\
      \ list is returned."
  classes: []
- file: graphrag/query/input/retrieval/text_units.py
  functions:
  - node_id: graphrag/query/input/retrieval/text_units.py::to_text_unit_dataframe
    name: to_text_unit_dataframe
    signature: 'def to_text_unit_dataframe(text_units: list[TextUnit]) -> pd.DataFrame'
    docstring: "Convert a list of text units to a pandas DataFrame.\n\nArgs:\n   \
      \ text_units (list[TextUnit]): The text units to convert into a DataFrame. If\
      \ the list is empty, an empty DataFrame is returned.\n\nReturns:\n    pd.DataFrame:\
      \ A DataFrame where each row corresponds to a text unit. The columns are:\n\
      \        - \"id\": the text unit's short_id\n        - \"text\": the text of\
      \ the text unit\n        - any additional attribute columns derived from the\
      \ first text unit's attributes keys (excluding \"id\" and \"text\"). Attribute\
      \ values are converted to strings; missing attributes result in empty strings.\n\
      \nRaises:\n    None: This function does not raise any exceptions."
  - node_id: graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
    name: get_candidate_text_units
    signature: "def get_candidate_text_units(\n    selected_entities: list[Entity],\n\
      \    text_units: list[TextUnit],\n) -> pd.DataFrame"
    docstring: "Get all text units that are associated to selected entities.\n\nArgs:\n\
      \    selected_entities (list[Entity]): Entities whose text_unit_ids (if any)\
      \ are used to select text units.\n    text_units (list[TextUnit]): The pool\
      \ of TextUnit objects to search.\n\nReturns:\n    pd.DataFrame: A DataFrame\
      \ containing the text units associated with the selected entities. The DataFrame\
      \ is produced by converting the selected TextUnit objects via to_text_unit_dataframe;\
      \ if no such text units are found, an empty DataFrame is returned."
  classes: []
- file: graphrag/query/llm/text_utils.py
  functions:
  - node_id: graphrag/query/llm/text_utils.py::batched
    name: batched
    signature: 'def batched(iterable: Iterator, n: int)'
    docstring: "\"\"\"\nBatch data into tuples of length n. The last batch may be\
      \ shorter.\n\nArgs:\n    iterable (Iterator): The input iterable to batch.\n\
      \    n (int): The batch size (must be at least 1).\n\nReturns:\n    Iterator[tuple]:\
      \ An iterator that yields batches as tuples of length n (the last batch may\
      \ be shorter).\n\nRaises:\n    ValueError: If n < 1.\n\"\"\""
  - node_id: graphrag/query/llm/text_utils.py::try_parse_json_object
    name: try_parse_json_object
    signature: 'def try_parse_json_object(input: str, verbose: bool = True) -> tuple[str,
      dict]'
    docstring: 'JSON cleaning and formatting utilities for strings that may contain
      JSON or JSON-like content produced by an LLM.


      Args:

      - input: str. The raw string to parse and clean.

      - verbose: bool. If True, log warnings or exceptions encountered during parsing.


      Returns:

      - tuple[str, dict]. The input string (potentially cleaned) and the parsed JSON
      object as a dict. If parsing ultimately fails, the dict will be empty.'
  - node_id: graphrag/query/llm/text_utils.py::chunk_text
    name: chunk_text
    signature: 'def chunk_text(text: str, max_tokens: int, tokenizer: Tokenizer |
      None = None)'
    docstring: "Chunk text by token length.\n\nArgs:\n    text (str): The input text\
      \ to chunk.\n    max_tokens (int): Maximum number of tokens per chunk.\n   \
      \ tokenizer (Tokenizer | None): Tokenizer to use for encoding/decoding. If None,\
      \ a default tokenizer is obtained via get_tokenizer(encoding_model=defs.ENCODING_MODEL).\n\
      \nReturns:\n    Iterator[str]: An iterator that yields chunk strings, each created\
      \ by decoding token sequences with at most max_tokens tokens.\n\nRaises:\n \
      \   ValueError: If max_tokens < 1."
  classes: []
- file: graphrag/query/question_gen/base.py
  functions:
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.generate
    name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate questions.\n\nArgs:\n    question_history: History of previously\
      \ generated questions.\n    context_data: Optional context data used to influence\
      \ generation; None if unavailable.\n    question_count: Number of questions\
      \ to generate.\n    kwargs: Additional keyword arguments for extensibility.\n\
      \nReturns:\n    QuestionResult: The generated results including the response\
      \ list, context_data, completion_time, llm_calls, and prompt_tokens.\n\nRaises:\n\
      \    NotImplementedError: If the method is not implemented by a subclass."
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.agenerate
    name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "\"\"\"Generate questions asynchronously.\"\"\"\n\nArgs:\n    question_history:\
      \ History of previously asked questions.\n    context_data: Optional context\
      \ data used to influence generation; None if unavailable.\n    question_count:\
      \ Number of questions to generate.\n    kwargs: Additional keyword arguments\
      \ for extensibility.\n\nReturns:\n    QuestionResult: The generated result containing\
      \ the questions and related data.\n\""
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize a Base Question Gen with the provided model and context\
      \ builder.\n\nArgs:\n    model (ChatModel): The language model interface used\
      \ for this base question generator.\n    context_builder (GlobalContextBuilder\
      \ | LocalContextBuilder): The builder that constructs the context for the questions.\n\
      \    tokenizer (Tokenizer | None): Optional tokenizer to use. If None, a tokenizer\
      \ appropriate for the model will be selected.\n    model_params (dict[str, Any]\
      \ | None): Optional parameters for the language model. If None, defaults to\
      \ an empty dict.\n    context_builder_params (dict[str, Any] | None): Optional\
      \ parameters for the context builder. If None, defaults to an empty dict.\n\n\
      Returns:\n    None"
  classes:
  - class_id: graphrag/query/question_gen/base.py::BaseQuestionGen
    name: BaseQuestionGen
    docstring: "BaseQuestionGen is a base class for generating questions using a language\
      \ model and context builders.\n\nPurpose:\n    Provide a common interface and\
      \ shared setup for question generation by coordinating a ChatModel with a context_builder\
      \ (GlobalContextBuilder or LocalContextBuilder) and an optional Tokenizer. Subclasses\
      \ implement the concrete generation logic.\n\nArgs:\n    model (ChatModel):\
      \ The language model interface used for this base question generator.\n    context_builder\
      \ (GlobalContextBuilder | LocalContextBuilder): The builder that constructs\
      \ the context for the questions.\n    tokenizer (Tokenizer | None): Optional\
      \ tokenizer to use. If None, a tokenizer appropriate for the model will be used.\n\
      \    model_params (dict[str, Any] | None): Optional parameters for configuring\
      \ the underlying model.\n    context_builder_params (dict[str, Any] | None):\
      \ Optional parameters for configuring the context builder.\n\nAttributes:\n\
      \    model: The language model interface used for generation.\n    context_builder:\
      \ The context builder instance used to assemble context data.\n    tokenizer:\
      \ Optional tokenizer instance used to tokenize prompts and outputs.\n    model_params:\
      \ Optional parameters for the model configuration.\n    context_builder_params:\
      \ Optional parameters for the context builder configuration.\n\nMethods:\n \
      \   generate(question_history: list[str], context_data: str | None, question_count:\
      \ int, **kwargs) -> QuestionResult:\n        Generate questions synchronously.\n\
      \    agenerate(question_history: list[str], context_data: str | None, question_count:\
      \ int, **kwargs) -> QuestionResult:\n        Generate questions asynchronously."
    methods:
    - name: generate
      signature: "def generate(\n        self,\n        question_history: list[str],\n\
        \        context_data: str | None,\n        question_count: int,\n       \
        \ **kwargs,\n    ) -> QuestionResult"
    - name: agenerate
      signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
        \        context_data: str | None,\n        question_count: int,\n       \
        \ **kwargs,\n    ) -> QuestionResult"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: GlobalContextBuilder | LocalContextBuilder,\n        tokenizer:\
        \ Tokenizer | None = None,\n        model_params: dict[str, Any] | None =\
        \ None,\n        context_builder_params: dict[str, Any] | None = None,\n \
        \   )"
- file: graphrag/query/question_gen/local_gen.py
  functions:
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.agenerate
    name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate a question based on the question history and context data.\n\
      \nIf context data is not provided, it will be generated by the local context\
      \ builder using the current question and conversation history, along with any\
      \ additional keyword arguments and the configured context_builder_params.\n\n\
      Args:\n    question_history: list[str] - History of previously asked questions.\n\
      \    context_data: str | None - Optional context data used to influence generation;\
      \ None to generate automatically.\n    question_count: int - Number of questions\
      \ to generate.\n    **kwargs: Additional keyword arguments passed to the local\
      \ context builder when constructing context data.\n\nReturns:\n    QuestionResult:\
      \ The generated results including:\n        response: list[str] - The generated\
      \ response split by newline.\n        context_data: dict[str, Any] - The context\
      \ data associated with this question. If context_data was provided, contains\
      \ {\"context_data\": <value>}; otherwise contains keys from the context builder\
      \ result (e.g., \"question_context\" and context_records).\n        completion_time:\
      \ float - Time elapsed for generation in seconds.\n        llm_calls: int -\
      \ Number of LLM API calls performed (typically 1).\n        prompt_tokens: int\
      \ - Number of tokens in the system prompt.\n\nRaises:\n    None: Exceptions\
      \ are caught within the function; on error, a QuestionResult with an empty response\
      \ is returned along with the current context data."
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.generate
    name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate a question based on the question history and context data.\n\
      \nIf context data is not provided, it will be generated by the local context\
      \ builder.\n\nArgs:\n    question_history (list[str]): History of previously\
      \ asked questions.\n    context_data (str | None): Optional context data; if\
      \ None, context data will be generated by the local context builder.\n    question_count\
      \ (int): Number of questions to generate.\n    kwargs: Additional keyword arguments\
      \ for extensibility (passed to the context builder and model configuration).\n\
      \nReturns:\n    QuestionResult: The generated results including the response\
      \ as a list of strings, context_data containing the question_context and context_records,\
      \ completion_time, llm_calls, and prompt_tokens.\n\nRaises:\n    Exception:\
      \ Exceptions are caught internally and do not propagate to callers; if an unexpected\
      \ error occurs during generation, it will be logged and a default QuestionResult\
      \ will be returned."
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback]\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize a LocalQuestionGen instance.\n\nArgs:\n    model: ChatModel\
      \ - The language model interface to use.\n    context_builder: LocalContextBuilder\
      \ - The builder that constructs the context for local question generation.\n\
      \    tokenizer: Tokenizer | None - Optional tokenizer to use.\n    system_prompt:\
      \ str - System prompt for question generation. Defaults to QUESTION_SYSTEM_PROMPT.\n\
      \    callbacks: list[BaseLLMCallback] | None - Optional callbacks for LLM events.\
      \ If None, an empty list is used.\n    model_params: dict[str, Any] | None -\
      \ Optional parameters to pass to the model.\n    context_builder_params: dict[str,\
      \ Any] | None - Optional parameters to pass to the context builder.\n\nReturns:\n\
      \    None"
  classes:
  - class_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen
    name: LocalQuestionGen
    docstring: "Generates questions using a local context builder and a language model.\n\
      \nThis class coordinates a LocalContextBuilder and a ChatModel to generate questions\
      \ based on a history of questions and optional context data. It provides asynchronous\
      \ and synchronous generation methods that return a QuestionResult, and it uses\
      \ a configurable system prompt.\n\nAttributes:\n    model: The language model\
      \ interface to use (ChatModel).\n    context_builder: The builder that constructs\
      \ context for local question generation (LocalContextBuilder).\n    tokenizer:\
      \ Optional tokenizer to use (Tokenizer | None).\n    system_prompt: System prompt\
      \ for question generation; defaults to QUESTION_SYSTEM_PROMPT (str).\n    callbacks:\
      \ Optional list of callbacks for model events (list[BaseLLMCallback] | None).\n\
      \    model_params: Optional parameters for the model (dict[str, Any] | None).\n\
      \    context_builder_params: Optional parameters for the context builder (dict[str,\
      \ Any] | None).\n\nArgs:\n    model: ChatModel - The language model interface\
      \ to use.\n    context_builder: LocalContextBuilder - The builder that constructs\
      \ the context for local question generation.\n    tokenizer: Tokenizer | None\
      \ - Optional tokenizer to use.\n    system_prompt: str - System prompt for question\
      \ generation. Defaults to QUESTION_SYSTEM_PROMPT.\n    callbacks: list[BaseLLMCallback]\
      \ | None - Optional callbacks for model events.\n    model_params: dict[str,\
      \ Any] | None - Optional parameters for the model.\n    context_builder_params:\
      \ dict[str, Any] | None - Optional parameters for the context builder.\n\nReturns:\n\
      \    QuestionResult - The generated QuestionResult from agenerate or generate.\n\
      \nRaises:\n    Exception - If underlying components fail during generation."
    methods:
    - name: agenerate
      signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
        \        context_data: str | None,\n        question_count: int,\n       \
        \ **kwargs,\n    ) -> QuestionResult"
    - name: generate
      signature: "def generate(\n        self,\n        question_history: list[str],\n\
        \        context_data: str | None,\n        question_count: int,\n       \
        \ **kwargs,\n    ) -> QuestionResult"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: LocalContextBuilder,\n        tokenizer: Tokenizer | None\
        \ = None,\n        system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks:\
        \ list[BaseLLMCallback] | None = None,\n        model_params: dict[str, Any]\
        \ | None = None,\n        context_builder_params: dict[str, Any] | None =\
        \ None,\n    )"
- file: graphrag/query/structured_search/base.py
  functions:
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Asynchronously search the given query.\n\nThis abstract method must\
      \ be implemented by subclasses. It performs an asynchronous\nsearch given a\
      \ query string and optional conversation history, returning a SearchResult.\n\
      \nArgs:\n    query (str): The search query to execute.\n    conversation_history\
      \ (ConversationHistory | None): Optional conversation history to consider during\
      \ the search. If provided, prior turns may influence results.\n    **kwargs:\
      \ Additional keyword arguments passed to the search implementation.\n\nReturns:\n\
      \    SearchResult: The result of the asynchronous search operation.\n\nRaises:\n\
      \    NotImplementedError: Subclasses must implement this method."
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream search for the given query.\nArgs:\n    query (str): The search\
      \ query to execute.\n    conversation_history (ConversationHistory | None):\
      \ Optional conversation history to consider during the search.\nReturns:\n \
      \   AsyncGenerator[str, None]: An asynchronous generator yielding strings representing\
      \ streamed search results.\nRaises:\n    NotImplementedError: Subclasses must\
      \ implement this method."
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
      \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None\
      \ = None,\n    )"
    docstring: "Initialize a BaseSearch instance with the provided model and context\
      \ builder.\n\nArgs:\n    model (ChatModel): The language model interface used\
      \ for this base search.\n    context_builder (T): The builder that constructs\
      \ the context for the search.\n    tokenizer (Tokenizer | None): Optional tokenizer\
      \ to use. If None, a tokenizer is selected via get_tokenizer().\n    model_params\
      \ (dict[str, Any] | None): Optional configuration parameters for the language\
      \ model.\n    context_builder_params (dict[str, Any] | None): Optional configuration\
      \ parameters for the context builder.\n\nReturns:\n    None: This constructor\
      \ initializes internal state and does not return a value."
  classes:
  - class_id: graphrag/query/structured_search/base.py::BaseSearch
    name: BaseSearch
    docstring: "Abstract base class for structured searches using a language model\
      \ and contextual builders.\n\nThis ABC defines the interface and provides common\
      \ configuration for search implementations that operate with a ChatModel, a\
      \ context builder, and an optional tokenizer. It also holds optional parameter\
      \ dictionaries for both the model and the context builder.\n\nKey attributes:\n\
      \  model: The language model interface used for this base search.\n  context_builder:\
      \ The builder that constructs the context for the search (generic type T).\n\
      \  tokenizer: Optional tokenizer to use. If None, a tokenizer is selected via\
      \ get_tokenizer().\n  model_params: Optional dictionary of parameters to configure\
      \ the language model.\n  context_builder_params: Optional dictionary of parameters\
      \ to configure the context builder.\n\nArgs:\n  model: The language model interface\
      \ used for this base search.\n  context_builder: The builder that constructs\
      \ the context for the search.\n  tokenizer: Optional tokenizer to use. If None,\
      \ a tokenizer is selected via get_tokenizer().\n  model_params: Optional dictionary\
      \ of parameters to configure the language model.\n  context_builder_params:\
      \ Optional dictionary of parameters to configure the context builder.\n\nRaises:\n\
      \  NotImplementedError: Subclasses must implement the abstract methods search\
      \ and stream_search."
    methods:
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
        \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    - name: stream_search
      signature: "def stream_search(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n    ) -> AsyncGenerator[str,\
        \ None]"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: T,\n        tokenizer: Tokenizer | None = None,\n    \
        \    model_params: dict[str, Any] | None = None,\n        context_builder_params:\
        \ dict[str, Any] | None = None,\n    )"
- file: graphrag/query/structured_search/basic_search/basic_context.py
  functions:
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext._map_ids
    name: _map_ids
    signature: def _map_ids(self) -> dict[str, str]
    docstring: "Map id to short_id in the text units.\n\nArgs:\n    self (object):\
      \ The instance containing the text_units attribute used to build the mapping.\n\
      \nReturns:\n    dict[str, str]: A mapping from each text unit's id to its short_id.\n\
      \nRaises:\n    AttributeError: If any text unit is missing 'id' or 'short_id'\
      \ attributes."
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
      \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
      \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col:\
      \ str = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "\"\"\"Build the context for the basic search mode.\n\nArgs:\n  query\
      \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
      \ | None): Optional conversation history to consider while constructing the\
      \ context.\n  k (int): Number of top related texts to retrieve.\n  max_context_tokens\
      \ (int): Maximum total number of tokens allowed for the context.\n  context_name\
      \ (str): Name assigned to the context section in the results (default \"Sources\"\
      ).\n  column_delimiter (str): Delimiter used to separate columns in the generated\
      \ context text.\n  text_id_col (str): Name of the column containing text identifiers.\n\
      \  text_col (str): Name of the column containing the text content.\n  **kwargs:\
      \ Additional keyword arguments that may influence how the context is built.\n\
      \nReturns:\n  ContextBuilderResult: The result containing the built context\
      \ for the basic search mode.\n\"\"\""
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
      \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
      \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = \"id\",\n    )"
    docstring: "Initialize a BasicSearchContext with the provided embedding model\
      \ and text unit embeddings and prepare internal mappings.\n\nArgs:\n    text_embedder:\
      \ EmbeddingModel The embedding model used to embed text for similarity search.\n\
      \    text_unit_embeddings: BaseVectorStore The vector store containing embeddings\
      \ for text units.\n    text_units: list[TextUnit] | None Optional list of text\
      \ units to consider.\n    tokenizer: Tokenizer | None Optional tokenizer to\
      \ use; if not provided, get_tokenizer() is used.\n    embedding_vectorstore_key:\
      \ str Key in the vector store for identifying text units (default: \"id\").\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  classes:
  - class_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext
    name: BasicSearchContext
    docstring: 'Builds and manages the context used by the basic search mode in graphrag.


      Purpose:

      The BasicSearchContext encapsulates the logic to construct a coherent, compact
      context for a user query by leveraging an embedding model and a vector store
      of text units. It can incorporate optional conversation history and token constraints
      when building the context via build_context, coordinating with the BasicContextBuilder
      to produce a ContextBuilderResult.


      Attributes:

      text_embedder: EmbeddingModel

      text_unit_embeddings: BaseVectorStore

      text_units: list[TextUnit] | None

      tokenizer: Tokenizer | None

      embedding_vectorstore_key: str


      Args:

      text_embedder (EmbeddingModel): The embedding model used to embed text for similarity
      search

      text_unit_embeddings (BaseVectorStore): The vector store containing embeddings
      for text units

      text_units (list[TextUnit] | None): Optional list of text units to consider

      tokenizer (Tokenizer | None): Optional tokenizer to use

      embedding_vectorstore_key (str): The key in the vector store that identifies
      items (default "id")


      Returns:

      None


      Raises:

      AttributeError: If any text unit is missing ''id'' or ''short_id'' attributes
      when mapping ids via _map_ids'
    methods:
    - name: _map_ids
      signature: def _map_ids(self) -> dict[str, str]
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        k: int\
        \ = 10,\n        max_context_tokens: int = 12_000,\n        context_name:\
        \ str = \"Sources\",\n        column_delimiter: str = \"|\",\n        text_id_col:\
        \ str = \"source_id\",\n        text_col: str = \"text\",\n        **kwargs,\n\
        \    ) -> ContextBuilderResult"
    - name: __init__
      signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
        \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
        \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
        \ str = \"id\",\n    )"
- file: graphrag/query/structured_search/basic_search/search.py
  functions:
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Build rag search context that fits a single context window and generates\
      \ an answer for the user query.\n\nArgs:\n  query: The user query to process.\n\
      \  conversation_history: Optional conversation history to incorporate into the\
      \ search context.\n  **kwargs: Additional keyword arguments passed to the context\
      \ builder and the model.\n\nReturns:\n  SearchResult: The search result containing\
      \ the generated response, context data, and timing information."
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Build basic search context that fits a single context window and stream\
      \ the answer for the user query.\n\nArgs:\n  query (str): The user query to\
      \ process.\n  conversation_history (ConversationHistory | None): Optional conversation\
      \ history to incorporate into the search context.\n\nReturns:\n  AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding strings representing chunks of the\
      \ generated answer as they are produced."
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    docstring: "Initialize a BasicSearch instance for basic search orchestration (internal\
      \ API).\n\nArgs:\n    model (ChatModel): The language model interface used for\
      \ this basic search.\n    context_builder (BasicContextBuilder): The builder\
      \ that constructs the context for the search.\n    tokenizer (Tokenizer | None):\
      \ Optional tokenizer to use.\n    system_prompt (str | None): System prompt\
      \ for the search. If None, uses BASIC_SEARCH_SYSTEM_PROMPT.\n    response_type\
      \ (str): Specifies the format of the response. Defaults to \"multiple paragraphs\"\
      .\n    callbacks (list[QueryCallbacks] | None): Optional list of query callbacks\
      \ to invoke.\n    model_params (dict[str, Any] | None): Optional parameters\
      \ to pass to the model.\n    context_builder_params (dict | None): Optional\
      \ parameters for the context builder.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch
    name: BasicSearch
    docstring: 'BasicSearch orchestrates the construction of a single-context-window
      search context and the generation (or streaming) of an answer for a user query.


      Purpose and responsibility:

      - Build a single-context-window search context

      - Orchestrate a language model (ChatModel) with a BasicContextBuilder to produce
      results

      - Support both full result generation via search and streaming output via stream_search


      Key attributes inferred from initialization:

      - model: The language model interface used for this basic search.

      - context_builder: The builder that constructs the context for the search.

      - tokenizer: Optional tokenizer to use.

      - system_prompt: System prompt for the search. If None, a default system prompt
      is used.

      - response_type: The type/format of the response. Default is "multiple paragraphs".

      - callbacks: Optional list of QueryCallbacks to handle events during search.

      - model_params: Additional parameters passed to the model.

      - context_builder_params: Additional parameters passed to the context builder.


      Args:

      model: The language model interface used for this basic search.

      context_builder: The builder that constructs the context for the search.

      tokenizer: Optional tokenizer to use.

      system_prompt: System prompt for the search. If None, the default system prompt
      is used.

      response_type: The type of response formatting. Default is "multiple paragraphs".

      callbacks: Optional list of QueryCallbacks to be invoked during search.

      model_params: Additional parameters passed to the model.

      context_builder_params: Additional parameters passed to the context builder.


      Returns:

      None: This initializer does not return a value.


      Raises:

      Exceptions raised by the underlying components (e.g., ChatModel, BasicContextBuilder)
      may be propagated to the caller.'
    methods:
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
        \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    - name: stream_search
      signature: "def stream_search(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n    ) -> AsyncGenerator[str,\
        \ None]"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: BasicContextBuilder,\n        tokenizer: Tokenizer | None\
        \ = None,\n        system_prompt: str | None = None,\n        response_type:\
        \ str = \"multiple paragraphs\",\n        callbacks: list[QueryCallbacks]\
        \ | None = None,\n        model_params: dict[str, Any] | None = None,\n  \
        \      context_builder_params: dict | None = None,\n    )"
- file: graphrag/query/structured_search/drift_search/action.py
  functions:
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.compute_score
    name: compute_score
    signature: 'def compute_score(self, scorer: Any)'
    docstring: "Compute the score for the action using the provided scorer.\n\nArgs:\n\
      \    scorer (Any): The scorer to compute the score.\n\nReturns:\n    None: The\
      \ method updates self.score with the value returned by scorer.compute_score(self.query,\
      \ self.answer); if the result is None, self.score is set to -inf."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__hash__
    name: __hash__
    signature: def __hash__(self) -> int
    docstring: "Return a hash value for the DriftAction object to enable hashing in\
      \ networkx.MultiDiGraph.\n\nAssumes queries are unique.\n\nArgs:\n    self:\
      \ The DriftAction instance.\n\nReturns:\n    int: Hash based on the query."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        query: str,\n        answer:\
      \ str | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
      \    )"
    docstring: "Initialize the DriftAction with a query, optional answer, and follow-up\
      \ actions.\n\nArgs:\n    query (str): The query for the action.\n    answer\
      \ (Optional[str]): The answer to the query, if available.\n    follow_ups (Optional[list[DriftAction]]):\
      \ A list of follow-up actions.\n\nReturns:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.serialize
    name: serialize
    signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str,
      Any]'
    docstring: "Serialize the action to a dictionary.\n\nSerializes the DriftAction\
      \ into a dictionary representation, including the core fields and, optionally,\
      \ serialized follow-up actions.\n\nArgs:\n    include_follow_ups (bool): Whether\
      \ to include follow-up actions in the serialization. The default is True; when\
      \ True, the result includes a \"follow_ups\" key containing a list of serialized\
      \ follow-up actions, each serialized by its own serialize() method.\n\nReturns:\n\
      \    dict[str, Any]: Serialized action as a dictionary with the following keys:\n\
      \        - \"query\": The query string.\n        - \"answer\": The answer.\n\
      \        - \"score\": The score.\n        - \"metadata\": Metadata dictionary.\n\
      \        - \"follow_ups\": (optional) List of serialized follow-up actions if\
      \ include_follow_ups is True (empty list if none)."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.is_complete
    name: is_complete
    signature: def is_complete(self) -> bool
    docstring: "Check if the action is complete (i.e., an answer is available).\n\n\
      Returns:\n    bool: True if an answer is available, False otherwise."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.from_primer_response
    name: from_primer_response
    signature: "def from_primer_response(\n        cls, query: str, response: str\
      \ | dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
    docstring: "Create a DriftAction from a DRIFTPrimer response.\n\nArgs:\n    query\
      \ (str): The query string.\n    response (str | dict[str, Any]): Primer response\
      \ data. The runtime accepts:\n        - a dictionary with keys:\n          \
      \  - follow_up_queries (list[dict[str, Any]]): actions to follow up with\n \
      \           - intermediate_answer: the answer to present\n            - score\
      \ (optional, numeric): a score for the action\n        - a JSON string that\
      \ decodes to such a dictionary.\n\nReturns:\n    DriftAction: A new DriftAction\
      \ instance populated from the response. The instance's\n    query is set to\
      \ the provided query; follow_ups, answer, and score are populated\n    from\
      \ the corresponding keys in the response.\n\nRaises:\n    ValueError: If the\
      \ response is not a dictionary or a JSON string that decodes to a dictionary.\n\
      \    ValueError: If a JSON string cannot be parsed or decodes to a non-dictionary\
      \ value."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.deserialize
    name: deserialize
    signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
    docstring: "Deserialize the action from a dictionary.\n\nArgs:\n    data (dict[str,\
      \ Any]): Serialized action data.\n\nReturns:\n    DriftAction: A deserialized\
      \ instance of DriftAction.\n\nRaises:\n    ValueError: If the 'query' key is\
      \ missing in serialized data."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__eq__
    name: __eq__
    signature: 'def __eq__(self, other: object) -> bool'
    docstring: "Check equality based on the query string.\n\nArgs:\n    other (object):\
      \ Another object to compare with.\n\nReturns:\n    bool: True if the other object\
      \ is a DriftAction with the same query, False otherwise."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.search
    name: search
    signature: 'def search(self, search_engine: Any, global_query: str, scorer: Any
      = None)'
    docstring: "Execute an asynchronous search using the search engine, and update\
      \ the action with the results.\n\nIf a scorer is provided, compute the score\
      \ for the action.\n\nArgs:\n    search_engine (Any): The search engine to execute\
      \ the query.\n    global_query (str): The global query string.\n    scorer (Any,\
      \ optional): Scorer to compute scores for the action.\n\nReturns:\n    DriftAction:\
      \ The updated action with the search results.\n\nRaises:\n    Exception: If\
      \ an error occurs during search execution or processing."
  classes:
  - class_id: graphrag/query/structured_search/drift_search/action.py::DriftAction
    name: DriftAction
    docstring: "DriftAction represents a single drift action within a structured drift\
      \ search workflow. It encapsulates a query, an optional answer, and an optional\
      \ list of follow-up actions, and it carries a score that may be computed later\
      \ by a scorer.\n\nArgs:\n  query (str): The query for the action.\n  answer\
      \ (Optional[str]): The answer to the query, if available.\n  follow_ups (Optional[List['DriftAction']]):\
      \ A list of follow-up actions.\n\nAttributes:\n  query (str): The action's query\
      \ string.\n  answer (Optional[str]): The answer to the query, if available.\n\
      \  follow_ups (Optional[List['DriftAction']]): Nested follow-up actions.\n \
      \ score (Optional[float]): The computed score for this action. May be None before\
      \ scoring.\n\nNotes:\n  score is initialized to None. Use compute_score(scorer)\
      \ to assign a numeric score based on the query and answer; if the scorer returns\
      \ None, score remains None.\n\nNotable methods:\n  compute_score(scorer): Compute\
      \ and assign the action's score using the provided scorer.\n  __hash__(self)\
      \ -> int: Return a hash based on the query to enable hashing in graphs. Assumes\
      \ queries are unique.\n  __init__(self, query: str, answer: Optional[str] =\
      \ None, follow_ups: Optional[List['DriftAction']] = None) -> None: Initialize\
      \ the action with a query, optional answer, and optional follow-ups.\n  serialize(self,\
      \ include_follow_ups: bool = True) -> dict[str, Any]: Serialize the action to\
      \ a dict; optionally include serialized follow-ups.\n  is_complete(self) ->\
      \ bool: Return True if an answer is present.\n  from_primer_response(cls, query:\
      \ str, response: str | dict[str, Any] | list[dict[str, Any]]) -> \"DriftAction\"\
      : Create a DriftAction from a DRIFTPrimer response.\n  deserialize(cls, data:\
      \ dict[str, Any]) -> \"DriftAction\": Deserialize an action from a dict.\n \
      \ __eq__(self, other: object) -> bool: Equality based on the query.\n  search(self,\
      \ search_engine: Any, global_query: str, scorer: Any = None) -> \"DriftAction\"\
      : Execute an asynchronous search and update the action; if a scorer is provided,\
      \ compute the score."
    methods:
    - name: compute_score
      signature: 'def compute_score(self, scorer: Any)'
    - name: __hash__
      signature: def __hash__(self) -> int
    - name: __init__
      signature: "def __init__(\n        self,\n        query: str,\n        answer:\
        \ str | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
        \    )"
    - name: serialize
      signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str,
        Any]'
    - name: is_complete
      signature: def is_complete(self) -> bool
    - name: from_primer_response
      signature: "def from_primer_response(\n        cls, query: str, response: str\
        \ | dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
    - name: deserialize
      signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
    - name: __eq__
      signature: 'def __eq__(self, other: object) -> bool'
    - name: search
      signature: 'def search(self, search_engine: Any, global_query: str, scorer:
        Any = None)'
- file: graphrag/query/structured_search/drift_search/drift_context.py
  functions:
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.init_local_context_builder
    name: init_local_context_builder
    signature: def init_local_context_builder(self) -> LocalSearchMixedContext
    docstring: "Initialize and return the local search mixed context built from the\
      \ current DRIFT context attributes.\n\nArgs:\n    self: The DRIFT drift context\
      \ builder instance.\n\nReturns:\n    LocalSearchMixedContext: The initialized\
      \ local context constructed from the builder's state, including:\n      - community_reports\n\
      \      - text_units\n      - entities\n      - relationships\n      - covariates\n\
      \      - entity_text_embeddings\n      - embedding_vectorstore_key\n      -\
      \ text_embedder\n      - tokenizer\n\nRaises:\n    None\n\nExamples:\n    context\
      \ = self.init_local_context_builder()"
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self, query: str, **kwargs\n    ) -> tuple[pd.DataFrame,\
      \ dict[str, int]]"
    docstring: "Build DRIFT search context.\n\nArgs\n  query: str\n      Search query\
      \ string.\n\nReturns\n  tuple[pd.DataFrame, dict[str, int]]: Top-k most similar\
      \ documents, and a dictionary containing the number of LLM calls, and prompts\
      \ and output tokens.\n\nRaises\n  ValueError: If no community reports are available,\
      \ or embeddings are incompatible."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.check_query_doc_encodings
    name: check_query_doc_encodings
    signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
      -> bool'
    docstring: "Check if the embeddings are compatible for direct comparison.\n\n\
      This function enforces concrete compatibility criteria:\n- Both embeddings are\
      \ non-None\n- They have the same container type (type(query_embedding) == type(embedding))\n\
      - They have the same length (len(query_embedding) == len(embedding))\n- The\
      \ inner element types are the same (type(query_embedding[0]) == type(embedding[0]))\n\
      \nNote: The function expects sequences of homogeneous elements (e.g., lists\
      \ of floats, numpy arrays of floats). It assumes non-empty embeddings. If both\
      \ embeddings have length 0, attempting to access the first element (index 0)\
      \ will raise IndexError.\n\nArgs\n----\nquery_embedding : Any\n    Embedding\
      \ of the query.\nembedding : Any\n    Embedding to compare against.\n\nReturns\n\
      -------\nbool: True if embeddings are compatible, otherwise False.\n\nRaises\n\
      ------\nIndexError\n    If embeddings are empty and the code attempts to access\
      \ the first element.\nTypeError\n    If inputs do not support len() or indexing,\
      \ or are not indexable."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.convert_reports_to_df
    name: convert_reports_to_df
    signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
    docstring: "Convert a list of CommunityReport objects to a pandas DataFrame.\n\
      \nArgs:\n    reports (list[CommunityReport]): List of CommunityReport objects.\n\
      \nReturns:\n    pd.DataFrame: DataFrame with report data.\n\nRaises:\n    ValueError:\
      \ If some reports are missing full content or full content embeddings."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
      \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n    \
      \    reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None =\
      \ None,\n        local_system_prompt: str | None = None,\n        local_mixed_context:\
      \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str | None = None,\n    )"
    docstring: "Initialize the DRIFT search context builder with necessary components.\n\
      \nThis constructor wires together the core components required for DRIFT-style\n\
      structured search, including the language model interface, embedding model,\n\
      entity context, prompts, and optional metadata. If some optional inputs are\
      \ not\nprovided, sensible defaults are created.\n\nArgs:\n    model (ChatModel):\
      \ The chat-based language model to drive queries.\n    text_embedder (EmbeddingModel):\
      \ Embedding model used to encode text.\n    entities (list[Entity]): Entities\
      \ present in the current context.\n    entity_text_embeddings (BaseVectorStore):\
      \ Vector store containing entity text embeddings.\n    text_units (list[TextUnit]\
      \ | None): Optional list of TextUnit objects for the context.\n    reports (list[CommunityReport]\
      \ | None): Optional list of CommunityReport objects.\n    relationships (list[Relationship]\
      \ | None): Optional relationships among entities.\n    covariates (dict[str,\
      \ list[Covariate]] | None): Optional covariates keyed by name.\n    tokenizer\
      \ (Tokenizer | None): Optional Tokenizer to use; if None, a tokenizer will be\
      \ created via get_tokenizer().\n    embedding_vectorstore_key (str): Key for\
      \ the embedding vector store; defaults to EntityVectorStoreKey.ID.\n    config\
      \ (DRIFTSearchConfig | None): Optional configuration for DRIFT search behavior;\
      \ if None, a default config is created.\n    local_system_prompt (str | None):\
      \ Optional override for the local system prompt; defaults to DRIFT_LOCAL_SYSTEM_PROMPT.\n\
      \    local_mixed_context (LocalSearchMixedContext | None): Optional prebuilt\
      \ local mixed context; if None, a new one is initialized.\n    reduce_system_prompt\
      \ (str | None): Optional prompt reduction instruction; defaults to DRIFT_REDUCE_PROMPT.\n\
      \    response_type (str | None): Optional specifier for the desired response\
      \ type.\n\nReturns:\n    None\n\nSide effects:\n    - Creates a default DRIFTSearchConfig\
      \ if none is provided.\n    - Creates or retrieves a tokenizer if one is not\
      \ supplied.\n    - Sets default local system and reduction prompts if not provided.\n\
      \    - May initialize a new LocalSearchMixedContext via init_local_context_builder()\
      \ when\n      local_mixed_context is not supplied.\n\nRaises:\n    - Propagates\
      \ any exceptions raised by get_tokenizer() or the LocalSearchMixedContext constructor."
  classes:
  - class_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder
    name: DRIFTSearchContextBuilder
    docstring: "DRIFTSearchContextBuilder wires together core DRIFT components to\
      \ assemble a coherent DRIFT search context used for retrieval and reasoning\
      \ over community reports, entities, covariates, and relationships. It serves\
      \ as the central integration point that binds the language model interface,\
      \ embedding model, entity context, prompts, and optional metadata into a single\
      \ context object consumed by DRIFT-style structured search workflows.\n\nKey\
      \ attributes:\n- model: The chat-based language model interface used for conversational\
      \ reasoning.\n- text_embedder: The embedding model used to encode text for similarity\
      \ search.\n- entities: A list of Entity objects representing known entities\
      \ in reports or queries.\n- entity_text_embeddings: A vector store containing\
      \ embeddings for entity mentions.\n- text_units: Optional list of TextUnit describing\
      \ units of text content.\n- reports: Optional pre-loaded list of CommunityReport\
      \ objects to consider.\n- relationships: Optional relationships between entities.\n\
      - covariates: Optional mapping of covariates, keyed by domain, to lists of Covariate.\n\
      - tokenizer: Optional Tokenizer used to tokenize prompts/text.\n- embedding_vectorstore_key:\
      \ Key to locate entity embeddings in the vector store; defaults to EntityVectorStoreKey.ID.\n\
      - config: Optional DRIFTSearchConfig configuring DRIFT behavior.\n- local_system_prompt:\
      \ Optional custom system prompt for local DRIFT search.\n- local_mixed_context:\
      \ Optional pre-built LocalSearchMixedContext; if omitted, one will be constructed\
      \ as needed.\n- reduce_system_prompt: Optional prompt used during reduction\
      \ steps.\n- response_type: Optional hint for response formatting.\n\nArgs:\n\
      - model (ChatModel): The chat-based language model interface.\n- text_embedder\
      \ (EmbeddingModel): The embedding model used to encode text for retrieval.\n\
      - entities (list[Entity]): Entities referenced in reports and queries.\n- entity_text_embeddings\
      \ (BaseVectorStore): Vector store containing embeddings for entity mentions.\n\
      - text_units (list[TextUnit] | None): Optional list of text units; defaults\
      \ to None.\n- reports (list[CommunityReport] | None): Optional pre-loaded reports;\
      \ defaults to None.\n- relationships (list[Relationship] | None): Optional relationships\
      \ between entities; defaults to None.\n- covariates (dict[str, list[Covariate]]\
      \ | None): Optional covariates by domain; defaults to None.\n- tokenizer (Tokenizer\
      \ | None): Optional tokenizer; if None, a tokenizer will be created as needed.\n\
      - embedding_vectorstore_key (str): Key for locating embeddings in the vector\
      \ store; defaults to EntityVectorStoreKey.ID.\n- config (DRIFTSearchConfig |\
      \ None): Optional DRIFT configuration; defaults to None.\n- local_system_prompt\
      \ (str | None): Optional local system prompt; defaults to None (uses DRIFT_LOCAL_SYSTEM_PROMPT).\n\
      - local_mixed_context (LocalSearchMixedContext | None): Optional pre-built local\
      \ mixed context; defaults to None.\n- reduce_system_prompt (str | None): Optional\
      \ reduction prompt; defaults to None (uses DRIFT_REDUCE_PROMPT).\n- response_type\
      \ (str | None): Optional response formatting hint; defaults to None.\n\nReturns:\n\
      - None: This is a constructor that initializes and wires the components into\
      \ the DRIFT search context.\n\nRaises:\n- ValueError: If required inputs are\
      \ missing or embeddings are incompatible for downstream comparison.\n\nNotes:\n\
      - When optional inputs are omitted, sensible defaults are created to enable\
      \ DRIFT-style search operations.\n\nExamples:\n- Basic usage:\n  builder = DRIFTSearchContextBuilder(\n\
      \      model=my_chat_model,\n      text_embedder=my_embedder,\n      entities=entities_list,\n\
      \      entity_text_embeddings=entity_embedding_store\n  )\n  local = builder.init_local_context_builder()\n\
      \  df, stats = builder.build_context(\"find reports mentioning CityA\")"
    methods:
    - name: init_local_context_builder
      signature: def init_local_context_builder(self) -> LocalSearchMixedContext
    - name: build_context
      signature: "def build_context(\n        self, query: str, **kwargs\n    ) ->\
        \ tuple[pd.DataFrame, dict[str, int]]"
    - name: check_query_doc_encodings
      signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
        -> bool'
    - name: convert_reports_to_df
      signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  text_embedder: EmbeddingModel,\n        entities: list[Entity],\n     \
        \   entity_text_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
        \ | None = None,\n        reports: list[CommunityReport] | None = None,\n\
        \        relationships: list[Relationship] | None = None,\n        covariates:\
        \ dict[str, list[Covariate]] | None = None,\n        tokenizer: Tokenizer\
        \ | None = None,\n        embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n\
        \        config: DRIFTSearchConfig | None = None,\n        local_system_prompt:\
        \ str | None = None,\n        local_mixed_context: LocalSearchMixedContext\
        \ | None = None,\n        reduce_system_prompt: str | None = None,\n     \
        \   response_type: str | None = None,\n    )"
- file: graphrag/query/structured_search/drift_search/primer.py
  functions:
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.split_reports
    name: split_reports
    signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
    docstring: "Split the input reports into folds to enable parallel processing.\n\
      \nArgs:\n    reports (pd.DataFrame): DataFrame of community reports.\n\nReturns:\n\
      \    list[pd.DataFrame]: List of report folds."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
      \ pd.DataFrame,\n    ) -> SearchResult"
    docstring: "Asynchronous search method that processes the query and returns a\
      \ SearchResult.\n\nArgs:\n    query (str): The search query.\n    top_k_reports\
      \ (pd.DataFrame): DataFrame containing the top-k reports.\n\nReturns:\n    SearchResult:\
      \ The search result containing the response and context data."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__call__
    name: __call__
    signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
    docstring: "Call method to process the query by expanding it and embedding the\
      \ result.\n\nThis asynchronous method delegates to expand_query to produce an\
      \ expanded\nquery text and a token-count dictionary, then computes the embedding\
      \ for the\nexpanded text using the text embedding model.\n\nArgs:\n    query\
      \ (str): The original search query.\n\nReturns:\n    tuple[list[float], dict[str,\
      \ int]]: A tuple containing\n        - the embedding vector (as a list of floats)\
      \ for the expanded query, and\n        - a token-count dictionary produced by\
      \ expand_query (e.g., containing\n          keys such as \"llm_calls\", \"prompt_tokens\"\
      , and \"output_tokens\").\n\nRaises:\n    Propagates exceptions raised by expand_query\
      \ or by the embedding model."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.expand_query
    name: expand_query
    signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
    docstring: "Expand the query using a random community report template.\n\nArgs:\n\
      \    query (str): The original search query.\n\nReturns:\n    tuple[str, dict[str,\
      \ int]]: Expanded query text and a dictionary with token usage details:\n  \
      \      llm_calls: number of language model calls made (usually 1)\n        prompt_tokens:\
      \ number of tokens in the prompt\n        output_tokens: number of tokens in\
      \ the model output"
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.decompose_query
    name: decompose_query
    signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
      \    ) -> tuple[dict, dict[str, int]]"
    docstring: "Decompose the query into subqueries using global guidance from the\
      \ provided community reports.\n\nArgs:\n    query (str): The original search\
      \ query.\n    reports (pd.DataFrame): DataFrame containing community reports.\
      \ Must include a 'full_content' column used to build the concatenated context\
      \ for the primer prompt.\n\nReturns:\n    tuple[dict, dict[str, int]]: A tuple\
      \ containing:\n        parsed_response (dict): The parsed JSON response produced\
      \ by the language model.\n        token_ct (dict[str, int]): Token usage counters\
      \ with keys:\n            llm_calls (int): number of language model calls performed\n\
      \            prompt_tokens (int): number of tokens in the constructed prompt\n\
      \            output_tokens (int): number of tokens in the model output\n\nRaises:\n\
      \    KeyError: if the reports DataFrame does not contain the required 'full_content'\
      \ column.\n    json.JSONDecodeError: if the model response content is not valid\
      \ JSON.\n    Exception: any exceptions raised by the chat model interaction\
      \ (propagated from the API call).\n\nNotes:\n    The function builds community_reports\
      \ by concatenating the 'full_content' field of all reports with double newlines,\
      \ and passes that into DRIFT_PRIMER_PROMPT."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n   \
      \     text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
      \        tokenizer: Tokenizer | None = None,\n    )"
    docstring: "Initialize the PrimerQueryProcessor.\n\nArgs:\n    chat_model (ChatModel):\
      \ The language model used to process the query.\n    text_embedder (EmbeddingModel):\
      \ The text embedding model.\n    reports (list[CommunityReport]): List of community\
      \ reports.\n    tokenizer (Tokenizer | None, optional): Token encoder for token\
      \ counting.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n\
      \        chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
      \    )"
    docstring: "Initialize the DRIFTPrimer.\n\nArgs:\n    config (DRIFTSearchConfig):\
      \ Configuration settings for DRIFT search.\n    chat_model (ChatModel): The\
      \ language model used for searching.\n    tokenizer (Tokenizer, optional): Tokenizer\
      \ for managing tokens. If not provided, a default tokenizer is obtained.\n\n\
      Returns:\n    None\n\nRaises:\n    None"
  classes:
  - class_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer
    name: DRIFTPrimer
    docstring: "DRIFTPrimer coordinates the DRIFT drift-search workflow for structured\
      \ query processing over community reports. It decomposes queries using global\
      \ guidance, splits the input reports into folds for parallel processing, and\
      \ executes asynchronous searches against a language model, using a tokenizer\
      \ to manage tokens.\n\nAttributes:\n    config (DRIFTSearchConfig): Configuration\
      \ settings for DRIFT search.\n    chat_model (ChatModel): The language model\
      \ used for searching.\n    tokenizer (Tokenizer): Tokenizer used to manage tokens\
      \ during processing.\n\nArgs:\n    config (DRIFTSearchConfig): Configuration\
      \ settings for DRIFT search.\n    chat_model (ChatModel): The language model\
      \ used for searching.\n    tokenizer (Tokenizer, optional): Tokenizer for managing\
      \ tokens. If not provided, a default tokenizer is obtained.\n\nReturns:\n  \
      \  None\n\nRaises:\n    None"
    methods:
    - name: split_reports
      signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
        \ pd.DataFrame,\n    ) -> SearchResult"
    - name: decompose_query
      signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
        \    ) -> tuple[dict, dict[str, int]]"
    - name: __init__
      signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n\
        \        chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
        \    )"
  - class_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor
    name: PrimerQueryProcessor
    docstring: "PrimerQueryProcessor expands a user query using a randomly selected\
      \ community report template and computes its embedding with the provided models.\
      \ It processes a single query per call.\n\nArgs:\n    chat_model (ChatModel):\
      \ The language model used to expand the query into an augmented form via a randomized\
      \ template.\n    text_embedder (EmbeddingModel): The embedding model used to\
      \ compute the dense vector for the expanded query.\n    reports (list[CommunityReport]):\
      \ A list of CommunityReport instances used as templates for expansion.\n   \
      \ tokenizer (Tokenizer | None): Optional tokenizer to count tokens during expansion\
      \ and embedding.\n\nReturns:\n    None\n\nRaises:\n    ValueError, TypeError,\
      \ RuntimeError: If inputs are invalid or underlying models fail.\n\nMethods:\n\
      \    __call__(self, query: str) -> tuple[list[float], dict[str, int]]\n    \
      \    Process a single query by expanding it (via expand_query) and then computing\
      \ its embedding.\n        Returns:\n            embedding (list[float]): The\
      \ embedding vector produced by text_embedder.\n            token_usage (dict[str,\
      \ int]): Token usage details from the expansion/embedding process (e.g., llm_calls,\
      \ prompt_tokens, output_tokens).\n\n    expand_query(self, query: str) -> tuple[str,\
      \ dict[str, int]]\n        Expand the input query using a randomly selected\
      \ community report template.\n        Returns:\n            expanded_query (str):\
      \ The expanded query text.\n            token_usage (dict[str, int]): Token\
      \ usage details from the expansion step (e.g., llm_calls, prompt_tokens, output_tokens)."
    methods:
    - name: __call__
      signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
    - name: expand_query
      signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
    - name: __init__
      signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n \
        \       text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
        \        tokenizer: Tokenizer | None = None,\n    )"
- file: graphrag/query/structured_search/drift_search/search.py
  functions:
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response
    name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
      \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
      \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
      \    ) -> str"
    docstring: "Reduce the response to a single comprehensive response.\n\nParameters\n\
      ----------\nresponses : str|dict[str, Any]\n    The responses to reduce. If\
      \ a string, it is treated as a single response; if a\n    dict, the function\
      \ extracts the \"answer\" field from each node in responses[\"nodes\"].\nquery\
      \ : str\n    The original query.\nllm_calls : dict[str, int]\n    Counter for\
      \ LLM calls performed during reduction. This dictionary is updated in\n    place;\
      \ after execution, llm_calls[\"reduce\"] will be set to 1.\nprompt_tokens :\
      \ dict[str, int]\n    Token counts for prompts used during reduction. This dictionary\
      \ is updated in\n    place; after execution, prompt_tokens[\"reduce\"] will\
      \ equal the total number of\n    tokens in the constructed search prompt plus\
      \ the encoded query.\noutput_tokens : dict[str, int]\n    Token counts for outputs\
      \ produced during reduction. This dictionary is updated in\n    place; after\
      \ execution, output_tokens[\"reduce\"] will equal the number of tokens in\n\
      \    the reduced response.\nllm_kwargs : dict[str, Any]\n    Additional keyword\
      \ arguments to pass to the LLM (passed to model.achat via\n    model_parameters).\n\
      \nReturns\n-------\nstr\n    The reduced (consolidated) response."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._process_primer_results
    name: _process_primer_results
    signature: "def _process_primer_results(\n        self, query: str, search_results:\
      \ SearchResult\n    ) -> DriftAction"
    docstring: "Process the results from the primer search to extract intermediate\
      \ answers and follow-up queries.\n\nArgs:\n    query (str): The original search\
      \ query.\n    search_results (SearchResult): The results from the primer search.\n\
      \nReturns:\n    DriftAction: Action generated from the primer response.\n\n\
      Raises:\n    RuntimeError: If no intermediate answers or follow-up queries are\
      \ found in the primer response.\n    ValueError: If the primer response is not\
      \ a list of dictionaries."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._search_step
    name: _search_step
    signature: "def _search_step(\n        self, global_query: str, search_engine:\
      \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]"
    docstring: "Perform an asynchronous search step (internal API) by executing each\
      \ DriftAction concurrently and collecting the results.\n\nArgs:\n    global_query\
      \ (str): The global query for the search.\n    search_engine (LocalSearch):\
      \ The local search engine instance.\n    actions (list[DriftAction]): A list\
      \ of actions to perform.\n\nReturns:\n    list[DriftAction]: The results from\
      \ executing the search actions asynchronously.\n\nRaises:\n    Exception: May\
      \ propagate exceptions raised by the underlying action searches."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response_streaming
    name: _reduce_response_streaming
    signature: "def _reduce_response_streaming(\n        self,\n        responses:\
      \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
      \ Any],\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream a reduced response by constructing a reduced prompt from the\
      \ provided responses and streaming tokens from the model.\n\nParameters\n----------\n\
      responses : str | dict[str, Any]\n    The responses to reduce. If a string,\
      \ treated as a single response; otherwise, if a dict,\n    extract the \"answer\"\
      \ value from each node in responses.get(\"nodes\", []) that has an \"answer\"\
      .\nquery : str\n    The original query.\nmodel_params : dict[str, Any]\n   \
      \ Parameters for the underlying model used during streaming.\n\nReturns\n-------\n\
      AsyncGenerator[str, None]\n    An asynchronous generator yielding streamed tokens\
      \ (strings) from the model. Each yielded token\n    corresponds to a portion\
      \ of the reduced response. Tokens are also emitted to registered callbacks\n\
      \    via on_llm_new_token as they arrive.\n\nRaises\n------\nException\n   \
      \ Propagates exceptions raised by the underlying model streaming or by registered\
      \ callbacks."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n\
      \        query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n    )"
    docstring: "Initialize the DRIFTSearch class.\n\nThis constructor wires core components\
      \ for DRIFT-style search, including the\nlanguage model interface, context,\
      \ token handling, and query lifecycle.\n\nArgs:\n    model (ChatModel): The\
      \ chat-based language model used for searching.\n    context_builder (DRIFTSearchContextBuilder):\
      \ Builder that holds DRIFT configuration and context.\n    tokenizer (Tokenizer,\
      \ optional): Token encoder used to tokenize input and manage tokens.\n     \
      \   If not provided, a default tokenizer is obtained via get_tokenizer().\n\
      \    query_state (QueryState, optional): State tracked for the current search\
      \ query.\n        If not provided, a new QueryState is created.\n    callbacks\
      \ (list[QueryCallbacks], optional): Callback handlers for query events.\n  \
      \      If not provided, an empty list is used.\n\nReturns:\n    None\n\nSide\
      \ effects:\n    - Assigns instance attributes for model, context_builder, tokenizer,\
      \ and query_state.\n    - Creates a DRIFTPrimer instance configured with the\
      \ current context and tokenizer.\n    - Initializes the local search component\
      \ by calling init_local_search(),\n      preparing a LocalSearch with parameters\
      \ derived from the DRIFT configuration."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
    name: init_local_search
    signature: def init_local_search(self) -> LocalSearch
    docstring: "Initialize a LocalSearch instance configured from the DRIFT search\
      \ configuration.\n\nThis method reads values from the DRIFTSearch instance's\
      \ configuration (via self.context_builder.config) to build the LocalSearch parameters,\
      \ including how context is built, how many tokens to consider, and how the OpenAI\
      \ model is prompted.\n\nParameters\n    None: This method does not accept explicit\
      \ parameters and relies on the DRIFTSearch instance state\n    (model, context_builder,\
      \ tokenizer, and config) to construct and configure the LocalSearch.\n\nReturns\n\
      \    LocalSearch: A LocalSearch instance configured with:\n        model: the\
      \ current ChatModel\n        system_prompt: the DRIFT context's system prompt\n\
      \        context_builder: the DRIFT local mixed context\n        tokenizer:\
      \ the tokenizer in use\n        model_params: OpenAI API parameters derived\
      \ from the local_search fields\n        context_builder_params: local context\
      \ parameters derived from the local_search fields\n        response_type: multiple\
      \ paragraphs\n        callbacks: any provided callbacks\n\nConfiguration details\
      \ used from DRIFT config\n    local_search_text_unit_prop: text unit property\
      \ used by the local search\n    local_search_community_prop: property for community\
      \ weighting\n    local_search_top_k_mapped_entities: maximum number of mapped\
      \ entities\n    local_search_top_k_relationships: maximum number of relationships\n\
      \    local_search_max_data_tokens: maximum tokens for local context\n    local_search_llm_max_gen_tokens:\
      \ maximum tokens for LLM generation\n    local_search_temperature: sampling\
      \ temperature\n    local_search_n: number of candidate generations\n    local_search_top_p:\
      \ nucleus sampling parameter\n    local_search_llm_max_gen_completion_tokens:\
      \ maximum tokens for LLM completion\n\nNotes\n    This method relies on the\
      \ presence and validity of the above config fields.\n    Missing or invalid\
      \ values may raise exceptions when constructing LocalSearch or its parameters.\n\
      \nRaises\n    ValueError: if required configuration values are missing or invalid\n\
      \    TypeError: if internal components are not properly initialized\n    Exception:\
      \ propagates any exception raised by LocalSearch construction or parameter derivation"
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Perform an asynchronous DRIFT search.\n\nArgs:\n    query (str): The\
      \ query to search for.\n    conversation_history (Any, optional): The conversation\
      \ history, if any.\n    reduce (bool, optional): Whether to reduce the response\
      \ to a single comprehensive response.\n    **kwargs: Additional keyword arguments\
      \ that may be used by the search implementation.\n\nReturns:\n    SearchResult:\
      \ The search result containing the response and context data.\n\nRaises:\n \
      \   ValueError: If the query is empty."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self, query: str, conversation_history:\
      \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
    docstring: "Perform a streaming DRIFT search asynchronously.\n\nArgs:\n    query\
      \ (str): The query to search for.\n    conversation_history (ConversationHistory\
      \ | None, optional): The conversation history.\n\nReturns:\n    AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding pieces of the reduced response as\
      \ they are produced.\n\nRaises:\n    Exception: If the underlying search or\
      \ streaming operations fail."
  classes:
  - class_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch
    name: DRIFTSearch
    docstring: 'DRIFTSearch orchestrates the DRIFT-style search workflow for structured
      queries by coordinating a language model, a DRIFT context builder, and local
      search components to iteratively refine results.


      Purpose:

      - Manage the end-to-end DRIFT search lifecycle, including initialization, prompt
      construction, local search steps, result aggregation, and optional streaming.


      Key responsibilities:

      - Wire together the language model interface, DRIFT context handling, tokenization,
      and the query lifecycle.

      - Expose methods to initialize local search, perform search steps, and stream
      results.

      - Coordinate reduction of model responses into a final answer and intermediate
      actions.


      Key attributes (inferred from constructor and methods):

      - model: The ChatModel used to interact with the language model.

      - context_builder: DRIFTSearchContextBuilder housing configuration and DRIFT
      context.

      - tokenizer: Optional Tokenizer used for token handling and prompting/streaming.

      - query_state: Optional QueryState tracking the current query status and results.

      - callbacks: Optional list[QueryCallbacks] for query lifecycle events.


      Notes:

      - This class delegates most operational logic to specialized components (e.g.,
      LocalSearch, DRIFTPrimer) to implement the DRIFT workflow.


      Raises:

      - None. This class does not raise exceptions; errors from underlying components
      may propagate to callers.'
    methods:
    - name: _reduce_response
      signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
        \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n       \
        \ prompt_tokens: dict[str, int],\n        output_tokens: dict[str, int],\n\
        \        **llm_kwargs,\n    ) -> str"
    - name: _process_primer_results
      signature: "def _process_primer_results(\n        self, query: str, search_results:\
        \ SearchResult\n    ) -> DriftAction"
    - name: _search_step
      signature: "def _search_step(\n        self, global_query: str, search_engine:\
        \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]"
    - name: _reduce_response_streaming
      signature: "def _reduce_response_streaming(\n        self,\n        responses:\
        \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
        \ Any],\n    ) -> AsyncGenerator[str, None]"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer\
        \ | None = None,\n        query_state: QueryState | None = None,\n       \
        \ callbacks: list[QueryCallbacks] | None = None,\n    )"
    - name: init_local_search
      signature: def init_local_search(self) -> LocalSearch
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
        \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
    - name: stream_search
      signature: "def stream_search(\n        self, query: str, conversation_history:\
        \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
- file: graphrag/query/structured_search/drift_search/state.py
  functions:
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.action_token_ct
    name: action_token_ct
    signature: def action_token_ct(self) -> dict[str, int]
    docstring: "Return the token counts across all actions in the graph.\n\nArgs:\n\
      \    self: The instance containing a graph attribute; each node in the graph\
      \ has metadata with keys 'llm_calls', 'prompt_tokens', and 'output_tokens'.\n\
      Returns:\n    dict[str, int]: A dictionary with keys 'llm_calls', 'prompt_tokens',\
      \ and 'output_tokens' mapping to the summed counts across all nodes."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize the drift query state with an empty graph.\n\nArgs:\n \
      \   self: The instance being initialized.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.serialize
    name: serialize
    signature: "def serialize(\n        self, include_context: bool = True\n    )\
      \ -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
    docstring: "Serialize the graph to a dictionary representation, optionally including\
      \ contextual information for nodes.\n\nArgs:\n- include_context (bool): If True,\
      \ return a 3-tuple consisting of the graph dictionary, a context_data dictionary,\
      \ and a string representation of the context_data. If False, return only the\
      \ graph dictionary.\n\nReturns:\n- If include_context is False: a dictionary\
      \ with the keys \"nodes\" and \"edges\". \"nodes\" is a list of dictionaries\
      \ for each node, each containing an \"id\" (int), fields from node.serialize(include_follow_ups=False),\
      \ and all attributes from self.graph.nodes[node]. \"edges\" is a list of dictionaries\
      \ with \"source\" (int), \"target\" (int), and \"weight\" (float, defaults to\
      \ 1.0).\n- If include_context is True: a tuple of three elements: (graph_dict,\
      \ context_data, context_text).\n  - graph_dict is the same dictionary as above\
      \ (with keys \"nodes\" and \"edges\").\n  - context_data is a dictionary mapping\
      \ a node query (string) to its context_data (any), built from nodes that have\
      \ a non-empty metadata.context_data and a query.\n  - context_text is the string\
      \ representation of context_data.\n\nNotes and edge cases:\n- The function returns\
      \ different shapes depending on include_context. Callers must handle both possibilities.\n\
      - If no context data exists, context_data will be {}, and context_text will\
      \ be \"{}\".\n- If the graph has no nodes, nodes and edges lists are empty;\
      \ IDs are assigned by enumeration order starting at 0.\n- We rely on node.serialize(include_follow_ups=False)\
      \ for the per-node payload; exact fields depend on the DriftAction implementation.\n\
      - Edge weights default to 1.0 if not present in edge_data."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.find_incomplete_actions
    name: find_incomplete_actions
    signature: def find_incomplete_actions(self) -> list[DriftAction]
    docstring: "Find all unanswered actions in the graph.\n\n        Args:\n     \
      \       self: The instance containing the graph where actions reside.\n\n  \
      \      Returns:\n            list[DriftAction]: A list of DriftAction objects\
      \ that are not complete."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_action
    name: add_action
    signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any]
      | None = None)'
    docstring: "Add an action node to the graph with optional metadata.\n\nArgs:\n\
      \    action: DriftAction to add as a node in the graph.\n    metadata: Optional\
      \ dict[str, Any] of node attributes to attach to the action. If None, no attributes\
      \ are added.\n\nReturns:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_all_follow_ups
    name: add_all_follow_ups
    signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
      \        follow_ups: list[DriftAction] | list[str],\n        weight: float =\
      \ 1.0,\n    )"
    docstring: "Add all follow-up actions and link them to the given action.\n\nArgs:\n\
      \    action: The parent DriftAction to link follow-up actions to.\n    follow_ups:\
      \ A list of follow-up actions to add. Each item can be a DriftAction or a string\
      \ query.\n    weight: The weight to apply to each relationship when linking\
      \ to the parent.\n\nReturns:\n    None. The follow-up actions are added to the\
      \ graph and connected to the given action.\n\nRaises:\n    None. The method\
      \ logs warnings for invalid input types but does not raise exceptions."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.deserialize
    name: deserialize
    signature: 'def deserialize(self, data: dict[str, Any])'
    docstring: "Deserialize the dictionary back to a graph.\n\nArgs:\n    data: Serialized\
      \ representation of the graph to deserialize. Contains \"nodes\" and \"edges\"\
      .\n\nReturns:\n    None: This method updates the graph in place and does not\
      \ return a value.\n\nRaises:\n    KeyError: If any node entry in data's \"nodes\"\
      \ list is missing the required \"id\" field."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.relate_actions
    name: relate_actions
    signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
      \ weight: float = 1.0\n    )"
    docstring: "Relate two actions in the graph.\n\nArgs:\n    self: The QueryState\
      \ instance.\n    parent: The parent DriftAction in the relation.\n    child:\
      \ The child DriftAction to be related to the parent.\n    weight: The weight\
      \ of the edge to add between the actions.\n\nReturns:\n    None. The function\
      \ mutates the internal graph by adding an edge with the specified weight.\n\n\
      Raises:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.rank_incomplete_actions
    name: rank_incomplete_actions
    signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
      \ float] | None = None\n    ) -> list[DriftAction]"
    docstring: "\"\"\"Rank all incomplete actions in the graph, optionally by a scorer.\n\
      \nArgs:\n    scorer: Callable[[DriftAction], float] | None. A function that\
      \ takes a DriftAction and returns a numeric score. If provided, actions are\
      \ scored and returned sorted by score in descending order. If None, actions\
      \ are returned in a random order.\n\nReturns:\n    list[DriftAction]: The incomplete\
      \ actions, either ranked by score or shuffled.\n\nRaises:\n    None\n\"\"\""
  classes:
  - class_id: graphrag/query/structured_search/drift_search/state.py::QueryState
    name: QueryState
    docstring: "Represents the state of a drift search query as a graph of DriftAction\
      \ nodes.\n\nArgs:\n    self: The instance being initialized. This class is instantiated\
      \ without external parameters.\n\nReturns:\n    None\n\nRaises:\n    None\n\n\
      Purpose/responsibility:\n    Manage a directed graph of DriftAction nodes, enabling\
      \ addition of actions, linking follow-ups, serialization/deserialization, and\
      \ ranking of incomplete actions during structured drift search.\n\nKey attributes:\n\
      \    graph (nx.DiGraph): The directed graph storing DriftAction nodes and their\
      \ relationships. Each node carries metadata with keys 'llm_calls', 'prompt_tokens',\
      \ and 'output_tokens'."
    methods:
    - name: action_token_ct
      signature: def action_token_ct(self) -> dict[str, int]
    - name: __init__
      signature: def __init__(self)
    - name: serialize
      signature: "def serialize(\n        self, include_context: bool = True\n   \
        \ ) -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
    - name: find_incomplete_actions
      signature: def find_incomplete_actions(self) -> list[DriftAction]
    - name: add_action
      signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any]
        | None = None)'
    - name: add_all_follow_ups
      signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
        \        follow_ups: list[DriftAction] | list[str],\n        weight: float\
        \ = 1.0,\n    )"
    - name: deserialize
      signature: 'def deserialize(self, data: dict[str, Any])'
    - name: relate_actions
      signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
        \ weight: float = 1.0\n    )"
    - name: rank_incomplete_actions
      signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
        \ float] | None = None\n    ) -> list[DriftAction]"
- file: graphrag/query/structured_search/global_search/community_context.py
  functions:
  - node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        entities: list[Entity] | None\
      \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
      \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
      \ | None = None,\n        random_state: int = 86,\n    )"
    docstring: "Initialize a GlobalCommunityContext instance with the provided data\
      \ and optional configuration.\n\nArgs:\n    community_reports: Reports for communities\
      \ to consider.\n    communities: Community objects used to build the hierarchy\
      \ and starting points.\n    entities: Optional list of Entity objects to include\
      \ in the context.\n    tokenizer: Tokenizer to use; if None, a default tokenizer\
      \ is obtained via get_tokenizer().\n    dynamic_community_selection: Enable\
      \ dynamic selection of communities during processing.\n    dynamic_community_selection_kwargs:\
      \ Optional keyword arguments for configuring DynamicCommunitySelection.\n  \
      \  random_state: Seed for random number generation (default 86).\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        use_community_summary: bool =\
      \ True,\n        column_delimiter: str = \"|\",\n        shuffle_data: bool\
      \ = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
      \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
      \ bool = True,\n        community_weight_name: str = \"occurrence\",\n     \
      \   normalize_community_weight: bool = True,\n        max_context_tokens: int\
      \ = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
      \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n  \
      \      **kwargs: Any,\n    ) -> ContextBuilderResult"
    docstring: "Prepare batches of community report data table as context data for\
      \ global search.\n\nArgs:\n  query: The user query to build context for.\n \
      \ conversation_history: Optional conversation history to consider while constructing\
      \ the context.\n  use_community_summary: Whether to use the community summary\
      \ in the context data.\n  column_delimiter: Delimiter used to separate columns\
      \ in the context representation.\n  shuffle_data: Whether to shuffle the data\
      \ before context assembly.\n  include_community_rank: Whether to include per-community\
      \ rank in the context.\n  min_community_rank: Minimum rank value to include\
      \ in the context.\n  community_rank_name: Name to assign to the rank field in\
      \ the context data.\n  include_community_weight: Whether to include per-community\
      \ weight in the context.\n  community_weight_name: Name to assign to the weight\
      \ field in the context data.\n  normalize_community_weight: Whether to normalize\
      \ the community weights.\n  max_context_tokens: Maximum allowed tokens for the\
      \ generated context.\n  context_name: Descriptive name for the context data\
      \ (e.g., \"Reports\").\n  conversation_history_user_turns_only: Whether to include\
      \ only user turns from the conversation history.\n  conversation_history_max_turns:\
      \ Maximum number of turns to include from the conversation history.\n  kwargs:\
      \ Additional keyword arguments.\n\nReturns:\n  ContextBuilderResult: The result\
      \ containing context chunks, context data, and related usage statistics.\n\n\
      Raises:\n  Exceptions raised by underlying components (e.g., conversation_history,\
      \ dynamic_community_selection, and build_community_context) during context construction."
  classes:
  - class_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext
    name: GlobalCommunityContext
    docstring: "GlobalCommunityContext is a global context builder for structured\
      \ search across multiple communities. It extends GlobalContextBuilder and coordinates\
      \ community reports, communities, optional entities, and tokenizer-driven text\
      \ processing to assemble a unified context used by the global search workflow.\
      \ It also supports optional dynamic community selection to tailor context content\
      \ to the query.\n\nArgs:\n  community_reports (list[CommunityReport]): Reports\
      \ for communities to consider.\n  communities (list[Community]): Community objects\
      \ used to build the hierarchy and starting points.\n  entities (list[Entity]\
      \ | None, optional): Optional list of Entity objects to include in the context.\n\
      \  tokenizer (Tokenizer | None, optional): Tokenizer to use; if None, a default\
      \ tokenizer is obtained via get_tokenizer.\n  dynamic_community_selection (bool):\
      \ Enable dynamic community selection.\n  dynamic_community_selection_kwargs\
      \ (dict[str, Any] | None, optional): Optional kwargs for dynamic selection.\n\
      \  random_state (int): Random seed for reproducibility.\n\nAttributes:\n  community_reports:\
      \ The provided CommunityReport objects.\n  communities: The provided Community\
      \ objects.\n  entities: Optional list of Entity objects included in the context.\n\
      \  tokenizer: The Tokenizer instance in use.\n  dynamic_community_selection:\
      \ Flag indicating if dynamic selection is enabled.\n  dynamic_community_selection_kwargs:\
      \ Additional kwargs for dynamic selection.\n  random_state: Random seed for\
      \ reproducibility.\n\nInitialization:\n  This constructor initializes the instance\
      \ with the provided data and configuration. It does not return a value. If tokenizer\
      \ is None, a default tokenizer is obtained via get_tokenizer. If dynamic_community_selection\
      \ is True, dynamic selection will be configured using dynamic_community_selection_kwargs.\
      \ The class relies on the base GlobalContextBuilder for shared behavior and\
      \ may raise TypeError or ValueError for invalid inputs.\n\nSee Also:\n  GlobalContextBuilder\n\
      \nMethods:\n  build_context: Prepare batches of community report data as context\
      \ data for global search. Returns a ContextBuilderResult."
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
        \        communities: list[Community],\n        entities: list[Entity] | None\
        \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
        \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
        \ | None = None,\n        random_state: int = 86,\n    )"
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        use_community_summary:\
        \ bool = True,\n        column_delimiter: str = \"|\",\n        shuffle_data:\
        \ bool = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
        \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
        \ bool = True,\n        community_weight_name: str = \"occurrence\",\n   \
        \     normalize_community_weight: bool = True,\n        max_context_tokens:\
        \ int = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
        \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n\
        \        **kwargs: Any,\n    ) -> ContextBuilderResult"
- file: graphrag/query/structured_search/global_search/search.py
  functions:
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n     \
      \   map_system_prompt: str | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str = \"multiple paragraphs\",\n  \
      \      allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
      \ str | None = None,\n        json_mode: bool = True,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n        max_data_tokens: int = 8000,\n        map_llm_params:\
      \ dict[str, Any] | None = None,\n        reduce_llm_params: dict[str, Any] |\
      \ None = None,\n        map_max_length: int = 1000,\n        reduce_max_length:\
      \ int = 2000,\n        context_builder_params: dict[str, Any] | None = None,\n\
      \        concurrent_coroutines: int = 32,\n    )"
    docstring: "Initialize a GlobalSearch instance (internal API).\n\nArgs:\n    model:\
      \ ChatModel - The language model interface used for this global search.\n  \
      \  context_builder: GlobalContextBuilder - The builder that constructs the context\
      \ for the search.\n    tokenizer: Tokenizer | None - Optional tokenizer to use;\
      \ if None, a default tokenizer will be used.\n    map_system_prompt: str | None\
      \ - System prompt for the map stage; if None, MAP_SYSTEM_PROMPT is used.\n \
      \   reduce_system_prompt: str | None - System prompt for the reduce stage; if\
      \ None, REDUCE_SYSTEM_PROMPT is used.\n    response_type: str - How to format\
      \ the response, e.g., \"multiple paragraphs\".\n    allow_general_knowledge:\
      \ bool - Whether to allow incorporating general knowledge beyond the provided\
      \ context.\n    general_knowledge_inclusion_prompt: str | None - Prompt guiding\
      \ inclusion of general knowledge; if None, GENERAL_KNOWLEDGE_INSTRUCTION is\
      \ used.\n    json_mode: bool - Whether to request responses in JSON format.\n\
      \    callbacks: list[QueryCallbacks] | None - Optional callbacks to handle search\
      \ lifecycle events.\n    max_data_tokens: int - Maximum number of tokens allocated\
      \ for data in the mapping stage.\n    map_llm_params: dict[str, Any] | None\
      \ - Parameters for the map LLM call.\n    reduce_llm_params: dict[str, Any]\
      \ | None - Parameters for the reduce LLM call.\n    map_max_length: int - Maximum\
      \ token length for map responses.\n    reduce_max_length: int - Maximum token\
      \ length for reduce responses.\n    context_builder_params: dict[str, Any] |\
      \ None - Additional parameters forwarded to the context builder.\n    concurrent_coroutines:\
      \ int - Maximum number of concurrent coroutines running during mapping.\n\n\
      Returns:\n    None"
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
    docstring: "Perform a global search.\n\nGlobal search mode includes two steps:\n\
      \n- Step 1: Run parallel LLM calls on communities' short summaries to generate\
      \ answer for each batch\n- Step 2: Combine the answers from step 2 to generate\
      \ the final answer\n\nArgs:\n    query: The search query.\n    conversation_history:\
      \ Optional conversation history to provide context for the search.\n    kwargs:\
      \ Additional keyword arguments for the search.\n\nReturns:\n    GlobalSearchResult:\
      \ The result of the global search."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._map_response_single_batch
    name: _map_response_single_batch
    signature: "def _map_response_single_batch(\n        self,\n        context_data:\
      \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
      \    ) -> SearchResult"
    docstring: "Generate an answer for a single chunk of community reports.\n\nArgs:\n\
      \    context_data (str): Contextual data for the current chunk of community\
      \ reports.\n    query (str): The query to be answered based on the provided\
      \ context.\n    max_length (int): Maximum length allowed for the generated response.\n\
      \    llm_kwargs (Any): Additional keyword arguments forwarded to the language\
      \ model (model_parameters).\n\nReturns:\n    SearchResult: The processed response\
      \ for this batch, including the parsed response, context data, timing, and token\
      \ usage.\n\nRaises:\n    None: This function handles exceptions internally and\
      \ does not propagate exceptions to the caller."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._stream_reduce_response
    name: _stream_reduce_response
    signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
      \ list[SearchResult],\n        query: str,\n        max_length: int,\n     \
      \   **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream and reduce multiple map responses into a single streamed output\
      \ by ranking key points and querying the LLM.\n\nArgs:\n    map_responses (list[SearchResult]):\
      \ List of SearchResult objects to extract key points from. Each result may contain\
      \ a response that is a list of dictionaries with keys 'answer' and 'score'.\n\
      \    query (str): User query string to pass as the prompt for the LLM during\
      \ streaming.\n    max_length (int): Maximum length to request in the reduce\
      \ system prompt (limits the generated content).\n    llm_kwargs (dict[str, Any]):\
      \ Additional keyword arguments forwarded to the language model streaming method\
      \ (e.g., model_parameters). This function forwards these to the underlying streaming\
      \ API via the async generator.\n\nReturns:\n    AsyncGenerator[str, None]: An\
      \ asynchronous generator yielding chunks of text produced by the streaming LLM.\n\
      \nNotes:\n- Key points are collected from all map_responses, filtered to keep\
      \ only entries with a positive 'score', and labeled with the originating analyst\
      \ index (Analyst 1, Analyst 2, ...).\n- If no positive-scoring key points exist\
      \ and allow_general_knowledge is False, the function yields NO_DATA_ANSWER and\
      \ terminates. This provides a canned empty answer to avoid hallucinations when\
      \ no relevant data is available.\n- If general knowledge is allowed (allow_general_knowledge\
      \ is True) and no data points exist, the NO_DATA_ANSWER path is skipped and\
      \ the LLM may supplement with generic knowledge.\n- The function enforces a\
      \ token budget via self.max_data_tokens, constructing text_data by concatenating\
      \ stacked analyst blocks until the token limit would be exceeded.\n- The constructed\
      \ report (text_data) feeds reduce_system_prompt via its format with report_data,\
      \ response_type, and max_length. If allow_general_knowledge is enabled, an additional\
      \ general knowledge inclusion prompt is appended.\n- The final prompt used to\
      \ query the LLM is built from search_prompt, and the function streams chunks\
      \ from self.model.achat_stream, yielding each chunk while notifying registered\
      \ callbacks through on_llm_new_token.\n- Analytic ordering is determined by\
      \ descending score after filtering, while analysts are preserved by their original\
      \ indices for labeling in the response."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream the global search response.\n\nArgs:\n    query: str\n    \
      \    The search query to process.\n    conversation_history: ConversationHistory\
      \ | None\n        Optional conversation history to provide context for the search.\n\
      \nReturns:\n    AsyncGenerator[str, None]\n        An asynchronous generator\
      \ yielding string fragments that represent streaming portions of the final answer.\
      \ Fragments are produced by the streaming reduction step as results become available.\n\
      \nRaises:\n    Exception\n        Propagates exceptions raised by the context\
      \ builder, mapping, and streaming components (e.g., I/O or LLM errors)."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._reduce_response
    name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
      \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
    docstring: "Combine all intermediate responses from multiple batches into a final\
      \ answer to the user query.\n\nArgs:\n    self: The instance of the containing\
      \ class.\n    map_responses: list[SearchResult]\n        The intermediate responses\
      \ collected from each batch.\n    query: str\n        The original user query.\n\
      \    llm_kwargs: dict\n        Additional keyword arguments to pass to the LLM\
      \ model during reduction.\n\nReturns:\n    SearchResult\n        The reduced\
      \ final response containing the generated answer, context data,\n        and\
      \ performance metrics such as completion time and token usage.\n\nRaises:\n\
      \    None\n        This method handles exceptions internally and does not raise\
      \ to callers."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
    name: _parse_search_response
    signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
      Any]]'
    docstring: "Parse the search response json and return a list of key points.\n\n\
      Parameters\n----------\nsearch_response : str\n    The search response json\
      \ string. Expected to contain a top-level object with a\n    \"points\" field\
      \ that is a list of key point objects. Each point should include\n    a \"description\"\
      \ and a \"score\".\n\nReturns\n-------\nlists[dict[str, Any]]\n    A list of\
      \ key points, where each point is a dictionary with keys \"answer\" and\n  \
      \  \"score\". The value for \"answer\" comes from the point's \"description\"\
      . The value\n    for \"score\" is the integer value of the point's \"score\"\
      . If the input cannot be\n    parsed or does not contain a valid \"points\"\
      \ list, a single default point is returned:\n    {\"answer\": \"\", \"score\"\
      : 0}.\n\nNotes\n-----\nNo exceptions are raised by this function. Parsing errors\
      \ or missing data result in the\ndefault return above."
  classes:
  - class_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch
    name: GlobalSearch
    docstring: 'GlobalSearch orchestrates a multi-step global search by querying multiple
      batches of community reports in parallel and reducing the results into a final
      answer.


      Purpose:

      Perform a structured global search by running parallel language model calls
      on batches of short summaries and then combining those results to produce a
      final answer.


      Key attributes:

      - model: ChatModel - The language model interface used for this global search.

      - context_builder: GlobalContextBuilder - The builder that constructs the context
      for the search.

      - tokenizer: Tokenizer | None - Optional tokenizer to use; if None, a default
      tokenizer will be used.

      - map_system_prompt: str | None - System prompt for the mapping stage.

      - reduce_system_prompt: str | None - System prompt for the reducing stage.

      - response_type: str - The response format; default "multiple paragraphs".

      - allow_general_knowledge: bool - Whether general knowledge is allowed during
      responses.

      - general_knowledge_inclusion_prompt: str | None - Prompt to include general
      knowledge in responses.

      - json_mode: bool - If True, use JSON-based parsing for responses.

      - callbacks: list[QueryCallbacks] | None - Callbacks invoked during processing.

      - max_data_tokens: int - Maximum tokens used for data pieces.

      - map_llm_params: dict[str, Any] | None - Parameters for map phase LLM calls.

      - reduce_llm_params: dict[str, Any] | None - Parameters for reduce phase LLM
      calls.

      - map_max_length: int - Maximum length for map outputs.

      - reduce_max_length: int - Maximum length for reduce outputs.

      - context_builder_params: dict[str, Any] | None - Parameters for the context
      builder.

      - concurrent_coroutines: int - Number of concurrent coroutines to run.


      Args:

      - model: ChatModel - The language model interface used for this global search.

      - context_builder: GlobalContextBuilder - The builder that constructs the context
      for the search.

      - tokenizer: Tokenizer | None - Optional tokenizer to use; if None, a default
      tokenizer will be used.

      - map_system_prompt: str | None - System prompt for the mapping stage.

      - reduce_system_prompt: str | None - System prompt for the reducing stage.

      - response_type: str - The response format; default "multiple paragraphs".

      - allow_general_knowledge: bool - Whether general knowledge is allowed during
      responses.

      - general_knowledge_inclusion_prompt: str | None - Prompt to include general
      knowledge in responses.

      - json_mode: bool - Whether to parse responses as JSON.

      - callbacks: list[QueryCallbacks] | None - Callbacks invoked during processing.

      - max_data_tokens: int - Maximum tokens used for data pieces.

      - map_llm_params: dict[str, Any] | None - Parameters for map phase LLM calls.

      - reduce_llm_params: dict[str, Any] | None - Parameters for reduce phase LLM
      calls.

      - map_max_length: int - Maximum length for map outputs.

      - reduce_max_length: int - Maximum length for reduce outputs.

      - context_builder_params: dict[str, Any] | None - Parameters for the context
      builder.

      - concurrent_coroutines: int - Number of concurrent coroutines to run.


      Returns:

      None.


      Raises:

      Exceptions raised by underlying components (e.g., ChatModel, GlobalContextBuilder,
      Tokenizer) may propagate.'
    methods:
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: GlobalContextBuilder,\n        tokenizer: Tokenizer |\
        \ None = None,\n        map_system_prompt: str | None = None,\n        reduce_system_prompt:\
        \ str | None = None,\n        response_type: str = \"multiple paragraphs\"\
        ,\n        allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
        \ str | None = None,\n        json_mode: bool = True,\n        callbacks:\
        \ list[QueryCallbacks] | None = None,\n        max_data_tokens: int = 8000,\n\
        \        map_llm_params: dict[str, Any] | None = None,\n        reduce_llm_params:\
        \ dict[str, Any] | None = None,\n        map_max_length: int = 1000,\n   \
        \     reduce_max_length: int = 2000,\n        context_builder_params: dict[str,\
        \ Any] | None = None,\n        concurrent_coroutines: int = 32,\n    )"
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
        \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
    - name: _map_response_single_batch
      signature: "def _map_response_single_batch(\n        self,\n        context_data:\
        \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
        \    ) -> SearchResult"
    - name: _stream_reduce_response
      signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
        \ list[SearchResult],\n        query: str,\n        max_length: int,\n   \
        \     **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
    - name: stream_search
      signature: "def stream_search(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n    ) -> AsyncGenerator[str,\
        \ None]"
    - name: _reduce_response
      signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
        \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
    - name: _parse_search_response
      signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
        Any]]'
- file: graphrag/query/structured_search/local_search/mixed_context.py
  functions:
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.filter_by_entity_keys
    name: filter_by_entity_keys
    signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
    docstring: "Filter entity text embeddings by entity keys.\n\nArgs:\n    entity_keys:\
      \ List of entity keys to filter by. May be a list of integers or a list of strings.\n\
      \nReturns:\n    None"
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        entities: list[Entity],\n  \
      \      entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
      \        text_units: list[TextUnit] | None = None,\n        community_reports:\
      \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n    )"
    docstring: "Initialize a LocalSearchMixedContext with the provided data and optional\
      \ configuration.\n\nArgs:\n    entities: list[Entity] The list of entities to\
      \ include.\n    entity_text_embeddings: BaseVectorStore The vector store containing\
      \ embeddings for entity text.\n    text_embedder: EmbeddingModel The embedding\
      \ model used to embed text for similarity search.\n    text_units: list[TextUnit]\
      \ | None Optional list of TextUnit objects to include.\n    community_reports:\
      \ list[CommunityReport] | None Optional list of CommunityReport objects to include.\n\
      \    relationships: list[Relationship] | None Optional list of Relationship\
      \ objects to include.\n    covariates: dict[str, list[Covariate]] | None Optional\
      \ mapping of covariates by key.\n    tokenizer: Tokenizer | None Optional tokenizer\
      \ to use; if None, get_tokenizer() will be used.\n    embedding_vectorstore_key:\
      \ str The key for the embedding vector store; defaults to EntityVectorStoreKey.ID.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
    name: _build_text_unit_context
    signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Rank matching text units and add them to the context window until\
      \ it hits the max_context_tokens limit.\n\nArgs:\n    selected_entities (list[Entity]):\
      \ Entities for which to collect and rank associated text units.\n    max_context_tokens\
      \ (int): Maximum number of tokens to include in the context.\n    return_candidate_context\
      \ (bool): If True, also compute and include candidate text units context data.\n\
      \    column_delimiter (str): Delimiter used to separate fields in the context\
      \ rows.\n    context_name (str): Name of the context section to populate (default\
      \ \"Sources\").\n\nReturns:\n    tuple[str, dict[str, pd.DataFrame]]: The textual\
      \ context and a mapping from context names to DataFrames containing the context\
      \ data."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
      \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n \
      \       conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
      \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
      \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
      \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
      \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
      \ str = \"number of relationships\",\n        include_relationship_weight: bool\
      \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n      \
      \  return_candidate_context: bool = False,\n        use_community_summary: bool\
      \ = False,\n        min_community_rank: int = 0,\n        community_context_name:\
      \ str = \"Reports\",\n        column_delimiter: str = \"|\",\n        **kwargs:\
      \ dict[str, Any],\n    ) -> ContextBuilderResult"
    docstring: "Build data context for local search prompt.\n\nBuild a context by\
      \ combining community reports and entity/relationship/covariate tables, and\
      \ text units using a predefined ratio set by text_unit_prop and community_prop.\n\
      \nArgs:\n  query (str): The user query to build context for.\n  conversation_history\
      \ (ConversationHistory | None): Optional conversation history to consider while\
      \ constructing the context.\n  include_entity_names (list[str] | None): Entity\
      \ names to explicitly include in the mapping.\n  exclude_entity_names (list[str]\
      \ | None): Entity names to exclude from consideration.\n  conversation_history_max_turns\
      \ (int | None): Maximum number of user turns from conversation history to include.\n\
      \  conversation_history_user_turns_only (bool): If True, only user turns from\
      \ the conversation history are used.\n  max_context_tokens (int): Maximum token\
      \ budget for the constructed context.\n  text_unit_prop (float): Proportion\
      \ of the context allocated to text units.\n  community_prop (float): Proportion\
      \ of the context allocated to community context.\n  top_k_mapped_entities (int):\
      \ Number of top entities to map from the query.\n  top_k_relationships (int):\
      \ Number of top relationships to include for local context.\n  include_community_rank\
      \ (bool): Whether to include community ranking information in the community\
      \ context.\n  include_entity_rank (bool): Whether to include entity ranking\
      \ information in the local context.\n  rank_description (str): Description used\
      \ when presenting rankings.\n  include_relationship_weight (bool): Whether to\
      \ include relationship weights in the local context.\n  relationship_ranking_attribute\
      \ (str): Attribute name used for ranking relationships.\n  return_candidate_context\
      \ (bool): If True, return candidate context alongside the main context data.\n\
      \  use_community_summary (bool): If True, use a summarized representation of\
      \ community context when available.\n  min_community_rank (int): Minimum rank\
      \ threshold for including a community.\n  community_context_name (str): Label/name\
      \ for the community context section.\n  column_delimiter (str): Delimiter used\
      \ to separate fields in generated context.\n  **kwargs (dict[str, Any]): Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \  ContextBuilderResult: The result containing the built context and associated\
      \ data.\n\nRaises:\n  ValueError: If the sum of community_prop and text_unit_prop\
      \ exceeds 1."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
    name: _build_community_context
    signature: "def _build_community_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
      \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
      \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
      \ dict[str, pd.DataFrame]]"
    docstring: "Add community data to the context window until it hits the max_context_tokens\
      \ limit.\n\nArgs:\n  selected_entities (list[Entity]): Entities selected for\
      \ which related communities should be added to the context.\n  max_context_tokens\
      \ (int): Maximum number of tokens to include for the community context.\n  use_community_summary\
      \ (bool): Whether to utilize a summarized representation of communities.\n \
      \ column_delimiter (str): Delimiter used between columns in the generated context.\n\
      \  include_community_rank (bool): Whether to include the community rank in the\
      \ context.\n  min_community_rank (int): Minimum community rank to include in\
      \ the context.\n  return_candidate_context (bool): If True, also compute and\
      \ return candidate context data.\n  context_name (str): Name of the context\
      \ section (default \"Reports\").\n\nReturns:\n  tuple[str, dict[str, pd.DataFrame]]:\
      \ A tuple consisting of\n    - context_text: String containing the assembled\
      \ community context; may be empty.\n    - context_data: Mapping from a lowercase\
      \ context name to a DataFrame with additional context information.\n\nRaises:\n\
      \  Exceptions raised by underlying calls (e.g., build_community_context, get_candidate_communities)\n\
      \  may propagate to the caller."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
    name: _build_local_context
    signature: "def _build_local_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
      \ bool = False,\n        rank_description: str = \"relationship count\",\n \
      \       include_relationship_weight: bool = False,\n        top_k_relationships:\
      \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n     \
      \   return_candidate_context: bool = False,\n        column_delimiter: str =\
      \ \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Build data context for local search prompt by combining entity/relationship/covariate\
      \ tables.\n\nArgs:\n  selected_entities (list[Entity]): Entities to include\
      \ in the context.\n  max_context_tokens (int): Maximum allowed tokens for the\
      \ generated context text. Parsing stops when adding a new entity would exceed\
      \ this limit.\n  include_entity_rank (bool): Whether to include the entity's\
      \ rank in the context rows.\n  rank_description (str): Description of the ranking\
      \ used in the entity context (default: 'relationship count').\n  include_relationship_weight\
      \ (bool): Whether to include the relationship weight in the generated context.\n\
      \  top_k_relationships (int): Number of top relationships to include for each\
      \ entity.\n  relationship_ranking_attribute (str): Attribute name used to rank\
      \ relationships (default: 'rank').\n  return_candidate_context (bool): If True,\
      \ return all candidate entities/relationships/covariates (not only those fitted\
      \ into the context window) and tag them accordingly.\n  column_delimiter (str):\
      \ Delimiter used to separate fields in the generated context.\n\nReturns:\n\
      \  tuple[str, dict[str, pd.DataFrame]]: The final context text and a mapping\
      \ from context section names to their corresponding DataFrames."
  classes:
  - class_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
    name: LocalSearchMixedContext
    docstring: "Local search context builder that mixes community data with local\
      \ entity/relationship/covariate context for structured searches.\n\nThis class\
      \ aggregates data from community reports, entity/relationship/covariate tables,\
      \ and text units to create a comprehensive context for local search prompts.\
      \ It relies on a vector store of entity text embeddings and a text embedding\
      \ model to rank and assemble relevant content.\n\nArgs:\n  entities: The list\
      \ of entities to include.\n  entity_text_embeddings: The vector store containing\
      \ embeddings for entity text.\n  text_embedder: The embedding model used to\
      \ embed text for similarity search.\n  text_units: Optional list of TextUnit.\n\
      \  community_reports: Optional list of CommunityReport.\n  relationships: Optional\
      \ list of Relationship.\n  covariates: Optional dict[str, list[Covariate]];\
      \ covariate data grouped by key.\n  tokenizer: Optional Tokenizer.\n  embedding_vectorstore_key:\
      \ Key to select the embedding store (default EntityVectorStoreKey.ID).\n\nReturns:\n\
      \  An initialized LocalSearchMixedContext instance.\n\nRaises:\n  Not specified\
      \ in the provided information."
    methods:
    - name: filter_by_entity_keys
      signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
    - name: __init__
      signature: "def __init__(\n        self,\n        entities: list[Entity],\n\
        \        entity_text_embeddings: BaseVectorStore,\n        text_embedder:\
        \ EmbeddingModel,\n        text_units: list[TextUnit] | None = None,\n   \
        \     community_reports: list[CommunityReport] | None = None,\n        relationships:\
        \ list[Relationship] | None = None,\n        covariates: dict[str, list[Covariate]]\
        \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
        \ str = EntityVectorStoreKey.ID,\n    )"
    - name: _build_text_unit_context
      signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
        \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
        \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
        \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    - name: build_context
      signature: "def build_context(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n        include_entity_names:\
        \ list[str] | None = None,\n        exclude_entity_names: list[str] | None\
        \ = None,\n        conversation_history_max_turns: int | None = 5,\n     \
        \   conversation_history_user_turns_only: bool = True,\n        max_context_tokens:\
        \ int = 8000,\n        text_unit_prop: float = 0.5,\n        community_prop:\
        \ float = 0.25,\n        top_k_mapped_entities: int = 10,\n        top_k_relationships:\
        \ int = 10,\n        include_community_rank: bool = False,\n        include_entity_rank:\
        \ bool = False,\n        rank_description: str = \"number of relationships\"\
        ,\n        include_relationship_weight: bool = False,\n        relationship_ranking_attribute:\
        \ str = \"rank\",\n        return_candidate_context: bool = False,\n     \
        \   use_community_summary: bool = False,\n        min_community_rank: int\
        \ = 0,\n        community_context_name: str = \"Reports\",\n        column_delimiter:\
        \ str = \"|\",\n        **kwargs: dict[str, Any],\n    ) -> ContextBuilderResult"
    - name: _build_community_context
      signature: "def _build_community_context(\n        self,\n        selected_entities:\
        \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
        \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
        \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
        \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
        \ dict[str, pd.DataFrame]]"
    - name: _build_local_context
      signature: "def _build_local_context(\n        self,\n        selected_entities:\
        \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
        \ bool = False,\n        rank_description: str = \"relationship count\",\n\
        \        include_relationship_weight: bool = False,\n        top_k_relationships:\
        \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n   \
        \     return_candidate_context: bool = False,\n        column_delimiter: str\
        \ = \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
- file: graphrag/query/structured_search/local_search/search.py
  functions:
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Builds a local search context that fits a single context window and\
      \ generates an answer for the user query.\n\nArgs:\n    query: The user query\
      \ to process.\n    conversation_history: Optional conversation history to incorporate\
      \ into the search context.\n    **kwargs: Additional keyword arguments passed\
      \ to the context builder and the model. May include drift_query to override\
      \ the query for drift.\n\nReturns:\n    SearchResult: The constructed search\
      \ result containing the response text, context data and text, completion time,\
      \ and token usage metadata.\n\nRaises:\n    None. All exceptions are caught\
      \ within the method and result in an empty response rather than propagating\
      \ errors."
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    docstring: "Initialize a LocalSearch instance for local search orchestration.\n\
      \nArgs:\n    model: ChatModel - The language model interface used for this local\
      \ search.\n    context_builder: LocalContextBuilder - The builder that constructs\
      \ the context for the local search.\n    tokenizer: Tokenizer | None - Optional\
      \ tokenizer to use.\n    system_prompt: str | None - System prompt for the local\
      \ search. If None, uses LOCAL_SEARCH_SYSTEM_PROMPT.\n    response_type: str\
      \ - The type of response formatting, e.g., \"multiple paragraphs\".\n    callbacks:\
      \ list[QueryCallbacks] | None - Optional list of query callbacks.\n    model_params:\
      \ dict[str, Any] | None - Additional parameters for the model.\n    context_builder_params:\
      \ dict | None - Additional parameters for the context builder.\n\nReturns:\n\
      \    None - The instance is initialized and nothing is returned."
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator"
    docstring: "Build local search context that fits a single context window and generate\
      \ answer for the user query.\n\nArgs:\n    query (str): The user query to process.\n\
      \    conversation_history (ConversationHistory | None): Optional conversation\
      \ history to incorporate into the search context.\n\nReturns:\n    AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding strings representing chunks of the\
      \ generated answer.\n\nRaises:\n    Exception: If an error occurs during streaming\
      \ or within the model or callbacks."
  classes:
  - class_id: graphrag/query/structured_search/local_search/search.py::LocalSearch
    name: LocalSearch
    docstring: "LocalSearch orchestrates local search operations by building a compact\
      \ context and querying a language model to generate an answer for a user query.\
      \ It coordinates the context builder, optional tokenizer, system prompt, and\
      \ callbacks to produce a structured search result or a streaming output.\n\n\
      Purpose:\n- Build a local search context that fits a single context window and\
      \ generates an answer for the user query.\n\nArgs:\n    model: The language\
      \ model interface used for this local search.\n    context_builder: The builder\
      \ that constructs the context for the local search.\n    tokenizer: Optional\
      \ tokenizer to use.\n    system_prompt: System prompt for the local search.\
      \ If None, uses the default system prompt.\n    response_type: The format of\
      \ the response produced by the search.\n    callbacks: The query callbacks to\
      \ be invoked during search.\n    model_params: Parameters for the underlying\
      \ language model.\n    context_builder_params: Parameters for the context builder.\n\
      \nReturns:\n- LocalSearch: An initialized LocalSearch instance.\n\nRaises:\n\
      - Exceptions from the underlying components (model, tokenizer, context builder)\
      \ may propagate to the caller."
    methods:
    - name: search
      signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
        \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    - name: __init__
      signature: "def __init__(\n        self,\n        model: ChatModel,\n      \
        \  context_builder: LocalContextBuilder,\n        tokenizer: Tokenizer | None\
        \ = None,\n        system_prompt: str | None = None,\n        response_type:\
        \ str = \"multiple paragraphs\",\n        callbacks: list[QueryCallbacks]\
        \ | None = None,\n        model_params: dict[str, Any] | None = None,\n  \
        \      context_builder_params: dict | None = None,\n    )"
    - name: stream_search
      signature: "def stream_search(\n        self,\n        query: str,\n       \
        \ conversation_history: ConversationHistory | None = None,\n    ) -> AsyncGenerator"
- file: graphrag/storage/blob_pipeline_storage.py
  functions:
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._abfs_url
    name: _abfs_url
    signature: 'def _abfs_url(self, key: str) -> str'
    docstring: "Get the ABFS URL for the given key.\n\nArgs:\n    key: The key identifying\
      \ the blob within the container and path prefix.\n\nReturns:\n    str: The ABFS\
      \ URL for the given key, formatted as abfs://{path}."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nArgs:\n    self: The instance of\
      \ the BlobPipelineStorage.\n\nReturns:\n    list[str]: The keys currently stored\
      \ in the storage.\n\nRaises:\n    NotImplementedError: Blob storage does yet\
      \ not support listing keys."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_json
    name: _set_df_json
    signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
    docstring: "Store a dataframe as JSON at the specified key in storage.\n\nThe\
      \ dataframe is serialized to JSON in records format with one JSON object per\
      \ line and written to the path derived from the provided key. Depending on configuration,\
      \ the write uses either a storage account with DefaultAzureCredential or a connection\
      \ string.\n\nArgs:\n    key: The key under which to store the JSON export of\
      \ the dataframe.\n    dataframe: The dataframe to serialize to JSON.\n\nReturns:\n\
      \    None\n\nRaises:\n    Exception: If an error occurs during serialization\
      \ or storage write."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_parquet
    name: _set_df_parquet
    signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
    docstring: "\"\"\"Set a parquet dataframe.\n\nStores the provided dataframe as\
      \ a Parquet file at the path derived from the key. If a storage account name\
      \ is configured and no connection string is provided, the Parquet file is written\
      \ using Azure storage options with the storage account name and DefaultAzureCredential;\
      \ otherwise, the Parquet file is written using the provided connection string.\n\
      \nArgs:\n    key: The key under which to store the parquet export of the dataframe.\n\
      \    dataframe: The dataframe to serialize and store.\n\nReturns:\n    None\n\
      \nRaises:\n    Exceptions raised by the dataframe serialization or the storage\
      \ write operations.\n\"\"\""
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find blobs in a container using a file pattern, as well as a custom\
      \ filter function.\n\nArgs:\n    base_dir: The name of the base container.\n\
      \    file_pattern: The file pattern to use.\n    file_filter: A dictionary of\
      \ key-value pairs to filter the blobs.\n    max_count: The maximum number of\
      \ blobs to return. If -1, all blobs are returned.\n\nReturns:\n    An iterator\
      \ of blob names and their corresponding regex matches."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the blob storage cache. This\
      \ implementation is a no-op and does not remove any blobs from the container.\
      \ To perform an actual clear, implement iteration over the container's blobs\
      \ and delete each one.\n\nReturns:\n    None: The coroutine completes without\
      \ returning a value."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get a value from the cache.\n\nArgs:\n    key: The key to retrieve\
      \ from the cache.\n    as_bytes: If True, return the raw bytes stored for the\
      \ key; if False, decode the data to a string using encoding.\n    encoding:\
      \ Encoding to use when decoding the value if as_bytes is False.\n\nReturns:\n\
      \    Any: The value associated with the key. If as_bytes is False, the value\
      \ is decoded to a string using the provided encoding or the object's default\
      \ encoding. If an error occurs, returns None.\n\nRaises:\n    None: This method\
      \ does not raise any exceptions; it logs a warning and returns None on error."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create the blob container if it does not exist.\n\nArgs:\n    self:\
      \ The instance of the class containing the BlobServiceClient and container configuration.\n\
      \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
      \ Blob Storage client operations may be raised."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Delete a key from the cache.\n\nArgs:\n    key (str): The key to delete.\n\
      \nReturns:\n    None"
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._container_exists
    name: _container_exists
    signature: def _container_exists(self) -> bool
    docstring: "Check if the configured blob container exists.\n\nArgs:\n    self:\
      \ The instance of the class that holds _container_name and _blob_service_client.\n\
      \nReturns:\n    bool: True if the container exists, otherwise False.\n\nRaises:\n\
      \    Exception: If the underlying blob service client call to list_containers\
      \ raises an exception."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set a value in the cache.\n\nArgs:\n  key: str \u2014 The key under\
      \ which to store the value.\n  value: Any \u2014 The value to store in the cache.\n\
      \  encoding: str | None \u2014 Optional encoding to use when encoding non-bytes\
      \ to bytes. If None, uses the default encoding.\n\nReturns:\n  None \u2014 The\
      \ method does not return a value.\n\nRaises:\n  Exception \u2014 Exceptions\
      \ raised by the underlying cache operation are caught and logged; they are not\
      \ propagated to the caller."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._keyname
    name: _keyname
    signature: 'def _keyname(self, key: str) -> str'
    docstring: "Get the key name.\n\nArgs:\n    key: The key to be joined with the\
      \ path prefix to form the full key path.\n\nReturns:\n    The full key name\
      \ as a string."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a new BlobPipelineStorage instance.\n\nThis constructor\
      \ selects the initialization flow as follows:\n- If a connection_string is provided,\
      \ create the BlobServiceClient from the connection string.\n- Otherwise, if\
      \ storage_account_blob_url is provided, create the BlobServiceClient using the\
      \ account URL and DefaultAzureCredential.\n- If neither is provided, raise a\
      \ ValueError.\n\ncontainer_name is required. Providing container_name as None\
      \ raises ValueError. If the container_name key is missing from kwargs, a KeyError\
      \ is raised by Python.\n\nbase_dir is optional and sets the path prefix within\
      \ the container (defaults to an empty string). encoding defaults to 'utf-8'.\n\
      \nArgs:\n  container_name: The container name to use for blob storage. This\
      \ key is required. If the key is missing from kwargs, a KeyError is raised.\
      \ If the value is None, a ValueError is raised.\n  connection_string: The Azure\
      \ Blob Storage connection string. If provided, used to initialize the client.\n\
      \  storage_account_blob_url: The URL of the storage account blob endpoint. Used\
      \ with DefaultAzureCredential when connection_string is not provided.\n  base_dir:\
      \ The base directory (path prefix) within the container. Optional. Defaults\
      \ to ''.\n  encoding: Encoding to use. Defaults to 'utf-8'.\n\nReturns:\n  None\n\
      \nRaises:\n  KeyError: If container_name is not provided in kwargs.\n  ValueError:\
      \ If container_name is None.\n  ValueError: If neither connection_string nor\
      \ storage_account_blob_url is provided.\n  ValueError: If storage_account_blob_url\
      \ is None when connection_string is not provided."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously check if a key exists in the cache.\n\nArgs:\n    key\
      \ (str): The cache key to check.\n\nReturns:\n    bool: True if a value exists\
      \ for the key in the cache, otherwise False.\n\nRaises:\n    Exception: If an\
      \ error occurs while performing the underlying blob storage operations."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict. If a key referenced\
      \ by file_filter is missing from item, a KeyError may be raised. Values should\
      \ be strings (or objects compatible with re.search).\n\nReturns:\n    bool:\
      \ True if no file_filter is defined. Otherwise, True if all key/value patterns\
      \ in file_filter match the corresponding fields in item using re.search; False\
      \ otherwise.\n\nRaises:\n    KeyError: If a key referenced by file_filter is\
      \ missing from the item."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "\"\"\"Delete the container if it exists.\n\nThis protected method\
      \ deletes the container using the internal container name (self._container_name).\
      \ Deletion is conditional on existence, as determined by _container_exists().\
      \ The deletion is performed via the BlobServiceClient stored on self._blob_service_client\
      \ and may raise Azure SDK exceptions if the operation fails.\n\nArgs:\n    None:\
      \ This method does not accept explicit parameters. It relies on internal state\
      \ (self._container_name, self._blob_service_client).\n\nReturns:\n    None:\
      \ This method does not return a value.\n\nRaises:\n    AzureError: Azure SDK\
      \ exceptions may be raised during the delete operation.\n\"\"\""
  - node_id: graphrag/storage/blob_pipeline_storage.py::validate_blob_container_name
    name: validate_blob_container_name
    signature: 'def validate_blob_container_name(container_name: str)'
    docstring: "Validate a blob container name against Azure rules.\n\nThis function\
      \ verifies the following constraints:\n- The name length is between 3 and 63\
      \ characters.\n- The name starts with a letter or a number.\n- All characters\
      \ are lowercase letters, numbers, or hyphen.\n- The name does not contain consecutive\
      \ hyphens.\n- The name does not end with a hyphen.\n\nArgs:\n    container_name\
      \ (str): The blob container name to be validated.\n\nReturns:\n    bool: True\
      \ if the container name is valid.\n    ValueError: If the input is invalid,\
      \ a ValueError instance describing the reason is returned (note: the function\
      \ does not raise exceptions; invalid input signals are returned)."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. If None, the current instance is returned.\n\
      \nReturns:\n    PipelineStorage: The current instance when name is None; otherwise\
      \ a new BlobPipelineStorage configured with base_dir set to the path formed\
      \ by joining the current path prefix and name."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._blobname
    name: _blobname
    signature: 'def _blobname(blob_name: str) -> str'
    docstring: "Normalize a blob name by removing the internal path prefix and any\
      \ leading slash.\n\nArgs:\n    blob_name: The original blob name as a string.\n\
      \nReturns:\n    The blob name with the path prefix (self._path_prefix) removed\
      \ if present, and any leading slash stripped.\n\nRaises:\n    None"
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "\"\"\"Get the creation date for the given key from Azure Blob storage\
      \ and format it with the local time zone.\n\nArgs:\n    key (str): The key for\
      \ which to retrieve the creation date from blob storage.\n\nReturns:\n    str:\
      \ The creation date as a string formatted with the local time zone. Returns\
      \ an empty string if an error occurs.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; it returns an empty string if an error occurs.\n\"\"\
      \""
  classes:
  - class_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
    name: BlobPipelineStorage
    docstring: "BlobPipelineStorage is an Azure Blob Storage backed implementation\
      \ of the PipelineStorage interface for caching pipeline data.\n\nSummary:\n\
      This class provides a blob-based storage backend for caching results and data\
      \ used by the GraphRag pipeline. It stores dataframe exports as JSON or Parquet,\
      \ supports retrieving values, finding blobs by pattern, and basic cache management.\
      \ Initialization selects between using a connection string or a storage account\
      \ URL with DefaultAzureCredential to create the BlobServiceClient, and requires\
      \ a container name.\n\nAttributes:\n  _blob_service_client (BlobServiceClient):\
      \ Client used to interact with Azure Blob Storage.\n  _container_name (str):\
      \ Name of the container in which blobs are stored.\n  _path_prefix (str): Path\
      \ prefix used to scope blob names within the container.\n\nArgs:\n  connection_string:\
      \ Optional Azure Blob Storage connection string. If provided, the BlobServiceClient\
      \ is created from this string.\n  storage_account_blob_url: Optional URL to\
      \ the storage account. If provided (and connection_string is not), the BlobServiceClient\
      \ is created using DefaultAzureCredential.\n  container_name: Name of the blob\
      \ container to use. The container will be created if it does not exist.\n\n\
      Raises:\n  ValueError: If neither connection_string nor storage_account_blob_url\
      \ is provided."
    methods:
    - name: _abfs_url
      signature: 'def _abfs_url(self, key: str) -> str'
    - name: keys
      signature: def keys(self) -> list[str]
    - name: _set_df_json
      signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
    - name: _set_df_parquet
      signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
    - name: find
      signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
        \        base_dir: str | None = None,\n        file_filter: dict[str, Any]\
        \ | None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
        \ Any]]]"
    - name: clear
      signature: def clear(self) -> None
    - name: get
      signature: "def get(\n        self, key: str, as_bytes: bool | None = False,\
        \ encoding: str | None = None\n    ) -> Any"
    - name: _create_container
      signature: def _create_container(self) -> None
    - name: delete
      signature: 'def delete(self, key: str) -> None'
    - name: _container_exists
      signature: def _container_exists(self) -> bool
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: _keyname
      signature: 'def _keyname(self, key: str) -> str'
    - name: __init__
      signature: 'def __init__(self, **kwargs: Any) -> None'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: item_filter
      signature: 'def item_filter(item: dict[str, Any]) -> bool'
    - name: _delete_container
      signature: def _delete_container(self) -> None
    - name: child
      signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    - name: _blobname
      signature: 'def _blobname(blob_name: str) -> str'
    - name: get_creation_date
      signature: 'def get_creation_date(self, key: str) -> str'
- file: graphrag/storage/cosmosdb_pipeline_storage.py
  functions:
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_database
    name: _delete_database
    signature: def _delete_database(self) -> None
    docstring: "Delete the database if it exists.\n\nDeletes the database associated\
      \ with this storage object if it exists. If the database is deleted, the internal\
      \ container reference is cleared to reflect that there is no active container\
      \ available until a new database/container is created.\n\nReturns:\n    None:\
      \ This method does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError:\
      \ If the database to delete does not exist.\n\nSide effects:\n    self._container_client\
      \ is set to None.\n    self._database_client is updated with the value returned\
      \ by delete_database."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a CosmosDB storage instance.\n\nArgs:\n  cosmosdb_account_url:\
      \ The URL of the Cosmos DB account. Used to initialize CosmosClient when a connection\
      \ string is not provided.\n  connection_string: The Cosmos DB connection string.\
      \ Used to initialize CosmosClient when provided.\n  base_dir: The database name\
      \ to create/use.\n  container_name: The container name to create/use.\n  encoding:\
      \ Encoding to use for data (default \"utf-8\").\nReturns:\n  None\nRaises:\n\
      \  ValueError: If no base_dir is provided for database name or if neither connection_string\
      \ nor cosmosdb_account_url is provided."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear all contents from storage.\n\nThis currently deletes the database,\
      \ including all containers and data within it.\nTODO: We should decide what\
      \ granularity of deletion is the ideal behavior (e.g. delete all items within\
      \ a container, delete the current container, delete the current database)\n\n\
      Returns:\n    None: The function does not return a value.\n\nRaises:\n    Exception:\
      \ If deletion fails."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"Keys listing is not supported for CosmosDB storage.\n\nArgs:\n\
      \    self: The instance of the storage.\n\nReturns:\n    None: This method does\
      \ not return any keys; it raises NotImplementedError.\n\nRaises:\n    NotImplementedError:\
      \ CosmosDB storage does not support listing keys.\n\"\"\""
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "Delete the container with the current container name if it exists.\n\
      \nArgs:\n    self: Any. The storage instance.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError: If the\
      \ container to delete does not exist."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> PipelineStorage'
    docstring: "Return the current storage instance (no new child is created).\n\n\
      This method accepts an optional name parameter for API compatibility but does\
      \ not create a new child. It returns the current instance (self).\n\nArgs:\n\
      \    name: str | None, optional name for the child storage. This parameter is\
      \ accepted for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self)."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create a Cosmos DB container for the current container name if it\
      \ doesn't exist.\n\nThis method creates or retrieves the container using the\
      \ current container name as the id and a partition key on the path \"/id\" (Hash).\
      \ It assigns the resulting container proxy to self._container_client. The operation\
      \ only proceeds if a database client exists.\n\nArgs:\n    self: The instance\
      \ of the class containing the Cosmos client references.\n\nReturns:\n    None.\
      \ Updates the internal _container_client attribute with the container proxy.\n\
      \nRaises:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Insert the contents of a file into a Cosmos DB container for the given\
      \ filename key.\n\nFor better optimization, the file is destructured such that\
      \ each row is a unique Cosmos DB item.\n\nArgs:\n  key (str): The filename key\
      \ under which to insert the item in the Cosmos DB container.\n  value (Any):\
      \ The content to insert. If value is bytes, it is treated as a parquet file\
      \ and each row becomes a separate item (with an id derived from the key prefix\
      \ and the row index or the existing row id). If the input row lacks an id, a\
      \ unique id is constructed as \"<prefix>:<index>\" and the prefix is tracked\
      \ for downstream handling. If value is not bytes, it is treated as a cache output\
      \ or stats.json and stored as a single item with id=key and body parsed from\
      \ JSON.\n  encoding (str | None): Optional encoding to use. This parameter is\
      \ unused by this method and is ignored.\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
      \ If the database or container are not initialized. This exception is raised\
      \ internally but is caught and logged within the method and is not propagated\
      \ to callers."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_database
    name: _create_database
    signature: def _create_database(self) -> None
    docstring: "Create the database if it doesn't exist.\n\nReturns:\n    None: This\
      \ method does not return a value.\n\nRaises:\n    CosmosHttpResponseError: If\
      \ the Cosmos DB service returns an HTTP error."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::_create_progress_status
    name: _create_progress_status
    signature: "def _create_progress_status(\n    num_loaded: int, num_filtered: int,\
      \ num_total: int\n) -> Progress"
    docstring: "Create a Progress object representing the current load progress.\n\
      \nArgs:\n    num_loaded: int. The number of files that have been loaded.\n \
      \   num_filtered: int. The number of files that were filtered out.\n    num_total:\
      \ int. The total number of items.\n\nReturns:\n    Progress: Progress object\
      \ with total_items=num_total, completed_items=num_loaded + num_filtered, and\
      \ description set to \"<num_loaded> files loaded (<num_filtered> filtered)\"\
      ."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Delete all cosmosdb items belonging to the given filename key.\n\n\
      This coroutine does nothing if the database or container client is not initialized.\
      \ For keys containing \".parquet\", it deletes all items whose id starts with\
      \ the prefix of the key; otherwise it deletes the single item with id equal\
      \ to the key and the corresponding partition key.\n\nIf a CosmosResourceNotFoundError\
      \ is raised, it is ignored. Any other exception is logged.\n\nArgs:\n    key:\
      \ The filename key identifying the items to delete.\n\nReturns:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Asynchronously fetch items from the Cosmos DB container that match\
      \ the given key.\n\nThis coroutine supports two modes:\n\n- as_bytes is truthy:\
      \ returns matching items as Parquet-encoded data. The data are collected by\
      \ querying items whose id starts with the derived prefix, converted to a DataFrame,\
      \ and then serialized to Parquet using DataFrame.to_parquet(). The return value\
      \ is Parquet data only if in-memory writing is supported by the underlying to_parquet\
      \ implementation; otherwise the exact return type may vary depending on the\
      \ runtime.\n- as_bytes is None or False: returns the body of a single item as\
      \ a JSON string.\n\nArgs:\n    key (str): The key used to query the container.\
      \ If as_bytes is True, this key is treated as a prefix for filtering by id.\n\
      \    as_bytes (bool | None): If True, return matching items as Parquet-encoded\
      \ data. If None or False, return the body of a single item as a JSON string.\n\
      \    encoding (str | None): Encoding to use for the returned data if applicable.\
      \ This parameter is currently unused.\n\nReturns:\n    Any: If as_bytes is True\
      \ and in-memory Parquet writing is supported, returns a Parquet-encoded bytes-like\
      \ object representing the matching items. If as_bytes is None or False, returns\
      \ a JSON string of the item body for the given key, or None if the database\
      \ or container client is not initialized or if an error occurs.\n\nRaises:\n\
      \    None: This coroutine handles exceptions internally, logs a warning, and\
      \ returns None on error."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict.\n\nReturns:\n    bool:\
      \ True if no file_filter is defined or if all key/value pairs in file_filter\
      \ match the corresponding fields in item using re.search.\n\nRaises:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._get_prefix
    name: _get_prefix
    signature: 'def _get_prefix(self, key: str) -> str'
    docstring: "\"Get the prefix of the filename key.\"\n\nArgs:\n  key: The filename\
      \ key as a string. The prefix is the substring before the first dot.\n\nReturns:\n\
      \  str: The prefix portion of the key (substring before the first dot)."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously determine whether the contents for the given filename\
      \ key exist in Cosmos DB storage.\n\nThis coroutine checks existence by querying\
      \ the Cosmos DB container. For parquet files, existence is determined by a prefix\
      \ match on the item id using STARTSWITH, since parquet data is stored with a\
      \ prefixed id. For non-parquet keys, existence is determined by an exact id\
      \ match (c.id == key).\n\nInitialization/readiness: If the storage has not been\
      \ initialized (database or container client not available), the method returns\
      \ False without performing I/O.\n\nAsync context: This is an async function\
      \ and should be awaited by callers. In this implementation it does not contain\
      \ explicit await expressions; the actual I/O occurs through the Cosmos client.\n\
      \nArgs:\n    key (str): The filename key to check for existence in Cosmos DB\
      \ storage.\n\nReturns:\n    bool: True if the item(s) exist in Cosmos DB storage,\
      \ otherwise False. Parquet keys return True if any item exists with an id starting\
      \ with the generated prefix; non-parquet keys return True only when exactly\
      \ one matching item exists.\n\nRaises:\n    Exception: Propagates exceptions\
      \ raised by the underlying Cosmos DB SDK during query/communication with the\
      \ service."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find documents in a Cosmos DB container using a file pattern regex\
      \ and optional file filter.\n\nArgs:\n    file_pattern (re.Pattern[str]): The\
      \ file pattern to use.\n    base_dir (str | None): The name of the base directory\
      \ (not used in Cosmos DB context).\n    file_filter (dict[str, Any] | None):\
      \ A dictionary of key-value pairs to filter the documents.\n    max_count (int):\
      \ The maximum number of documents to return. If -1, all documents are returned.\n\
      \nReturns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator of document\
      \ IDs and their corresponding regex matches."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date for the given key from Cosmos DB storage and\
      \ format it with the local time zone.\n\nArgs:\n    key (str): The key for which\
      \ to retrieve the creation date.\n\nReturns:\n    str: The creation date as\
      \ a string formatted with the local time zone. Returns an empty string if the\
      \ key is not found or an error occurs.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; it returns an empty string on error."
  classes:
  - class_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
    name: CosmosDBPipelineStorage
    docstring: 'CosmosDBPipelineStorage is a Cosmos DB-backed storage backend that
      stores and retrieves data in a Cosmos DB container. It implements the storage
      interface used by Graphrag to manage databases, containers, and items, including
      creation/deletion of databases and containers, insertion of file contents, retrieval,
      and query-based operations. It maintains an internal reference to the active
      container via _container_client, and this reference is cleared when the database
      is deleted or when no container exists, reflecting the lifecycle of the underlying
      Cosmos DB resources.


      Summary:

      - Provides a persistent storage layer backed by Azure Cosmos DB for file-based
      data.

      - Manages the lifecycle of a database and container and exposes item-level operations
      consistent with the PipelineStorage API.

      - Keeps an internal _container_client to interact with the current container;
      this is cleared when the database/container is removed.


      Args:

      - cosmosdb_account_url: The URL of the Cosmos DB account. Used to initialize
      CosmosClient when a connection string is not provided.

      - connection_string: The Cosmos DB connection string. Used to initialize CosmosClient
      when provided.

      - base_dir: The database name to create/use.

      - container_name: The container name to create/use.

      - encoding: Encoding to use for serialization/deserialization (e.g., utf-8).


      Returns:

      - None: This constructor does not return a value.


      Raises:

      - CosmosHttpResponseError: If the Cosmos DB service returns an HTTP error during
      initialization or resource creation.'
    methods:
    - name: _delete_database
      signature: def _delete_database(self) -> None
    - name: __init__
      signature: 'def __init__(self, **kwargs: Any) -> None'
    - name: clear
      signature: def clear(self) -> None
    - name: keys
      signature: def keys(self) -> list[str]
    - name: _delete_container
      signature: def _delete_container(self) -> None
    - name: child
      signature: 'def child(self, name: str | None) -> PipelineStorage'
    - name: _create_container
      signature: def _create_container(self) -> None
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: _create_database
      signature: def _create_database(self) -> None
    - name: delete
      signature: 'def delete(self, key: str) -> None'
    - name: get
      signature: "def get(\n        self, key: str, as_bytes: bool | None = None,\
        \ encoding: str | None = None\n    ) -> Any"
    - name: item_filter
      signature: 'def item_filter(item: dict[str, Any]) -> bool'
    - name: _get_prefix
      signature: 'def _get_prefix(self, key: str) -> str'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: find
      signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
        \        base_dir: str | None = None,\n        file_filter: dict[str, Any]\
        \ | None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
        \ Any]]]"
    - name: get_creation_date
      signature: 'def get_creation_date(self, key: str) -> str'
- file: graphrag/storage/factory.py
  functions:
  - node_id: graphrag/storage/factory.py::StorageFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, storage_type: str) -> bool'
    docstring: "Check if the given storage type is supported.\n\nArgs:\n    storage_type\
      \ (str): The type identifier for the storage.\n\nReturns:\n    bool: True if\
      \ the storage type is registered in the registry, False otherwise."
  - node_id: graphrag/storage/factory.py::StorageFactory.create_storage
    name: create_storage
    signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
    docstring: "Create a storage object from the provided type.\n\nArgs:\n    storage_type:\
      \ The type of storage to create.\n    kwargs: Additional keyword arguments for\
      \ the storage constructor.\n\nReturns:\n    A PipelineStorage instance.\n\n\
      Raises:\n    ValueError: If the storage type is not registered."
  - node_id: graphrag/storage/factory.py::StorageFactory.register
    name: register
    signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
      \ PipelineStorage]\n    ) -> None"
    docstring: "Register a custom storage implementation.\n\nArgs:\n    storage_type\
      \ (str): The type identifier for the storage.\n    creator (Callable[..., PipelineStorage]):\
      \ A class or callable that creates an instance of PipelineStorage.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/storage/factory.py::StorageFactory.get_storage_types
    name: get_storage_types
    signature: def get_storage_types(cls) -> list[str]
    docstring: "Get the registered storage implementations.\n\nArgs:\n    cls: The\
      \ class on which this classmethod is invoked.\n\nReturns:\n    list[str]: The\
      \ list of registered storage type keys (i.e., the keys of cls._registry)."
  classes:
  - class_id: graphrag/storage/factory.py::StorageFactory
    name: StorageFactory
    docstring: "StorageFactory: Registry-based factory for pipeline storage backends.\n\
      \nPurpose:\nProvides a centralized registry that maps storage type identifiers\
      \ to creator callables for concrete PipelineStorage implementations (BlobPipelineStorage,\
      \ CosmosDBPipelineStorage, FilePipelineStorage, MemoryPipelineStorage). It enables\
      \ checking supported types, creating storage instances, registering new types,\
      \ and listing available storage types.\n\nAttributes:\n    _registry: ClassVar[dict[str,\
      \ Callable[..., PipelineStorage]]]\n        Registry mapping storage type keys\
      \ to their creator callables.\n\nMethods:\n    is_supported_type(cls, storage_type:\
      \ str) -> bool:\n        Check if the given storage_type is registered.\n\n\
      \    create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage:\n\
      \        Create a storage object from the provided type.\n\n    register(cls,\
      \ storage_type: str, creator: Callable[..., PipelineStorage]) -> None:\n   \
      \     Register a custom storage implementation.\n\n    get_storage_types(cls)\
      \ -> list[str]:\n        Get the registered storage implementations.\n\nReturns:\n\
      \    For is_supported_type: bool indicating support.\n    For create_storage:\
      \ a PipelineStorage instance.\n    For register: None (side effect: registry\
      \ updated).\n    For get_storage_types: list of registered storage type keys.\n\
      \nRaises:\n    ValueError: If the storage type is not registered when attempting\
      \ to create storage."
    methods:
    - name: is_supported_type
      signature: 'def is_supported_type(cls, storage_type: str) -> bool'
    - name: create_storage
      signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
    - name: register
      signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
        \ PipelineStorage]\n    ) -> None"
    - name: get_storage_types
      signature: def get_storage_types(cls) -> list[str]
- file: graphrag/storage/file_pipeline_storage.py
  functions:
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "\"\"\"Clear all entries under the root directory.\n\nRemoves directories\
      \ recursively and unlinks files directly under the root directory, effectively\
      \ clearing the storage contents.\n\nArgs:\n    self: The FilePipelineStorage\
      \ instance to operate on.\n\nReturns:\n    None\n\nRaises:\n    OSError: If\
      \ a filesystem operation fails during removal of files or directories.\n\"\"\
      \""
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nThe keys represent file names located\
      \ directly in the root directory (self._root_dir). Only files are included;\
      \ directories are ignored. This operation is non-recursive and may return an\
      \ empty list if no files are present. The keys are provided as file names (not\
      \ full paths).\n\nArgs:\n  self (FilePipelineStorage): The instance of the FilePipelineStorage.\n\
      \nReturns:\n  list[str]: The file names (not full paths) of files directly under\
      \ root_dir; directories are excluded. The list may be empty if no files exist."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Return a storage instance for a child or the current instance if no\
      \ name is provided.\n\nArgs:\n    name: str | None, optional name for the child\
      \ storage. If None, the current instance is returned.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self) when name is None; otherwise a new FilePipelineStorage\
      \ representing the child storage located at the path formed by joining the current\
      \ root_dir with the provided name."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict. If a key referenced\
      \ by file_filter is missing from item, a KeyError may be raised. Values should\
      \ be strings (or objects compatible with re.search).\n\nReturns:\n    bool:\
      \ True if no file_filter is defined; otherwise, True only if all re.search(value,\
      \ item[key]) checks succeed for every key, value pair in file_filter.\n\nRaises:\n\
      \    KeyError: If item lacks a key referenced by file_filter.\n    TypeError:\
      \ If item[key] is not a string (and thus not compatible with re.search).\n \
      \   re.error: If a regex pattern from file_filter is invalid.\n\nNotes:\n  \
      \  This function relies on file_filter from outer scope. To improve robustness\
      \ and testability, consider passing file_filter as an explicit parameter to\
      \ the function."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find files in the storage that match a compiled file pattern and optional\
      \ metadata-based filtering.\n\nArgs:\n    file_pattern (re.Pattern[str]): A\
      \ compiled regular expression to match file paths.\n    base_dir (str | None):\
      \ Base directory to search within. If None, search starts from the storage root.\n\
      \    file_filter (dict[str, Any] | None): Optional dictionary mapping named\
      \ group keys to regular expressions; only items whose corresponding fields match\
      \ are yielded.\n    max_count (int): The maximum number of results to yield.\
      \ If -1, yield all matches.\n\nReturns:\n    Iterator[tuple[str, dict[str, Any]]]:\
      \ An iterator yielding tuples of (filename, group) where filename is the path\
      \ relative to the storage root (without a leading path separator) and group\
      \ is the dictionary of named groups extracted by file_pattern.\n\nRaises:\n\
      \    KeyError: If file_filter references a key not present in the named groups\
      \ produced by file_pattern."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a file-based storage backend.\n\nThis constructor prepares\
      \ the storage root by creating the base_dir if it does not exist and sets the\
      \ encoding used for file I/O.\n\nArgs:\n  base_dir (str): Directory path where\
      \ files are stored. Defaults to the empty string (uses the current working directory).\n\
      \  encoding (str): Text encoding for file operations. Defaults to \"utf-8\"\
      .\n\nReturns:\n  None\n\nRaises:\n  OSError: If the root directory cannot be\
      \ created or accessed due to filesystem errors.\n  Other exceptions may be raised\
      \ for permission issues or unexpected filesystem failures."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage._read_file
    name: _read_file
    signature: "def _read_file(\n        self,\n        path: str | Path,\n      \
      \  as_bytes: bool | None = False,\n        encoding: str | None = None,\n  \
      \  ) -> Any"
    docstring: "Read the contents of a file asynchronously.\n\nArgs:\n    path: The\
      \ path to the file to read. (str | Path)\n    as_bytes: When True, read in binary\
      \ mode and return bytes; otherwise read in text mode. (bool | None)\n    encoding:\
      \ Encoding to use when reading as text. If None and as_bytes is False, uses\
      \ self._encoding. (str | None)\n\nReturns:\n    The contents of the file. Returns\
      \ bytes if as_bytes is True, otherwise a string.\n\nRaises:\n    Exceptions\
      \ raised by the underlying file I/O (e.g., open or read errors) may be propagated."
  - node_id: graphrag/storage/file_pipeline_storage.py::join_path
    name: join_path
    signature: 'def join_path(file_path: str, file_name: str) -> Path'
    docstring: "Join a base path with the relative components of a file name to form\
      \ a new pathlib.Path.\n\nThis function uses the parent directory and the file\
      \ name from file_name and joins them with file_path to produce the final path.\
      \ Note that if file_name is an absolute path, pathlib will discard the leading\
      \ path components of file_path and the result will correspond to the absolute\
      \ path described by file_name's components.\n\nArgs:\n    file_path (str): Base\
      \ path to join with the file's relative components (parent and name).\n    file_name\
      \ (str): Path-like string; its parent directory and file name are used for the\
      \ join.\n\nReturns:\n    pathlib.Path: The resulting Path object."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get the contents of a file identified by key from the storage root\
      \ or directly from the provided path if present.\n\nArgs:\n    key: str - The\
      \ file key or path relative to the root storage.\n    as_bytes: bool | None\
      \ - If True, read the file as bytes; otherwise read as text.\n    encoding:\
      \ str | None - Encoding to use when reading text. Ignored when reading bytes.\n\
      \nReturns:\n    Any - The file contents, read as bytes when as_bytes is True,\
      \ otherwise as text, or None if no file is found.\n\nRaises:\n    Propagates\
      \ exceptions raised by has() or _read_file() during I/O operations."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set a value for the given key in the file-based storage.\n\nArgs:\n\
      \    key: str\n        The key, used as the relative path under the storage\
      \ root.\n    value: Any\n        The value to store. If value is bytes, write\
      \ in binary mode; otherwise write as text.\n    encoding: str | None\n     \
      \   Encoding to use when writing text. If None, uses the default encoding.\n\
      \nReturns:\n    None\n        This coroutine completes after the value has been\
      \ written."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Check whether a file for the given key exists in the storage.\n\n\
      This coroutine checks the existence of the file located at the path formed by\
      \ joining the storage root directory with the provided key.\n\nArgs:\n    key\
      \ (str): The key (relative path) to check within the storage root directory.\n\
      \nReturns:\n    bool: True if the file exists, False otherwise."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Asynchronously delete the item associated with the given key from\
      \ storage.\n\nIf the key exists, delete the corresponding file; if the key does\
      \ not exist, this operation is a no-op. This method is asynchronous.\n\nArgs:\n\
      \    key (str): The key of the item to delete.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    OSError: If a filesystem operation\
      \ fails during removal of the file."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date of a file.\n\nArgs:\n    key (str): The key\
      \ of the file for which to retrieve the creation date.\n\nReturns:\n    str:\
      \ The creation date as a string formatted with the local time zone.\n\nRaises:\n\
      \    FileNotFoundError: If the file does not exist at the constructed path.\n\
      \    OSError: If an OS error occurs while accessing file metadata."
  classes:
  - class_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
    name: FilePipelineStorage
    docstring: 'File-based storage backend for a pipeline that stores items as individual
      files under a root directory. This class implements the PipelineStorage interface
      and provides filesystem-backed operations to read, write, delete, list keys,
      clear storage, and find files by pattern.


      Attributes:

      - _root_dir: str - Root directory for storage; created if missing.

      - _encoding: str - Text encoding used for file I/O.


      Args:

      - base_dir: Directory path where files are stored. Defaults to the empty string
      (uses the current working directory).

      - encoding: Text encoding for file operations. Defaults to "utf-8".


      Returns:

      - None


      Raises:

      - OSError: If a filesystem operation fails during initialization or storage
      operations.

      - FileNotFoundError: If a file is not found when querying creation date or reading
      a specific item.'
    methods:
    - name: clear
      signature: def clear(self) -> None
    - name: keys
      signature: def keys(self) -> list[str]
    - name: child
      signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    - name: item_filter
      signature: 'def item_filter(item: dict[str, Any]) -> bool'
    - name: find
      signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
        \        base_dir: str | None = None,\n        file_filter: dict[str, Any]\
        \ | None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
        \ Any]]]"
    - name: __init__
      signature: 'def __init__(self, **kwargs: Any) -> None'
    - name: _read_file
      signature: "def _read_file(\n        self,\n        path: str | Path,\n    \
        \    as_bytes: bool | None = False,\n        encoding: str | None = None,\n\
        \    ) -> Any"
    - name: get
      signature: "def get(\n        self, key: str, as_bytes: bool | None = False,\
        \ encoding: str | None = None\n    ) -> Any"
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: delete
      signature: 'def delete(self, key: str) -> None'
    - name: get_creation_date
      signature: 'def get_creation_date(self, key: str) -> str'
- file: graphrag/storage/memory_pipeline_storage.py
  functions:
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nArgs:\n    self (MemoryPipelineStorage):\
      \ The instance of the MemoryPipelineStorage.\n\nReturns:\n    list[str]: The\
      \ keys currently stored in the storage."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Asynchronously delete the given key from the storage.\n\nArgs:\n \
      \   key (str): The key to delete.\n\nReturns:\n    None\n\nRaises:\n    KeyError:\
      \ If the key does not exist in the storage."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "\"\"\"Return True if the given key exists in the storage.\n\nArgs:\n\
      \    key (str): The key to check for.\n\nReturns:\n    bool: True if the key\
      \ exists in the storage, False otherwise.\n\"\"\""
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Asynchronously set the value for the given key in memory storage.\n\
      \nThis method updates the in-memory storage dictionary by assigning the provided\
      \ value to the specified key. The encoding parameter is accepted for compatibility\
      \ but is not used in this implementation.\n\nArgs:\n    key (str): The key to\
      \ set the value for.\n    value (Any): The value to set.\n    encoding (str\
      \ | None): Optional encoding to apply when serializing the value (unused).\n\
      \nReturns:\n    None: The coroutine completes when the value has been set.\n\
      \nRaises:\n    None: This coroutine does not raise any exceptions."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize in-memory storage backend.\n\nThis constructor initializes\
      \ the internal storage by creating an empty\ndictionary bound to self._storage\
      \ and by calling the base class initializer.\n\nArgs:\n    self (MemoryPipelineStorage):\
      \ The instance being initialized. No additional\n        parameters.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "\"\"\"Get the value for the given key from in-memory storage.\n\n\
      Args:\n    key (str): The key to retrieve the value for.\n    as_bytes (bool\
      \ | None): Unused. This parameter is accepted for API compatibility but is ignored\
      \ by this backend.\n    encoding (str | None): Unused. This parameter is accepted\
      \ for API compatibility but is ignored by this backend.\n\nReturns:\n    Any:\
      \ The value associated with the key if present; None if the key is not in storage.\"\
      \"\""
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. This parameter is accepted for API compatibility\
      \ but is ignored.\n\nReturns:\n    PipelineStorage: A new MemoryPipelineStorage\
      \ instance representing the child storage."
  classes:
  - class_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage
    name: MemoryPipelineStorage
    docstring: "Memory-based storage backend for pipeline data.\n\nThis class implements\
      \ an in-memory storage backend that stores key-value pairs in a\nPython dictionary\
      \ in memory and conforms to the PipelineStorage interface. It provides\nfast,\
      \ non-persistent storage for pipeline data and supports creating child storages\
      \ for\nisolated namespaces.\n\nAttributes:\n- _storage: dict[str, Any] storing\
      \ the in-memory mapping of keys to values.\n\nArgs:\n  self (MemoryPipelineStorage):\
      \ The instance being initialized. No additional parameters.\n\nReturns:\n  None"
    methods:
    - name: clear
      signature: def clear(self) -> None
    - name: keys
      signature: def keys(self) -> list[str]
    - name: delete
      signature: 'def delete(self, key: str) -> None'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: __init__
      signature: def __init__(self)
    - name: get
      signature: "def get(\n        self, key: str, as_bytes: bool | None = None,\
        \ encoding: str | None = None\n    ) -> Any"
    - name: child
      signature: 'def child(self, name: str | None) -> "PipelineStorage"'
- file: graphrag/storage/pipeline_storage.py
  functions:
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Return True if the given key exists in the storage.\n\nArgs:\n   \
      \ key: The key to check for.\n\nReturns:\n    output - True if the key exists\
      \ in the storage, False otherwise."
  - node_id: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
    name: get_timestamp_formatted_with_local_tz
    signature: 'def get_timestamp_formatted_with_local_tz(timestamp: datetime) ->
      str'
    docstring: "\"\"\"Get the formatted timestamp with the local time zone.\n\nArgs:\n\
      \    timestamp (datetime): The timestamp to format in the local time zone.\n\
      \nReturns:\n    str: The timestamp represented as 'YYYY-MM-DD HH:MM:SS \xB1\
      HHMM' in the local time zone.\n\nRaises:\n    None: This function does not raise\
      \ any exceptions.\n\"\"\""
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find files in the storage that match a compiled file_pattern, with\
      \ optional base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern\
      \ (re.Pattern[str]): A compiled regular expression to match file paths.\n  \
      \  base_dir (str | None): The base directory to search within. If None, search\
      \ starts from the storage root.\n    file_filter (dict[str, Any] | None): Optional\
      \ dictionary of metadata field names to values to filter on. Implementations\
      \ may perform exact-value matching against the file's metadata; keys correspond\
      \ to metadata attributes present in the returned dictionaries. If None, no additional\
      \ filtering is applied.\n    max_count (int): Maximum number of results to yield.\
      \ -1 means no limit. When a non-negative value is provided, at most that many\
      \ results are yielded.\n\nReturns:\n    Iterator[tuple[str, dict[str, Any]]]:\
      \ An iterator yielding (path, metadata) pairs where path is the matched file\
      \ path as a string and metadata is a dictionary of attributes describing the\
      \ file. The exact metadata keys are implementation-dependent and may vary between\
      \ storage backends.\n\nNotes:\n    This is an abstract method. Concrete subclasses\
      \ must provide an implementation.\n\nRaises:\n    Exceptions raised depend on\
      \ the concrete subclass implementation (e.g., I/O or filesystem errors)."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date for the given key.\n\nArgs:\n    key (str):\
      \ The key for which to retrieve the creation date.\n\nReturns:\n    str: The\
      \ creation date as a string."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage.\n\nThis coroutine\
      \ clears all data stored in the storage backend and does not return a value.\
      \ It must be awaited.\n\nArgs:\n    self: The storage instance.\n\nReturns:\n\
      \    None: The coroutine completes without returning a value.\n\nRaises:\n \
      \   Exception: If the storage backend fails to clear the storage."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set the value for the given key.\n\nThis is an asynchronous operation\
      \ that stores the provided value under the given key. If the key already exists,\
      \ its value will be overwritten.\n\nArgs:\n    key (str): The key to set the\
      \ value for.\n    value (Any): The value to set.\n    encoding (str | None):\
      \ Optional encoding to apply when serializing the value.\n\nReturns:\n    None:\
      \ This coroutine completes when the value has been stored.\n\nRaises:\n    StorageError:\
      \ If a storage-related error occurs during storage."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"List all keys in the storage.\n\nArgs:\n    self (PipelineStorage):\
      \ The instance of the PipelineStorage.\n\nReturns:\n    list[str]: The keys\
      \ currently stored in the storage.\n\nRaises:\n    NotImplementedError: If key\
      \ listing is not supported by the storage backend.\n\"\"\""
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get the value for the given key.\n\n        Args:\n            key\
      \ (str): The key to retrieve the value for.\n            as_bytes (bool | None):\
      \ If True, return the value as bytes. If None, use the backend's default representation.\n\
      \            encoding (str | None): The text encoding to use when decoding bytes\
      \ to str. If None, the backend's default encoding is used.\n\n        Returns:\n\
      \            Any: The value for the given key. The concrete return type depends\
      \ on as_bytes and encoding:\n                - If as_bytes is True: bytes\n\
      \                - If as_bytes is False or None and encoding is not None: str\
      \ decoded using the provided encoding\n                - If as_bytes is None\
      \ and encoding is None: backend-specific type or None\n\n        Raises:\n \
      \           KeyError: If the key does not exist in storage.\n            ValueError:\
      \ If the provided encoding is invalid or not supported for the stored value.\n\
      \            RuntimeError: If a backend-specific error occurs."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create or return a child storage instance.\n\nThis method creates\
      \ and returns a dedicated child storage instance. The optional name parameter\
      \ is accepted for API compatibility but may be ignored by the implementing class.\
      \ The behavior with the name (whether it selects a specific child or is ignored)\
      \ is determined by the concrete implementation.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. This parameter is accepted for API compatibility\
      \ but may be ignored by the implementation.\n\nReturns:\n    PipelineStorage:\
      \ The child storage instance corresponding to the provided name."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "\"\"\"Delete the given key from the storage.\n\nArgs:\n    key (str):\
      \ The key to delete.\n\nReturns:\n    None\n\"\"\""
  classes:
  - class_id: graphrag/storage/pipeline_storage.py::PipelineStorage
    name: PipelineStorage
    docstring: "Abstract base class for storage backends used by the pipeline.\n\n\
      Defines the interface for a key-value style storage with support for existence\
      \ checks,\npattern-based file discovery, retrieval with optional byte/encoding\
      \ handling, listing\nkeys, creation date retrieval, and deletion. Implementations\
      \ may back the storage with\nin-memory structures, filesystem, database, or\
      \ remote services.\n\nAttributes:\n  Not defined at this level. Concrete implementations\
      \ may define internal state and\n  configuration."
    methods:
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: find
      signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
        \        base_dir: str | None = None,\n        file_filter: dict[str, Any]\
        \ | None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
        \ Any]]]"
    - name: get_creation_date
      signature: 'def get_creation_date(self, key: str) -> str'
    - name: clear
      signature: def clear(self) -> None
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: keys
      signature: def keys(self) -> list[str]
    - name: get
      signature: "def get(\n        self, key: str, as_bytes: bool | None = None,\
        \ encoding: str | None = None\n    ) -> Any"
    - name: child
      signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    - name: delete
      signature: 'def delete(self, key: str) -> None'
- file: graphrag/tokenizer/get_tokenizer.py
  functions:
  - node_id: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
    name: get_tokenizer
    signature: "def get_tokenizer(\n    model_config: LanguageModelConfig | None =\
      \ None,\n    encoding_model: str = ENCODING_MODEL,\n) -> Tokenizer"
    docstring: "Get the tokenizer for the given model configuration or fallback to\
      \ a tiktoken based tokenizer.\n\nArgs:\n    model_config: LanguageModelConfig\
      \ | None, optional\n        The model configuration. If not provided or model_config.encoding_model\
      \ is manually set,\n        use a tiktoken based tokenizer. Otherwise, use a\
      \ LitellmTokenizer based on the model name.\n        LiteLLM supports token\
      \ encoding/decoding for the range of models it supports.\n    encoding_model:\
      \ str, optional\n        A tiktoken encoding model to use if no model configuration\
      \ is provided. Only used if a\n        model configuration is not provided.\n\
      \nReturns:\n    Tokenizer: An instance of a Tokenizer.\n\nRaises:\n    None"
  classes: []
- file: graphrag/tokenizer/litellm_tokenizer.py
  functions:
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
      \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from\
      \ the list of tokens.\n\nRaises:\n    Exception: If decoding fails due to an\
      \ underlying error in the decoding process."
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.__init__
    name: __init__
    signature: 'def __init__(self, model_name: str) -> None'
    docstring: "\"\"\"Initialize the LiteLLM Tokenizer.\n\nArgs:\n    model_name (str):\
      \ The name of the LiteLLM model to use for tokenization.\n\nReturns:\n    None:\
      \ This initializer does not return a value.\n\"\"\""
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "Encode the given text into a list of tokens using the configured Litellm\
      \ model.\n\nArgs:\n    self (LitellmTokenizer): The instance of LitellmTokenizer.\
      \ The encoding model is determined by the model_name attribute.\n    text (str):\
      \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
      \ the encoded text.\n\nRaises:\n    Exception: If encoding fails due to underlying\
      \ encoder errors or model issues."
  classes:
  - class_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer
    name: LitellmTokenizer
    docstring: "LitellmTokenizer provides text-to-token and token-to-text conversions\
      \ using a Litellm model.\n\nIt wraps the Litellm encode and decode functions\
      \ to operate on a single model identified by model_name.\n\nArgs:\n    model_name\
      \ (str): The name of the Litellm model to use for tokenization.\n\nReturns:\n\
      \    None: This initializer does not return a value.\n\nRaises:\n    Exception:\
      \ If initialization fails due to an underlying error.\n\nAttributes:\n    model_name\
      \ (str): The name of the Litellm model used for tokenization."
    methods:
    - name: decode
      signature: 'def decode(self, tokens: list[int]) -> str'
    - name: __init__
      signature: 'def __init__(self, model_name: str) -> None'
    - name: encode
      signature: 'def encode(self, text: str) -> list[int]'
- file: graphrag/tokenizer/tiktoken_tokenizer.py
  functions:
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "\"\"\"Decode a list of tokens back into a string.\n\nArgs:\n    tokens\
      \ (list[int]): A list of tokens to decode.\n\nReturns:\n    str: The decoded\
      \ string from the list of tokens.\n\nRaises:\n    Exception: If decoding fails\
      \ due to an underlying error in the encoding.\n\"\"\""
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.__init__
    name: __init__
    signature: 'def __init__(self, encoding_name: str) -> None'
    docstring: "Initialize the Tiktoken Tokenizer.\n\nArgs:\n    encoding_name (str):\
      \ The name of the Tiktoken encoding to use for tokenization.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "Encode the given text into a list of tokens.\n\nArgs:\n    text (str):\
      \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
      \ the encoded text."
  classes:
  - class_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer
    name: TiktokenTokenizer
    docstring: "TiktokenTokenizer is a Tokenizer implementation that uses the tiktoken\
      \ library to encode and decode text using a specified encoding.\n\nParameters:\n\
      \  encoding_name (str): The name of the Tiktoken encoding to use for tokenization.\n\
      \nAttributes:\n  encoding_name (str): The name of the Tiktoken encoding used\
      \ for tokenization.\n  encoding: The tiktoken encoding object created from encoding_name.\n\
      \nNotes:\n  - Encoding and decoding are performed via the underlying tiktoken\
      \ encoding.\n  - encode(text) returns a list of token IDs for the provided text.\n\
      \  - decode(tokens) returns the string corresponding to the given list of token\
      \ IDs."
    methods:
    - name: decode
      signature: 'def decode(self, tokens: list[int]) -> str'
    - name: __init__
      signature: 'def __init__(self, encoding_name: str) -> None'
    - name: encode
      signature: 'def encode(self, text: str) -> list[int]'
- file: graphrag/tokenizer/tokenizer.py
  functions:
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "\"\"\"Encode the given text into a list of tokens.\n\nArgs:\n    text\
      \ (str): The input text to encode.\n\nReturns:\n    list[int]: A list of tokens\
      \ representing the encoded text.\n\nRaises:\n    NotImplementedError: The encode\
      \ method must be implemented by subclasses.\n\"\"\""
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.num_tokens
    name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
    docstring: "Return the number of tokens in the given text.\n\nParameters\n   \
      \ text (str): The input text to analyze.\n\nReturns\n    int: The number of\
      \ tokens in the input text.\n\nRaises\n    NotImplementedError: If the encode\
      \ method is not implemented by a subclass."
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
      \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from\
      \ the list of tokens.\n\nRaises:\n    NotImplementedError: If the decode method\
      \ has not been implemented by subclasses."
  classes:
  - class_id: graphrag/tokenizer/tokenizer.py::Tokenizer
    name: Tokenizer
    docstring: 'Abstract interface for tokenization operations.


      This abstract base class defines the contract for encoding text into a sequence
      of token identifiers, determining how many tokens a given text would yield,
      and decoding a list of tokens back into text. Subclasses must implement encode,
      num_tokens, and decode.


      Key attributes:

      None defined at this level. Concrete implementations may expose internal state
      such as a vocabulary or encoding rules.


      Returns:

      This class does not produce values by itself. The concrete methods return the
      types described by their signatures when implemented in subclasses (list[int]
      for encode, int for num_tokens, and str for decode).


      Raises:

      NotImplementedError: The encode, num_tokens, and decode methods must be implemented
      by subclasses.'
    methods:
    - name: encode
      signature: 'def encode(self, text: str) -> list[int]'
    - name: num_tokens
      signature: 'def num_tokens(self, text: str) -> int'
    - name: decode
      signature: 'def decode(self, tokens: list[int]) -> str'
- file: graphrag/utils/api.py
  functions:
  - node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a text-based similarity search.\n\nThis method computes an\
      \ embedding for the input text using the provided text_embedder. If the resulting\
      \ embedding is truthy (i.e., not None or an empty result), it delegates to similarity_search_by_vector\
      \ with that embedding and the specified k. If the embedding is falsy, it returns\
      \ an empty list, indicating no results.\n\nArgs:\n    text (str): The input\
      \ text to search for similar documents.\n    text_embedder (TextEmbedder): A\
      \ callable that returns a list of floats representing the embedding of the input\
      \ text.\n    k (int): The number of top results to return. Defaults to 10.\n\
      \    **kwargs: Additional keyword arguments passed to downstream search methods.\n\
      \nReturns:\n    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
      \ objects, sorted by score in descending order and truncated to k results.\n\
      \nRaises:\n    Propagates exceptions raised by the text_embedder or by the underlying\
      \ similarity_search_by_vector call.\n\nNotes:\n    - If text is None or empty,\
      \ text_embedder(text) may raise or return a falsy value, in which case this\
      \ method returns [].\n    - The internal flow is: compute the embedding via\
      \ text_embedder, then perform a vector search only if the embedding is truthy;\
      \ otherwise, return an empty list."
  - node_id: graphrag/utils/api.py::load_search_prompt
    name: load_search_prompt
    signature: 'def load_search_prompt(root_dir: str, prompt_config: str | None) ->
      str | None'
    docstring: "Load the search prompt from disk if configured.\n\nIf not, leave it\
      \ empty - the search functions will load their defaults.\n\nArgs:\n    root_dir:\
      \ Root directory path as a string where the prompt_config is resolved.\n   \
      \ prompt_config: Optional path to the prompt file, relative to root_dir. If\
      \ provided, the function will attempt to load the file if it exists.\n\nReturns:\n\
      \    The contents of the prompt file decoded as UTF-8 if found, otherwise None.\n\
      \nRaises:\n    OSError: If a filesystem error occurs while reading the prompt\
      \ file.\n    UnicodeDecodeError: If the prompt file contents cannot be decoded\
      \ as UTF-8."
  - node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search across all configured embedding\
      \ stores and merge results.\n\nArgs:\n  query_embedding: list[float] - Embedding\
      \ vector to search with\n  k: int - Number of top results to return\n  kwargs:\
      \ Any - Additional keyword arguments for compatibility; not used directly by\
      \ this method\n\nReturns:\n  list[VectorStoreSearchResult] - Top-k results merged\
      \ from all stores, sorted by score in descending order\n\nRaises:\n  Exception\
      \ - Exceptions raised by the underlying embedding stores may propagate to the\
      \ caller."
  - node_id: graphrag/utils/api.py::MultiVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id across the configured vector stores.\n\
      \nArgs:\n    id: The composite identifier used to locate the document. It should\
      \ be formatted as \"<internal_id>-<index_name>\", where\n        <internal_id>\
      \ is the id within the specific vector store and <index_name> is the name of\
      \ that store.\n\nReturns:\n    VectorStoreDocument: The document corresponding\
      \ to the provided id from the matching vector store.\n\nRaises:\n    ValueError:\
      \ If the index name extracted from the id is not found among the configured\
      \ index stores."
  - node_id: graphrag/utils/api.py::update_context_data
    name: update_context_data
    signature: "def update_context_data(\n    context_data: Any,\n    links: dict[str,\
      \ Any],\n) -> Any"
    docstring: "Update context data with index_name and index_id fields derived from\
      \ the links mapping.\n\nArgs:\n    context_data (dict[str, pandas.DataFrame]):\
      \ The context data to update. Each value should be a DataFrame containing records\
      \ with an 'id' field and other relevant columns. The dict's keys typically include\
      \ 'reports', 'entities', 'relationships', 'claims', and 'sources'.\n    links\
      \ (dict[str, Any]): A dictionary of links to the original dataframes. Expected\
      \ to contain the following mappings:\n        - 'community_reports': dict-like\
      \ mapping int(id) -> {'index_name', 'id'}\n        - 'entities': dict-like mapping\
      \ int(id) -> {'index_name', 'id'}\n        - 'relationships': dict-like mapping\
      \ int(id) -> {'index_name', 'id'}\n        - 'covariates': dict-like mapping\
      \ int(id) -> {'index_name', 'id'}\n        - 'text_units': dict-like mapping\
      \ int(id) -> {'index_name', 'id'}\n\nReturns:\n    dict[str, list[dict]]: The\
      \ updated context data. For each key in the input, the function returns a list\
      \ of dictionaries representing the records augmented with:\n        - index_name\
      \ and index_id based on the provided links\n        - Additional key-specific\
      \ transformations (e.g., 'entity' trimmed to the portion before the first dash\
      \ for 'entities', 'source' and 'target' trimmed for 'relationships', etc.)\n\
      \nRaises:\n    KeyError: If required keys are missing in the links dictionary\
      \ or if expected fields are missing in a context_data entry (e.g., missing 'id').\n\
      \    TypeError: If context_data is not a dict[str, pandas.DataFrame] or if a\
      \ value does not support to_dict(orient='records').\n    ValueError: If an entry['id']\
      \ cannot be cast to int."
  - node_id: graphrag/utils/api.py::MultiVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
      \        index_names: list[str],\n    )"
    docstring: "Initialize a MultiVectorStore with embedding stores and index names.\n\
      \nArgs:\n    embedding_stores: list[BaseVectorStore]\n        The vector stores\
      \ to combine in this multi-store.\n    index_names: list[str]\n        The corresponding\
      \ index names for each embedding store.\n\nReturns:\n    None"
  - node_id: graphrag/utils/api.py::reformat_context_data
    name: reformat_context_data
    signature: 'def reformat_context_data(context_data: dict) -> dict'
    docstring: "Reformats context_data for all query responses.\n\nReformats a dictionary\
      \ of dataframes into a dictionary of lists. One list entry for each\nrecord.\
      \ Records are grouped by original dictionary keys.\n\nNote: depending on which\
      \ query algorithm is used, the context_data may not contain the same information\
      \ (keys).\nIn this case, the default behavior will be to set these keys as empty\
      \ lists to preserve a standard output format.\n\nArgs:\n    context_data: dict\n\
      \        A mapping from key to either a pandas DataFrame-like object with to_dict(orient='records')\n\
      \        or to a dict, or to None. DataFrames are converted to a list of dictionaries\
      \ representing\n        records. If a value is already a dict, it is used as-is.\
      \ If a key yields no records, the\n        key will be left with its default\
      \ empty list.\n\nReturns:\n    dict\n        A dictionary containing the reformatted\
      \ data. It starts with the keys\n        \"reports\", \"entities\", \"relationships\"\
      , \"claims\", and \"sources\" initialized to empty\n        lists; for input\
      \ keys with data, the corresponding value is replaced with the list of\n   \
      \     records (or the dict if the input value was a dict).\n\nRaises:\n    None"
  - node_id: graphrag/utils/api.py::MultiVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector store.\n\nArgs:\n    documents: List\
      \ of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
      \ Whether to overwrite existing data in the vector store if True; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented"
  - node_id: graphrag/utils/api.py::create_storage_from_config
    name: create_storage_from_config
    signature: 'def create_storage_from_config(output: StorageConfig) -> PipelineStorage'
    docstring: "Create a storage object from the config.\n\nArgs:\n    output: StorageConfig\
      \ The storage configuration to create a storage object from.\n\nReturns:\n \
      \   PipelineStorage: The created storage object.\n\nRaises:\n    ValueError:\
      \ If the storage type is not registered."
  - node_id: graphrag/utils/api.py::MultiVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
      \ list[str] | list[int]\n    The IDs to include in the filter.\n\nReturns:\n\
      \  Any\n    The constructed query filter to filter documents by the provided\
      \ IDs.\n\nRaises:\n  NotImplementedError\n    If the method is not implemented."
  - node_id: graphrag/utils/api.py::MultiVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
      \ arguments for connecting to the vector storage.\n\nReturns:\n    Any: The\
      \ result of the connection operation.\n\nRaises:\n    NotImplementedError: If\
      \ the method is not implemented."
  - node_id: graphrag/utils/api.py::create_cache_from_config
    name: create_cache_from_config
    signature: 'def create_cache_from_config(cache: CacheConfig, root_dir: str) ->
      PipelineCache'
    docstring: "Create a cache object from the given CacheConfig by delegating to\
      \ CacheFactory.\n\nArgs:\n  cache: CacheConfig The cache configuration to create\
      \ the cache object from. The configuration is dumped to a dict via model_dump();\
      \ the dictionary must include a \"type\" key used to determine the concrete\
      \ cache implementation.\n  root_dir: str The root directory to merge into the\
      \ cache creation kwargs.\n\nReturns:\n  PipelineCache: The created cache object.\n\
      \nRaises:\n  Exception: May raise any exception raised by the underlying CacheFactory\
      \ during cache creation.\n\nNotes:\n  This function uses cache.model_dump()\
      \ to obtain the configuration, reads the \"type\" field for the cache_type,\
      \ and merges root_dir into the kwargs before invoking CacheFactory."
  - node_id: graphrag/utils/api.py::truncate
    name: truncate
    signature: 'def truncate(text: str, max_length: int) -> str'
    docstring: "Truncate a string to a maximum length.\n\nIf the input text length\
      \ is greater than max_length, return the first max_length characters followed\
      \ by \"...[truncated]\". Otherwise, return the input text unchanged.\n\nArgs:\n\
      \    text: str\n        The text to truncate.\n    max_length: int\n       \
      \ The maximum allowed length of the text.\n\nReturns:\n    str\n        The\
      \ possibly truncated string. If truncation occurred, the returned string ends\
      \ with \"...[truncated]\".\n\nRaises:\n    TypeError: If text is not a string\
      \ or max_length is not an integer."
  - node_id: graphrag/utils/api.py::get_embedding_store
    name: get_embedding_store
    signature: "def get_embedding_store(\n    config_args: dict[str, dict],\n    embedding_name:\
      \ str,\n) -> BaseVectorStore"
    docstring: "\"\"\"Get the embedding description store.\n\nGiven a mapping of index\
      \ configurations in config_args and a target embedding name embedding_name,\
      \ construct and connect the appropriate vector store(s). If there is only a\
      \ single configured index, this returns that single vector store; otherwise\
      \ it returns a MultiVectorStore that aggregates multiple vector stores across\
      \ indexes.\n\nArgs:\n    config_args: dict[str, dict]\n        Configuration\
      \ for one or more embedding indexes. Each key is an index identifier and each\
      \ value is a dictionary of store configuration options used to instantiate a\
      \ vector store.\n    embedding_name: str\n        Name of the embedding to configure\
      \ and fetch the store for.\n\nReturns:\n    BaseVectorStore\n        A vector\
      \ store capable of handling the requested embedding. If multiple indexes are\
      \ configured, a MultiVectorStore is returned.\n\nRaises:\n    Exception\n  \
      \      If underlying vector store creation or connection fails.\n\"\"\""
  classes:
  - class_id: graphrag/utils/api.py::MultiVectorStore
    name: MultiVectorStore
    docstring: Unified interface that combines multiple vector stores into a single
      multi-store for cross-store search and retrieval.
    methods:
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    - name: __init__
      signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
        \        index_names: list[str],\n    )"
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> Any'
- file: graphrag/utils/cli.py
  functions:
  - node_id: graphrag/utils/cli.py::dir_exist
    name: dir_exist
    signature: def dir_exist(path)
    docstring: "\"\"\"Check for directory existence.\n\nArgs:\n    path (str): Path\
      \ to the directory.\n\nReturns:\n    str: The input path if the directory exists.\n\
      \nRaises:\n    argparse.ArgumentTypeError: If the directory does not exist.\n\
      \"\"\""
  - node_id: graphrag/utils/cli.py::file_exist
    name: file_exist
    signature: def file_exist(path)
    docstring: "Check that the given path points to an existing file.\n\nArgs:\n \
      \   path (str): Path to the file to validate. May be a string or Path object.\n\
      \nReturns:\n    str: The input path if the file exists.\n\nRaises:\n    argparse.ArgumentTypeError:\
      \ If the file does not exist.\n\nNotes:\n    This check uses Path.is_file()\
      \ to verify that the path refers to a regular file."
  - node_id: graphrag/utils/cli.py::redact
    name: redact
    signature: 'def redact(config: dict) -> str'
    docstring: "Sanitize secrets in a configuration object by redacting sensitive\
      \ fields.\n\nThis function traverses the input configuration and redacts values\
      \ for keys identified as sensitive. The redaction keys are currently hard-coded\
      \ as api_key, connection_string, container_name, and organization. If a sensitive\
      \ key's value is None, that key is omitted from the resulting JSON instead of\
      \ being redacted. The function recursively processes nested dictionaries and\
      \ lists. The set of sensitive keys is hard-coded but designed to be extendable.\n\
      \nReturns:\n    str: A JSON-formatted string with sensitive values replaced\
      \ by \"==== REDACTED ====\" and with None-valued sensitive keys omitted.\n\n\
      Raises:\n    TypeError: If the resulting object cannot be serialized to JSON\
      \ (for example, when non-serializable values are present in the input).\n\n\
      Notes:\n    json.dumps is used for serialization; non-serializable input values\
      \ will trigger a TypeError. To customize redaction behavior, modify the sensitive-keys\
      \ set or implement a separate configuration mechanism."
  - node_id: graphrag/utils/cli.py::redact_dict
    name: redact_dict
    signature: 'def redact_dict(config: dict) -> dict'
    docstring: "Redact sensitive values in a dictionary.\n\nArgs:\n    config (dict):\
      \ The configuration dictionary to redact.\n\nReturns:\n    dict: A new dictionary\
      \ with sensitive keys redacted. Keys in {\"api_key\", \"connection_string\"\
      , \"container_name\", \"organization\"} will have their values replaced with\
      \ \"==== REDACTED ====\" when not None. Nested dictionaries and lists are processed\
      \ recursively; non-dictionary/list values are preserved."
  classes: []
- file: graphrag/utils/storage.py
  functions:
  - node_id: graphrag/utils/storage.py::write_table_to_storage
    name: write_table_to_storage
    signature: "def write_table_to_storage(\n    table: pd.DataFrame, name: str, storage:\
      \ PipelineStorage\n) -> None"
    docstring: "Write a table to storage.\n\nArgs:\n  table: pd.DataFrame\n      The\
      \ DataFrame to write to storage.\n  name: str\n      Base name for the parquet\
      \ file to be stored as name.parquet.\n  storage: PipelineStorage\n      The\
      \ storage backend to which the parquet file will be written.\n\nReturns:\n \
      \ None\n\nRaises:\n  Exception: Exceptions raised by the storage backend during\
      \ the write operation may propagate."
  - node_id: graphrag/utils/storage.py::storage_has_table
    name: storage_has_table
    signature: 'def storage_has_table(name: str, storage: PipelineStorage) -> bool'
    docstring: "Check if a table exists in storage.\n\nArgs:\n    name: The name of\
      \ the table to check (without the extension).\n    storage: The storage backend\
      \ implementing PipelineStorage.\n\nReturns:\n    bool: True if a file named\
      \ \"<name>.parquet\" exists in storage, False otherwise.\n\nRaises:\n    Propagates\
      \ exceptions raised by storage.has."
  - node_id: graphrag/utils/storage.py::load_table_from_storage
    name: load_table_from_storage
    signature: 'def load_table_from_storage(name: str, storage: PipelineStorage) ->
      pd.DataFrame'
    docstring: "Load a parquet from the storage instance.\n\nArgs:\n  name: str\n\
      \      Base name for the parquet file to load. The expected file name is {name}.parquet.\n\
      \  storage: PipelineStorage\n      The storage backend to read the parquet file\
      \ from.\n\nReturns:\n  pd.DataFrame\n      DataFrame loaded from the parquet\
      \ file stored at {name}.parquet.\n\nRaises:\n  ValueError\n      Could not find\
      \ {name}.parquet in storage!\n  Exception\n      Exceptions raised by the storage\
      \ backend or parquet reader during the load operation may propagate."
  - node_id: graphrag/utils/storage.py::delete_table_from_storage
    name: delete_table_from_storage
    signature: 'def delete_table_from_storage(name: str, storage: PipelineStorage)
      -> None'
    docstring: "Delete a table from storage.\n\nArgs:\n  name (str): The base name\
      \ of the parquet file to delete, without extension.\n  storage (PipelineStorage):\
      \ The storage backend to delete the parquet file from.\n\nReturns:\n  None\n\
      \nRaises:\n  Exception: Exceptions raised by the storage backend during the\
      \ delete operation may propagate."
  classes: []
- file: graphrag/vector_stores/azure_ai_search.py
  functions:
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search.\n\nThis method constructs\
      \ a VectorizedQuery using the provided query_embedding and k, and issues a search\
      \ via the configured database connection. It returns a list of VectorStoreSearchResult\
      \ objects, each containing a VectorStoreDocument and the corresponding similarity\
      \ score.\n\nArgs:\n  query_embedding: list[float] - Embedding vector to search\
      \ with\n  k: int - Number of nearest neighbors to retrieve\n  kwargs: Any -\
      \ Additional keyword arguments (passed through)\n\nReturns:\n  list[VectorStoreSearchResult]\
      \ - List of matching documents with scores\n\nRaises:\n  Exception - If the\
      \ underlying search operation fails or the Azure client raises an error"
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: 'Connect to AI search vector storage.


      Args:

      - url: The endpoint URL for the Azure AI Search service.

      - api_key: Optional API key for authentication. If provided, AzureKeyCredential
      is used; otherwise DefaultAzureCredential is used.

      - audience: Optional audience to pass to the client.

      - vector_search_profile_name: Optional name for the vector search profile. Defaults
      to "vectorSearchProfile".


      Returns:

      - None


      Raises:

      - ValueError: Azure AI Search expects url.'
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a text-based similarity search.\n\nArgs:\n    text (str):\
      \ The input text to search for similar documents.\n    text_embedder (TextEmbedder):\
      \ The callable used to compute an embedding for the input text.\n    k (int):\
      \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
      \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
      \ VectorStoreSearchResult objects.\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Builds a query filter to filter documents by a list of IDs.\n\nArgs:\n\
      \  include_ids: list[str] | list[int] The IDs to include in the filter. If None\
      \ or an empty\n    list is provided, the filter is cleared (set to None) and\
      \ None is returned.\n\nReturns:\n  Any The constructed query filter string to\
      \ filter documents by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize Azure AI Search vector store by delegating to the base\
      \ class constructor.\n\nArgs:\n    vector_store_schema_config: VectorStoreSchemaConfig\
      \ - The schema configuration for the vector store.\n    **kwargs: Any - Additional\
      \ keyword arguments forwarded to the base class initializer.\n\nReturns:\n \
      \   None\n\nRaises:\n    Exceptions raised by the base class __init__ are propagated."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id with its id, text, vector, and attributes\
      \ populated from the stored document.\n\nRaises:\n    json.JSONDecodeError:\
      \ If the attributes field cannot be decoded as JSON.\n    Exception: If the\
      \ underlying database connection fails to retrieve the document."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into an Azure AI Search index.\n\nArgs:\n    documents\
      \ (list[VectorStoreDocument]): Documents to load into the index. Each document\
      \ has id, vector, text, and attributes. Only documents with a non-null vector\
      \ are uploaded.\n    overwrite (bool): If True, delete and recreate the index\
      \ before loading.\n\nReturns:\n    None: This method does not return a value.\n\
      \nRaises:\n    Exception: If an error occurs during index creation or document\
      \ upload (propagates from underlying Azure SDK calls)."
  classes:
  - class_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
    name: AzureAISearchVectorStore
    docstring: "Azure AI Search vector store integrating Azure Cognitive Search to\
      \ provide vector-based retrieval.\n\nThis class provides vector similarity search,\
      \ text-based search, and document load/retrieval backed by Azure AI Search.\
      \ It delegates initialization to the base vector store class and is configured\
      \ via VectorStoreSchemaConfig.\n\nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig\
      \ - The schema configuration for the vector store.\n  **kwargs: Any - Additional\
      \ keyword arguments forwarded to the base class initializer.\n\nReturns:\n \
      \ None\n\nRaises:\n  Exceptions raised by the base class __init__ are propagated."
    methods:
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> Any'
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: __init__
      signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
        \ **kwargs: Any\n    ) -> None"
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
- file: graphrag/vector_stores/base.py
  functions:
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform ANN search by vector.\n\nArgs:\n    self: The instance of\
      \ the class.\n    query_embedding: list[float] The embedding vector to search\
      \ with.\n    k: int The number of top results to return.\n    **kwargs: Any\
      \ Additional keyword arguments that may influence the search.\n\nReturns:\n\
      \    list[VectorStoreSearchResult]: The top-k search results as VectorStoreSearchResult\
      \ objects.\n\nRaises:\n    NotImplementedError: If the method is not implemented\
      \ by a subclass."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
    docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
      \ arguments for connecting to the vector storage.\n\nReturns:\n    None"
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids\
      \ (Optional[list[str] | list[int]]): The IDs to include in the filter. If None\
      \ or an empty list is provided, the filter is cleared (set to None) and None\
      \ is returned. Concrete implementations may define different handling for empty\
      \ input.\n\nReturns:\n  Any: The constructed query filter to filter documents\
      \ by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector-store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
      \ - List of VectorStoreDocument objects to load into the vector store.\n   \
      \ overwrite: bool - If True, overwrite existing data in the vector store; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        vector_store_schema_config:\
      \ VectorStoreSchemaConfig,\n        db_connection: Any | None = None,\n    \
      \    document_collection: Any | None = None,\n        query_filter: Any | None\
      \ = None,\n        **kwargs: Any,\n    )"
    docstring: "Initialize the base vector store with the provided configuration and\
      \ optional resources.\n\nThis initializer assigns the given resources to instance\
      \ attributes and extracts\nconfiguration fields from vector_store_schema_config\
      \ to set the store's metadata\nand vector properties. It does not call super().__init__.\n\
      \nArgs:\n  vector_store_schema_config (VectorStoreSchemaConfig): The schema\
      \ configuration for the vector store. This is used to set index_name, id_field,\
      \ text_field, vector_field, attributes_field, and vector_size on the instance.\n\
      \  db_connection (Any | None): Optional database connection.\n  document_collection\
      \ (Any | None): Optional document collection.\n  query_filter (Any | None):\
      \ Optional query filter to apply to queries.\n  kwargs (Any): Additional keyword\
      \ arguments captured for later use and stored in self.kwargs.\n\nReturns:\n\
      \  None\n\nRaises:\n  None"
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform ANN search by text.\n\nArgs:\n    self: The instance\
      \ of the class.\n    text: str The input text to search for similar documents.\n\
      \    text_embedder: TextEmbedder The callable used to compute an embedding for\
      \ the input text.\n    k: int The number of top results to return.\n    **kwargs:\
      \ Any Additional keyword arguments.\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ A list of matching VectorStoreSearchResult objects.\n\nRaises:\n    NotImplementedError:\
      \ If the method is not implemented.\n\"\"\""
  classes:
  - class_id: graphrag/vector_stores/base.py::BaseVectorStore
    name: BaseVectorStore
    docstring: "Abstract base class for vector stores used by GraphRAG.\n\nThis class\
      \ defines the core interface and common lifecycle for vector-based storage and\
      \ retrieval of documents. Subclasses must implement the storage backend and\
      \ search logic for both vector and text queries, while this class provides a\
      \ consistent initialization surface and shared attributes.\n\nArgs:\n    vector_store_schema_config\
      \ (VectorStoreSchemaConfig): The schema configuration for this vector store,\
      \ including vector dimensions and field mappings.\n    db_connection (Any, optional):\
      \ Optional database or resource handle used to connect to the underlying storage.\
      \ Defaults to None.\n    document_collection (Any, optional): Optional existing\
      \ collection of documents within the store. Defaults to None.\n    query_filter\
      \ (Any, optional): Optional default filter applied when retrieving documents.\
      \ Defaults to None.\n    **kwargs (Any): Additional keyword arguments for subclass-specific\
      \ behavior.\n\nAttributes:\n    vector_store_schema_config: The configuration\
      \ used to configure this store's schema and vector properties.\n    db_connection:\
      \ Optional store connection/resource.\n    document_collection: Optional stored\
      \ document collection.\n    query_filter: Optional default query filter.\n\n\
      Abstract methods (must be implemented by subclasses):\n    connect(self, **kwargs):\
      \ Establish a connection to the vector store.\n    load_documents(self, documents,\
      \ overwrite=True): Load documents into the store.\n    similarity_search_by_vector(self,\
      \ query_embedding, k=10, **kwargs): ANN search by vector.\n    similarity_search_by_text(self,\
      \ text, text_embedder, k=10, **kwargs): ANN search by text.\n    search_by_id(self,\
      \ id): Retrieve a document by its identifier.\n    filter_by_id(self, include_ids):\
      \ Build a filter to limit results by IDs.\n\nNotes:\n    This base class is\
      \ not intended to be instantiated directly. Concrete implementations should\
      \ provide the specifics of the storage backend and search mechanics."
    methods:
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> None'
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
    - name: __init__
      signature: "def __init__(\n        self,\n        vector_store_schema_config:\
        \ VectorStoreSchemaConfig,\n        db_connection: Any | None = None,\n  \
        \      document_collection: Any | None = None,\n        query_filter: Any\
        \ | None = None,\n        **kwargs: Any,\n    )"
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
- file: graphrag/vector_stores/cosmosdb.py
  functions:
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._database_exists
    name: _database_exists
    signature: def _database_exists(self) -> bool
    docstring: "Check if the configured Cosmos DB database exists.\n\nReturns:\n \
      \   bool: True if the database exists, False otherwise.\n\nRaises:\n    CosmosHttpResponseError:\
      \ If there is an HTTP error while listing databases."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a text-based similarity search.\n\nArgs:\n    text (str):\
      \ The input text to search for similar documents.\n    text_embedder (TextEmbedder):\
      \ The callable used to compute an embedding for the input text.\n    k (int):\
      \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
      \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
      \ VectorStoreSearchResult objects.\n\nRaises:\n    ..."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_database
    name: _delete_database
    signature: def _delete_database(self) -> None
    docstring: "Delete the database if it exists.\n\nReturns:\n    None: This method\
      \ does not return a value."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Builds a query filter to filter documents by a list of ids.\n\nArgs:\n\
      \  include_ids: list[str] | list[int]\n    The IDs to include in the filter.\
      \ If None or an empty list is provided, the filter is cleared (set to None)\
      \ and None is returned.\n\nReturns:\n  Any\n    The constructed query filter\
      \ string to filter documents by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize CosmosDB vector store by delegating to the base class constructor.\n\
      \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema\
      \ configuration for the vector store.\n  **kwargs: Any - Additional keyword\
      \ arguments forwarded to the base class initializer.\n\nReturns:\n  None\n\n\
      Raises:\n  Exceptions raised by the base class __init__ are propagated."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.cosine_similarity
    name: cosine_similarity
    signature: def cosine_similarity(a, b)
    docstring: "Cosine similarity between two vectors.\n\nArgs:\n  a: First vector.\n\
      \  b: Second vector.\n\nReturns:\n  float: The cosine similarity between a and\
      \ b. If either vector has zero magnitude, returns 0.0."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_database
    name: _create_database
    signature: def _create_database(self) -> None
    docstring: "Create the database if it doesn't exist.\n\nThis method uses the Cosmos\
      \ client to ensure the database exists and then obtains a database client for\
      \ subsequent operations.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    CosmosHttpResponseError: If the Cosmos DB service\
      \ returns an HTTP error."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create the Cosmos DB container for the current container name if it\
      \ doesn't exist.\n\nConfigures a partition key based on the id field, a vector\
      \ embedding policy for the vector field and size, and an indexing policy. It\
      \ first attempts to apply a diskANN vector index; if that raises CosmosHttpResponseError,\
      \ it retries without vector indexes to ensure compatibility.\n\nArgs:\n    self:\
      \ Any. The instance containing the Cosmos DB client and vector store configuration.\n\
      \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
      \ Cosmos client operations may be raised."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to CosmosDB vector storage.\n\nArgs:\n    connection_string:\
      \ The Cosmos DB connection string. If provided, a CosmosClient is created from\
      \ it.\n    url: The Cosmos DB account URL. Used when connection_string is not\
      \ provided.\n    database_name: The name of the database to use. This must be\
      \ provided.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If neither connection_string\
      \ nor url is provided.\n    ValueError: If database_name is not provided.\n\
      \    ValueError: If index_name is empty or not provided."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the CosmosDB vector store.\n\nIf overwrite is\
      \ True, the existing container will be deleted and recreated before loading.\
      \ Documents with non-null vectors are stored. Each document is stored as an\
      \ item with fields corresponding to the configured id_field, vector_field, text_field,\
      \ and attributes_field (JSON-serialized). Upload uses upsert semantics; existing\
      \ items with the same id will be updated.\n\nArgs:\n    documents (list[VectorStoreDocument]):\
      \ List of VectorStoreDocument objects to load into CosmosDB. Only documents\
      \ with a non-null vector are stored. Each stored item includes fields corresponding\
      \ to the configured id_field, vector_field, text_field, and attributes_field.\n\
      \    overwrite (bool): If True, delete and recreate the container before loading\
      \ documents; otherwise preserve existing data.\n\nReturns:\n    None\n\nRaises:\n\
      \    ValueError: If the container client is not initialized."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._container_exists
    name: _container_exists
    signature: def _container_exists(self) -> bool
    docstring: "Check if the configured Cosmos DB container exists in the database.\n\
      \nArgs:\n    self: The instance that holds _container_name and _database_client\
      \ used to query Cosmos DB.\n\nReturns:\n    bool: True if a container with id\
      \ equal to self._container_name exists, otherwise False. The check is performed\
      \ against the 'id' field of containers returned by list_containers.\n\nRaises:\n\
      \    CosmosHttpResponseError: If an HTTP error occurs while listing containers.\
      \ This method does not catch it."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id in the Cosmos DB vector store.\n\nArgs:\n\
      \    id: The identifier of the document to retrieve.\n\nReturns:\n    VectorStoreDocument:\
      \ The document corresponding to the provided id with its id, vector, text, and\
      \ attributes populated from the stored item.\n\nRaises:\n    ValueError: If\
      \ the container client is not initialized."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "\"\"\"Delete the vector store container from the database if it exists.\n\
      \nArgs:\n    self: The instance of the class containing the vector store and\
      \ database client.\n\nReturns:\n    None\n\nRaises:\n    CosmosHttpResponseError:\
      \ If the underlying Cosmos DB client call to delete the container fails.\n\"\
      \"\""
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear the vector store by deleting its container and database.\n\n\
      Deletes the underlying container and database used to store vectors by delegating\
      \ to _delete_container() and _delete_database().\n\nRaises:\n    CosmosHttpResponseError:\
      \ If the underlying Cosmos DB client call to delete the container or database\
      \ fails."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search against the CosmosDB container.\n\
      \nArgs:\n  query_embedding: Embedding vector to search with\n  k: Number of\
      \ top results to return\n  kwargs: Additional keyword arguments for compatibility;\
      \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]:\
      \ Top-k results as VectorStoreSearchResult objects\n\nRaises:\n  ValueError:\
      \ If the container client is not initialized."
  classes:
  - class_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
    name: CosmosDBVectorStore
    docstring: "CosmosDBVectorStore is a Cosmos DB-backed vector store implementation\
      \ for GraphRAG. It manages storage, retrieval, and indexing of document vectors\
      \ and metadata in Azure Cosmos DB and supports text-based and vector-based similarity\
      \ search.\n\nArgs:\n  vector_store_schema_config (VectorStoreSchemaConfig):\
      \ The schema configuration for the vector store, including field mappings for\
      \ id, vector, text, and attributes, as well as database/container naming.\n\
      \  **kwargs: Additional keyword arguments forwarded to the base class initializer.\n\
      \nReturns:\n  None\n\nRaises:\n  CosmosHttpResponseError: If an HTTP error occurs\
      \ while interacting with Cosmos DB (e.g., during database/container operations\
      \ or listing resources)."
    methods:
    - name: _database_exists
      signature: def _database_exists(self) -> bool
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: _delete_database
      signature: def _delete_database(self) -> None
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: __init__
      signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
        \ **kwargs: Any\n    ) -> None"
    - name: cosine_similarity
      signature: def cosine_similarity(a, b)
    - name: _create_database
      signature: def _create_database(self) -> None
    - name: _create_container
      signature: def _create_container(self) -> None
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> Any'
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
    - name: _container_exists
      signature: def _container_exists(self) -> bool
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    - name: _delete_container
      signature: def _delete_container(self) -> None
    - name: clear
      signature: def clear(self) -> None
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
- file: graphrag/vector_stores/factory.py
  functions:
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.create_vector_store
    name: create_vector_store
    signature: "def create_vector_store(\n        cls,\n        vector_store_type:\
      \ str,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n     \
      \   **kwargs: dict,\n    ) -> BaseVectorStore"
    docstring: "Create a vector store object from the provided type via a registry\
      \ lookup.\n\nThis function looks up the registered vector store implementation\
      \ by vector_store_type and instantiates it by passing vector_store_schema_config\
      \ and any additional keyword arguments to the concrete vector store constructor.\
      \ The concrete vector store may require or accept additional kwargs; these are\
      \ forwarded via kwargs.\n\nArgs:\n    vector_store_type (str): The type identifier\
      \ for the vector store to create.\n    vector_store_schema_config (VectorStoreSchemaConfig):\
      \ Configuration describing the vector store schema; it is forwarded to the concrete\
      \ vector store implementation.\n    **kwargs: Additional keyword arguments for\
      \ the concrete vector store constructor.\n\nReturns:\n    BaseVectorStore: A\
      \ vector store instance.\n\nRaises:\n    ValueError: If the vector store type\
      \ is not registered."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.get_vector_store_types
    name: get_vector_store_types
    signature: def get_vector_store_types(cls) -> list[str]
    docstring: "Get the registered vector store implementations.\n\nArgs:\n    cls:\
      \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
      \ The list of registered vector store type keys (i.e., the keys of cls._registry)."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
    docstring: "Check if the given vector store type is supported.\n\nArgs:\n    cls:\
      \ type The class reference (classmethod parameter).\n    vector_store_type:\
      \ str The type identifier for the vector store.\n\nReturns:\n    bool: True\
      \ if vector_store_type is registered in the registry, False otherwise."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.register
    name: register
    signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]\n    ) -> None"
    docstring: "Register a custom vector store implementation.\n\nStores the provided\
      \ creator in the internal registry under the given vector_store_type. The registration\n\
      does not enforce any factory semantics; the creator is stored as-is and will\
      \ be invoked at runtime by\nVectorStoreFactory.create_vector_store with vector_store_schema_config\
      \ and any additional keyword arguments.\n\nArgs:\n    vector_store_type (str):\
      \ The type identifier for the vector store.\n    creator (Callable[..., BaseVectorStore]):\
      \ A class or callable that creates an instance of BaseVectorStore. The creator\n\
      \        is registered as-is and is invoked later with vector_store_schema_config\
      \ and kwargs.\n\nReturns:\n    None"
  classes:
  - class_id: graphrag/vector_stores/factory.py::VectorStoreFactory
    name: VectorStoreFactory
    docstring: "VectorStoreFactory is a registry-based factory for constructing vector\
      \ store instances from a registry of concrete implementations.\n\nPurpose\n\
      This class maintains a registry that maps vector_store_type keys (strings) to\
      \ creator callables that return BaseVectorStore instances. It exposes classmethods\
      \ to instantiate vector stores by type, list supported types, verify support\
      \ for a type, and register new implementations.\n\nClass Attributes\n  _registry\
      \ (ClassVar[dict[str, Callable[..., BaseVectorStore]]]): Internal registry mapping\
      \ vector_store_type keys to creator callables. Each creator should return a\
      \ BaseVectorStore when invoked with a VectorStoreSchemaConfig and any forwarded\
      \ keyword arguments.\n\nMethods\n  create_vector_store(\n      cls,\n      vector_store_type:\
      \ str,\n      vector_store_schema_config: VectorStoreSchemaConfig,\n      **kwargs:\
      \ dict\n  ) -> BaseVectorStore\n  Create a vector store instance by looking\
      \ up the registered creator for the given vector_store_type and invoking it\
      \ with vector_store_schema_config and any additional keyword arguments. If the\
      \ type is not registered, raises an error (e.g., KeyError or ValueError). Exceptions\
      \ raised by the concrete vector store constructor are propagated.\n\n  get_vector_store_types(cls)\
      \ -> list[str]\n  Return a list of registered vector_store_type keys.\n\n  is_supported_type(cls,\
      \ vector_store_type: str) -> bool\n  Return True if vector_store_type is registered;\
      \ otherwise False.\n\n  register(cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]) -> None\n  Register a new vector store implementation under\
      \ the given vector_store_type. The provided creator is stored and invoked by\
      \ create_vector_store. Raises ValueError if vector_store_type is already registered.\n\
      \nNotes\n The factory delegates all construction details to the registered creators.\
      \ The concrete creators may require or accept additional kwargs, which are forwarded\
      \ by create_vector_store."
    methods:
    - name: create_vector_store
      signature: "def create_vector_store(\n        cls,\n        vector_store_type:\
        \ str,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n   \
        \     **kwargs: dict,\n    ) -> BaseVectorStore"
    - name: get_vector_store_types
      signature: def get_vector_store_types(cls) -> list[str]
    - name: is_supported_type
      signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
    - name: register
      signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
        \ BaseVectorStore]\n    ) -> None"
- file: graphrag/vector_stores/lancedb.py
  functions:
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a similarity search using a given input text.\n\nArgs:\n\
      \    text: The input text to search for similar documents.\n    text_embedder:\
      \ TextEmbedder used to compute an embedding for the input text.\n    k: The\
      \ number of top results to return.\n    **kwargs: Additional keyword arguments.\n\
      \nReturns:\n    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
      \ objects.\n\nRaises:\n    Exception: If text_embedder or the underlying similarity\
      \ search raise.\n\"\"\""
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id: The identifier of the\
      \ document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document corresponding\
      \ to the provided id. If a matching document is found, its id, text, vector,\
      \ and attributes are populated; otherwise a VectorStoreDocument with id set\
      \ to the provided id and text=None, vector=None is returned."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into LanceDB vector storage.\n\nArgs:\n    documents:\
      \ List of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
      \ If True, overwrite existing table data; otherwise, append to the table.\n\n\
      Returns:\n    None\n\nRaises:\n    May raise exceptions from LanceDB operations\
      \ during table creation or data insertion."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
      \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a vector-based similarity search against the LanceDB\
      \ document collection.\n\nQuery the underlying document collection for documents\
      \ whose embeddings are close to the provided query_embedding. If a query filter\
      \ has been configured (via filter_by_id), the search results are restricted\
      \ to that subset using a prefilter.\n\nThe top-k results are returned in order\
      \ of increasing distance. Each result is a VectorStoreSearchResult containing:\n\
      - document: VectorStoreDocument with id, text, vector, and attributes (attributes\
      \ parsed from JSON in the attributes field)\n- score: 1 - abs(float(doc[\"_distance\"\
      ])) (a similarity score in [0, 1])\n\nArgs:\n  query_embedding: list[float]\
      \ | np.ndarray - Embedding vector to search with\n  k: int - Number of top results\
      \ to return\n  **kwargs: Any - Additional keyword arguments for compatibility;\
      \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]\
      \ - Top-k results with associated documents and similarity scores\n\nRaises:\n\
      \  ValueError, TypeError, RuntimeError - If the input embedding is invalid or\
      \ the search operation fails due to backend issues.\n\nNotes:\n  - If self.query_filter\
      \ is set, results are filtered by the provided condition before applying the\
      \ top-k limit.\n  - The attributes field is parsed from JSON; ensure the underlying\
      \ column contains valid JSON.\n\n\"\"\""
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
      \ list[str] | list[int]\n    The IDs to include in the filter. If the list is\
      \ empty, the filter is cleared\n    (set to None) and None is returned.\n\n\
      Returns:\n  Any\n    The constructed query filter string to filter documents\
      \ by the provided IDs, or None\n    if no IDs are provided."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize LanceDB vector store by delegating to the base class constructor.\n\
      \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema\
      \ configuration for the vector store.\n  **kwargs: Any - Additional keyword\
      \ arguments forwarded to the base class initializer.\n\nReturns:\n  None\n\n\
      Raises:\n  Exceptions raised by the base class __init__ are propagated...."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to LanceDB vector storage.\n\nArgs:\n    db_uri (str): The\
      \ LanceDB database URI to connect to. This value must be supplied via kwargs\
      \ with the key 'db_uri'.\n\nNotes:\n    - If self.index_name is set and a table\
      \ with that name exists in the connected database, the function will open that\
      \ table and assign it to self.document_collection.\n\nReturns:\n    None\n\n\
      Raises:\n    KeyError: If 'db_uri' is not provided in kwargs.\n    Exception:\
      \ If the underlying LanceDB library raises an exception during connect or table\
      \ open."
  classes:
  - class_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
    name: LanceDBVectorStore
    docstring: "LanceDB-backed vector store that uses LanceDB for storing and querying\
      \ document embeddings.\n\nThis class extends BaseVectorStore to provide similarity\
      \ search by text and by vector, loading documents into LanceDB, filtering by\
      \ IDs, and connecting to a LanceDB database. Key attributes include vector_store_schema_config,\
      \ index_name (optional), and document_collection (the LanceDB table handle).\n\
      \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema\
      \ configuration for the vector store.\n  **kwargs: Any - Additional keyword\
      \ arguments forwarded to the base class initializer.\n\nReturns:\n  None\n\n\
      Raises:\n  Exceptions raised by the base class __init__ are propagated."
    methods:
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float] | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: __init__
      signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
        \ **kwargs: Any\n    ) -> None"
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> Any'
- file: tests/conftest.py
  functions:
  - node_id: tests/conftest.py::pytest_addoption
    name: pytest_addoption
    signature: def pytest_addoption(parser)
    docstring: "Register the pytest option --run_slow to run slow tests.\n\nParameters:\n\
      \    parser: object\n        The pytest parser object used to register command-line\
      \ options.\n\nReturns:\n    None\n        The function does not return a value."
  classes: []
- file: tests/integration/language_model/test_factory.py
  functions:
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat
    name: achat
    signature: "def achat(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> ModelResponse"
    docstring: "Asynchronous achat that returns a fixed content response for the given\
      \ prompt.\n\nArgs:\n    prompt (str): The input prompt. The content is fixed\
      \ and does not depend on the prompt.\n    history (list | None): Optional conversation\
      \ history. May be unused by this placeholder implementation.\n    **kwargs (Any):\
      \ Additional keyword arguments. May be ignored.\n\nReturns:\n    ModelResponse:\
      \ The response for the prompt. The BaseModelResponse contains output with content=\"\
      content\". The output.full_response is None.\n\nRaises:\n    None documented\
      \ for this placeholder implementation.\n\nNotes:\n    This is a placeholder\
      \ implementation. It does not engage a real chat model; it always returns the\
      \ fixed content string."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
      \        ) -> list[list[float]]"
    docstring: "Asynchronously compute embeddings for a batch of input texts.\n\n\
      Args:\n    text_list: A batch of text inputs to generate embeddings for.\n\n\
      Returns:\n    list[list[float]]: A batch of embeddings corresponding to the\
      \ input texts."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat
    name: chat
    signature: "def chat(\n            self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n        ) -> ModelResponse"
    docstring: "Process a chat prompt and return the corresponding model response.\n\
      \nArgs:\n    self: The instance of the class that contains this chat method.\n\
      \    prompt (str): The prompt to send to the model.\n    history (list | None):\
      \ Optional chat history to provide context for the model.\n    **kwargs: Additional\
      \ keyword arguments passed to the underlying model.\n\nReturns:\n    ModelResponse:\
      \ The model response object. In this implementation, it is a BaseModelResponse\n\
      \        containing a BaseModelOutput with content set to \"content\" and full_response\
      \ set to {\"key\": \"value\"}."
  - node_id: tests/integration/language_model/test_factory.py::test_create_custom_chat_model
    name: test_create_custom_chat_model
    signature: def test_create_custom_chat_model()
    docstring: "Test creating and using a custom chat model via the ModelFactory and\
      \ ModelManager.\n\nThis test defines a CustomChatModel with chat and achat methods,\
      \ registers it with the factory,\ncreates an instance via ModelManager, and\
      \ asserts behavior of the methods (achat returning\ncontent-only response and\
      \ chat returning content with a full_response payload).\n\nReturns:\n    None"
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "Asynchronously generate an embedding for the input text.\n\nArgs:\n\
      \  text: The input text to generate the embedding for.\n  kwargs: Additional\
      \ keyword arguments passed to the embedding model.\n\nReturns:\n  list[float]:\
      \ A list of floating-point numbers representing the embedding.\n\nRaises:\n\
      \  This function does not raise any exceptions."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n            self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n        ) -> Generator[str, None]"
    docstring: "Stream a chat response for the given prompt.\n\nArgs:\n  prompt: str\
      \ \u2014 The text to generate a response for.\n  history: list | None \u2014\
      \ The conversation history.\n  **kwargs: Any \u2014 Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n  Generator[str, None] \u2014 The generator\
      \ that yields strings representing the response."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "Compute embeddings for a batch of input texts.\n\nArgs:\n    text_list:\
      \ list[str] - A batch of text inputs to generate embeddings for.\n    **kwargs:\
      \ Any - Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    list[list[float]] - A batch of embeddings corresponding to the input texts."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n            self, prompt: str, history: list |\
      \ None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
    docstring: "\"\"\"Stream Chat with the Model using the given prompt.\n\nArgs:\n\
      \  prompt: The prompt to chat with.\n  history: The conversation history.\n\
      \  kwargs: Additional arguments to pass to the Model.\n\nReturns:\n  An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \  Propagates exceptions raised by the underlying model call or streaming response.\n\
      \"\"\""
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n  text: The input\
      \ text to generate the embedding for.\n  kwargs: Additional keyword arguments\
      \ passed to the embedding model.\n\nReturns:\n  list[float]: A list of floating-point\
      \ numbers representing the embedding.\n\nRaises:\n  This function does not raise\
      \ any exceptions...."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with optional keyword arguments.\n\nThis __init__\
      \ accepts arbitrary keyword arguments (kwargs) but does not store or use them.\
      \ No state is initialized or modified, and no side effects occur.\n\nArgs:\n\
      \  kwargs: dict[str, Any] - keyword arguments provided to the initializer. They\
      \ are ignored.\n\nReturns:\n  None"
  - node_id: tests/integration/language_model/test_factory.py::test_create_custom_embedding_llm
    name: test_create_custom_embedding_llm
    signature: def test_create_custom_embedding_llm()
    docstring: "Asynchronous test for creating a custom embedding LLM and validating\
      \ its methods.\n\nArgs:\n    None: The function takes no parameters.\n\nReturns:\n\
      \    None: This test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion in the test fails."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
      \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
      \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None..."
  classes:
  - class_id: tests/integration/language_model/test_factory.py::CustomChatModel
    name: CustomChatModel
    docstring: "Lightweight test-oriented chat model that provides synchronous, streaming,\
      \ and asynchronous chat interfaces.\n\nPurpose:\n    Simulate basic chat interactions\
      \ for tests by offering predictable responses across methods. It does not maintain\
      \ internal state between calls and ignores any initialization keyword arguments.\n\
      \nAttributes:\n    None - the class does not store internal state.\n\nArgs:\n\
      \    kwargs: dict[str, Any] - Optional keyword arguments provided to the initializer.\
      \ They are ignored.\n\nReturns:\n    None"
    methods:
    - name: achat
      signature: "def achat(\n            self, prompt: str, history: list | None\
        \ = None, **kwargs: Any\n        ) -> ModelResponse"
    - name: chat
      signature: "def chat(\n            self, prompt: str, history: list | None =\
        \ None, **kwargs: Any\n        ) -> ModelResponse"
    - name: chat_stream
      signature: "def chat_stream(\n            self, prompt: str, history: list |\
        \ None = None, **kwargs: Any\n        ) -> Generator[str, None]"
    - name: achat_stream
      signature: "def achat_stream(\n            self, prompt: str, history: list\
        \ | None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
    - name: __init__
      signature: def __init__(self, **kwargs)
  - class_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel
    name: CustomEmbeddingModel
    docstring: "CustomEmbeddingModel provides synchronous and asynchronous embeddings\
      \ for text inputs, including batch processing. It is stateless and accepts arbitrary\
      \ keyword arguments at initialization, which are ignored.\n\nArgs:\n  kwargs:\
      \ dict[str, Any] - keyword arguments provided to the initializer. They are ignored.\n\
      \nReturns:\n  None\n\nMethods:\n  aembed_batch(self, text_list: list[str], **kwargs)\
      \ -> list[list[float]]: Asynchronously compute embeddings for a batch of input\
      \ texts.\n    Returns: A list of embeddings, where each embedding is a list\
      \ of floats corresponding to the input texts.\n    Raises: Exceptions from the\
      \ underlying embedding process may propagate.\n  aembed(self, text: str, **kwargs)\
      \ -> list[float]: Asynchronously generate an embedding for the input text.\n\
      \    Returns: A list of floats representing the embedding.\n    Raises: Exceptions\
      \ from the underlying embedding process may propagate.\n  embed_batch(self,\
      \ text_list: list[str], **kwargs) -> list[list[float]]: Compute embeddings for\
      \ a batch of input texts.\n    Returns: A list of embeddings corresponding to\
      \ the input texts.\n    Raises: Exceptions from the underlying embedding process\
      \ may propagate.\n  embed(self, text: str, **kwargs) -> list[float]: Generate\
      \ an embedding for the input text.\n    Returns: A list of floats representing\
      \ the embedding.\n    Raises: Exceptions from the underlying embedding process\
      \ may propagate.\n  __init__(self, **kwargs): Initialize the instance with arbitrary\
      \ keyword arguments; no initialization is performed.\n    Returns: None\n  \
      \  Raises: None"
    methods:
    - name: aembed_batch
      signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
        \        ) -> list[list[float]]"
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    - name: embed
      signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    - name: __init__
      signature: def __init__(self, **kwargs)
- file: tests/integration/logging/test_factory.py
  functions:
  - node_id: tests/integration/logging/test_factory.py::test_create_blob_logger
    name: test_create_blob_logger
    signature: def test_create_blob_logger()
    docstring: "Test for creating a blob logger via LoggerFactory (skipped in this\
      \ environment).\n\nThis test is marked with pytest.mark.skip(reason=\"Blob storage\
      \ emulator is not available in this environment\"). If executed, it would construct\
      \ a kwargs dictionary containing: type: \"blob\", connection_string: WELL_KNOWN_BLOB_STORAGE_KEY,\
      \ base_dir: \"testbasedir\", container_name: \"testcontainer\". It would then\
      \ call LoggerFactory.create_logger(ReportingType.blob.value, kwargs) and assert\
      \ that the resulting logger is an instance of BlobWorkflowLogger.\n\nReturns:\n\
      \    None"
  - node_id: tests/integration/logging/test_factory.py::test_create_unknown_logger
    name: test_create_unknown_logger
    signature: def test_create_unknown_logger()
    docstring: "Tests that creating an unknown logger type raises a ValueError.\n\n\
      Args:\n    None: The test function does not take any parameters.\n\nReturns:\n\
      \    None: The test does not return a value.\n\nRaises:\n    ValueError: Unknown\
      \ reporting type: unknown"
  - node_id: tests/integration/logging/test_factory.py::test_get_logger_types
    name: test_get_logger_types
    signature: def test_get_logger_types()
    docstring: "Verify that built-in logger types are registered and returned by LoggerFactory.get_logger_types.\n\
      \nThis test retrieves the list of registered logger types from LoggerFactory\
      \ and asserts that\nReportingType.file.value and ReportingType.blob.value are\
      \ present in the result.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If the expected logger types are not present in the result."
  - node_id: tests/integration/logging/test_factory.py::test_register_and_create_custom_logger
    name: test_register_and_create_custom_logger
    signature: def test_register_and_create_custom_logger()
    docstring: 'Test registering and creating a custom logger type.


      This test registers a custom logger type named "custom" using LoggerFactory.register
      with a factory function, creates a logger via LoggerFactory.create_logger("custom",
      {}), and asserts that the factory was invoked and the returned logger is the
      expected instance. It also asserts that the created logger has the initialized
      attribute set to True and that "custom" is present in the list of registered
      logger types and is reported as supported.'
  classes: []
- file: tests/integration/logging/test_standard_logging.py
  functions:
  - node_id: tests/integration/logging/test_standard_logging.py::test_standard_logging
    name: test_standard_logging
    signature: def test_standard_logging()
    docstring: "Test that standard logging returns a logger whose name matches the\
      \ requested value 'graphrag.test' by asserting logger.name == 'graphrag.test'.\n\
      \nArgs:\n    none: This test takes no parameters.\n\nReturns:\n    None: This\
      \ test returns no value.\n\nRaises:\n    AssertionError: If the logger name\
      \ does not equal 'graphrag.test'."
  - node_id: tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
    name: test_logger_hierarchy
    signature: def test_logger_hierarchy()
    docstring: "Test that logger hierarchy works correctly.\n\nArgs:\n    None: This\
      \ test does not accept any parameters.\n\nReturns:\n    None: The test does\
      \ not return a value.\n\nRaises:\n    AssertionError: If the logger hierarchy\
      \ does not propagate the root level to child loggers as expected."
  - node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
    name: test_init_loggers_file_config
    signature: def test_init_loggers_file_config()
    docstring: "Test that init_loggers works with file configuration.\n\nReturns:\n\
      \    None"
  - node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
    name: test_init_loggers_file_verbose
    signature: def test_init_loggers_file_verbose()
    docstring: "Initialize logging for graphrag using the provided GraphRagConfig.\n\
      \nA logger named \"graphrag\" is configured with a handler derived from the\
      \ given configuration. The log level is set to DEBUG when verbose is True, otherwise\
      \ INFO. Before attaching the new handler, all existing handlers on the logger\
      \ are removed; any FileHandler instances are closed to avoid resource leaks\
      \ and duplicate logs.\n\nArgs:\n    config (GraphRagConfig): The GraphRagConfig\
      \ instance providing logging settings.\n    verbose (bool): If True, set the\
      \ log level to DEBUG; otherwise INFO.\n    filename (str): The log filename\
      \ to use for the log output. Defaults to DEFAULT_LOG_FILENAME.\n\nReturns:\n\
      \    None"
  - node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
    name: test_init_loggers_custom_filename
    signature: def test_init_loggers_custom_filename()
    docstring: "Test that init_loggers writes logs to a custom filename.\n\nThis test\
      \ creates a temporary Graphrag configuration, initializes the loggers with a\
      \ custom\nfilename (\"custom-log.log\"), and asserts that a file named logs/custom-log.log\
      \ is created inside\nthe temporary root directory. It then cleans up by closing\
      \ and removing any FileHandler instances\nattached to the graphrag logger.\n\
      \nReturns:\n    None: The test function does not return a value."
  classes: []
- file: tests/integration/storage/test_blob_pipeline_storage.py
  functions:
  - node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_get_creation_date
    name: test_get_creation_date
    signature: def test_get_creation_date()
    docstring: 'Test that BlobPipelineStorage.get_creation_date returns a correctly
      formatted creation timestamp for a blob.


      Returns:

      A string representing the creation date of the blob, formatted as "%Y-%m-%d
      %H:%M:%S %z".

      Type: str'
  - node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_dotprefix
    name: test_dotprefix
    signature: def test_dotprefix()
    docstring: Test that a dot-prefix path is handled correctly by BlobPipelineStorage
      when setting and listing files. The test creates a storage with path_prefix='.'
      and writes input/christmas.txt, then searches for .txt files and asserts that
      the resulting path is ['input/christmas.txt'].
  - node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_child
    name: test_child
    signature: def test_child()
    docstring: "Test that a child BlobPipelineStorage can be created from a parent\
      \ storage and used to perform basic file operations.\n\nReturns:\n    None\n\
      \    The function does not return a value."
  - node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_find
    name: test_find
    signature: def test_find()
    docstring: "Asynchronous test for BlobPipelineStorage.find and basic file operations.\n\
      \nThis test creates a BlobPipelineStorage instance, verifies that there are\
      \ no matching .txt files under the input base_dir, creates input/christmas.txt\
      \ and then confirms it is found, creates test.txt and confirms both files are\
      \ listed, reads the content of test.txt, and finally deletes test.txt and ensures\
      \ it no longer exists. The container is cleaned up at the end.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  classes: []
- file: tests/integration/storage/test_cosmosdb_storage.py
  functions:
  - node_id: tests/integration/storage/test_cosmosdb_storage.py::test_get_creation_date
    name: test_get_creation_date
    signature: def test_get_creation_date()
    docstring: 'Test that CosmosDBPipelineStorage.get_creation_date returns a correctly
      formatted creation timestamp.


      This test creates a storage, writes a JSON file, retrieves its creation date
      using get_creation_date, and asserts that the returned string matches the expected
      format "%Y-%m-%d %H:%M:%S %z" when parsed as a datetime.


      Returns:

      None

      Type: None'
  - node_id: tests/integration/storage/test_cosmosdb_storage.py::test_clear
    name: test_clear
    signature: def test_clear()
    docstring: "Test that CosmosDBPipelineStorage.clear() removes all stored items\
      \ and resets internal client references.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: tests/integration/storage/test_cosmosdb_storage.py::test_child
    name: test_child
    signature: def test_child()
    docstring: "Test that a child CosmosDBPipelineStorage can be created from a parent\
      \ storage.\n\nThe test initializes a CosmosDBPipelineStorage, uses the child()\
      \ method to obtain a child storage, and asserts that the returned object is\
      \ a CosmosDBPipelineStorage.\n\nReturns:\n    None"
  - node_id: tests/integration/storage/test_cosmosdb_storage.py::test_find
    name: test_find
    signature: def test_find()
    docstring: "Test the integration behavior of CosmosDBPipelineStorage.find() in\
      \ a test container. This test creates a storage instance, verifies an empty\
      \ result set, adds JSON files, verifies the listing order, reads content, checks\
      \ existence, deletes a file, and clears storage at the end.\n\nReturns:\n  \
      \  None\n\nRaises:\n    None"
  classes: []
- file: tests/integration/storage/test_factory.py
  functions:
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.get
    name: get
    signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
      \ encoding: str | None = None\n        ) -> Any"
    docstring: "Asynchronous method to retrieve the value for a given key from storage.\n\
      \nThis interface method is intended for storage backends to implement. The actual\
      \ return\nvalue is implementation-dependent. In the base implementation available\
      \ here, this call\nreturns None.\n\nArgs:\n    key (str): The key to retrieve.\n\
      \    as_bytes (bool | None): If True, return the value as bytes. If None, use\
      \ the backend's\n        default behavior. The exact semantics are backend-dependent.\n\
      \    encoding (str | None): Optional encoding to apply when returning the value\
      \ as a string.\n        Its interpretation is backend-dependent.\n\nReturns:\n\
      \    None: The value retrieval is not implemented in the base class and returns\
      \ None. Concrete\n    backends may return the stored value, or may raise an\
      \ error if the key is not found.\n\nRaises:\n    Exception: Backend-specific\
      \ errors may be raised for not-found conditions or other I/O errors.\n     \
      \   Implementations define their own exception types."
  - node_id: tests/integration/storage/test_factory.py::test_register_class_directly_works
    name: test_register_class_directly_works
    signature: def test_register_class_directly_works()
    docstring: 'Test that StorageFactory allows direct class registration and can
      instantiate the registered class.


      This test registers a concrete CustomStorage class directly with StorageFactory,
      verifies it is registered and reported as supported, and creates an instance
      to ensure the registration path works.


      Args:

      - None: This test has no parameters.


      Returns:

      - None: This test does not return a value; it uses assertions to verify StorageFactory
      behavior.


      Notes:

      - Scope: direct class registration via StorageFactory for StorageFactory behavior
      verification.'
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "\"\"\"Get the creation date for the given key.\n\nArgs:\n    key (str):\
      \ The key for which to retrieve the creation date.\n\nReturns:\n    str: The\
      \ creation date as a string.\n\"\"\""
  - node_id: tests/integration/storage/test_factory.py::test_get_storage_types
    name: test_get_storage_types
    signature: def test_get_storage_types()
    docstring: "Verify that StorageFactory.get_storage_types returns a collection\
      \ containing the values of the built-in storage types.\n\nThe test asserts that\
      \ StorageType.file.value, StorageType.memory.value, StorageType.blob.value,\
      \ and StorageType.cosmosdb.value are present in the returned collection.\n\n\
      Returns:\n    None"
  - node_id: tests/integration/storage/test_factory.py::test_create_blob_storage
    name: test_create_blob_storage
    signature: def test_create_blob_storage()
    docstring: "Test creating a blob storage via the StorageFactory.\n\nReturns:\n\
      \    None\n\nRaises:\n    AssertionError: If the created storage is not an instance\
      \ of BlobPipelineStorage."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Deletes the item associated with the specified key from storage.\n\
      \nArgs:\n    key: The key of the item to delete.\n\nReturns:\n    None"
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.find
    name: find
    signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
      \            base_dir: str | None = None,\n            file_filter: dict[str,\
      \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]"
    docstring: "Find files in the storage that match a compiled file_pattern, with\
      \ optional base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern\
      \ (re.Pattern[str]): A compiled regular expression to match file paths.\n  \
      \  base_dir (str | None): The base directory to search within. If None, search\
      \ starts from the storage root.\n    file_filter (dict[str, Any] | None): Optional\
      \ dictionary of metadata-based filters to apply when selecting files.\n    max_count\
      \ (int): Maximum number of results to return. A value of -1 means no limit.\n\
      \nReturns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator yielding tuples\
      \ of (path, metadata) for each matching file."
  - node_id: tests/integration/storage/test_factory.py::test_create_file_storage
    name: test_create_file_storage
    signature: def test_create_file_storage()
    docstring: "Test creating a file-based storage via the StorageFactory.\n\nReturns:\n\
      \    None\n\nRaises:\n    AssertionError: if the created storage is not an instance\
      \ of FilePipelineStorage."
  - node_id: tests/integration/storage/test_factory.py::test_create_unknown_storage
    name: test_create_unknown_storage
    signature: def test_create_unknown_storage()
    docstring: "Test that creating an unknown storage type raises a ValueError.\n\n\
      Returns:\n    None\n\nRaises:\n    ValueError: Unknown storage type: unknown"
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: 'Return a list of keys stored in the storage.


      Args:

      self: The storage instance.


      Returns:

      list[str]: A list of keys as strings.


      Raises:

      This implementation does not raise any exceptions.'
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "\"\"\"Return the current storage instance (no new child is created).\n\
      \nThis method accepts an optional name parameter for API compatibility but does\
      \ not create a new child. It returns the current instance (self).\n\nArgs:\n\
      \    name (str | None): Optional name for the child storage. This parameter\
      \ is accepted for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self).\n\"\"\""
  - node_id: tests/integration/storage/test_factory.py::test_register_and_create_custom_storage
    name: test_register_and_create_custom_storage
    signature: def test_register_and_create_custom_storage()
    docstring: 'Test registering and creating a custom storage type.


      This test registers a custom storage type named "custom" using StorageFactory.register
      with a factory function, creates storage via StorageFactory.create_storage,
      and verifies that the factory was invoked and that the returned storage is the
      expected instance. It also asserts that the created instance has initialized
      set to True, and that "custom" appears in StorageFactory.get_storage_types()
      and is reported as a supported type by StorageFactory.is_supported_type.


      Returns: None'
  - node_id: tests/integration/storage/test_factory.py::test_create_cosmosdb_storage
    name: test_create_cosmosdb_storage
    signature: def test_create_cosmosdb_storage()
    docstring: "Test creating a CosmosDB storage via the StorageFactory.\n\nThis test\
      \ is skipped on non-Windows platforms because the Cosmos DB emulator is only\
      \ available on Windows runners.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If the created storage is not an instance of CosmosDBPipelineStorage."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously checks whether the given key exists in storage.\n\n\
      This coroutine should be awaited to obtain the result.\n\nArgs:\n    key: The\
      \ key to check for existence.\n\nReturns:\n    bool: True if the key exists,\
      \ False otherwise. Note: This is a placeholder implementation that always returns\
      \ False. Subclasses should override this method to provide a real existence\
      \ check.\n\nRaises:\n    Exception: If a storage backend encounters an error\
      \ during the check."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage backend. This no-op\
      \ implementation does not modify any stored data.\n\nThis coroutine completes\
      \ without returning a value.\n\nParameters:\n    None\n\nReturns:\n    None\n\
      \nRaises:\n    None: This no-op implementation does not raise exceptions."
  - node_id: tests/integration/storage/test_factory.py::test_create_memory_storage
    name: test_create_memory_storage
    signature: def test_create_memory_storage()
    docstring: "Test creating a memory storage via StorageFactory using StorageType.memory.value\
      \ with an empty kwargs dict, and verify the result is a MemoryPipelineStorage\
      \ instance.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ if the created storage is not an instance of MemoryPipelineStorage."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set the value for the given key.\n\nAsynchronously set the value for\
      \ the given key.\n\nArgs:\n    key (str): The key to set the value for.\n  \
      \  value (Any): The value to set.\n    encoding (str | None): Optional encoding\
      \ to apply when serializing the value.\n\nReturns:\n    None: This coroutine\
      \ completes when the value has been set."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
      \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
      \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None"
  classes:
  - class_id: tests/integration/storage/test_factory.py::CustomStorage
    name: CustomStorage
    docstring: "CustomStorage: Test double implementing the PipelineStorage interface\
      \ for integration tests.\n\nThis class provides a minimal storage backend used\
      \ in tests to validate StorageFactory behavior by implementing the methods defined\
      \ in PipelineStorage: get, get_creation_date, delete, find, keys, child, has,\
      \ clear, and set. The constructor accepts arbitrary keyword arguments which\
      \ are ignored.\n\nArgs:\n    kwargs (dict[str, Any]): Keyword arguments passed\
      \ to the initializer. They are ignored.\n\nReturns:\n    None\n\nRaises:\n \
      \   None"
    methods:
    - name: get
      signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
        \ encoding: str | None = None\n        ) -> Any"
    - name: get_creation_date
      signature: 'def get_creation_date(self, key: str) -> str'
    - name: delete
      signature: 'def delete(self, key: str) -> None'
    - name: find
      signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
        \            base_dir: str | None = None,\n            file_filter: dict[str,\
        \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
        \ dict[str, Any]]]"
    - name: keys
      signature: def keys(self) -> list[str]
    - name: child
      signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    - name: has
      signature: 'def has(self, key: str) -> bool'
    - name: clear
      signature: def clear(self) -> None
    - name: set
      signature: 'def set(self, key: str, value: Any, encoding: str | None = None)
        -> None'
    - name: __init__
      signature: def __init__(self, **kwargs)
- file: tests/integration/storage/test_file_pipeline_storage.py
  functions:
  - node_id: tests/integration/storage/test_file_pipeline_storage.py::test_find
    name: test_find
    signature: def test_find()
    docstring: "Asynchronous test for FilePipelineStorage.find and basic get/set/delete\
      \ operations.\n\nThis test is asynchronous and uses await to verify the behavior\
      \ of FilePipelineStorage:\n- find: lists .txt files under a base directory matching\
      \ a pattern\n- get: reads file contents (including verifying the existence)\n\
      - set: creates a new file with specified content\n- delete: removes the file\
      \ and confirms it is deleted\n\nNote: Failures may raise assertions or other\
      \ exceptions during test execution.\n\nArgs:\n    None: The function does not\
      \ accept any input parameters. Type: None\n\nReturns:\n    None: The test does\
      \ not return a value. Type: None\n\nRaises:\n    AssertionError: If any assertion\
      \ in the test fails.\n    Exception: If any other error occurs during test execution."
  - node_id: tests/integration/storage/test_file_pipeline_storage.py::test_child
    name: test_child
    signature: def test_child()
    docstring: "\"\"\"\nTest that a child FilePipelineStorage can be created from\
      \ a parent storage and used to perform basic file operations.\n\nReturns:\n\
      \    None\n        The function does not return a value.\n\"\"\""
  - node_id: tests/integration/storage/test_file_pipeline_storage.py::test_get_creation_date
    name: test_get_creation_date
    signature: def test_get_creation_date()
    docstring: "Test that FilePipelineStorage.get_creation_date returns a correctly\
      \ formatted creation timestamp for a blob. The test uses the fixture file tests/fixtures/text/input/dulce.txt\
      \ and asserts that the returned string matches the format '%Y-%m-%d %H:%M:%S\
      \ %z'.\n\nArgs:\n  None\n\nReturns:\n  str - A timestamp string formatted as\
      \ '%Y-%m-%d %H:%M:%S %z' as produced by FilePipelineStorage.get_creation_date\
      \ for the fixture file.\n\nRaises:\n  ValueError: If the returned creation_date\
      \ string cannot be parsed using the expected format '%Y-%m-%d %H:%M:%S %z'."
  classes: []
- file: tests/integration/vector_stores/test_azure_ai_search.py
  functions:
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store
    name: vector_store
    signature: def vector_store(self, mock_search_client, mock_index_client)
    docstring: "Create an Azure AI Search vector store fixture.\n\nArgs:\n    self:\
      \ The test class instance.\n    mock_search_client: Mock Azure AI Search client\
      \ to be assigned to vector_store.db_connection.\n    mock_index_client: Mock\
      \ Azure AI Search index client to be assigned to vector_store.index_client.\n\
      \nReturns:\n    AzureAISearchVectorStore: The configured Azure AI Search vector\
      \ store instance."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.none_embedder
    name: none_embedder
    signature: 'def none_embedder(text: str) -> None'
    docstring: "A placeholder embedder function used in tests that accepts a text\
      \ string and returns None.\n\nArgs:\n    text: str\n        The input text to\
      \ be embedded. The function does not perform embedding and returns None.\n\n\
      Returns:\n    None\n        The function returns no value."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store_custom
    name: vector_store_custom
    signature: def vector_store_custom(self, mock_search_client, mock_index_client)
    docstring: "Create an Azure AI Search vector store fixture with custom field mappings.\n\
      \nArgs:\n    self: The test class instance.\n    mock_search_client: Mock Azure\
      \ AI Search client to be assigned to vector_store.db_connection.\n    mock_index_client:\
      \ Mock Azure AI Search index client to be assigned to vector_store.index_client.\n\
      \nReturns:\n    AzureAISearchVectorStore: The configured Azure AI Search vector\
      \ store instance."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_embedder
    name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
    docstring: "A simple text embedder used for testing that returns a fixed embedding\
      \ vector. The embedding is independent of the input text and always returns\
      \ [0.1, 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text: Input text to embed.\n\nReturns:\n\
      \    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4, 0.5].\n\n\
      Raises:\n    None: This function does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_customization
    name: test_vector_store_customization
    signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
      \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
      \    )"
    docstring: "Test vector store customization with Azure AI Search.\n\nArgs:\n \
      \   self: The test case instance.\n    vector_store_custom: Custom Azure AI\
      \ Search vector store used in the test.\n    sample_documents: Documents used\
      \ to load into the vector store.\n    mock_search_client: Mock for the Azure\
      \ AI Search search client.\n    mock_index_client: Mock for the Azure AI Search\
      \ index client.\n\nReturns:\n    None."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_operations
    name: test_vector_store_operations
    signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
      \ mock_search_client, mock_index_client\n    )"
    docstring: "Test basic vector store operations with Azure AI Search.\n\nArgs:\n\
      \  self: The test case instance.\n  vector_store: AzureAISearchVectorStore used\
      \ in the test.\n  sample_documents: Documents loaded into the vector store for\
      \ testing.\n  mock_search_client: Mock for the Azure AI Search search client.\n\
      \  mock_index_client: Mock for the Azure AI Search index client.\n\nReturns:\n\
      \  None."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_search_client
    name: mock_search_client
    signature: def mock_search_client(self)
    docstring: "Create and yield a mock Azure AI Search client for tests.\n\nArgs:\n\
      \    self: TestAzureAISearchVectorStore instance.\n\nReturns:\n    MagicMock:\
      \ The mocked Azure AI Search client (SearchClient) instance produced by patch."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.sample_documents
    name: sample_documents
    signature: def sample_documents(self)
    docstring: "Create sample documents for testing.\n\nParameters:\n    self: Instance\
      \ of the test class used by pytest to provide fixture context.\n\nReturns:\n\
      \    List[VectorStoreDocument]: A list of VectorStoreDocument objects with ids\
      \ \"doc1\" and \"doc2\",\n        texts \"This is document 1\" and \"This is\
      \ document 2\",\n        vectors [0.1, 0.2, 0.3, 0.4, 0.5] and [0.2, 0.3, 0.4,\
      \ 0.5, 0.6],\n        and attributes {\"title\": \"Doc 1\", \"category\": \"\
      test\"} and {\"title\": \"Doc 2\", \"category\": \"test\"}."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_empty_embedding
    name: test_empty_embedding
    signature: def test_empty_embedding(self, vector_store, mock_search_client)
    docstring: "Test similarity search by text with empty embedding.\n\nArgs:\n  \
      \  self: The test method's instance.\n    vector_store: The AzureAISearchVectorStore\
      \ instance under test.\n    mock_search_client: Mock search client used to verify\
      \ interactions.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_index_client
    name: mock_index_client
    signature: def mock_index_client(self)
    docstring: "Create a mock Azure AI Search index client.\n\nArgs:\n    self (TestAzureAISearchVectorStore):\
      \ The test class instance.\n\nReturns:\n    MagicMock: The mocked Azure AI Search\
      \ index client instance produced by patching SearchIndexClient.\n\nRaises:\n\
      \    None"
  classes:
  - class_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore
    name: TestAzureAISearchVectorStore
    docstring: 'TestAzureAISearchVectorStore: Test suite for AzureAISearchVectorStore
      integration with Azure Cognitive Search.


      This class provides integration-style tests for the AzureAISearchVectorStore
      using mocked Azure Cognitive Search clients. It validates interaction with the
      search and indexing layers, embedding handling, and basic vector-store operations
      without requiring live services. The tests are driven by fixtures that configure
      a vector store instance, supply embedder behavior, and provide sample documents,
      while patching the SearchClient and SearchIndexClient to simulate Azure AI Search
      behavior.


      Key fixtures and test coverage:

      - vector_store: a configured AzureAISearchVectorStore instance bound to mocked
      search and index clients.

      - none_embedder: a placeholder embedder that returns None to exercise edge cases.

      - vector_store_custom: a vector store configured with custom field mappings.

      - mock_embedder: a simple embedder returning a fixed embedding vector.

      - mock_search_client: a mocked Azure AI Search SearchClient used in tests.

      - mock_index_client: a mocked Azure AI Search SearchIndexClient used in tests.

      - sample_documents: a small list of VectorStoreDocument objects used for load/search
      scenarios.

      - test_empty_embedding: tests behavior when embedding yields an empty or None
      vector.

      - test_vector_store_customization: tests that custom field mappings are honored
      during indexing/search.

      - test_vector_store_operations: tests basic vector store operations (add/load/search)
      with mocks.'
    methods:
    - name: vector_store
      signature: def vector_store(self, mock_search_client, mock_index_client)
    - name: none_embedder
      signature: 'def none_embedder(text: str) -> None'
    - name: vector_store_custom
      signature: def vector_store_custom(self, mock_search_client, mock_index_client)
    - name: mock_embedder
      signature: 'def mock_embedder(text: str) -> list[float]'
    - name: test_vector_store_customization
      signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
        \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
        \    )"
    - name: test_vector_store_operations
      signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
        \ mock_search_client, mock_index_client\n    )"
    - name: mock_search_client
      signature: def mock_search_client(self)
    - name: sample_documents
      signature: def sample_documents(self)
    - name: test_empty_embedding
      signature: def test_empty_embedding(self, vector_store, mock_search_client)
    - name: mock_index_client
      signature: def mock_index_client(self)
- file: tests/integration/vector_stores/test_cosmosdb.py
  functions:
  - node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_customization
    name: test_vector_store_customization
    signature: def test_vector_store_customization()
    docstring: "Test vector store customization with CosmosDB.\n\nArgs:\n    None:\
      \ The function does not accept any parameters.\n\nReturns:\n    None"
  - node_id: tests/integration/vector_stores/test_cosmosdb.py::test_clear
    name: test_clear
    signature: def test_clear()
    docstring: '"""Test clearing the vector store.


      Initializes a CosmosDBVectorStore with index_name "testclear", connects to the
      Cosmos DB instance using WELL_KNOWN_COSMOS_CONNECTION_STRING and database_name
      "testclear", loads a VectorStoreDocument, verifies it can be retrieved by its
      id, clears the store, and asserts that the underlying database no longer exists
      via _database_exists().


      Returns:

      None

      """'
  - node_id: tests/integration/vector_stores/test_cosmosdb.py::mock_embedder
    name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
    docstring: "Return a fixed embedding vector for testing.\n\nArgs:\n    text: Input\
      \ text to embed.\n\nReturns:\n    list[float]: The fixed embedding vector [0.1,\
      \ 0.2, 0.3, 0.4, 0.5].\n\nRaises:\n    None: This function does not raise any\
      \ exceptions."
  - node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_operations
    name: test_vector_store_operations
    signature: def test_vector_store_operations()
    docstring: "Test basic vector store operations with CosmosDB.\n\nArgs:\n    None:\
      \ The function does not accept any parameters.\n\nReturns:\n    None: The test\
      \ does not return a value.\n\nRises:\n    Exception: Exceptions may be raised\
      \ during test execution."
  classes: []
- file: tests/integration/vector_stores/test_factory.py
  functions:
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
    docstring: "Performs a similarity search using the provided text.\n\nArgs:\n \
      \   text (str): The query text to search.\n    text_embedder (Any): The embedder\
      \ used to convert the text into embeddings.\n    k (int): The number of results\
      \ to return. Defaults to 10.\n    **kwargs: Additional keyword arguments passed\
      \ to the underlying search implementation.\n\nReturns:\n    list: A list of\
      \ search results. In this implementation, returns an empty list.\n\nRaises:\n\
      \    None: This function does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_factory.py::test_create_unknown_vector_store
    name: test_create_unknown_vector_store
    signature: def test_create_unknown_vector_store()
    docstring: "Test that creating an unknown vector store type raises a ValueError.\n\
      \nReturns:\n    None\n\nRaises:\n    ValueError: Unknown vector store type:\
      \ unknown"
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.filter_by_id
    name: filter_by_id
    signature: def filter_by_id(self, include_ids)
    docstring: "Filter vector store results by a set of IDs.\n\nArgs:\n    include_ids:\
      \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    Any\
      \ - The filtered results. This implementation returns a dictionary (empty by\
      \ default in the test)."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.connect
    name: connect
    signature: def connect(self, **kwargs)
    docstring: "Connect to the vector store.\n\nThis base implementation is a placeholder/no-op\
      \ and should be overridden by subclasses to establish an actual connection.\n\
      \nArgs:\n    kwargs: Arbitrary keyword arguments.\n\nReturns:\n    None\n\n\
      Raises:\n    None: This base implementation does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_factory.py::test_create_cosmosdb_vector_store
    name: test_create_cosmosdb_vector_store
    signature: def test_create_cosmosdb_vector_store()
    docstring: "Test creating a CosmosDB vector store via the VectorStoreFactory.\n\
      \nReturns:\n    None"
  - node_id: tests/integration/vector_stores/test_factory.py::test_get_vector_store_types
    name: test_get_vector_store_types
    signature: def test_get_vector_store_types()
    docstring: "Verify that VectorStoreFactory.get_vector_store_types returns a collection\
      \ containing the values of built-in vector store types LanceDB, AzureAISearch,\
      \ and CosmosDB.\n\nArgs:\n    None\n\nReturns:\n    List[str] - a collection\
      \ of built-in vector store type values from VectorStoreFactory.get_vector_store_types().\
      \ Note: The test function itself does not return a value; it asserts that the\
      \ expected values are present in the collection.\n\nRaises:\n    None"
  - node_id: tests/integration/vector_stores/test_factory.py::test_register_and_create_custom_vector_store
    name: test_register_and_create_custom_vector_store
    signature: def test_register_and_create_custom_vector_store()
    docstring: '"""Test registering and creating a custom vector store type."""'
  - node_id: tests/integration/vector_stores/test_factory.py::test_create_azure_ai_search_vector_store
    name: test_create_azure_ai_search_vector_store
    signature: def test_create_azure_ai_search_vector_store()
    docstring: "Test creating an Azure AI Search vector store using the VectorStoreFactory.\n\
      \nArgs:\n    None: The test function has no input parameters.\n\nReturns:\n\
      \    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If the created vector_store is not an instance of AzureAISearchVectorStore."
  - node_id: tests/integration/vector_stores/test_factory.py::test_is_supported_type
    name: test_is_supported_type
    signature: def test_is_supported_type()
    docstring: "Test that VectorStoreFactory.is_supported_type returns True for built-in\
      \ vector store type values LanceDB, AzureAISearch, CosmosDB and returns False\
      \ for an unknown type string.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: tests/integration/vector_stores/test_factory.py::test_register_class_directly_works
    name: test_register_class_directly_works
    signature: def test_register_class_directly_works()
    docstring: 'Test that registering a class directly works (VectorStoreFactory allows
      this).


      Args:

      - None: This test has no parameters.


      Returns:

      - None: This test does not return a value.


      Raises:

      - None: This test does not raise any exceptions.'
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
    docstring: "Perform a similarity search by vector and return the top-k results.\n\
      \nArgs:\n  self: The instance of the class.\n  query_embedding: list[float]\
      \ The embedding vector to search with.\n  k: int The number of top results to\
      \ return.\n  **kwargs: Any Additional keyword arguments that may influence the\
      \ search.\n\nReturns:\n  list: The top-k search results. This placeholder implementation\
      \ returns an empty list."
  - node_id: tests/integration/vector_stores/test_factory.py::test_create_lancedb_vector_store
    name: test_create_lancedb_vector_store
    signature: def test_create_lancedb_vector_store()
    docstring: "Test creating a LanceDB vector store via the VectorStoreFactory.\n\
      \nArgs:\n    None: This test takes no input parameters.\n\nReturns:\n    None:\
      \ The test does not return a value.\n\nRaises:\n    AssertionError: If the created\
      \ vector_store is not an instance of LanceDBVectorStore, or if the vector_store's\
      \ index_name does not match the expected value."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Internal API: Initialize the CustomVectorStore by forwarding keyword\
      \ arguments to the base class initializer.\n\nArgs:\n  kwargs: dict of keyword\
      \ arguments forwarded to BaseVectorStore.__init__\n\nReturns:\n  None\n\nRaises:\n\
      \  Propagates exceptions raised by BaseVectorStore.__init__."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.load_documents
    name: load_documents
    signature: def load_documents(self, documents, overwrite=True)
    docstring: "Load documents into the vector store.\n\nArgs:\n  documents (list[VectorStoreDocument]):\
      \ List of VectorStoreDocument objects to load into the vector store.\n  overwrite\
      \ (bool): If True, overwrite existing data in the vector store; otherwise, preserve\
      \ existing data.\n\nReturns:\n  None\n\nNotes:\n  - This base implementation\
      \ is a placeholder and intentionally does nothing. Subclasses should override\
      \ to provide concrete loading behavior.\n  - No input validation is performed\
      \ in this base method.\n  - If documents is empty, the method performs no action.\n\
      \  - Overwrite semantics are intended for the concrete implementation; callers\
      \ should ensure documents meet preconditions (e.g., required fields, IDs) as\
      \ required by the concrete store."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.search_by_id
    name: search_by_id
    signature: def search_by_id(self, id)
    docstring: "\"\"\"\nSearch for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id.\n\"\"\""
  classes:
  - class_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore
    name: CustomVectorStore
    docstring: "Test utility vector store used in tests that forwards initialization\
      \ to the base vector store.\n\nThis class provides a minimal implementation\
      \ of the vector store interface to support tests by forwarding keyword arguments\
      \ to BaseVectorStore.__init__.\n\nArgs:\n  kwargs: dict of keyword arguments\
      \ forwarded to BaseVectorStore.__init__\n\nReturns:\n  None\n\nRaises:\n  Propagates\
      \ exceptions raised by BaseVectorStore.__init__"
    methods:
    - name: similarity_search_by_text
      signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
    - name: filter_by_id
      signature: def filter_by_id(self, include_ids)
    - name: connect
      signature: def connect(self, **kwargs)
    - name: similarity_search_by_vector
      signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
    - name: __init__
      signature: def __init__(self, **kwargs)
    - name: load_documents
      signature: def load_documents(self, documents, overwrite=True)
    - name: search_by_id
      signature: def search_by_id(self, id)
- file: tests/integration/vector_stores/test_lancedb.py
  functions:
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents_categories
    name: sample_documents_categories
    signature: def sample_documents_categories(self)
    docstring: "Create sample documents with different categories for testing.\n\n\
      Args:\n    self: Instance of the test class used by pytest to provide fixture\
      \ context.\n\nReturns:\n    List[VectorStoreDocument]: A list of VectorStoreDocument\
      \ objects with\n        varying category attributes in the attributes dictionary\
      \ to support\n        category-based tests (e.g., animals and vehicles)."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents
    name: sample_documents
    signature: def sample_documents(self)
    docstring: "Create sample documents for testing.\n\nArgs:\n    self: Instance\
      \ of the test class used by pytest to provide fixture context.\n\nReturns:\n\
      \    List[VectorStoreDocument]: A list of three VectorStoreDocument objects\
      \ representing\n        the sample documents with ids \"1\", \"2\", and \"3\"\
      ; texts \"This is document 1\",\n        \"This is document 2\", \"This is document\
      \ 3\"; vectors as [0.1, 0.2, 0.3, 0.4, 0.5],\n        [0.2, 0.3, 0.4, 0.5, 0.6],\
      \ and [0.3, 0.4, 0.5, 0.6, 0.7]; and attributes including\n        \"title\"\
      \ as \"Doc 1\"/\"Doc 2\"/\"Doc 3\" and \"category\" as \"test\" for each.\n\n\
      Raises:\n    None"
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_empty_collection
    name: test_empty_collection
    signature: def test_empty_collection(self)
    docstring: "Test creating an empty LanceDB collection, deleting a loaded document,\
      \ and then adding a new document.\n\nArgs:\n    self: The test case instance.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_customization
    name: test_vector_store_customization
    signature: def test_vector_store_customization(self, sample_documents)
    docstring: "Test vector store customization with LanceDB.\n\nArgs:\n    self:\
      \ The test case instance.\n    sample_documents: list[VectorStoreDocument] -\
      \ Documents used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\
      \nRaises:\n    AssertionError: If any assertion in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_filter_search
    name: test_filter_search
    signature: def test_filter_search(self, sample_documents_categories)
    docstring: "Test filtered search with LanceDB to verify that filtering by document\
      \ IDs correctly constrains the results of a vector similarity search to the\
      \ filtered documents. Key steps: load the sample_documents_categories into the\
      \ vector store, apply filter_by_id(['1','2']), run similarity_search_by_vector\
      \ with a sample vector and k=3, and assert that the results respect the filter\
      \ (at most two results, no document with id '3', and all result ids are within\
      \ {'1','2'}).\n\nArgs:\n  self: The test case instance.\n  sample_documents_categories:\
      \ list[VectorStoreDocument] - Documents loaded into the LanceDB vector store\
      \ for this test.\n\nReturns:\n  None.\n\nRaises:\n  AssertionError: If any assertion\
      \ in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_operations
    name: test_vector_store_operations
    signature: def test_vector_store_operations(self, sample_documents)
    docstring: "Test basic vector store operations with LanceDB.\n\nArgs:\n    self:\
      \ The test case instance.\n    sample_documents: list[VectorStoreDocument] -\
      \ Documents used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\
      \nRaises:\n    AssertionError: If any assertion in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.mock_embedder
    name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
    docstring: "A simple text embedder used for testing that returns a fixed embedding\
      \ vector. The embedding is independent of the input text and always returns\
      \ [0.1, 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text (str): Input text to embed.\n\
      \nReturns:\n    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4,\
      \ 0.5].\n\nRaises:\n    None: This function does not raise any exceptions."
  classes:
  - class_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore
    name: TestLanceDBVectorStore
    docstring: 'Integration tests for LanceDBVectorStore integration.


      Purpose:

      Test the LanceDB-backed vector store implementation (LanceDBVectorStore) by
      exercising common operations such as creating and deleting collections, loading
      documents, performing vector similarity searches, applying filters, and ensuring
      basic vector store functionality works as expected in an integration test context.


      Summary:

      This test class uses sample_documents and sample_documents_categories helpers
      to generate VectorStoreDocument instances and relies on a simple mock_embedder
      that returns a fixed embedding [0.1, 0.2, 0.3, 0.4, 0.5]. It includes tests
      for:

      - test_empty_collection

      - test_vector_store_customization

      - test_filter_search

      - test_vector_store_operations


      Inferred key attributes:

      No explicit instance attributes are documented; the tests rely on the imports
      of VectorStoreDocument and LanceDBVectorStore, as well as the helper methods.'
    methods:
    - name: sample_documents_categories
      signature: def sample_documents_categories(self)
    - name: sample_documents
      signature: def sample_documents(self)
    - name: test_empty_collection
      signature: def test_empty_collection(self)
    - name: test_vector_store_customization
      signature: def test_vector_store_customization(self, sample_documents)
    - name: test_filter_search
      signature: def test_filter_search(self, sample_documents_categories)
    - name: test_vector_store_operations
      signature: def test_vector_store_operations(self, sample_documents)
    - name: mock_embedder
      signature: 'def mock_embedder(text: str) -> list[float]'
- file: tests/mock_provider.py
  functions:
  - node_id: tests/mock_provider.py::MockChatLLM.achat
    name: achat
    signature: "def achat(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    docstring: "Return the next response in the predefined list, cycling through available\
      \ responses using modulo arithmetic. If there are no configured responses, returns\
      \ an empty content response.\n\nArgs:\n    prompt: The input prompt to process.\n\
      \    history: Optional list of previous messages for context.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler.\n\n\
      Returns:\n    ModelResponse: The next response in the predefined sequence. If\
      \ the next item is a BaseModel, it will be used as the response payload. If\
      \ the item is a plain string, it will be wrapped in a response object (the wrapper\
      \ BaseModelResponse) containing that string as content. When no responses are\
      \ configured, a BaseModelResponse with empty content is returned.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying chat logic or input validation."
  - node_id: tests/mock_provider.py::MockChatLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
      \ | None = None,\n        config: LanguageModelConfig | None = None,\n     \
      \   json: bool = False,\n        **kwargs: Any,\n    )"
    docstring: "Initialize a mock chat LLM provider with optional responses and configuration.\n\
      \nArgs:\n    responses: List[str | BaseModel] | None. A list of responses to\
      \ return in sequence. Each item can be a string or a BaseModel.\n    config:\
      \ LanguageModelConfig | None. Optional configuration. If provided and it has\
      \ a.responses attribute, those will be used instead of the responses argument.\n\
      \    json: bool. JSON serialization option (present for compatibility; not used\
      \ by this initializer).\n    kwargs: Any. Additional keyword arguments passed\
      \ to the initializer.\n\nReturns:\n    None. This constructor initializes internal\
      \ state and does not return a value.\n\nRaises:\n    None. This initializer\
      \ does not raise exceptions by itself."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n  \
      \  text_list: A batch of text inputs to generate embeddings for.\n    **kwargs:\
      \ Additional keyword arguments (e.g., model parameters).\n\nReturns:\n    list[list[float]]:\
      \ A batch of embeddings corresponding to the input texts."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n  text_list:\
      \ A batch of input texts to generate embeddings for.\n  **kwargs: Additional\
      \ keyword arguments (e.g., model parameters).\n\nReturns:\n  list[list[float]]:\
      \ A batch of embeddings corresponding to the input texts."
  - node_id: tests/mock_provider.py::MockChatLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Asynchronously stream the configured responses from the mock provider.\n\
      \nThis generator yields each configured response in order. It does not use the\
      \ input\nprompt or history for generation.\n\nArgs:\n    prompt (str): The input\
      \ prompt to process. This implementation ignores it.\n    history (list | None):\
      \ Optional conversation history. This implementation ignores it.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying handler.\n\nReturns:\n\
      \    AsyncGenerator[str, None]: An asynchronous generator yielding response\
      \ strings. If a configured response is a BaseModel, it is converted to JSON\
      \ using model_dump_json(); otherwise the response is yielded as-is.\n\nRaises:\n\
      \    None"
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The\
      \ input text to generate the embedding for.\n    kwargs: Additional keyword\
      \ arguments passed to the embedding model.\n\nReturns:\n    list[float]: A list\
      \ of floating-point numbers representing the embedding.\n\nRaises:\n    This\
      \ function does not raise any exceptions."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The\
      \ input text to generate the embedding for.\n    kwargs: Additional keyword\
      \ arguments passed to the embedding model.\n\nReturns:\n    list[float]: A list\
      \ of floating-point numbers representing the embedding.\n\nRaises:\n    This\
      \ function does not raise any exceptions...."
  - node_id: tests/mock_provider.py::MockChatLLM.chat
    name: chat
    signature: "def chat(\n        self,\n        prompt: str,\n        history: list\
      \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    docstring: "Return the next response in the configured sequence.\n\nThis mock\
      \ chat provider cycles through configured responses and returns them one at\
      \ a time. If no responses are configured, an empty response with content \"\"\
      \ is returned.\n\nArgs:\n    prompt (str): The input prompt to process. The\
      \ mock uses no prompt data to generate the response.\n    history (list | None):\
      \ Optional history for context. Not used by this mock implementation.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler. These\
      \ are ignored by this mock implementation but accepted for compatibility.\n\n\
      Returns:\n    ModelResponse: The next response in the configured list as a BaseModelResponse.\
      \ The response content is accessible via response.output.content, and if the\
      \ stored response was a BaseModel it will be serialized to JSON for the content\
      \ and exposed via parsed_response.\n\nNotes:\n    - The next response is selected\
      \ using a modulo operation on the internal index and then the index is incremented.\n\
      \    - If a response is a Pydantic BaseModel, its JSON representation is used\
      \ as the content (via model_dump_json()). The original BaseModel is exposed\
      \ in parsed_response.\n    - If no responses are configured, the content is\
      \ an empty string \"\" and parsed_response is None."
  - node_id: tests/mock_provider.py::MockChatLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
    docstring: "Not yet implemented: chat_stream for the mock provider.\n\nThis mock\
      \ chat_stream is not implemented and will raise NotImplementedError if called.\
      \ When implemented, it would yield strings from an internal responses list,\
      \ in order, ignoring the prompt and history.\n\nArgs:\n    prompt (str): The\
      \ input prompt to process. This mock ignores the prompt data.\n    history (list\
      \ | None): Optional conversation history. This mock ignores history.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler.\n\n\
      Returns:\n    None\n    Type: None\n\nRaises:\n    NotImplementedError"
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any)'
    docstring: "Initialize a mock embedding LLM provider.\n\nArgs:\n    kwargs: Additional\
      \ keyword arguments (ignored).\n\nReturns:\n    None"
  classes:
  - class_id: tests/mock_provider.py::MockChatLLM
    name: MockChatLLM
    docstring: 'MockChatLLM is a configurable mock chat language model provider used
      for testing. It cycles through a predefined sequence of responses and can emit
      them synchronously via chat() or asynchronously via achat_stream(). It supports
      an optional LanguageModelConfig override and a json flag to indicate that responses
      should be treated as JSON where applicable.


      Args:

      - responses: List[str | BaseModel] | None. A list of responses to return in
      sequence. Each item can be a string or a BaseModel.

      - config: LanguageModelConfig | None. Optional configuration. If provided and
      it has a responses attribute, those will be used instead of the responses argument.

      - json: bool. If True, responses are interpreted as JSON content where applicable
      when constructing outputs.

      - kwargs: Any. Additional keyword arguments forwarded to the underlying chat
      handler.


      Returns:

      - None. Initializes a MockChatLLM instance.


      Raises:

      - NotImplementedError: If chat_stream is called since this streaming interface
      is not yet implemented.


      Notes:

      - If config is provided and exposes a responses attribute, those responses are
      used in place of the explicit responses argument.

      - The non-streaming chat() method returns the next response in the configured
      sequence, cycling through via modulo arithmetic. If no responses are configured,
      an empty content response is returned. The streaming interface for this mock
      is provided via achat_stream(), which yields each configured response in order
      and ignores the input prompt and history. The chat_stream() method is not implemented
      and will raise NotImplementedError if invoked.


      Examples:

      - Basic usage with string responses: create MockChatLLM(responses=["Hi","Hello"])
      and call chat() to retrieve responses in order, cycling when the end is reached.'
    methods:
    - name: achat
      signature: "def achat(\n        self,\n        prompt: str,\n        history:\
        \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    - name: __init__
      signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
        \ | None = None,\n        config: LanguageModelConfig | None = None,\n   \
        \     json: bool = False,\n        **kwargs: Any,\n    )"
    - name: achat_stream
      signature: "def achat_stream(\n        self,\n        prompt: str,\n       \
        \ history: list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str,\
        \ None]"
    - name: chat
      signature: "def chat(\n        self,\n        prompt: str,\n        history:\
        \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    - name: chat_stream
      signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
        \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
  - class_id: tests/mock_provider.py::MockEmbeddingLLM
    name: MockEmbeddingLLM
    docstring: '# TODO: Document this class'
    methods:
    - name: aembed_batch
      signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs:\
        \ Any\n    ) -> list[list[float]]"
    - name: embed_batch
      signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    - name: aembed
      signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    - name: embed
      signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    - name: __init__
      signature: 'def __init__(self, **kwargs: Any)'
- file: tests/notebook/test_notebooks.py
  functions:
  - node_id: tests/notebook/test_notebooks.py::_notebook_run
    name: _notebook_run
    signature: 'def _notebook_run(filepath: Path)'
    docstring: 'Execute a notebook via nbconvert and collect error outputs.


      Args:

      - filepath: Path to the notebook file to execute.


      Returns:

      - list: A list of error outputs collected from the executed notebook cells.


      Raises:

      - subprocess.CalledProcessError: If the nbconvert command fails to execute.'
  - node_id: tests/notebook/test_notebooks.py::test_notebook
    name: test_notebook
    signature: 'def test_notebook(notebook_path: Path)'
    docstring: "Test that a notebook executes without errors.\n\nArgs:\n    notebook_path:\
      \ Path to the notebook file to test.\n\nReturns:\n    None\n\nRaises:\n    subprocess.CalledProcessError:\
      \ If the nbconvert command fails to execute."
  classes: []
- file: tests/smoke/test_fixtures.py
  functions:
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__assert_indexer_outputs
    name: __assert_indexer_outputs
    signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
      \ dict[str, dict[str, Any]]\n    )"
    docstring: "Assert that the indexer outputs conform to the provided workflow configuration.\n\
      \nArgs:\n    self: The instance of the containing class.\n    root: Path to\
      \ the root directory containing the indexer outputs (expects an output subdirectory\
      \ with stats.json).\n    workflow_config: Mapping of workflow names to their\
      \ configuration dictionaries. Each config may include:\n        - expected_artifacts:\
      \ List[str] of artifact files to validate (parquet files).\n        - max_runtime:\
      \ Optional number specifying the maximum allowed runtime for the workflow.\n\
      \        - row_range: List[int] with two elements [min_rows, max_rows] for the\
      \ number of rows in each artifact.\n        - nan_allowed_columns: Optional[List[str]]\
      \ of column names that may contain NaN values.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If the output folder does not exist, if the reported workflows\
      \ do not match the configured ones, if a runtime constraint is violated, or\
      \ if artifact checks fail (row count or NaN values)."
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_indexer
    name: __run_indexer
    signature: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
      \ str,\n    )"
    docstring: "Run the indexer command for the given root and input file type and\
      \ ensure it completes successfully by invoking uv run poe index, including --verbose\
      \ when a debug flag is set.\n\nArgs:\n    root: Path\n        Path to the root\
      \ directory used for indexing.\n    input_file_type: str\n        The input\
      \ file type. This parameter is accepted for interface compatibility but is not\
      \ used in the function body.\n\nReturns:\n    None\n\nRaises:\n    AssertionError\n\
      \        If the indexer finishes with a non-zero return code."
  - node_id: tests/smoke/test_fixtures.py::cleanup
    name: cleanup
    signature: 'def cleanup(skip: bool = False)'
    docstring: 'Decorator to cleanup the output and cache folders after each test.


      Args:

      - skip: bool, optional. If True, skip cleanup of output and cache folders. Default
      is False.


      Returns:

      - A decorator that wraps a test function and performs cleanup after the test.


      Raises:

      - AssertionError: If the wrapped function raises an AssertionError, it is re-raised.'
  - node_id: tests/smoke/test_fixtures.py::prepare_azurite_data
    name: prepare_azurite_data
    signature: 'def prepare_azurite_data(input_path: str, azure: dict) -> Callable[[],
      None]'
    docstring: "Prepare Azurite test data for the fixtures.\n\nThis coroutine uses\
      \ the azure configuration to create or reset a blob storage\ncontainer, uploads\
      \ test data from the local input directory (txt and csv files),\nand returns\
      \ a callable that will delete the container when invoked.\n\nArgs:\n  input_path:\
      \ Path on disk containing test input data. The function looks for an\n    input\
      \ subdirectory with .txt and .csv files to upload.\n  azure: Dictionary with\
      \ Azure/Azurite configuration. Expected keys include:\n    input_container:\
      \ name of the blob container to use\n    input_base_dir: optional base directory\
      \ inside the container for the uploaded files\n\nReturns:\n  A callable with\
      \ no arguments that deletes the blob container when called.\n\nRaises:\n  Exceptions\
      \ raised by BlobPipelineStorage operations or file I/O"
  - node_id: tests/smoke/test_fixtures.py::_load_fixtures
    name: _load_fixtures
    signature: def _load_fixtures()
    docstring: "Load all fixtures from the tests/fixtures directory and return their\
      \ configurations (internal helper).\n\nIf GH_PAGES is set, only the min-csv\
      \ fixture is loaded; otherwise all subdirectories under tests/fixtures are considered.\n\
      \nReturns:\n  list of tuple (str, dict): a list where each item is a pair consisting\
      \ of the subfolder name and the parsed JSON configuration loaded from config.json\
      \ for that fixture. The first entry is omitted in order to disable the azure\
      \ blob connection test.\n\nRaises:\n  FileNotFoundError: If a subfolder is missing\
      \ config.json.\n  json.JSONDecodeError: If config.json cannot be parsed as JSON."
  - node_id: tests/smoke/test_fixtures.py::pytest_generate_tests
    name: pytest_generate_tests
    signature: def pytest_generate_tests(metafunc)
    docstring: 'Generate parameterized tests for all test functions in this module.


      Args:

      metafunc: The pytest metafunc object used to inspect, filter, and parametrize
      tests.


      Returns:

      None


      Raises:

      KeyError: If the expected per-function configuration is missing from metafunc.cls.params
      for the given function name.

      AttributeError: If expected attributes are not present on metafunc (e.g., metafunc.function
      or metafunc.cls).'
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_query
    name: __run_query
    signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
    docstring: "Run a uv run poe query command using the provided root and query_config\
      \ and return the subprocess result.\n\nArgs:\n  root: Path to the root directory\
      \ for the command. The path is resolved to an absolute POSIX string and passed\
      \ to --root.\n  query_config: dict[str, str]. Configuration for the query. Must\
      \ include:\n      method: The value for --method.\n      query: The value for\
      \ --query.\n      community_level: Optional; the value for --community-level.\
      \ If omitted, defaults to 2.\n\nReturns:\n  subprocess.CompletedProcess: The\
      \ result of subprocess.run invoked with capture_output=True and text=True.\n\
      \nRaises:\n  None"
  - node_id: tests/smoke/test_fixtures.py::wrapper
    name: wrapper
    signature: def wrapper(*args, **kwargs)
    docstring: "Wrapper around the wrapped test function that forwards arguments and\
      \ cleans up after execution when not skipped.\n\nArgs:\n    args: Positional\
      \ arguments forwarded to the wrapped function.\n    kwargs: Keyword arguments\
      \ forwarded to the wrapped function; must include input_path used for cleanup.\n\
      \nReturns:\n    Any: The return value of the wrapped function.\n\nRaises:\n\
      \    AssertionError: The wrapped function's AssertionError is re-raised."
  - node_id: tests/smoke/test_fixtures.py::decorator
    name: decorator
    signature: def decorator(func)
    docstring: 'Decorator factory that wraps a test function to forward all positional
      and keyword arguments to the wrapped function and to perform post-execution
      cleanup of test artefacts.


      Parameters:

      - skip (bool): If True, skip performing cleanup after the wrapped function returns.
      Defaults to False.


      Returns:

      - Callable[[Callable[..., Any]], Callable[..., Any]]: A decorator that can be
      applied to a test function. The decorated function will forward all positional
      and keyword arguments to the wrapped function and, after execution, clean up
      the output and cache directories under the root path derived from the input_path
      keyword argument, unless skip is True.


      Notes:

      - The cleanup targets are root/output and root/cache, where root is obtained
      from kwargs["input_path"].

      - input_path must be provided in the call to the decorated function, as it is
      used to determine the cleanup root.

      - All positional and keyword arguments are forwarded to the wrapped function.
      Any AssertionError raised by the wrapped function is propagated.'
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.test_fixture
    name: test_fixture
    signature: "def test_fixture(\n        self,\n        input_path: str,\n     \
      \   input_file_type: str,\n        workflow_config: dict[str, dict[str, Any]],\n\
      \        query_config: list[dict[str, str]],\n    )"
    docstring: "Prepare Azurite test data for the fixtures. This coroutine uses the\
      \ azure configuration to create or reset a blob storage container, uploads test\
      \ data from the local input directory (txt and csv files), and returns a callable\
      \ that will delete the container when invoked.\n\nArgs:\n  input_path: Path\
      \ on disk containing test input data. The function looks for an input subdirectory\
      \ with .txt and .csv files to upload.\n  azure: Dictionary with Azure/Azurite\
      \ configuration. Expected keys include: ...\n\nReturns:\n  Callable[[], None]:\
      \ A callable that will delete the container when invoked.\n\nRaises:\n  Exceptions\
      \ related to Azure/Azurite operations (not specified in the provided data)."
  classes:
  - class_id: tests/smoke/test_fixtures.py::TestIndexer
    name: TestIndexer
    docstring: 'TestIndexer is a test helper that coordinates setup, execution, and
      validation of smoke-test fixtures for the indexer and its queries. It encapsulates
      helpers to run the indexer, issue queries, verify indexer outputs against a
      workflow configuration, and prepare blob-storage backed test data for fixtures
      used by the test suite. The class orchestrates test preparation, execution,
      and validation within the tests/smoke/test_fixtures.py module.


      Attributes:

      - The class does not expose explicit attributes in the excerpt; it relies on
      internal helpers and runtime parameters to perform operations.


      Methods:

      __assert_indexer_outputs(self, root: Path, workflow_config: dict[str, dict[str,
      Any]]) -> None

      - Asserts that the indexer outputs under root conform to the provided workflow_config.

      - Returns: None

      - Raises: AssertionError if outputs do not match; ValueError for invalid inputs.


      __run_indexer(self, root: Path, input_file_type: str) -> subprocess.CompletedProcess

      - Runs the indexer command for the given root and input_file_type and returns
      the subprocess result.

      - Returns: CompletedProcess representing the executed command.

      - Raises: subprocess.CalledProcessError if the command fails.


      __run_query(self, root: Path, query_config: dict[str, str]) -> subprocess.CompletedProcess

      - Runs a query against the indexer using the provided root and query_config
      and returns the subprocess result.

      - Returns: CompletedProcess of the query command.

      - Raises: subprocess.CalledProcessError on failure.


      test_fixture(self, input_path: str, input_file_type: str, workflow_config: dict[str,
      dict[str, Any]], query_config: list[dict[str, str]]) -> Callable[[], None]

      - Prepares test fixture data in the blob-storage-like backend used by the tests,
      uploading data from the specified input_path and configuring the test run as
      described by workflow_config and query_config.

      - Returns: A no-argument callable that cleans up the created resources when
      invoked.

      - Raises: ValueError if inputs are invalid; RuntimeError for storage-related
      failures.'
    methods:
    - name: __assert_indexer_outputs
      signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
        \ dict[str, dict[str, Any]]\n    )"
    - name: __run_indexer
      signature: "def __run_indexer(\n        self,\n        root: Path,\n       \
        \ input_file_type: str,\n    )"
    - name: __run_query
      signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
    - name: test_fixture
      signature: "def test_fixture(\n        self,\n        input_path: str,\n   \
        \     input_file_type: str,\n        workflow_config: dict[str, dict[str,\
        \ Any]],\n        query_config: list[dict[str, str]],\n    )"
- file: tests/unit/config/test_config.py
  functions:
  - node_id: tests/unit/config/test_config.py::test_missing_openai_required_api_key
    name: test_missing_openai_required_api_key
    signature: def test_missing_openai_required_api_key() -> None
    docstring: 'Test that missing required API keys for OpenAI models raise ValidationError.


      This test builds a model configuration lacking API keys for OpenAIChat and asserts
      that create_graphrag_config raises ValidationError. It then changes the chat
      model type to OpenAIEmbedding and asserts that a ValidationError is raised again,
      this time due to a missing API key for the embedding model.'
  - node_id: tests/unit/config/test_config.py::test_missing_azure_api_key
    name: test_missing_azure_api_key
    signature: def test_missing_azure_api_key() -> None
    docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
      \ is configured with APIKey authentication but no API key is provided, and that\
      \ switching to AzureManagedIdentity does not raise an error.\n\nReturns:\n \
      \   None\n\nRaises:\n    pydantic.ValidationError: If the model configuration\
      \ is invalid (e.g., missing API key for an Azure OpenAI Chat model with APIKey\
      \ authentication)."
  - node_id: tests/unit/config/test_config.py::test_conflicting_auth_type
    name: test_conflicting_auth_type
    signature: def test_conflicting_auth_type() -> None
    docstring: "Test that a conflicting authentication type raises ValidationError\
      \ when a model configuration specifies AzureManagedIdentity for an OpenAIChat\
      \ model.\n\nReturns:\n    None\n\nRaises:\n    ValidationError: If the models\
      \ configuration contains a conflicting auth_type."
  - node_id: tests/unit/config/test_config.py::test_conflicting_azure_api_key
    name: test_conflicting_azure_api_key
    signature: def test_conflicting_azure_api_key() -> None
    docstring: "Test that configuring an Azure OpenAI Chat model with Azure Managed\
      \ Identity and an API key raises a ValidationError.\n\nReturns:\n    None\n\n\
      Raises:\n    ValidationError: If the models configuration includes an api_key\
      \ while auth_type is AzureManagedIdentity for an Azure OpenAI Chat model, making\
      \ the config invalid."
  - node_id: tests/unit/config/test_config.py::test_missing_azure_api_base
    name: test_missing_azure_api_base
    signature: def test_missing_azure_api_base() -> None
    docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
      \ configuration is missing the required api_base field.\n\nParameters:\n   \
      \ None\n\nReturns:\n    None\n\nRaises:\n    pydantic.ValidationError: If the\
      \ model configuration is invalid due to a missing api_base in an Azure OpenAI\
      \ Chat model."
  - node_id: tests/unit/config/test_config.py::test_missing_azure_api_version
    name: test_missing_azure_api_version
    signature: def test_missing_azure_api_version() -> None
    docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
      \ configuration is missing the required api_version field.\n\nArgs:\n    None:\
      \ This test does not take any parameters.\n\nReturns:\n    None: This test does\
      \ not return a value.\n\nRaises:\n    pydantic.ValidationError: If the model\
      \ configuration is invalid due to a missing api_version in an Azure OpenAI Chat\
      \ model."
  - node_id: tests/unit/config/test_config.py::test_default_config
    name: test_default_config
    signature: def test_default_config() -> None
    docstring: "\"\"\"Test that the default Graphrag configuration is created as expected.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If the actual Graphrag\
      \ configuration does not match the expected configuration.\n    ValidationError:\
      \ If the input configuration dictionary cannot be validated by pydantic when\
      \ creating the config.\n\"\"\""
  - node_id: tests/unit/config/test_config.py::test_load_minimal_config
    name: test_load_minimal_config
    signature: def test_load_minimal_config() -> None
    docstring: "Test loading a minimal Graphrag configuration and verify it matches\
      \ the expected default Graphrag configuration for the given root directory.\n\
      \nReturns:\n    None"
  - node_id: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
    name: test_load_config_with_cli_overrides
    signature: def test_load_config_with_cli_overrides() -> None
    docstring: 'Test that load_config applies CLI overrides for the output base directory
      when an environment variable is provided.


      The test patches the environment to include CUSTOM_API_KEY, derives the root_dir
      from the minimal_config fixture, overrides the output base directory via cli_overrides,
      loads the configuration, and asserts that the resulting GraphRagConfig matches
      the expected configuration with the overridden base directory.'
  - node_id: tests/unit/config/test_config.py::test_load_config_missing_env_vars
    name: test_load_config_missing_env_vars
    signature: def test_load_config_missing_env_vars() -> None
    docstring: "Load configuration from a file and environment variables.\n\nNote:\
      \ Loading may depend on environment variables (for example API keys) in addition\
      \ to the configuration file.\n\nArgs:\n    root_dir (Path): The root directory\
      \ to search for the config file.\n    config_filepath (Path | None): The path\
      \ to the config file. If None, searches for the config file in root_dir.\n \
      \   cli_overrides (dict[str, Any] | None): Flat dictionary of CLI overrides.\
      \ Example: {'output.base_dir': 'override_value'}. Overrides are applied after\
      \ loading the base configuration.\n\nReturns:\n    GraphRagConfig: The loaded\
      \ configuration.\n\nRaises:\n    FileNotFoundError: If the config file cannot\
      \ be found.\n    KeyError: If a required environment variable is missing.\n\
      \    ValidationError: If the loaded configuration fails validation (e.g., invalid\
      \ structure or types)."
  classes: []
- file: tests/unit/config/utils.py
  functions:
  - node_id: tests/unit/config/utils.py::assert_prune_graph_configs
    name: assert_prune_graph_configs
    signature: "def assert_prune_graph_configs(\n    actual: PruneGraphConfig, expected:\
      \ PruneGraphConfig\n) -> None"
    docstring: "Asserts that actual and expected PruneGraphConfig have equal values\
      \ for the prune-related fields: min_node_freq, max_node_freq_std, min_node_degree,\
      \ max_node_degree_std, min_edge_weight_pct, remove_ego_nodes, and lcc_only.\n\
      \nArgs:\n    actual: PruneGraphConfig\n        The actual prune graph configuration\
      \ to validate.\n    expected: PruneGraphConfig\n        The expected prune graph\
      \ configuration to compare against.\n\nReturns:\n    None\n        This function\
      \ does not return a value. It raises AssertionError if any of the\n        prune\
      \ graph configuration fields differ between actual and expected.\n\nRaises:\n\
      \    AssertionError\n        If any of the asserted fields differ between actual\
      \ and expected."
  - node_id: tests/unit/config/utils.py::assert_extract_claims_configs
    name: assert_extract_claims_configs
    signature: "def assert_extract_claims_configs(\n    actual: ClaimExtractionConfig,\
      \ expected: ClaimExtractionConfig\n) -> None"
    docstring: "\"\"\"Assert that the actual and expected ClaimExtractionConfig instances\
      \ are equal.\n\nThis helper asserts that the fields enabled, prompt, description,\
      \ max_gleanings, strategy, and model_id on actual match those on expected.\n\
      \nArgs:\n    actual: The actual ClaimExtractionConfig instance produced by the\
      \ code under test.\n    expected: The expected ClaimExtractionConfig instance\
      \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If any of the compared fields differ between actual and expected.\n\"\"\""
  - node_id: tests/unit/config/utils.py::assert_community_reports_configs
    name: assert_community_reports_configs
    signature: "def assert_community_reports_configs(\n    actual: CommunityReportsConfig,\
      \ expected: CommunityReportsConfig\n) -> None"
    docstring: "Assert that two CommunityReportsConfig objects have the same values\
      \ for their configuration fields.\n\nArgs:\n    actual: The actual CommunityReportsConfig\
      \ instance.\n    expected: The expected CommunityReportsConfig instance.\n\n\
      Returns:\n    None\n\nRaises:\n    AssertionError: If graph_prompt, text_prompt,\
      \ max_length, max_input_length, strategy, or model_id differ between actual\
      \ and expected."
  - node_id: tests/unit/config/utils.py::assert_chunking_configs
    name: assert_chunking_configs
    signature: 'def assert_chunking_configs(actual: ChunkingConfig, expected: ChunkingConfig)
      -> None'
    docstring: "Assert that two ChunkingConfig objects have equal values for the configured\
      \ fields.\n\nArgs:\n    actual: ChunkingConfig to compare against expected.\n\
      \    expected: ChunkingConfig containing the expected values.\n\nReturns:\n\
      \    None\n\nRaises:\n    AssertionError: If any of the checked fields do not\
      \ match: size, overlap, group_by_columns, strategy, encoding_model, prepend_metadata,\
      \ chunk_size_includes_metadata."
  - node_id: tests/unit/config/utils.py::assert_cache_configs
    name: assert_cache_configs
    signature: 'def assert_cache_configs(actual: CacheConfig, expected: CacheConfig)
      -> None'
    docstring: "Assert that two CacheConfig objects have identical field values.\n\
      \nArgs:\n    actual: The actual CacheConfig to validate.\n    expected: The\
      \ expected CacheConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If any of the fields differ: type, base_dir, connection_string,\
      \ container_name, storage_account_blob_url, cosmosdb_account_url."
  - node_id: tests/unit/config/utils.py::assert_global_search_configs
    name: assert_global_search_configs
    signature: "def assert_global_search_configs(\n    actual: GlobalSearchConfig,\
      \ expected: GlobalSearchConfig\n) -> None"
    docstring: "Assert that actual and expected GlobalSearchConfig objects have equal\
      \ configuration values.\n\nThe function asserts equality for the following fields:\
      \ map_prompt, reduce_prompt, knowledge_prompt, max_context_tokens, data_max_tokens,\
      \ map_max_length, reduce_max_length, dynamic_search_threshold, dynamic_search_keep_parent,\
      \ dynamic_search_num_repeats, dynamic_search_use_summary, dynamic_search_max_level.\n\
      \nArgs:\n    actual: GlobalSearchConfig - The actual configuration to validate.\n\
      \    expected: GlobalSearchConfig - The expected configuration to compare against.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError - If the configurations\
      \ do not match."
  - node_id: tests/unit/config/utils.py::assert_text_analyzer_configs
    name: assert_text_analyzer_configs
    signature: "def assert_text_analyzer_configs(\n    actual: TextAnalyzerConfig,\
      \ expected: TextAnalyzerConfig\n) -> None"
    docstring: "Assert that two TextAnalyzerConfig objects are equal for all relevant\
      \ fields.\n\nArgs:\n    actual: TextAnalyzerConfig - The actual configuration\
      \ to validate.\n    expected: TextAnalyzerConfig - The expected configuration\
      \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError -\
      \ If any of the compared fields do not match."
  - node_id: tests/unit/config/utils.py::assert_language_model_configs
    name: assert_language_model_configs
    signature: "def assert_language_model_configs(\n    actual: LanguageModelConfig,\
      \ expected: LanguageModelConfig\n) -> None"
    docstring: "Assert that actual and expected LanguageModelConfig objects have equivalent\
      \ field values, including optional responses when present.\n\nArgs:\n    actual:\
      \ LanguageModelConfig instance containing the actual configuration to validate.\n\
      \    expected: LanguageModelConfig instance containing the expected configuration\
      \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If any corresponding field differs between actual and expected."
  - node_id: tests/unit/config/utils.py::assert_reporting_configs
    name: assert_reporting_configs
    signature: "def assert_reporting_configs(\n    actual: ReportingConfig, expected:\
      \ ReportingConfig\n) -> None"
    docstring: 'Assert that two ReportingConfig objects have identical field values.


      Parameters:

      - actual: ReportingConfig - The actual ReportingConfig instance.

      - expected: ReportingConfig - The expected ReportingConfig instance to compare
      against.


      Returns:

      - None


      Raises:

      - AssertionError: If any of the fields differ between actual and expected.'
  - node_id: tests/unit/config/utils.py::assert_cluster_graph_configs
    name: assert_cluster_graph_configs
    signature: "def assert_cluster_graph_configs(\n    actual: ClusterGraphConfig,\
      \ expected: ClusterGraphConfig\n) -> None"
    docstring: "Assert that actual and expected ClusterGraphConfig objects are equal\
      \ for cluster graph settings.\n\nArgs:\n    actual: ClusterGraphConfig\n   \
      \     The actual cluster graph configuration to validate.\n    expected: ClusterGraphConfig\n\
      \        The expected cluster graph configuration to validate.\n\nReturns:\n\
      \    None\n        The function does not return a value. It will raise AssertionError\
      \ if the compared fields differ.\n\nRaises:\n    AssertionError\n        If\
      \ actual and expected values for max_cluster_size, use_lcc, or seed differ."
  - node_id: tests/unit/config/utils.py::assert_output_configs
    name: assert_output_configs
    signature: 'def assert_output_configs(actual: StorageConfig, expected: StorageConfig)
      -> None'
    docstring: "Assert that two StorageConfig objects have identical field values.\n\
      \nArgs:\n    actual: The actual StorageConfig to validate.\n    expected: The\
      \ expected StorageConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If any of the fields differ: type, base_dir, connection_string,\
      \ container_name, storage_account_blob_url, cosmosdb_account_url."
  - node_id: tests/unit/config/utils.py::assert_drift_search_configs
    name: assert_drift_search_configs
    signature: "def assert_drift_search_configs(\n    actual: DRIFTSearchConfig, expected:\
      \ DRIFTSearchConfig\n) -> None"
    docstring: "Assert that two DRIFTSearchConfig objects have equal drift-search\
      \ configuration values.\n\nArgs:\n    actual: The actual DRIFTSearchConfig to\
      \ validate.\n    expected: The expected DRIFTSearchConfig to compare against.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any corresponding fields\
      \ differ between actual and expected."
  - node_id: tests/unit/config/utils.py::assert_update_output_configs
    name: assert_update_output_configs
    signature: "def assert_update_output_configs(\n    actual: StorageConfig, expected:\
      \ StorageConfig\n) -> None"
    docstring: "Assert that two StorageConfig objects have identical field values\
      \ for update output configurations.\n\nArgs:\n    actual: The actual StorageConfig\
      \ to validate.\n    expected: The expected StorageConfig to compare against.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any of the fields differ:\
      \ type, base_dir, connection_string, container_name, storage_account_blob_url,\
      \ cosmosdb_account_url...."
  - node_id: tests/unit/config/utils.py::assert_input_configs
    name: assert_input_configs
    signature: 'def assert_input_configs(actual: InputConfig, expected: InputConfig)
      -> None'
    docstring: "Assert that two InputConfig objects have identical field values.\n\
      \nArgs:\n    actual: The actual InputConfig to validate.\n    expected: The\
      \ expected InputConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If any of the fields differ: storage.type, file_type, storage.base_dir,\
      \ storage.connection_string, storage.storage_account_blob_url, storage.container_name,\
      \ encoding, file_pattern, file_filter, text_column, title_column, metadata."
  - node_id: tests/unit/config/utils.py::assert_vector_store_configs
    name: assert_vector_store_configs
    signature: "def assert_vector_store_configs(\n    actual: dict[str, VectorStoreConfig],\n\
      \    expected: dict[str, VectorStoreConfig],\n)"
    docstring: "Assert that two dictionaries of VectorStoreConfig objects are equal.\n\
      \nArgs:\n    actual: dict[str, VectorStoreConfig]\n        Actual mapping of\
      \ vector store names to VectorStoreConfig objects to validate.\n    expected:\
      \ dict[str, VectorStoreConfig]\n        Expected mapping of vector store names\
      \ to VectorStoreConfig objects.\n\nReturns:\n    None\n        This function\
      \ does not return a value; it raises AssertionError on mismatches.\n\nRaises:\n\
      \    AssertionError\n        If actual and expected do not match in type, length,\
      \ keys, or any VectorStoreConfig attributes\n        (type, db_uri, url, api_key,\
      \ audience, container_name, overwrite, database_name)."
  - node_id: tests/unit/config/utils.py::assert_extract_graph_configs
    name: assert_extract_graph_configs
    signature: "def assert_extract_graph_configs(\n    actual: ExtractGraphConfig,\
      \ expected: ExtractGraphConfig\n) -> None"
    docstring: "Assert that the actual and expected ExtractGraphConfig instances have\
      \ equal values for their core fields.\n\nArgs:\n    actual: The actual ExtractGraphConfig\
      \ instance produced by the code under test.\n    expected: The expected ExtractGraphConfig\
      \ instance to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If any of the fields prompt, entity_types, max_gleanings, strategy, or model_id\
      \ differ between actual and expected."
  - node_id: tests/unit/config/utils.py::assert_umap_configs
    name: assert_umap_configs
    signature: 'def assert_umap_configs(actual: UmapConfig, expected: UmapConfig)
      -> None'
    docstring: "Assert that the enabled attribute of two UmapConfig objects matches.\
      \ Only the enabled attribute is checked; other fields are not compared.\n\n\
      Note: If full equivalence is intended, align the implementation or docstring\
      \ accordingly.\n\nParameters:\n    actual (UmapConfig): The actual UmapConfig\
      \ to validate.\n    expected (UmapConfig): The expected UmapConfig to compare\
      \ against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: if actual.enabled\
      \ != expected.enabled"
  - node_id: tests/unit/config/utils.py::assert_snapshots_configs
    name: assert_snapshots_configs
    signature: "def assert_snapshots_configs(\n    actual: SnapshotsConfig, expected:\
      \ SnapshotsConfig\n) -> None"
    docstring: "Assert that two SnapshotsConfig objects have equal embeddings and\
      \ graphml configurations.\n\nArgs:\n    actual: The actual SnapshotsConfig instance.\n\
      \    expected: The expected SnapshotsConfig instance.\n\nReturns:\n    None\n\
      \nRaises:\n    AssertionError: If actual.embeddings != expected.embeddings or\
      \ actual.graphml != expected.graphml."
  - node_id: tests/unit/config/utils.py::assert_basic_search_configs
    name: assert_basic_search_configs
    signature: "def assert_basic_search_configs(\n    actual: BasicSearchConfig, expected:\
      \ BasicSearchConfig\n) -> None"
    docstring: "Assert that two BasicSearchConfig objects have equal prompt and k\
      \ values.\n\nArgs:\n    actual: BasicSearchConfig to compare against expected\n\
      \    expected: BasicSearchConfig to compare with actual\n\nReturns:\n    None\n\
      \nRaises:\n    AssertionError: If actual.prompt != expected.prompt or actual.k\
      \ != expected.k"
  - node_id: tests/unit/config/utils.py::assert_embed_graph_configs
    name: assert_embed_graph_configs
    signature: "def assert_embed_graph_configs(\n    actual: EmbedGraphConfig, expected:\
      \ EmbedGraphConfig\n) -> None"
    docstring: "Assert that actual and expected EmbedGraphConfig instances have equal\
      \ values for their core fields.\n\nArgs:\n    actual: EmbedGraphConfig. The\
      \ actual EmbedGraphConfig instance produced by the code under test.\n    expected:\
      \ EmbedGraphConfig. The expected EmbedGraphConfig instance to compare against.\n\
      \nReturns:\n    None. The function does not return a value.\n\nRaises:\n   \
      \ AssertionError: If any of the fields enabled, dimensions, num_walks, walk_length,\
      \ window_size, iterations, random_seed, or use_lcc differ between actual and\
      \ expected."
  - node_id: tests/unit/config/utils.py::assert_local_search_configs
    name: assert_local_search_configs
    signature: "def assert_local_search_configs(\n    actual: LocalSearchConfig, expected:\
      \ LocalSearchConfig\n) -> None"
    docstring: "Assert that two LocalSearchConfig objects have equal local search\
      \ configuration values.\n\nArgs:\n    actual: LocalSearchConfig to compare against\
      \ expected.\n    expected: LocalSearchConfig to compare with actual.\n\nReturns:\n\
      \    None\n\nRaises:\n    AssertionError: If any corresponding fields differ\
      \ between actual and expected."
  - node_id: tests/unit/config/utils.py::assert_summarize_descriptions_configs
    name: assert_summarize_descriptions_configs
    signature: "def assert_summarize_descriptions_configs(\n    actual: SummarizeDescriptionsConfig,\
      \ expected: SummarizeDescriptionsConfig\n) -> None"
    docstring: "\"\"\"Assert that two SummarizeDescriptionsConfig objects have identical\
      \ fields: prompt, max_length, strategy, and model_id.\n\nArgs:\n    actual (SummarizeDescriptionsConfig):\
      \ The actual config to compare.\n    expected (SummarizeDescriptionsConfig):\
      \ The expected config to compare against.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If any of the compared fields differ between actual and\
      \ expected.\n\"\"\""
  - node_id: tests/unit/config/utils.py::get_default_graphrag_config
    name: get_default_graphrag_config
    signature: 'def get_default_graphrag_config(root_dir: str | None = None) -> GraphRagConfig'
    docstring: "Return a GraphRagConfig instance configured with the default Graphrag\
      \ settings.\n\nArgs:\n    root_dir: Optional[str] - root directory to include\
      \ in the returned GraphRagConfig. If None, the root_dir key is not set.\n\n\
      Returns:\n    GraphRagConfig: The GraphRagConfig created by merging graphrag_config_defaults\
      \ with DEFAULT_MODEL_CONFIG, and including root_dir when provided."
  - node_id: tests/unit/config/utils.py::assert_text_embedding_configs
    name: assert_text_embedding_configs
    signature: "def assert_text_embedding_configs(\n    actual: TextEmbeddingConfig,\
      \ expected: TextEmbeddingConfig\n) -> None"
    docstring: "Assert that two TextEmbeddingConfig objects are equal for all relevant\
      \ fields.\n\nArgs:\n    actual: TextEmbeddingConfig - The actual configuration\
      \ to validate.\n    expected: TextEmbeddingConfig - The expected configuration\
      \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError -\
      \ If any of the compared fields do not match."
  - node_id: tests/unit/config/utils.py::assert_extract_graph_nlp_configs
    name: assert_extract_graph_nlp_configs
    signature: "def assert_extract_graph_nlp_configs(\n    actual: ExtractGraphNLPConfig,\
      \ expected: ExtractGraphNLPConfig\n) -> None"
    docstring: "Assert that two ExtractGraphNLPConfig objects are equal for all relevant\
      \ fields.\n\nArgs:\n    actual: ExtractGraphNLPConfig - The actual configuration\
      \ to validate.\n    expected: ExtractGraphNLPConfig - The expected configuration\
      \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError -\
      \ If any of the compared fields do not match...."
  - node_id: tests/unit/config/utils.py::assert_graphrag_configs
    name: assert_graphrag_configs
    signature: 'def assert_graphrag_configs(actual: GraphRagConfig, expected: GraphRagConfig)
      -> None'
    docstring: "Assert that two GraphRagConfig objects are equivalent by validating\
      \ all nested configurations match.\n\nArgs:\n    actual: GraphRagConfig - The\
      \ actual GraphRagConfig instance to validate.\n    expected: GraphRagConfig\
      \ - The expected GraphRagConfig instance to compare against.\n\nReturns:\n \
      \   None\n\nRaises:\n    AssertionError: If any of the fields differ between\
      \ actual and expected."
  classes: []
- file: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py
  functions:
  - node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
    name: test_sort_context
    signature: def test_sort_context()
    docstring: "Test that sort_context returns a non-null context and that the token\
      \ count matches platform-dependent expectations.\n\nReturns:\n    None. This\
      \ is a unit test function and does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion fails."
  - node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
    name: test_sort_context_max_tokens
    signature: def test_sort_context_max_tokens()
    docstring: "Test that sort_context respects the max_context_tokens constraint\
      \ by returning a non-null context whose token count is less than or equal to\
      \ the specified maximum.\n\nParameters:\n    None: This test has no input parameters.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails."
  classes: []
- file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  functions:
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph
    name: _create_strongly_connected_graph
    signature: def _create_strongly_connected_graph(self, digraph=False)
    docstring: "Create and return a test graph with a linear chain of edges and node\
      \ attributes.\n\nCreates an undirected Graph by default; if digraph is True,\
      \ a DiGraph is created. Adds nodes \"1\", \"2\", \"3\", \"4\" with node_name\
      \ attributes 1, 2, 3, 4, and adds edges (\"4\",\"5\") with degree=4, (\"3\"\
      ,\"4\") with degree=3, (\"2\",\"3\") with degree=2, and (\"1\",\"2\") with degree=1.\n\
      \nArgs:\n    self: The instance of the test class.\n    digraph: bool, whether\
      \ to create a directed graph (DiGraph) instead of an undirected Graph.\n\nReturns:\n\
      \    graph: networkx.Graph or networkx.DiGraph, created according to the digraph\
      \ flag."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph_with_edges_flipped
    name: _create_strongly_connected_graph_with_edges_flipped
    signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
    docstring: "Create and return a test graph used for stability tests of the largest\
      \ connected component. This variant builds a five-node graph where the first\
      \ edge introduces the new node 5 (edge 5-4). The remaining edges form a path:\
      \ 4-3, 3-2, 2-1, with edge attributes degree=4, degree=3, degree=2, and degree=1\
      \ respectively. If digraph is False, an undirected Graph is created; if digraph\
      \ is True, a directed DiGraph is created and the edges are oriented accordingly.\
      \ The function name indicates the first edge's orientation is flipped relative\
      \ to the non-flipped version (which uses 4-5). The graph contains five nodes\
      \ labeled 1 through 5; nodes 1-4 have node_name attributes 1-4, and node 5 is\
      \ introduced by the first edge.\n\nArgs:\n    self: The instance of the test\
      \ class. Type: unittest.TestCase\n    digraph (bool): If True, create a directed\
      \ graph (nx.DiGraph); otherwise an undirected graph (nx.Graph).\n\nReturns:\n\
      \    graph (networkx.Graph or networkx.DiGraph): The constructed graph.\n\n\
      Raises:\n    None"
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
    name: test_undirected_graph_run_twice_produces_same_graph
    signature: def test_undirected_graph_run_twice_produces_same_graph(self)
    docstring: "Verify that running stable_largest_connected_component on an undirected\
      \ graph twice yields the same graph, even if the input edges are flipped.\n\n\
      Args:\n    self (TestStableLCC): Instance of the test class.\n\nReturns:\n \
      \   None: This test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If the graphs produced by stable_largest_connected_component differ."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
    name: test_directed_graph_keeps_source_target_intact
    signature: def test_directed_graph_keeps_source_target_intact(self)
    docstring: "Test that a directed graph keeps source and target intact when computing\
      \ the stable largest connected component.\n\nArgs:\n    self (TestStableLCC):\
      \ Instance of the test class.\n\nReturns:\n    None: This test does not return\
      \ a value.\n\nRaises:\n    AssertionError: If the directed edges in the input\
      \ and output graphs differ, indicating the edge directions were not preserved."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
    name: test_directed_graph_run_twice_produces_same_graph
    signature: def test_directed_graph_run_twice_produces_same_graph(self)
    docstring: "Test that running stable_largest_connected_component on a directed\
      \ graph twice yields the same graph.\n\nArgs:\n    self (TestStableLCC): Instance\
      \ of the test class.\n\nReturns:\n    None: This test does not return a value.\n\
      \nRaises:\n    AssertionError: If the graphs produced by stable_largest_connected_component\
      \ differ between runs."
  classes:
  - class_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC
    name: TestStableLCC
    docstring: 'TestStableLCC is a unit test suite for validating the stability and
      correctness of the stable_largest_connected_component function across both undirected
      and directed graphs.


      Purpose:

      - Verify determinism: ensure stable_largest_connected_component returns identical
      graphs on repeated runs, even when input edges are flipped.

      - Preserve directed relationships: ensure that, for DiGraph inputs, the source
      and target directions are preserved in the resulting components.

      - Validate consistency across graph types: cover both undirected and directed
      graphs.


      Key helpers:

      - _create_strongly_connected_graph(digraph=False): helper to construct a representative
      linear chain graph with node attributes; by default undirected Graph, DiGraph
      when digraph=True.

      - _create_strongly_connected_graph_with_edges_flipped(digraph=False): helper
      to construct a five-node graph used for stability tests; flips edges scenario;
      supports Graph or DiGraph.


      Tests provided:

      - test_undirected_graph_run_twice_produces_same_graph

      - test_directed_graph_keeps_source_target_intact

      - test_directed_graph_run_twice_produces_same_graph


      Note: This docstring focuses on high-level intent and test coverage; it omits
      implementation details.'
    methods:
    - name: _create_strongly_connected_graph
      signature: def _create_strongly_connected_graph(self, digraph=False)
    - name: _create_strongly_connected_graph_with_edges_flipped
      signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
    - name: test_undirected_graph_run_twice_produces_same_graph
      signature: def test_undirected_graph_run_twice_produces_same_graph(self)
    - name: test_directed_graph_keeps_source_target_intact
      signature: def test_directed_graph_keeps_source_target_intact(self)
    - name: test_directed_graph_run_twice_produces_same_graph
      signature: def test_directed_graph_run_twice_produces_same_graph(self)
- file: tests/unit/indexing/input/test_csv_loader.py
  functions:
  - node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
    name: test_csv_loader_one_file
    signature: def test_csv_loader_one_file()
    docstring: "Test loading a single CSV file using the input loader and verify the\
      \ resulting DataFrame.\n\nThis test constructs an InputConfig configured for\
      \ CSV files in a single directory, creates storage via create_storage_from_config,\
      \ loads documents with create_input, and asserts:\n- the resulting DataFrame\
      \ has shape (2, 4)\n- the first row in the title column is \"input.csv\"\n\n\
      Returns:\n  None. This test does not return a value.\n\nRaises:\n  ValueError:\
      \ If the storage type is not registered in create_storage_from_config."
  - node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
    name: test_csv_loader_one_file_with_title
    signature: def test_csv_loader_one_file_with_title()
    docstring: "Asynchronous test that loads a single CSV file with a title column\
      \ using the input loader.\n\nConfigures InputConfig to read CSV files from tests/unit/indexing/input/data/one-csv\
      \ with a file_pattern that matches CSVs and sets title_column to \"title\".\
      \ It loads documents via create_input and asserts that the resulting DataFrame\
      \ has shape (2, 4) and that the first row in the title column is \"Hello\".\n\
      \nReturns:\n    None"
  - node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
    name: test_csv_loader_one_file_with_metadata
    signature: def test_csv_loader_one_file_with_metadata()
    docstring: "Async test that loads a single CSV file with metadata and validates\
      \ the loaded DataFrame and metadata content.\n\nArgs:\n    None: This test does\
      \ not take any parameters.\n\nReturns:\n    None: The test does not return a\
      \ value; it performs assertions to verify behavior.\n\nRaises:\n    AssertionError:\
      \ If the loaded DataFrame shape or metadata content does not match the expected\
      \ values."
  - node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
    name: test_csv_loader_multiple_files
    signature: def test_csv_loader_multiple_files()
    docstring: "Test loading multiple CSV files and verify the resulting DataFrame\
      \ shape.\n\nConfigures an InputConfig to load CSV files from tests/unit/indexing/input/data/multiple-csvs,\
      \ using file_type csv and a pattern that matches CSV files, creates storage\
      \ from the config, loads the documents, and asserts the DataFrame shape is (4,\
      \ 4).\n\nReturns:\n  None"
  classes: []
- file: tests/unit/indexing/input/test_json_loader.py
  functions:
  - node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
    name: test_json_loader_one_file_one_object
    signature: def test_json_loader_one_file_one_object()
    docstring: "Test loading a single JSON file containing one object and validating\
      \ the loaded data.\n\nThis test builds an InputConfig using json input, loads\
      \ documents via create_input, and asserts that the resulting DataFrame has shape\
      \ (1, 4) and that the title field equals \"input.json\".\n\nReturns:\n    None\n\
      \nRaises:\n    AssertionError: If the loaded data does not match the expected\
      \ shape or values."
  - node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
    name: test_json_loader_one_file_multiple_objects
    signature: def test_json_loader_one_file_multiple_objects()
    docstring: "Test loading a JSON file containing multiple objects and verify the\
      \ loaded DataFrame.\n\nArgs:\n    None: This test function does not accept any\
      \ parameters.\n\nReturns:\n    None: The function does not return a value.\n\
      \nRaises:\n    AssertionError: If the loaded data does not match the expected\
      \ shape or values."
  - node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
    name: test_json_loader_one_file_with_title
    signature: def test_json_loader_one_file_with_title()
    docstring: "Test loading a single JSON file with an explicit title column ('title')\
      \ and verify that the resulting DataFrame has shape (1, 4) and the title value\
      \ equals 'Hello'.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the\
      \ loaded data does not match the expected shape or values."
  - node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
    name: test_json_loader_one_file_with_metadata
    signature: def test_json_loader_one_file_with_metadata()
    docstring: "Test loading a single JSON file with metadata and validating that\
      \ the metadata is included in the resulting DataFrame.\n\nArgs:\n    None\n\n\
      Returns:\n    None\n\nRaises:\n    AssertionError: If the loaded data does not\
      \ match the expected shape or metadata values."
  - node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
    name: test_json_loader_multiple_files
    signature: def test_json_loader_multiple_files()
    docstring: "Test loading multiple JSON files from tests/unit/indexing/input/data/multiple-jsons\
      \ and verifying the loaded DataFrame shape (4, 4).\n\nThe test builds an InputConfig\
      \ for JSON input using a file pattern that matches JSON files, creates a storage\
      \ object from the config, loads documents via create_input, and asserts that\
      \ the resulting DataFrame has shape (4, 4). This is a test function and does\
      \ not return any data.\n\nArgs:\n    None: This test function does not accept\
      \ any parameters.\n\nReturns:\n    None: The test does not return a value.\n\
      \nRaises:\n    AssertionError: If the loaded data does not have the expected\
      \ shape (4, 4).\n    ValueError: If storage creation from the config fails."
  classes: []
- file: tests/unit/indexing/input/test_txt_loader.py
  functions:
  - node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
    name: test_txt_loader_one_file
    signature: def test_txt_loader_one_file()
    docstring: "Test loading a single TXT file using the input loader and verify the\
      \ resulting DataFrame shape and title.\n\nArgs:\n    None: This test function\
      \ does not accept any parameters.\n\nReturns:\n    None: This test does not\
      \ return a value.\n\nRaises:\n    None: This test does not raise exceptions\
      \ under normal operation."
  - node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
    name: test_txt_loader_one_file_with_metadata
    signature: def test_txt_loader_one_file_with_metadata()
    docstring: "Test loading a single TXT file with metadata and verify the metadata\
      \ content.\n\nArgs:\n    None: This test does not accept any parameters.\n\n\
      Returns:\n    None: The test does not return a value; it performs assertions\
      \ to verify behavior.\n\nRaises:\n    AssertionError: If the loaded DataFrame\
      \ shape or metadata content does not match the expected values."
  - node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
    name: test_txt_loader_multiple_files
    signature: def test_txt_loader_multiple_files()
    docstring: "Test loading multiple TXT files using the input loader and verify\
      \ the resulting DataFrame shape (2, 4).\n\nArgs:\n    None: This test function\
      \ does not accept any parameters.\n\nReturns:\n    None: This test does not\
      \ return a value.\n\nRaises:\n    None: This test does not raise exceptions\
      \ under normal operation."
  classes: []
- file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  functions:
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_default
    name: test_get_num_total_default
    signature: def test_get_num_total_default()
    docstring: "Compute the total number of elements in a DataFrame column.\n\nArgs:\n\
      \    output: pandas.DataFrame The DataFrame containing the target column.\n\
      \    column: str The name of the column to process.\n\nReturns:\n    int The\
      \ total number of elements in the specified column; strings contribute 1 each,\
      \ non-string entries contribute their length."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_array
    name: test_get_num_total_array
    signature: def test_get_num_total_array()
    docstring: "Compute the total number of elements in a DataFrame column, counting\
      \ strings as a single element and non-string entries by their length.\n\nArgs:\n\
      \    output (pd.DataFrame): The DataFrame containing the target column.\n  \
      \  column (str): The name of the column to process.\n\nReturns:\n    int: The\
      \ total number of elements in the specified column; strings contribute 1 each,\
      \ non-string entries contribute their length."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_str
    name: test_run_strategy_str
    signature: def test_run_strategy_str()
    docstring: 'Run the given chunking strategy on the input data and return the produced
      text chunks as strings.


      Args:

      - strategy_exec (Callable): The strategy function to execute to generate TextChunk
      objects. It should accept (input, config, tick) and return a list of TextChunk-like
      objects that expose a text_chunk attribute.

      - input: str | list[str] | list[tuple[str, str]]: The input data to chunk. May
      be a single string, a list of strings, or a list of (document_id, text) tuples
      depending on the strategy.

      - config: Any: Configuration for chunking, including size, overlap, and encoding
      model. This can be a real configuration object or a mock used in tests.

      - tick: Any: Progress ticker used to report progress.


      Returns:

      - list[str]: The produced text chunks, extracted from each TextChunk as text_chunk,
      in the same order as produced by the strategy.


      Raises:

      - Propagates any exception raised by strategy_exec. If a produced chunk lacks
      a text_chunk attribute, an AttributeError may be raised.


      Examples:

      - run_strategy(my_strategy, "text", cfg, tick) -> ["text"]'
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_str
    name: test_run_strategy_arr_str
    signature: def test_run_strategy_arr_str()
    docstring: "Run the given chunking strategy on the input data and return the produced\
      \ chunks.\n\nArgs:\n    strategy_exec: ChunkStrategy\n        The strategy function\
      \ to execute to generate TextChunk objects. It should accept\n        (input,\
      \ config, tick) and return a list of TextChunk-like objects that expose a\n\
      \        text_chunk attribute.\n    input: ChunkInput\n        The input data\
      \ to chunk. May be a string or a list of strings, or a list of tuples\n    \
      \    of (document_id, text content).\n    config: ChunkingConfig\n        Configuration\
      \ for chunking, including size, overlap, and encoding model.\n    tick: ProgressTicker\n\
      \        Progress ticker used during execution.\n\nReturns:\n    list[str |\
      \ tuple[list[str] | None, str, int]]\n    The produced chunks. Each element\
      \ is either a string (for string-based input) or a tuple\n    of the form (list[str]\
      \ | None, str, int) representing the chunked content, a representative\n   \
      \ text, and the token count for that chunk."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple
    name: test_run_strategy_arr_tuple
    signature: def test_run_strategy_arr_tuple()
    docstring: 'Test run_strategy with input as a list of (text, token) tuples.


      Verifies that when input is [("text test for run strategy", "3"), ("use for
      strategy", "5")] and the strategy returns two TextChunk objects with text_chunk
      values corresponding to the input texts and n_tokens values 5 and 3, run_strategy
      returns a list of tuples where each tuple is: ( [corresponding source texts],
      text_chunk, n_tokens as int ). For the given setup, the expected result is:


      - ( ["text test for run strategy"], "text test for run strategy", 5 )

      - ( ["use for strategy"], "use for strategy", 3 )


      The test uses mocks for config and tick and asserts the produced value matches
      the expected list.'
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple_same_doc
    name: test_run_strategy_arr_tuple_same_doc
    signature: def test_run_strategy_arr_tuple_same_doc()
    docstring: "Run the given chunking strategy on the input data and return the produced\
      \ chunks.\n\nArgs:\n    strategy_exec (ChunkStrategy): The strategy function\
      \ to execute to generate TextChunk-like objects. It should accept (input, config,\
      \ tick) and return a list of objects exposing a text_chunk attribute.\n    input\
      \ (ChunkInput): The input data to chunk. May be a string or a list of strings,\
      \ or a list of tuples (text, token) as used in tests.\n    config (ChunkingConfig):\
      \ Configuration for chunking, including size, overlap, and encoding model.\n\
      \    tick (ProgressTicker): Progress ticker used to report progress.\n\nReturns:\n\
      \    list[str | tuple[list[str] | None, str, int]]: A list of produced chunks.\
      \ Each item is either a string or a tuple where\n        the first element is\
      \ a list of source texts (or None), the second element is the text chunk content,\
      \ and the\n        third element is the number of tokens.\n\nRaises:\n    Propagates\
      \ exceptions raised by strategy_exec or invalid input processing."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_tokens
    name: test_load_strategy_tokens
    signature: def test_load_strategy_tokens()
    docstring: "Load the strategy callable for the given ChunkStrategyType.\n\nArgs:\n\
      \    strategy (ChunkStrategyType): The type of chunk strategy to load. If ChunkStrategyType.tokens,\
      \ the tokens strategy is returned. If ChunkStrategyType.sentence, NLP resources\
      \ are bootstrapped and the sentences strategy is returned.\n\nReturns:\n   \
      \ ChunkStrategy: The loaded strategy callable corresponding to the provided\
      \ strategy type.\n\nRaises:\n    ValueError: If an unknown strategy is provided."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_sentence
    name: test_load_strategy_sentence
    signature: def test_load_strategy_sentence()
    docstring: "Load and return the strategy callable for the given chunking strategy.\n\
      \nArgs:\n    strategy (ChunkStrategyType): The type of chunk strategy to load.\
      \ If ChunkStrategyType.tokens, the tokens strategy is returned. If ChunkStrategyType.sentence,\
      \ NLP resources are bootstrapped and the sentences strategy is returned.\n\n\
      Returns:\n    ChunkStrategy: The loaded strategy callable corresponding to the\
      \ provided strategy type.\n\nRaises:\n    ValueError: If an unknown strategy\
      \ is provided."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_none
    name: test_load_strategy_none
    signature: def test_load_strategy_none()
    docstring: "Load and return the strategy callable for a given chunking strategy\
      \ type.\n\nArgs:\n    strategy (ChunkStrategyType): The type of chunk strategy\
      \ to load. If ChunkStrategyType.tokens, the tokens strategy is returned. If\
      \ ChunkStrategyType.sentence, NLP resources are bootstrapped and the sentences\
      \ strategy is returned.\n\nReturns:\n    ChunkStrategy: The loaded strategy\
      \ callable corresponding to the provided strategy type.\n\nRaises:\n    ValueError:\
      \ If an unknown strategy is provided."
  - node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_chunk_text
    name: test_chunk_text
    signature: def test_chunk_text(mock_progress_ticker, mock_run_strategy, mock_load_strategy)
    docstring: "Chunk a piece of text into smaller pieces.\n\nThis function chunks\
      \ the text contained in the specified DataFrame column into smaller pieces according\
      \ to the given chunking strategy and encoding model. It loads the configured\
      \ chunking strategy, processes the input texts, and reports progress via the\
      \ provided callbacks. The function returns a pandas Series containing the resulting\
      \ chunks.\n\nArgs:\n    input (pd.DataFrame): DataFrame containing the data\
      \ to chunk.\n    column (str): The name of the column containing the text to\
      \ chunk. This can be a column with plain text, or a column with a list/tuple\
      \ of (doc_id, text).\n    size (int): The chunk size in tokens.\n    overlap\
      \ (int): The number of tokens to overlap between adjacent chunks.\n    encoding_model\
      \ (str): The encoding model to use for chunking.\n    strategy (ChunkStrategyType):\
      \ The strategy to use for chunking (e.g., sentence, word). See graphrag.config.enums.ChunkStrategyType\
      \ for supported values.\n    callbacks (WorkflowCallbacks): Object exposing\
      \ progress reporting callbacks (e.g., a progress attribute or method).\n\nReturns:\n\
      \    pd.Series: A Series containing the generated chunks. The exact shape depends\
      \ on the input and strategy.\n\nRaises:\n    ValueError: If an unknown or unsupported\
      \ strategy is provided.\n\nExamples:\n    chunk_text(df, \"text\", size=10,\
      \ overlap=2, encoding_model=\"model\", strategy=ChunkStrategyType.sentence,\
      \ callbacks=my_callbacks)"
  classes: []
- file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  functions:
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
    name: setup_method
    signature: def setup_method(self, method)
    docstring: "Set up the test environment before each test by invoking bootstrap().\n\
      \nArgs:\n    method: object\n        The test method currently being executed.\
      \ Provided by the test framework.\n\nReturns:\n    None\n        Type: None\n\
      \nRaises:\n    ImportError: If the bootstrap process fails during initialization\
      \ of required resources."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
    name: test_basic_functionality
    signature: def test_basic_functionality(self)
    docstring: "Test basic sentence splitting without metadata.\n\nVerifies that a\
      \ single input document containing two sentences is split into two TextChunk\
      \ objects with the expected text_chunk values, and that each chunk references\
      \ the first document (source_doc_indices == [0]). Also ensures the progress\
      \ ticker is invoked exactly once with the value 1.\n\nArgs:\n  self (TestRunSentences):\
      \ The test case instance.\n\nReturns:\n  None\n\nRaises:\n  None\n\nExamples:\n\
      \  Input:\n    [\"This is a test. Another sentence.\"]\n  Expected:\n    - Two\
      \ chunks:\n        - chunks[0].text_chunk == \"This is a test.\"\n        -\
      \ chunks[1].text_chunk == \"Another sentence.\"\n      - All chunks have source_doc_indices\
      \ == [0]\n      - tick.assert_called_once_with(1)\n  Notes:\n    No metadata\
      \ is attached to the produced chunks in this test."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
    name: test_multiple_documents
    signature: def test_multiple_documents(self)
    docstring: "Test processing multiple input documents into separate chunks and\
      \ verify correct chunk origins and progress tick behavior.\n\nThe test provides\
      \ two input documents: \\\"First. Document.\\\" and \\\"Second. Doc.\\\" which\
      \ should be chunked into four sentences (two per document). Each resulting TextChunk\
      \ should have its source_doc_indices set to the index of its originating document\
      \ (the first two chunks originate from document 0, the last two from document\
      \ 1). The test also asserts that the progress tick is invoked once for each\
      \ input document (two total).\n\nArgs:\n    self: The test case instance.\n\n\
      Returns:\n    None\n\nRaises:\n    None"
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
    name: test_mixed_whitespace_handling
    signature: def test_mixed_whitespace_handling(self)
    docstring: "Chunks text into multiple parts by sentence.\n\nArgs:\n  input: list[str]\
      \ - list of input documents to chunk into sentences.\n  _config: ChunkingConfig\
      \ - chunking configuration (unused by this strategy).\n  tick: ProgressTicker\
      \ - progress reporter; invoked to indicate progress after processing each input\
      \ document.\n\nReturns:\n  Iterable[TextChunk] - yields TextChunk objects for\
      \ each sentence, with text_chunk set to the sentence and source_doc_indices\
      \ containing the index of the source document.\n\nRaises:"
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_encode
    name: test_get_encoding_fn_encode
    signature: def test_get_encoding_fn_encode(mock_get_encoding)
    docstring: "Get encoding functions for a given encoding model.\n\nArgs:\n- encoding_name:\
      \ str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
      \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
      \ list[int]] - Encodes input text into token ids using the selected encoding;\
      \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
      \ str] - Decodes a list of token ids back into a string using the selected encoding."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_decode
    name: test_get_encoding_fn_decode
    signature: def test_get_encoding_fn_decode(mock_get_encoding)
    docstring: "Get encoding functions for a given encoding model.\n\nParameters:\n\
      - encoding_name: str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
      \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
      \ list[int]] - Encodes input text into token ids using the selected encoding;\
      \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
      \ str] - Decodes a list of token ids back into a string using the selected encoding.\n\
      \nRaises:\n- Exception: Propagates exceptions raised by the underlying tiktoken.get_encoding\
      \ or encoding object."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
    name: test_basic_functionality
    signature: def test_basic_functionality(self, mock_get_encoding)
    docstring: "Chunks text into chunks based on encoding tokens.\n\nArgs:\n    input:\
      \ list[str] - The input texts to be chunked.\n    config: ChunkingConfig - Chunking\
      \ configuration. Uses: size (number of tokens per chunk), overlap (number of\
      \ overlapping tokens between consecutive chunks), encoding_model (name of the\
      \ encoding model used to tokenize).\n    tick: ProgressTicker - Progress reporter;\
      \ invoked to indicate progress.\n\nReturns:\n    Iterable[TextChunk] - An iterable\
      \ of TextChunk objects.\n\nRaises:\n    None"
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
    name: test_non_string_input
    signature: def test_non_string_input(self, mock_get_encoding)
    docstring: "Test handling of non-string input (e.g., numbers) when tokenizing\
      \ text.\n\nArgs:\n  - self: The test case instance.\n  - mock_get_encoding:\
      \ The patched tiktoken.get_encoding function; a Mock that returns a mock encoder\
      \ used to encode/decode tokens.\n\nReturns:\n  - None. This test does not return\
      \ a value.\n\nRaises:\n  - None. No exceptions are expected during the test\
      \ execution."
  classes:
  - class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences
    name: TestRunSentences
    docstring: 'TestRunSentences validates the sentence-splitting behavior of the
      chunk_text strategies used by the indexing module. The tests focus on observable
      outcomes: input documents are split into sentences, each sentence is emitted
      as a TextChunk with the exact sentence text, and every chunk carries a reference
      to its originating document via source_doc_indices. A progress reporter is exercised
      to indicate per-document processing progress without asserting a specific invocation
      count. Setup initializes required resources by calling bootstrap before tests
      run. The suite examines basic functionality with a single document, handling
      of multiple documents, and mixed/edge whitespace scenarios.'
    methods:
    - name: setup_method
      signature: def setup_method(self, method)
    - name: test_basic_functionality
      signature: def test_basic_functionality(self)
    - name: test_multiple_documents
      signature: def test_multiple_documents(self)
    - name: test_mixed_whitespace_handling
      signature: def test_mixed_whitespace_handling(self)
  - class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens
    name: TestRunTokens
    docstring: "Unit tests for the RunTokens tokenization strategy used to chunk text\
      \ into token-based chunks for indexing.\n\nArgs:\n  self: The test case instance.\n\
      \nReturns:\n  None. The test class contains test methods that perform assertions\
      \ and do not return a value.\n\nRaises:\n  AssertionError: If any test assertion\
      \ fails during execution."
    methods:
    - name: test_basic_functionality
      signature: def test_basic_functionality(self, mock_get_encoding)
    - name: test_non_string_input
      signature: def test_non_string_input(self, mock_get_encoding)
- file: tests/unit/indexing/test_init_content.py
  functions:
  - node_id: tests/unit/indexing/test_init_content.py::uncomment_line
    name: uncomment_line
    signature: 'def uncomment_line(line: str) -> str'
    docstring: 'Uncomments a line by removing a leading "# " prefix, preserving indentation.


      Args:

      - line: str - input line that may start with whitespace followed by "# " to
      be removed.


      Returns:

      - str - the line with the first occurrence of a leading "# " removed, preserving
      the original indentation.


      Raises:

      - None'
  - node_id: tests/unit/indexing/test_init_content.py::test_init_yaml
    name: test_init_yaml
    signature: def test_init_yaml()
    docstring: "Load configuration parameters into a plain dictionary suitable for\
      \ subsequent GraphRagConfig validation.\n\nArgs:\n    values: dict[str, Any]\
      \ | None\n        Configuration values to pass into the GraphRagConfig validation\
      \ step. If None, defaults are applied.\n    root_dir: str | None\n        Root\
      \ directory for the project; used to resolve relative paths within the configuration.\n\
      \nReturns:\n    dict[str, Any]\n        A dictionary of configuration values\
      \ ready to be consumed by GraphRagConfig.model_validate to produce a GraphRagConfig\
      \ instance.\n\nExamples:\n    data = {...}  # your input dictionary\n    config\
      \ = create_graphrag_config(data)\n    GraphRagConfig.model_validate(config,\
      \ strict=True)"
  - node_id: tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
    name: test_init_yaml_uncommented
    signature: def test_init_yaml_uncommented()
    docstring: "Test that uncommenting the YAML in INIT_YAML produces a valid GraphRagConfig.\n\
      \nReturns:\n    None (type: None)\n\nRaises:\n    ValidationError: If the configuration\
      \ values do not satisfy pydantic validation."
  classes: []
- file: tests/unit/indexing/text_splitting/test_text_splitting.py
  functions:
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.encode
    name: encode
    signature: def encode(self, text)
    docstring: "Encode the input text as a list of Unicode code points.\n\nArgs:\n\
      \    text: str\n        The input text to encode as Unicode code points.\n\n\
      Returns:\n    list[int]\n        A list of integers where each integer is the\
      \ Unicode code point of the\n        corresponding character in text.\n\nRaises:\n\
      \    TypeError:\n        If text is None or not iterable."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_bool
    name: test_split_text_str_bool
    signature: def test_split_text_str_bool()
    docstring: "Test that TokenTextSplitter.split_text returns an empty list when\
      \ the input is None.\n\nThis test initializes a TokenTextSplitter with chunk_size=5\
      \ and chunk_overlap=2, calls split_text with None, and asserts that the result\
      \ is [].\n\nReturns:\n    None: this test does not return a value.\n\nRaises:\n\
      \    AssertionError: if the result is not an empty list."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::encode
    name: encode
    signature: 'def encode(text: str) -> list[int]'
    docstring: "Encode the input text into token IDs using the configured encoding\
      \ model.\n\nArgs:\n    text (str): The input to encode. If not a string, it\
      \ will be converted to a string.\n\nReturns:\n    list[int]: The encoded token\
      \ IDs produced by the encoding model.\n\nRaises:\n    Exception: If encoding\
      \ fails with the configured encoding model...."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_large_input
    name: test_split_text_large_input
    signature: def test_split_text_large_input(mock_split)
    docstring: "Tests that TokenTextSplitter.split_text handles a large input by delegating\
      \ to split_single_text_on_tokens and returning the expected number of chunks.\n\
      \nArgs:\n    mock_split: The patched mock for split_single_text_on_tokens used\
      \ to simulate splitting behavior.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
      \ If the resulting number of chunks is not 2000 or if the patched function was\
      \ not called exactly once. No exceptions are expected under normal execution."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_token_text_splitter
    name: test_token_text_splitter
    signature: def test_token_text_splitter(mock_tokenizer, mock_split_text)
    docstring: "Test that TokenTextSplitter.split_text delegates to split_single_text_on_tokens\
      \ with the given text and tokenizer.\n\nParameters:\n    mock_tokenizer (MagicMock):\
      \ Patch object that mocks the tokenizer factory; its return_value is the mocked\
      \ tokenizer used as the tokenizer argument to split_text.\n    mock_split_text\
      \ (MagicMock): Patch object for split_single_text_on_tokens; its return value\
      \ is the expected list of chunks.\n\nReturns:\n    None: This test does not\
      \ return a value.\n\nRaises:\n    AssertionError: If the expected call to split_single_text_on_tokens\
      \ is not made with the correct arguments."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.decode
    name: decode
    signature: def decode(self, token_ids)
    docstring: "Decode token ids to string by converting each integer to a character\
      \ and concatenating.\n\nArgs:\n    token_ids: An iterable of integers representing\
      \ token IDs to decode into a string.\n\nReturns:\n    str: The decoded string.\n\
      \nRaises:\n    TypeError: If token_ids contains non-integer elements or elements\
      \ cannot be processed by chr.\n    ValueError: If token_id is outside the valid\
      \ range for chr (0 <= id <= 0x10FFFF)."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_int
    name: test_split_text_str_int
    signature: def test_split_text_str_int()
    docstring: "Test that TokenTextSplitter.split_text raises TypeError when the input\
      \ is an integer (non-string). \n\nReturns:\n    None: this test does not return\
      \ a value.\n\nRaises:\n    TypeError: if the input to split_text is not a string\
      \ (e.g., an integer)."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::decode
    name: decode
    signature: 'def decode(tokens: list[int]) -> str'
    docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
      \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from\
      \ the list of tokens.\n\nRaises:\n    Exception: If decoding fails due to an\
      \ underlying error in the encoding."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_empty
    name: test_split_text_str_empty
    signature: def test_split_text_str_empty()
    docstring: "Test that TokenTextSplitter.split_text returns an empty list when\
      \ the input is an empty string.\n\nThis test initializes a TokenTextSplitter\
      \ with chunk_size=5 and chunk_overlap=2, calls split_text with an empty string,\
      \ and asserts that the result is [].\n\nReturns:\n    None: this test does not\
      \ return a value.\n\nRaises:\n    AssertionError: if the result is not an empty\
      \ list."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_noop_text_splitter
    name: test_noop_text_splitter
    signature: def test_noop_text_splitter() -> None
    docstring: "Test that NoopTextSplitter.split_text returns input unchanged.\n\n\
      This test constructs a NoopTextSplitter and asserts that:\n- split_text(\"some\
      \ text\") yields [\"some text\"]\n- split_text([\"some\", \"text\"]) yields\
      \ [\"some\", \"text\"]\n\nReturns:\n    None: This test does not return a value."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens
    name: test_split_single_text_on_tokens
    signature: def test_split_single_text_on_tokens()
    docstring: "Split a single text into chunks using the provided tokenizer.\n\n\
      Args:\n    text: str The input text to split into chunks.\n    tokenizer: TokenChunkerOptions\
      \ The tokenizer configuration used to encode the text into tokens and decode\
      \ chunks. It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\
      \nReturns:\n    list[str] The list of chunked text strings produced.\n\nRaises:\n\
      \    Exception: If the underlying tokenizer raises an error during encoding\
      \ or decoding operations."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
    name: test_split_single_text_on_tokens_no_overlap
    signature: def test_split_single_text_on_tokens_no_overlap()
    docstring: "Split a single text into chunks using the provided tokenizer.\n\n\
      Args:\n    text (str): The input text to split into chunks.\n    tokenizer (TokenChunkerOptions):\
      \ The tokenizer configuration used to encode the text into tokens and decode\
      \ chunks. It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\
      \nReturns:\n    list[str]: The list of chunked text strings produced.\n\nRaises:\n\
      \    Exception: If the underlying tokenizer raises an error during encoding\
      \ or decoding operations."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_multiple_texts_on_tokens
    name: test_split_multiple_texts_on_tokens
    signature: def test_split_multiple_texts_on_tokens()
    docstring: 'Test that split_multiple_texts_on_tokens calls the tick callback when
      processing multiple texts.


      This test creates a tokenizer configured with a mock tokenizer, passes two texts
      to split_multiple_texts_on_tokens, and asserts that the tick callback is invoked.'
  classes:
  - class_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer
    name: MockTokenizer
    docstring: "MockTokenizer is a simple, stateless tokenizer used in unit tests\
      \ to deterministically encode and decode text by Unicode code points. It simulates\
      \ a Unicode code point based tokenizer without relying on a real tokenizer implementation.\n\
      \nAttributes:\n- Stateless; no persistent state or instance attributes are required\
      \ or maintained.\n\nDeterminism:\n- All operations are deterministic: encode\
      \ uses ord() per character; decode reconstructs the string exactly from code\
      \ points.\n\nUsage:\nmock = MockTokenizer()\ntext = 'Test'\ntokens = mock.encode(text)\n\
      # tokens would be [84, 101, 115, 116]\ndecoded = mock.decode(tokens)\n# decoded\
      \ would be 'Test'\n\nMethods:\nencode(text)\n  Description: Encode the input\
      \ text as a list of Unicode code points.\n  Args:\n    text: str. The input\
      \ text to encode as Unicode code points.\n  Returns:\n    list[int]. A list\
      \ of integers where each integer is the Unicode code point of the corresponding\
      \ character in text.\n  Raises:\n    TypeError: If text is None or not a string\
      \ (or not iterable).\n\ndecode(token_ids)\n  Description: Decode token ids to\
      \ string by converting each integer to a character and concatenating.\n  Args:\n\
      \    token_ids: An iterable of integers representing token IDs to decode into\
      \ a string.\n  Returns:\n    str: The decoded string.\n  Raises:\n    TypeError:\
      \ If token_ids contains non-integer elements or elements cannot be processed\
      \ by chr.\n    ValueError: If any token_id is outside the valid Unicode range\
      \ for chr (e.g., not in 0 <= id <= 0x10FFFF)."
    methods:
    - name: encode
      signature: def encode(self, text)
    - name: decode
      signature: def decode(self, token_ids)
- file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  functions:
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
    name: test_run_extract_graph_single_document_correct_entities_returned
    signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
    docstring: 'Tests that run_extract_graph returns the expected entity titles for
      a single document.


      Args:

      - self: The test method instance (TestRunChain).


      Returns:

      - None: The test does not return a value.


      Raises:

      - AssertionError: If the assertion verifying the returned entities fails.'
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
    name: test_run_extract_graph_multiple_documents_correct_entities_returned
    signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
      \        self,\n    )"
    docstring: "Tests that run_extract_graph returns the expected entity titles for\
      \ multiple documents.\n\nArgs:\n    self (TestRunChain): The test method instance.\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If the assertion verifying the returned entities fails."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
    name: test_run_extract_graph_multiple_documents_correct_edges_returned
    signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
    docstring: "Verify that run_extract_graph returns a graph with the correct edges\
      \ when given multiple documents.\n\nArgs:\n    self (TestRunChain): The test\
      \ method instance.\n\nReturns:\n    None: The test does not return a value.\n\
      \nRaises:\n    AssertionError: If the expected edges are not found in the graph."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
      \        self,\n    )"
    docstring: "Test that run_extract_graph maps entity source_ids across multiple\
      \ documents correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion fails."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
      \        self,\n    )"
    docstring: "Test that run_extract_graph maps edge source_ids across multiple documents\
      \ correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion fails."
  classes:
  - class_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain
    name: TestRunChain
    docstring: 'TestRunChain is a unittest.TestCase subclass that validates the graph
      intelligence entity extraction workflow exercised by run_extract_graph. The
      tests focus on ensuring that, given a set of input Document objects and a mocked
      LLM, the resulting graph contains the expected entities and edges, and that
      entity/edge source_ids are consistently mapped across multiple documents. The
      class provides a high-level overview of the testing goal and the scope of what
      is being verified, without enumerating individual test methods.


      Purpose

      - Verify that the graph produced by run_extract_graph correctly captures entities
      and relationships for single- and multi-document inputs.

      - Ensure that source_id mappings for both entities and edges are propagated
      across documents as expected.


      Scope and responsibilities

      - Uses a controlled mock LLM to simulate responses and drive deterministic results.

      - Exercises the graph-building behavior of the Graph Intelligence strategy,
      not the LLM implementation itself.


      Key attributes

      - mock_llm: a mock language model used to supply deterministic responses during
      tests.

      - documents: input Document objects used to drive run_extract_graph.

      - expected_graph: abstracted expectations used to verify correctness of the
      produced graph.


      Usage

      - Part of the unit test suite, discovered and run via standard unittest discovery.

      - Requires no external resources beyond the provided mock LLM.


      Note

      - This class documents the overall testing goal and scope rather than listing
      individual test methods.'
    methods:
    - name: test_run_extract_graph_single_document_correct_entities_returned
      signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
    - name: test_run_extract_graph_multiple_documents_correct_entities_returned
      signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
        \        self,\n    )"
    - name: test_run_extract_graph_multiple_documents_correct_edges_returned
      signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
    - name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
      signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
        \        self,\n    )"
    - name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
      signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
        \        self,\n    )"
- file: tests/unit/indexing/verbs/helpers/mock_llm.py
  functions:
  - node_id: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
    name: create_mock_llm
    signature: 'def create_mock_llm(responses: list[str | BaseModel], name: str =
      "mock") -> ChatModel'
    docstring: "Creates a mock LLM that returns the given responses.\n\nArgs:\n  \
      \  responses (list[str | BaseModel]): The responses to be returned by the mock\
      \ LLM.\n    name (str): The name of the mock LLM. Defaults to \"mock\".\n\n\
      Returns:\n    ChatModel: A mock ChatModel configured to return the provided\
      \ responses.\n\nRaises:\n    Exception: If an error occurs while creating or\
      \ retrieving the mock chat model via ModelManager."
  classes: []
- file: tests/unit/litellm_services/test_rate_limiter.py
  functions:
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rate_limiter_validation
    name: test_rate_limiter_validation
    signature: def test_rate_limiter_validation()
    docstring: 'Test rate limiter creation covering both valid and invalid parameter
      scenarios.


      This test exercises creating a rate limiter with valid parameters as well as
      handling various invalid inputs, including an unknown strategy, missing TPM/RPM,
      negative RPM or TPM, and an invalid period_in_seconds. The test function takes
      no explicit parameters and returns None implicitly.'
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::_run_rate_limiter
    name: _run_rate_limiter
    signature: "def _run_rate_limiter(\n    rate_limiter: RateLimiter,\n    # Acquire\
      \ cost\n    input_queue: Queue[int | None],\n    # time value\n    output_queue:\
      \ Queue[float | None],\n)"
    docstring: "Run the RateLimiter and record timestamps for each acquired token\
      \ count.\n\nContinuously reads token_count from input_queue; when a token_count\
      \ is None, the function exits. For each non-None token_count, it enters the\
      \ rate limiter with acquire(token_count=token_count) and, after a successful\
      \ acquisition, pushes the current time (time.time()) to output_queue.\n\nArgs:\n\
      \    rate_limiter: The RateLimiter instance used to enforce rate limits.\n \
      \   input_queue: Queue[int | None]. Each non-None value represents the number\
      \ of tokens to acquire; None signals termination.\n    output_queue: Queue[float\
      \ | None]. Timestamps (time.time()) are put here after acquisition; may contain\
      \ None values.\n\nReturns:\n    None: This function does not return a value.\n\
      \nRaises:\n    Exceptions propagated from the underlying RateLimiter or queue\
      \ operations."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_binning
    name: test_binning
    signature: def test_binning()
    docstring: "Bin time values into consecutive time-based intervals.\n\nArgs:\n\
      \    time_values: list[float] - Input time values to bin.\n    time_interval:\
      \ int - Size of each time interval.\n\nReturns:\n    list[list[float]] - A list\
      \ of bins, where each inner list contains the values that fall into the corresponding\
      \ time interval. The i-th bin contains values in the interval [i * time_interval,\
      \ (i + 1) * time_interval).\n\nRaises:\n    None..."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
    name: test_rpm
    signature: def test_rpm()
    docstring: "Test that the rate limiter enforces RPM limits.\n\nArgs:\n    None\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails\
      \ during the test."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm
    name: test_tpm
    signature: def test_tpm()
    docstring: "Test that the rate limiter enforces TPM limits.\n\nThis test creates\
      \ a rate limiter with a static strategy configured with a TPM and a period,\n\
      then issues _num_requests token acquisitions with token_count set to _tokens_per_request\
      \ and\nrecords the time elapsed for each. The recorded times are binned into\
      \ intervals of _period_in_seconds.\nThe test expects the number of bins to be\
      \ equal to ceil((_num_requests * _tokens_per_request) / _tpm)\nand that the\
      \ maximum number of requests per bin is _tpm // _tokens_per_request.\n\nReturns:\n\
      \    None\n\nRaises:\n    AssertionError: If TPM constraints are violated."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
    name: test_token_in_request_exceeds_tpm
    signature: def test_token_in_request_exceeds_tpm()
    docstring: 'Test that the rate limiter allows for requests that use more tokens
      than the TPM.


      A rate limiter could be configured with a tpm of 1000 but a request may use
      2000 tokens,

      greater than the tpm limit but still below the context window limit of the underlying
      model.

      In this case, the request should still be allowed to proceed but may take up
      its own rate limit bin.'
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
    name: test_rpm_and_tpm_with_rpm_as_limiting_factor
    signature: def test_rpm_and_tpm_with_rpm_as_limiting_factor()
    docstring: "\"\"\"Test that the rate limiter enforces RPM and TPM limits.\n\n\
      Args:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If any\
      \ assertion fails during the test.\n\"\"\""
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
    name: test_rpm_and_tpm_with_tpm_as_limiting_factor
    signature: def test_rpm_and_tpm_with_tpm_as_limiting_factor()
    docstring: "Test that the rate limiter enforces TPM limits when TPM is the limiting\
      \ factor.\n\nThis test configures a static rate limiter with rpm, tpm and a\
      \ period, and issues _num_requests acquisitions\nwith token_count set to _tokens_per_request\
      \ (non-zero). It records the elapsed time for each acquisition and bins\nthe\
      \ results into intervals of _period_in_seconds. The TPM value drives the binning\
      \ and per bin capacity.\n\nExpected behavior:\n- Number of bins equals ceil((_num_requests\
      \ * _tokens_per_request) / _tpm).\n- Maximum number of requests per bin equals\
      \ _tpm // _tokens_per_request.\n\nArgs:\n    None\n\nReturns:\n    None\n\n\
      Raises:\n    AssertionError: If any assertion fails during the test."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
    name: test_rpm_threaded
    signature: def test_rpm_threaded()
    docstring: "Test that the rate limiter enforces RPM limits in a threaded environment.\n\
      \nArgs:\n  None\n\nReturns:\n  None\n\nRaises:\n  AssertionError: If any assertion\
      \ fails during the test."
  - node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
    name: test_tpm_threaded
    signature: def test_tpm_threaded()
    docstring: "Test that the rate limiter enforces TPM limits in a threaded environment.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails\
      \ during the test."
  classes: []
- file: tests/unit/litellm_services/test_retries.py
  functions:
  - node_id: tests/unit/litellm_services/test_retries.py::test_retries_async
    name: test_retries_async
    signature: "def test_retries_async(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None"
    docstring: "Test various retry strategies asynchronously.\n\nArgs:\n    strategy:\
      \ The retry strategy to use.\n    max_retries: The maximum number of retry attempts.\n\
      \    max_retry_wait: The maximum wait time between retries.\n    expected_time:\
      \ The minimum elapsed time expected for the retries to complete.\nReturns:\n\
      \    None\nRaises:\n    ValueError"
  - node_id: tests/unit/litellm_services/test_retries.py::test_retries
    name: test_retries
    signature: "def test_retries(\n    strategy: str, max_retries: int, max_retry_wait:\
      \ int, expected_time: float\n) -> None"
    docstring: "\"\"\"Test various retry strategies by exercising a mock function\
      \ that raises an exception to verify retry behavior and timing.\n\nArgs\n  \
      \  strategy: The retry strategy to use.\n    max_retries: The maximum number\
      \ of retry attempts.\n    max_retry_wait: The maximum wait time between retries.\n\
      \    expected_time: The minimum elapsed time expected for the retries to complete.\n\
      \nReturns\n    None\n\nRaises\n    ValueError: Raised by the mock function to\
      \ simulate a failed operation.\n\"\"\""
  - node_id: tests/unit/litellm_services/test_retries.py::mock_func
    name: mock_func
    signature: def mock_func()
    docstring: "Mock function used for testing retries.\n\nReturns:\n    None: This\
      \ function does not return normally because it always raises ValueError.\n\n\
      Raises:\n    ValueError: Mock error for testing retries"
  classes: []
- file: tests/unit/litellm_services/utils.py
  functions:
  - node_id: tests/unit/litellm_services/utils.py::assert_stagger
    name: assert_stagger
    signature: 'def assert_stagger(time_values: list[float], stagger: float)'
    docstring: "\"\"\"Assert that consecutive time values are at least the specified\
      \ stagger apart.\n\nArgs:\n    time_values (list[float]): Sequence of time values.\n\
      \    stagger (float): Minimum allowed difference between consecutive values.\n\
      \nReturns:\n    None: This function does not return a value.\n\nRaises:\n  \
      \  AssertionError: If any consecutive pair of time values is closer than stagger.\n\
      \"\"\""
  - node_id: tests/unit/litellm_services/utils.py::bin_time_intervals
    name: bin_time_intervals
    signature: "def bin_time_intervals(\n    time_values: list[float], time_interval:\
      \ int\n) -> list[list[float]]"
    docstring: "Bin time values into consecutive time-based intervals.\n\nArgs:\n\
      \    time_values: list[float] - Input time values to bin.\n    time_interval:\
      \ int - Size of each time interval.\n\nReturns:\n    list[list[float]] - A list\
      \ of bins, where each inner list contains the values that fall into the corresponding\
      \ time interval. The i-th bin contains values in the interval [i * time_interval,\
      \ (i + 1) * time_interval).\n\nRaises:\n    None"
  - node_id: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
    name: assert_max_num_values_per_period
    signature: "def assert_max_num_values_per_period(\n    periods: list[list[float]],\
      \ max_values_per_period: int\n)"
    docstring: "Assert the maximum number of values per period.\n\nArgs:\n    periods:\
      \ list[list[float]] - a list of periods; each period is a list of time values\n\
      \    max_values_per_period: int - the maximum number of values allowed per period\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError - if any period contains\
      \ more values than max_values_per_period"
  classes: []
- file: tests/unit/query/context_builder/test_entity_extraction.py
  functions:
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
    docstring: "Connect to the vector store.\n\nArgs:\n    kwargs: Arbitrary keyword\
      \ arguments.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: Raised\
      \ to indicate the method is not implemented."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.__init__
    name: __init__
    signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
    docstring: "Initialize the MockBaseVectorStore with the provided documents for\
      \ testing.\n\nArgs:\n    documents: list[VectorStoreDocument] - Documents to\
      \ initialize the mock vector store with.\n\nReturns:\n    None"
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Filter vector store documents by a set of IDs.\n\nArgs:\n    include_ids:\
      \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    list[VectorStoreDocument]\
      \ - The documents from self.documents whose id is in include_ids."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Performs a length-based similarity search over stored documents (embeddings\
      \ are ignored).\n\nArgs:\n    text (str): The query text to compare against\
      \ documents by their character length.\n    text_embedder (TextEmbedder): Provided\
      \ for API compatibility but not used by this implementation; embeddings are\
      \ not computed.\n    k (int): The number of top results to return. Defaults\
      \ to 10.\n    **kwargs (Any): Additional keyword arguments passed to the underlying\
      \ search implementation (not used).\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ A list of VectorStoreSearchResult objects sorted by increasing score; the\
      \ score is the absolute difference between the length of the input text and\
      \ the document's text length. If document.text is None, it is treated as an\
      \ empty string. Only the top k results are returned.\n\nRaises:\n    Not specified.\
      \ This method does not document any specific exceptions."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
      \ - List of VectorStoreDocument objects to load into the vector store.\n   \
      \ overwrite: bool - If True, overwrite existing data in the vector store; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Return the top-k documents using a deterministic mock search that\
      \ ignores the query embedding. This implementation does not perform real vector\
      \ similarity; instead it returns the first k documents with a fixed score of\
      \ 1. If k exceeds the number of available documents, all documents up to that\
      \ limit are returned.\n\nArgs:\n    self: The instance of the class containing\
      \ the documents.\n    query_embedding: list[float] The embedding vector to search\
      \ with. This value is ignored in the current implementation.\n    k: int The\
      \ number of top results to return.\n    **kwargs: Any additional keyword arguments\
      \ that may influence compatibility but are not used.\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ The top-k search results. Each result contains the associated document and\
      \ a fixed score of 1."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Return the first document with its id set to the provided id.\n\n\
      Args:\n    id (str): The identifier to assign to the first stored document before\
      \ returning.\n\nReturns:\n    VectorStoreDocument: The document whose id is\
      \ set to the provided id."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::test_map_query_to_entities
    name: test_map_query_to_entities
    signature: def test_map_query_to_entities()
    docstring: "Maps a user query to the corresponding Entity objects by performing\
      \ a semantic similarity search over a vector store of entity descriptions and\
      \ mapping the retrieved documents back to Entity records.\n\nArgs:\n- query:\
      \ str. The query string to search for relevant entities. If empty, the function\
      \ returns the top-k entities ordered by rank (see oversample_scaler for behavior\
      \ when non-empty).\n- text_embedding_vectorstore: BaseVectorStore. The vector\
      \ store used to perform semantic similarity search over entity descriptions.\n\
      - text_embedder: EmbeddingModel. The model used to encode the query into an\
      \ embedding for similarity search.\n- all_entities_dict: dict[str, Entity].\
      \ Mapping from the vector-store document identifiers (as determined by embedding_vectorstore_key)\
      \ to the corresponding Entity objects.\n- embedding_vectorstore_key: str. Key\
      \ indicating how documents in text_embedding_vectorstore identify an entity\
      \ (for example, EntityVectorStoreKey.ID or EntityVectorStoreKey.TITLE).\n- include_entity_names:\
      \ list[str] | None. Optional list of entity titles/names to explicitly include\
      \ in the results.\n- exclude_entity_names: list[str] | None. Optional list of\
      \ entity titles/names to exclude from the results.\n- k: int. Maximum number\
      \ of entities to return. If query is empty, up to k entities are returned, ordered\
      \ by rank with the highest rank first.\n- oversample_scaler: int. Multiplier\
      \ controlling how many candidate documents to fetch beyond k to improve robustness\
      \ of the top-k selection. A value of 1 disables oversampling; higher values\
      \ fetch more candidates. Default in the implementation is 2, but tests may override\
      \ this.\n\nReturns:\n- list[Entity]. The matched Entity objects, in the order\
      \ determined by the search and ranking, up to length k.\n\nRaises:\n- ValueError:\
      \ If k <= 0 or oversample_scaler < 1.\n- KeyError: If a retrieved document\u2019\
      s identifier cannot be found in all_entities_dict.\n- TypeError: If argument\
      \ types are invalid.\n\nNotes:\n- Vector-store to-entity mapping depends on\
      \ embedding_vectorstore_key. When embedding_vectorstore_key is ID, documents\
      \ store the Entity.id; when TITLE, documents store the Entity.title. This mapping\
      \ is how the function resolves a document back to an Entity.\n- Edge cases:\n\
      \  - Empty query returns the top-k entities by rank (highest numeric rank first).\n\
      \  - If no candidates are produced or all candidates are filtered out, an empty\
      \ list is returned.\n\nExample notes:\n- If embedding_vectorstore_key is EntityVectorStoreKey.ID\
      \ and a document has id equal to an Entity.id, map_query_to_entities will return\
      \ the corresponding Entity from all_entities_dict.\n- If embedding_vectorstore_key\
      \ is EntityVectorStoreKey.TITLE and a document has id equal to an Entity.title,\
      \ the function will resolve to that Entity via all_entities_dict."
  classes:
  - class_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore
    name: MockBaseVectorStore
    docstring: "MockBaseVectorStore is a test helper that provides a lightweight,\
      \ in-memory vector store for unit tests. It is designed to mirror essential\
      \ aspects of a real vector store without performing real embeddings or persisting\
      \ data, offering deterministic, mock search and retrieval behavior.\n\nArgs:\n\
      \    documents (list[VectorStoreDocument]): Documents to initialize the mock\
      \ vector store with for testing.\n\nAttributes:\n    documents (list[VectorStoreDocument]):\
      \ Documents stored in the mock vector store used for retrieval and search in\
      \ tests.\n\nNotes:\n    - Deterministic results: search and retrieval behavior\
      \ is fixed and repeatable across runs.\n    - No persistence: data exists only\
      \ for the lifetime of the test process.\n    - This class provides a test-facing\
      \ implementation that mirrors the BaseVectorStore interface as needed by tests,\
      \ without embedding calculations.\n\nMethods:\n    connect(**kwargs: Any) ->\
      \ None\n        No-op connection helper used in tests. Accepts arbitrary keyword\
      \ arguments and performs no action.\n\n    __init__(self, documents: list[VectorStoreDocument])\
      \ -> None\n        Initialize the mock vector store with the provided documents\
      \ for testing.\n\n    filter_by_id(include_ids: list[str] | list[int]) -> list[VectorStoreDocument]\n\
      \        Return the documents whose id is in include_ids.\n\n    similarity_search_by_text(\n\
      \        self, text: str, text_embedder: TextEmbedder, k: int = 10, **kwargs:\
      \ Any\n    ) -> list[VectorStoreSearchResult]\n        Perform a deterministic,\
      \ length-based search ignoring embeddings. Returns up to k results based on\
      \ stored documents with a fixed score.\n\n    load_documents(\n        self,\
      \ documents: list[VectorStoreDocument], overwrite: bool = True\n    ) -> None\n\
      \        Load documents into the vector store; if overwrite is True, replace\
      \ existing data; otherwise, append.\n\n    similarity_search_by_vector(\n  \
      \      self, query_embedding: list[float], k: int = 10, **kwargs: Any\n    )\
      \ -> list[VectorStoreSearchResult]\n        Return the top-k documents using\
      \ a deterministic mock search that ignores the query embedding; returns the\
      \ first k documents with a fixed score of 1.\n\n    search_by_id(self, id: str)\
      \ -> VectorStoreDocument\n        Return the first stored document with its\
      \ id set to the provided id."
    methods:
    - name: connect
      signature: 'def connect(self, **kwargs: Any) -> None'
    - name: __init__
      signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
    - name: filter_by_id
      signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    - name: similarity_search_by_text
      signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
        \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: load_documents
      signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
        \ overwrite: bool = True\n    ) -> None"
    - name: similarity_search_by_vector
      signature: "def similarity_search_by_vector(\n        self, query_embedding:\
        \ list[float], k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    - name: search_by_id
      signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
- file: tests/unit/query/input/retrieval/test_entities.py
  functions:
  - node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_id
    name: test_get_entity_by_id
    signature: def test_get_entity_by_id()
    docstring: "Get entity by id.\n\nArgs:\n    entities (dict[str, Entity]): Mapping\
      \ of entity IDs to Entity objects.\n    value (str): The id value to look up.\
      \ If value is a valid UUID, also try the same value with dashes removed.\n\n\
      Returns:\n    Entity | None: The matching Entity if found, otherwise None.\n\
      \nRaises:\n    None"
  - node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_key
    name: test_get_entity_by_key
    signature: def test_get_entity_by_key()
    docstring: "Get an Entity by key from a collection.\n\nThis helper searches through\
      \ an iterable of Entity objects and returns the first Entity whose attribute\
      \ named by key equals the provided value. If value is a string that represents\
      \ a UUID (with or without dashes), the comparison also considers the same UUID\
      \ with dashes removed to accommodate both representations.\n\nParameters:\n\
      - entities: Iterable[Entity]. The collection of Entity objects to search.\n\
      - key: str. The attribute name on Entity to compare.\n- value: str | int. The\
      \ value to match against the attribute. If value is a UUID string, also compare\
      \ the undashed form of the UUID.\n\nReturns:\n- Entity | None: The first matching\
      \ Entity if found, otherwise None.\n\nRaises:\n- AttributeError: If any item\
      \ in entities does not have the attribute named by key.\n\nExamples:\n- No match\
      \ returns None:\n  get_entity_by_key([Entity(id=\"id1\", short_id=\"sid1\",\
      \ title=\"title1\")], \"id\", \"00000000-0000-0000-0000-000000000000\") -> None\n\
      - Match by dashed UUID:\n  get_entity_by_key([\n    Entity(id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\"\
      , short_id=\"sid1\", title=\"title1\"),\n    Entity(id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
      , short_id=\"sid2\", title=\"title2\"),\n    Entity(id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      , short_id=\"sid3\", title=\"title3\"),\n  ],\n  \"id\",\n  \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
      ,\n) -> Entity(id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\", short_id=\"sid3\"\
      , title=\"title3\")\n- Match by undashed UUID:\n  get_entity_by_key([\n    Entity(id=\"\
      2da37c7a50a844d4aa2cfd401e19976c\", short_id=\"sid1\", title=\"title1\"),\n\
      \    Entity(id=\"c4f9356445074ee4b10298add401a965\", short_id=\"sid2\", title=\"\
      title2\"),\n    Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"\
      sid3\", title=\"title3\"),\n  ],\n  \"id\",\n  \"7c6f2bc947c9445393a3d2e174a02cd9\"\
      ,\n) -> Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\", title=\"\
      title3\")\n- Match by numeric rank:\n  get_entity_by_key([\n    Entity(id=\"\
      id1\", short_id=\"sid1\", title=\"title1\", rank=1),\n    Entity(id=\"id2\"\
      , short_id=\"sid2\", title=\"title2a\", rank=2),\n    Entity(id=\"id3\", short_id=\"\
      sid3\", title=\"title3\", rank=3),\n  ],\n  \"rank\",\n  2,\n) -> Entity(id=\"\
      id2\", short_id=\"sid2\", title=\"title2a\", rank=2)"
  classes: []
- file: tests/unit/utils/test_embeddings.py
  functions:
  - node_id: tests/unit/utils/test_embeddings.py::test_create_index_name
    name: test_create_index_name
    signature: def test_create_index_name()
    docstring: "Create an index name for the embedding store.\n\nThis function creates\
      \ a string by prefixing the embedding's index with the container_name and replacing\
      \ dots in the embedding_name with dashes to accommodate vector stores that do\
      \ not support dots.\n\nArgs:\n    container_name (str): The partition/prefix\
      \ for the index name.\n    embedding_name (str): The embedding name; must be\
      \ one of the supported embeddings defined in graphrag.index.config.embeddings.\n\
      \    validate (bool): Whether to validate the embedding_name against the supported\
      \ list. Defaults to True.\n\nReturns:\n    str: The generated index name in\
      \ the format \"<container_name>-<embedding_name_with_dashes>\".\n\nRaises:\n\
      \    KeyError: If validate is True and the embedding_name is not a supported\
      \ embedding."
  - node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_throws
    name: test_create_index_name_invalid_embedding_throws
    signature: def test_create_index_name_invalid_embedding_throws()
    docstring: "Create an index name for the embedding store.\n\nArgs:\n    container_name\
      \ (str): Partition identifier used for differentiating multiple embedding sets\
      \ within a vector store; it is added as a prefix to the index name.\n    embedding_name\
      \ (str): The fixed embedding name; the available list is defined in graphrag.index.config.embeddings.\n\
      \    validate (bool): If True, validate embedding_name and raise KeyError for\
      \ invalid names; if False, skip validation.\n\nReturns:\n    str: The constructed\
      \ index name with dots in embedding_name replaced by dashes and concatenated\
      \ with container_name using a dash (e.g., 'default-entity-title').\n\nRaises:\n\
      \    KeyError: If validate is True and embedding_name is not a valid embedding."
  - node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_does_not_throw
    name: test_create_index_name_invalid_embedding_does_not_throw
    signature: def test_create_index_name_invalid_embedding_does_not_throw()
    docstring: "Create an index name for the embedding store by prefixing the container\
      \ name and normalizing the embedding name.\n\nThe container_name parameter is\
      \ used for partitioning across multiple embedding sets within a vector store\
      \ and is added as a prefix to the index name.\nThe embedding_name is fixed,\
      \ with the available list defined in graphrag.index.config.embeddings. Dots\
      \ in the embedding_name are replaced with dashes to accommodate vector stores\
      \ that do not support dots.\n\nArgs:\n    container_name (str): Partition identifier\
      \ used for differentiating multiple embedding sets within a vector store; it\
      \ is added as a prefix to the index name.\n    embedding_name (str): The embedding\
      \ name; the available list is defined in graphrag.index.config.embeddings.\n\
      \    validate (bool): If True, validate embedding_name and raise KeyError if\
      \ invalid; if False, skip validation.\n\nReturns:\n    str: The constructed\
      \ index name.\n\nRaises:\n    KeyError: If validate is True and embedding_name\
      \ is not a valid embedding name."
  classes: []
- file: tests/unit/utils/test_encoding.py
  functions:
  - node_id: tests/unit/utils/test_encoding.py::test_encode_basic
    name: test_encode_basic
    signature: def test_encode_basic()
    docstring: "Get a tokenizer configured for the provided model configuration or\
      \ encoding model.\n\nThis function returns a Tokenizer suitable for the given\
      \ model configuration or for the specified encoding model. If a model_config\
      \ is not provided, or if model_config.encoding_model is explicitly set, a tiktoken-based\
      \ tokenizer is returned using the encoding_model. If a model_config is provided\
      \ and encoding_model is not set, a LitellmTokenizer is instantiated based on\
      \ the model name found in the configuration.\n\nParameters:\n- model_config\
      \ (LanguageModelConfig | None): The model configuration to determine which tokenizer\
      \ to instantiate. If None or if model_config.encoding_model is set, a tiktoken-based\
      \ tokenizer is returned.\n- encoding_model (str): The tiktoken encoding to use\
      \ when falling back to a tiktoken-based tokenizer.\n\nReturns:\n- Tokenizer:\
      \ The tokenizer instance configured for the provided model configuration or\
      \ encoding model.\n\nRaises:\n- ValueError, TypeError: If the inputs are invalid\
      \ or incompatible with the available tokenizers.\n\nExamples:\n    # Use tiktoken\
      \ tokenizer with a specified encoding\n    tokenizer = get_tokenizer(encoding_model=\"\
      cl100k_base\")\n    tokens = tokenizer.encode(\"abc def\")\n\n    # Use LitellmTokenizer\
      \ based on a model configuration\n    cfg = LanguageModelConfig(name=\"my-model\"\
      , encoding_model=None)\n    tokenizer2 = get_tokenizer(model_config=cfg)\n \
      \   tokens2 = tokenizer2.encode(\"hello world\")"
  - node_id: tests/unit/utils/test_encoding.py::test_num_tokens_empty_input
    name: test_num_tokens_empty_input
    signature: def test_num_tokens_empty_input()
    docstring: "Test that the tokenizer returns zero tokens for an empty string.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If the token count for\
      \ empty input is not zero."
  classes: []
- file: tests/verbs/test_create_base_text_units.py
  functions:
  - node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
    name: test_create_base_text_units
    signature: def test_create_base_text_units()
    docstring: "Test the creation of base text units by running the workflow and validating\
      \ the produced text_units table against the expected test data.\n\nReturns:\n\
      \    None. This test does not return a value; it asserts correctness by comparing\
      \ actual output to expected data.\n\nRaises:\n    Exception: Exceptions raised\
      \ by the underlying test utilities may propagate to the caller."
  - node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
    name: test_create_base_text_units_metadata
    signature: def test_create_base_text_units_metadata()
    docstring: "Asynchronous test for creating base text units with metadata.\n\n\
      Args:\n    None: This test does not take any parameters.\n\nReturns:\n    None\n\
      \nRaises:\n    Exception: Exceptions raised by the underlying test utilities\
      \ may propagate to the caller."
  - node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
    name: test_create_base_text_units_metadata_included_in_chunk
    signature: def test_create_base_text_units_metadata_included_in_chunk()
    docstring: "Asynchronous test for creating base text units with metadata included\
      \ in a chunk when chunk_size_includes_metadata is True and metadata is prepended.\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    Exception:\
      \ Exceptions raised by the underlying test utilities or storage backends may\
      \ propagate to the caller."
  classes: []
- file: tests/verbs/test_create_communities.py
  functions:
  - node_id: tests/verbs/test_create_communities.py::test_create_communities
    name: test_create_communities
    signature: def test_create_communities()
    docstring: "Test the create_communities workflow by generating final communities\
      \ and validating the produced output against the test dataset.\n\nArgs:\n  \
      \  None: This test function does not accept any parameters.\n\nReturns:\n  \
      \  None: The test does not return a value; it performs assertions to verify\
      \ correctness.\n\nRaises:\n    Exception: Exceptions raised by the helper utilities\
      \ used in this test (load_test_table, create_test_context, create_graphrag_config,\
      \ run_workflow, load_table_from_storage, or compare_outputs) may propagate."
  classes: []
- file: tests/verbs/test_create_community_reports.py
  functions:
  - node_id: tests/verbs/test_create_community_reports.py::test_create_community_reports
    name: test_create_community_reports
    signature: def test_create_community_reports()
    docstring: "Test the create_community_reports workflow by generating community\
      \ reports and validating the produced output against a test dataset.\n\nThe\
      \ test loads test data into a test context, configures a mock language model\
      \ with predefined responses, runs the workflow to generate community reports,\
      \ and asserts that the resulting community_reports table matches the expected\
      \ test data, including specific checks for mock-driven fields and the presence\
      \ of all final columns.\n\nReturns:\n  None: The test does not return a value;\
      \ it performs assertions to verify correctness.\n\nRaises:\n  Exception: Exceptions\
      \ raised by the helper utilities used in this test (load_test_table, create_test_context,\
      \ load_table_from_storage) may propagate."
  classes: []
- file: tests/verbs/test_create_final_documents.py
  functions:
  - node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents
    name: test_create_final_documents
    signature: def test_create_final_documents()
    docstring: "Test the final documents creation workflow.\n\nThis asynchronous test\
      \ loads the expected documents data, initializes a test context with text_units\
      \ storage, constructs a Graphrag config, executes the final documents workflow,\
      \ loads the produced documents from storage, and compares the actual output\
      \ to the expected data. It also verifies that all columns listed in DOCUMENTS_FINAL_COLUMNS\
      \ are present in the produced table.\n\nReturns:\n    None\n\nRaises:\n    Exception:\
      \ If any step in setup, execution, or validation fails."
  - node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
    name: test_create_final_documents_with_metadata_column
    signature: def test_create_final_documents_with_metadata_column()
    docstring: "Test the final documents workflow when a metadata column is provided.\n\
      \nThe test builds a test context with storage for text_units, creates a Graphrag\
      \ config, enables metadata extraction by setting config.input.metadata = [\"\
      title\"], simulates the metadata construction during initial input loading by\
      \ calling update_document_metadata, captures the expected documents table from\
      \ storage, runs the final documents workflow, then loads the actual documents\
      \ table from storage, and compares the two results. It also asserts that every\
      \ column listed in DOCUMENTS_FINAL_COLUMNS is present in the produced final\
      \ documents table.\n\nReturns:\n    None\nRaises:\n    Exception: Exceptions\
      \ raised by the utilities used in the test (e.g., load_table_from_storage, update_document_metadata,\
      \ run_workflow) may propagate."
  classes: []
- file: tests/verbs/test_create_final_text_units.py
  functions:
  - node_id: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
    name: test_create_final_text_units
    signature: def test_create_final_text_units()
    docstring: "Test the asynchronous creation of final text units and validate the\
      \ produced output against the expected test table.\n\nReturns:\n    None. This\
      \ test does not return a value; it asserts correctness by comparing actual output\
      \ to expected data."
  classes: []
- file: tests/verbs/test_extract_covariates.py
  functions:
  - node_id: tests/verbs/test_extract_covariates.py::test_extract_covariates
    name: test_extract_covariates
    signature: def test_extract_covariates()
    docstring: "\"\"\"Test the covariates extraction workflow using a mocked language\
      \ model response.\n\nReturns:\n    None: The test does not return a value.\n\
      \nRaises:\n    Exception: Exceptions raised by underlying utilities (e.g., load_test_table\
      \ or load_table_from_storage) may propagate during test execution.\n\"\"\""
  classes: []
- file: tests/verbs/test_extract_graph.py
  functions:
  - node_id: tests/verbs/test_extract_graph.py::test_extract_graph
    name: test_extract_graph
    signature: def test_extract_graph()
    docstring: Test that the extract_graph workflow runs with a test context and mocked
      LLM responses, persists the resulting entities and relationships to storage,
      and validates the stored graph for expected schema and content (including a
      mocked description).
  classes: []
- file: tests/verbs/test_extract_graph_nlp.py
  functions:
  - node_id: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
    name: test_extract_graph_nlp
    signature: def test_extract_graph_nlp()
    docstring: "Run the extract_graph_nlp workflow against a test context and verify\
      \ the produced entities and relationships in storage.\n\nThis async test creates\
      \ a test context with text units, builds a Graphrag config using DEFAULT_MODEL_CONFIG,\
      \ executes the workflow, and then loads the resulting tables to assert exact\
      \ row counts and schema.\n\nReturns:\n    None\n        This test does not return\
      \ a value; it performs assertions to validate correctness.\n\nRaises:\n    Exception\n\
      \        Exceptions may propagate from the underlying operations used in the\
      \ test (e.g., test utilities, storage I/O, or workflow execution)."
  classes: []
- file: tests/verbs/test_finalize_graph.py
  functions:
  - node_id: tests/verbs/test_finalize_graph.py::_prep_tables
    name: _prep_tables
    signature: def _prep_tables()
    docstring: "Prepare test tables for the finalize_graph workflow by loading test\
      \ data into a test context, dropping final columns that wouldn't be present\
      \ in inputs (x, y, degree from entities and combined_degree from relationships),\
      \ and returning the initialized context.\n\nReturns:\n    PipelineRunContext:\
      \ The initialized pipeline run context with the test data loaded into its output\
      \ storage.\n\nRaises:\n    Exception: Exceptions raised by create_test_context,\
      \ load_test_table, or write_table_to_storage may propagate."
  - node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph
    name: test_finalize_graph
    signature: def test_finalize_graph()
    docstring: "Test that the finalize_graph workflow produces final entities and\
      \ relationships tables with default coordinates.\n\nThis test prepares a test\
      \ context, creates a GraphRag configuration using the default model setup, executes\
      \ the finalize_graph workflow, and then loads the resulting tables from storage\
      \ to verify structure and values. Specifically, it asserts that:\n- the x and\
      \ y coordinates are zero sums when embedding/UMAP are disabled by default,\n\
      - all columns defined in ENTITIES_FINAL_COLUMNS are present in the entities\
      \ table,\n- all columns defined in RELATIONSHIPS_FINAL_COLUMNS are present in\
      \ the relationships table.\n\nReturns:\n    None\n        No return value.\n\
      \nRaises:\n    Exception\n        Exceptions raised by the underlying utilities\
      \ used in this test (e.g., _prep_tables, create_test_context, load_test_table,\
      \ or storage I/O) may propagate."
  - node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
    name: test_finalize_graph_umap
    signature: def test_finalize_graph_umap()
    docstring: "Test the finalize_graph workflow with UMAP enabled to verify that\
      \ x and y coordinates are produced and that final tables contain the expected\
      \ columns.\n\nArgs:\n    None: This function does not take any parameters.\n\
      \nReturns:\n    None: This is an asynchronous test function and does not return\
      \ a value.\n\nRaises:\n    Exception: Exceptions raised by create_test_context,\
      \ load_test_table, or write_table_to_storage may propagate."
  classes: []
- file: tests/verbs/test_generate_text_embeddings.py
  functions:
  - node_id: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
    name: test_generate_text_embeddings
    signature: def test_generate_text_embeddings()
    docstring: "Test the generate_text_embeddings workflow using a mock embedding\
      \ model and validate produced embeddings.\n\nThis test creates a test context\
      \ with several storage tables, configures the graphrag embedding to use a mock\
      \ embedding model,\nruns the generate_text_embeddings workflow, and asserts\
      \ that:\n- For every embedding field, a parquet named embeddings.{field}.parquet\
      \ exists in the output storage\n- embeddings.entity.description has exactly\
      \ two columns: id and embedding\n- embeddings.document.text has exactly two\
      \ columns: id and embedding\n\nThe test relies on utilities: create_test_context,\
      \ create_graphrag_config, run_workflow, and load_table_from_storage.\n\nArgs:\n\
      \    None\n\nReturns:\n    None\n\nRaises:\n    Exception: Exceptions raised\
      \ by underlying test utilities or storage operations may propagate."
  classes: []
- file: tests/verbs/test_pipeline_state.py
  functions:
  - node_id: tests/verbs/test_pipeline_state.py::run_workflow_1
    name: run_workflow_1
    signature: "def run_workflow_1(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n)"
    docstring: "Async function that initializes the pipeline state by setting context.state[\"\
      count\"] to 1 and returns a WorkflowFunctionOutput with result=None.\n\nArgs:\n\
      \    _config: Configuration for Graphrag configuration\n    context: The PipelineRunContext\
      \ for the current run; its state is updated by this function\n\nReturns:\n \
      \   WorkflowFunctionOutput: The function output; the result is None in this\
      \ implementation"
  - node_id: tests/verbs/test_pipeline_state.py::run_workflow_2
    name: run_workflow_2
    signature: "def run_workflow_2(  # noqa: RUF029\n    _config: GraphRagConfig,\
      \ context: PipelineRunContext\n)"
    docstring: "Async function that increments the pipeline state's count by one.\n\
      \nArgs:\n    _config: GraphRagConfig - Configuration for Graphrag configuration\n\
      \    context: PipelineRunContext - The PipelineRunContext for the current run;\
      \ its state is updated by this function\n\nReturns:\n    WorkflowFunctionOutput:\
      \ The function output; the result is None in this implementation.\n\nRaises:\n\
      \    KeyError: If 'count' is not present in context.state when attempting to\
      \ increment."
  - node_id: tests/verbs/test_pipeline_state.py::test_pipeline_state
    name: test_pipeline_state
    signature: def test_pipeline_state()
    docstring: "Test that the pipeline run context state can be updated by workflows.\n\
      \nTwo workflows are registered and executed in sequence, and the test asserts\
      \ the state's count becomes 2.\n\nReturns:\n    None\n        This test does\
      \ not return a value."
  - node_id: tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
    name: test_pipeline_existing_state
    signature: def test_pipeline_existing_state()
    docstring: "Test that an existing state value in the pipeline run context can\
      \ be updated by a workflow.\n\nOnly workflow_2 is registered and executed; the\
      \ test initializes the run context with state={\"count\": 4}, runs the pipeline,\
      \ and asserts the final state count is 5.\n\nReturns:\n    None\n        This\
      \ test does not return a value.\n\nRaises:\n    AssertionError\n        If the\
      \ final state count is not 5."
  classes: []
- file: tests/verbs/test_prune_graph.py
  functions:
  - node_id: tests/verbs/test_prune_graph.py::test_prune_graph
    name: test_prune_graph
    signature: def test_prune_graph()
    docstring: "Test that pruning the graph results in 20 entities.\n\nReturns:\n\
      \    None\n        No return value.\n\nRaises:\n    Exception\n        Exceptions\
      \ may propagate from the underlying operations used in the test (e.g., test\
      \ utilities, storage I/O, or workflow execution)."
  classes: []
- file: tests/verbs/util.py
  functions:
  - node_id: tests/verbs/util.py::load_test_table
    name: load_test_table
    signature: 'def load_test_table(output: str) -> pd.DataFrame'
    docstring: "Load a test table from parquet data using the provided workflow output\
      \ name.\n\nArgs:\n    output: The workflow output name, typically the workflow\
      \ name, used to locate the parquet file at tests/verbs/data/{output}.parquet.\n\
      \nReturns:\n    pd.DataFrame: The DataFrame read from the specified parquet\
      \ file."
  - node_id: tests/verbs/util.py::compare_outputs
    name: compare_outputs
    signature: "def compare_outputs(\n    actual: pd.DataFrame, expected: pd.DataFrame,\
      \ columns: list[str] | None = None\n) -> None"
    docstring: "Compare the actual and expected dataframes, optionally specifying\
      \ columns to compare. This function uses pandas.testing.assert_series_equal\
      \ to compare columns and intentionally omits the id column from value checks.\
      \ If a mismatch is found, the function prints the Expected and Actual values\
      \ for debugging before raising an AssertionError.\n\nArgs:\n    actual: The\
      \ actual DataFrame produced by the workflow.\n    expected: The expected DataFrame\
      \ against which to compare the actual DataFrame.\n    columns: Optional list\
      \ of column names to compare. If None, all columns from expected are compared.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If the number of rows differs\
      \ or any compared column's values (excluding id) differ, or if a column listed\
      \ in columns is not present in actual."
  - node_id: tests/verbs/util.py::update_document_metadata
    name: update_document_metadata
    signature: 'def update_document_metadata(metadata: list[str], context: PipelineRunContext)'
    docstring: "Asynchronously load the documents table from storage, create a new\
      \ metadata column containing per-row dictionaries built from the selected metadata\
      \ columns, and write the updated table back to storage.\n\nThis function loads\
      \ the documents table from the provided storage backend, builds a dictionary\
      \ for each row from the specified metadata columns, assigns it to a new 'metadata'\
      \ column, and persists the changes back to storage.\n\nArgs:\n  metadata: List[str]\
      \ - Names of the columns to include in each per-row metadata dictionary.\n \
      \ context: PipelineRunContext - Runtime context containing the output_storage\
      \ used to load and write the documents table.\n\nReturns:\n  None\n\nRaises:\n\
      \  Exception: Exceptions raised by the storage backend during load or write\
      \ operations may propagate."
  - node_id: tests/verbs/util.py::create_test_context
    name: create_test_context
    signature: 'def create_test_context(storage: list[str] | None = None) -> PipelineRunContext'
    docstring: "Create a test context with test tables loaded into storage.\n\nArgs:\n\
      \    storage: list[str] | None\n        A list of test table names to load from\
      \ test data and write into the\n        context's output storage. If None, only\
      \ the documents table is loaded and stored.\n\nReturns:\n    PipelineRunContext\n\
      \        The initialized pipeline run context with the test data loaded into\
      \ its\n        output storage.\n\nRaises:\n    Exception: Exceptions raised\
      \ by load_test_table or write_table_to_storage may propagate."
  classes: []
- file: unified-search-app/app/app_logic.py
  functions:
  - node_id: unified-search-app/app/app_logic.py::load_knowledge_model
    name: load_knowledge_model
    signature: 'def load_knowledge_model(sv: SessionVariables)'
    docstring: "\"\"\"Load knowledge model from the datasource and populate the session\
      \ variables with the loaded model data.\n\nArgs:\n    sv (SessionVariables):\
      \ The session variables container to be updated with the loaded knowledge model\n\
      \        data, including entities, relationships, covariates, community reports,\
      \ communities, and text units.\n        The function also resets generated_questions\
      \ and selected_question.\n\nReturns:\n    SessionVariables: The same sv object\
      \ after it has been populated with the knowledge model data.\n\nRaises:\n  \
      \  Propagates exceptions raised by load_model or datasource access as encountered.\n\
      \"\"\""
  - node_id: unified-search-app/app/app_logic.py::dataset_name
    name: dataset_name
    signature: 'def dataset_name(key: str, sv: SessionVariables) -> str'
    docstring: "Get dataset name.\n\nArgs:\n    key: The dataset key to look up.\n\
      \    sv: SessionVariables containing dataset information; sv.datasets.value\
      \ is an iterable of objects with key and name attributes.\n\nReturns:\n    The\
      \ name of the dataset whose key matches the provided key.\n\nRaises:\n    AttributeError:\
      \ If no dataset with the given key is found, since the implementation accesses\
      \ .name on None."
  - node_id: unified-search-app/app/app_logic.py::run_global_search_question_generation
    name: run_global_search_question_generation
    signature: "def run_global_search_question_generation(\n    query: str,\n    sv:\
      \ SessionVariables,\n) -> SearchResult"
    docstring: "Run global search question generation process.\n\nArgs:\n  query:\
      \ The search query string used to generate questions from the global search.\n\
      \  sv: The SessionVariables instance containing configuration and state for\
      \ the current session.\n\nReturns:\n  SearchResult: The result of the global\
      \ search question generation, including the search_type set to Global, the textual\
      \ response, and the context data (a dict of context data if available, otherwise\
      \ an empty dict)."
  - node_id: unified-search-app/app/app_logic.py::run_global_search
    name: run_global_search
    signature: 'def run_global_search(query: str, sv: SessionVariables) -> SearchResult'
    docstring: "Run global search.\n\nArgs:\n    query: str\n        The search query\
      \ string used to perform the global search.\n    sv: SessionVariables\n    \
      \    The SessionVariables instance containing configuration and state for the\
      \ current session.\n\nReturns:\n    SearchResult\n        The result of the\
      \ global search, including the search_type (Global), the textual response, and\
      \ the context data.\n\nRaises:\n    Exception\n        If an error occurs during\
      \ the global search process."
  - node_id: unified-search-app/app/app_logic.py::run_drift_search
    name: run_drift_search
    signature: "def run_drift_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    docstring: "\"\"\"Run drift search.\n\nExecute a drift-based search using the\
      \ Drift search engine, calling the Drift API with the\nconfiguration and session\
      \ data provided, and return a SearchResult containing the response and\nassociated\
      \ context data for display.\n\nArgs:\n    query: The search query string to\
      \ send to the Drift search API.\n    sv: SessionVariables containing configuration\
      \ and data used to perform the drift search\n        (graphrag_config, entities,\
      \ communities, community_reports, text_units, relationships,\n        and dataset_config\
      \ with community_level).\n\nReturns:\n    SearchResult: The drift search result\
      \ containing the response and its context data.\n\"\"\""
  - node_id: unified-search-app/app/app_logic.py::run_local_search
    name: run_local_search
    signature: "def run_local_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    docstring: "Run local search.\n\nArgs:\n    query: str\n        The search query\
      \ string used to perform the local search.\n    sv: SessionVariables\n     \
      \   The SessionVariables instance containing configuration and state for the\
      \ current session.\n\nReturns:\n    SearchResult\n        The result of the\
      \ local search, including the search_type (Local), the textual response, and\
      \ the context data.\n\nRaises:\n    Exception\n        If an error occurs during\
      \ local search execution (e.g., API call or UI state access)."
  - node_id: unified-search-app/app/app_logic.py::run_basic_search
    name: run_basic_search
    signature: "def run_basic_search(\n    query: str,\n    sv: SessionVariables,\n\
      ) -> SearchResult"
    docstring: "Run basic search.\n\nArgs:\n    query: str\n        The search query\
      \ string used to perform the basic search.\n    sv: SessionVariables\n     \
      \   The SessionVariables instance containing configuration and state for the\
      \ current session.\n\nReturns:\n    SearchResult\n        The result of the\
      \ basic search, including the search_type (Basic), the textual response, and\
      \ the context data.\n\nRaises:\n    Exception\n        If an error occurs during\
      \ the basic search process (e.g., API call failure)."
  - node_id: unified-search-app/app/app_logic.py::run_generate_questions
    name: run_generate_questions
    signature: 'def run_generate_questions(query: str, sv: SessionVariables)'
    docstring: "Run global search to generate questions for the dataset.\n\nArgs:\n\
      \  query (str): The search query string used to generate questions from the\
      \ global search.\n  sv (SessionVariables): The SessionVariables instance containing\
      \ configuration and state for the current session.\n\nReturns:\n  tuple[SearchResult,\
      \ ...]: The results of the global search question generation tasks, in the order\
      \ they were added.\n\nRaises:\n  Exception: Exceptions raised by the inner coroutines\
      \ may propagate."
  - node_id: unified-search-app/app/app_logic.py::load_dataset
    name: load_dataset
    signature: 'def load_dataset(dataset: str, sv: SessionVariables)'
    docstring: 'Load the selected dataset and initialize related session state. This
      function updates several fields on the session variables container (sv) and,
      when possible, loads the corresponding data source and knowledge model.


      Args:

      - dataset (str): The dataset key to load. This is a key from sv.datasets.value,
      not a UI element.

      - sv (SessionVariables): The session variables container that will be updated
      in place with the selected dataset''s metadata and configuration, including
      sv.dataset.value, sv.dataset_config.value, sv.datasource.value, and sv.graphrag_config.value.
      If a matching dataset configuration is found, a data source is created from
      its path and the graphrag settings are read from that source; the function then
      calls load_knowledge_model(sv) to populate the knowledge model.


      Returns:

      - None


      Notes:

      - If the dataset key is not found in sv.datasets.value (i.e., sv.dataset_config.value
      becomes None), the function will not create a data source, will not read settings,
      and will not load the knowledge model.

      - This function may raise exceptions originating from create_datasource or read_settings
      (depending on the dataset path and settings file) or from load_knowledge_model,
      if any of those operations fail.

      - The dataset parameter refers to a dataset key, not a user interface element.'
  - node_id: unified-search-app/app/app_logic.py::run_all_searches
    name: run_all_searches
    signature: 'def run_all_searches(query: str, sv: SessionVariables) -> list[SearchResult]'
    docstring: "Run all enabled search engines and return the results.\n\nArgs:\n\
      \    query: str\n        The search query string used by the enabled searches.\n\
      \    sv: SessionVariables\n        The SessionVariables instance containing\
      \ configuration and state used to determine which searches to run.\n\nReturns:\n\
      \    list[SearchResult]\n        The results from the enabled searches.\n\n\
      Raises:\n    Exception\n        If an error occurs during the execution of any\
      \ of the searches."
  classes: []
- file: unified-search-app/app/home_page.py
  functions:
  - node_id: unified-search-app/app/home_page.py::main
    name: main
    signature: def main()
    docstring: "Render the main Streamlit UI for the application as an asynchronous\
      \ coroutine that renders the UI as a side effect.\n\nThis coroutine renders\
      \ the primary GraphRAG UI by constructing the layout and widgets and displaying\
      \ them via Streamlit calls. It takes no parameters and returns None.\n\nArgs:\n\
      \    None: This function takes no parameters.\n\nReturns:\n    None: This coroutine\
      \ does not return a value."
  - node_id: unified-search-app/app/home_page.py::on_click_reset
    name: on_click_reset
    signature: 'def on_click_reset(sv: SessionVariables)'
    docstring: "Reset the relevant session variables on reset action.\n\nArgs:\n \
      \   sv (SessionVariables): The session variables container; resets sv.generated_questions.value\
      \ to [], sv.selected_question.value to '', and sv.show_text_input.value to True.\n\
      \nReturns:\n    None: This function does not return a value."
  - node_id: unified-search-app/app/home_page.py::on_change
    name: on_change
    signature: 'def on_change(sv: SessionVariables)'
    docstring: "Updates the current question in the session variables from the Streamlit\
      \ session state.\n\nParameters:\n    sv (SessionVariables): The session variables\
      \ container; updates sv.question.value from the input.\n\nReturns:\n    None:\
      \ This function does not return a value.\n\nRaises:\n    KeyError: If the key\
      \ 'question_input' is not present in st.session_state."
  classes: []
- file: unified-search-app/app/knowledge_loader/data_prep.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_community_report_data
    name: get_community_report_data
    signature: "def get_community_report_data(\n    _datasource: Datasource,\n) ->\
      \ pd.DataFrame"
    docstring: "\"\"\"Return a dataframe with community report data from the indexed-data.\n\
      \nArgs:\n    _datasource: Datasource to read the community report data from\
      \ the indexed-data.\n\nReturns:\n    A dataframe with community report data\
      \ loaded from the indexed-data.\n\nRaises:\n    Exception: If the underlying\
      \ data source read operation fails.\n\"\"\""
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_communities_data
    name: get_communities_data
    signature: "def get_communities_data(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    docstring: "\"\"\"Return a dataframe with communities data from the indexed-data.\n\
      \nArgs:\n    _datasource: Datasource to read the communities data from the indexed-data.\n\
      \nReturns:\n    A dataframe with communities data loaded from the indexed-data.\n\
      \nRaises:\n    Exception: If the underlying data source read operation fails.\n\
      \"\"\""
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_text_unit_data
    name: get_text_unit_data
    signature: 'def get_text_unit_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    docstring: "Return a dataframe containing text units (i.e. chunks of text from\
      \ raw documents) from the indexed data.\n\nArgs:\n    dataset: str The dataset\
      \ identifier.\n    _datasource: Datasource The data source to read text units\
      \ from.\n\nReturns:\n    pd.DataFrame: A dataframe containing the text unit\
      \ records from the indexed data.\n\nRaises:\n    Exception: If reading from\
      \ the datasource or processing fails."
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_entity_data
    name: get_entity_data
    signature: 'def get_entity_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    docstring: "Return a dataframe with entity data from the indexed data.\n\nReads\
      \ entity data from the configured table via the provided data source (config.entity_table).\
      \ The function prints the number of entity records and the dataset name as a\
      \ side effect and is cached with Streamlit's cache_data decorator using TTL\
      \ from config.default_ttl.\n\nArgs:\n    dataset: The dataset name to load.\n\
      \    _datasource: The data source used to read the entity data from the indexed\
      \ data.\n\nReturns:\n    pd.DataFrame: A dataframe containing the entity data\
      \ loaded from the indexed data.\n\nRaises:\n    Exception: If reading from the\
      \ data source fails."
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_relationship_data
    name: get_relationship_data
    signature: 'def get_relationship_data(dataset: str, _datasource: Datasource) ->
      pd.DataFrame'
    docstring: "Return a dataframe with entity-entity relationship data from the indexed-data.\n\
      \nReads relationship data from the configured table via the provided _datasource\
      \ (config.relationship_table). The function prints the number of relationship\
      \ records and the dataset name as a side effect and is cached with Streamlit's\
      \ cache_data decorator using TTL from config.default_ttl.\n\nArgs:\n    dataset:\
      \ str \u2014 The dataset name to load.\n    _datasource: Datasource \u2014 The\
      \ Datasource descriptor used to access the data from the configured relationship\
      \ table.\n\nReturns:\n    pd.DataFrame \u2014 DataFrame containing the relationship\
      \ data.\n\nRaises:\n    Exception \u2014 If reading from the data source fails."
  - node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_covariate_data
    name: get_covariate_data
    signature: 'def get_covariate_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    docstring: "\"\"\"Return a dataframe with covariate data from the indexed-data.\n\
      \nArgs:\n    dataset (str): The dataset identifier to load covariates for.\n\
      \    _datasource (Datasource): The data source to query for covariates.\n\n\
      Returns:\n    pd.DataFrame: A DataFrame containing covariate data loaded for\
      \ the specified dataset.\n\nRaises:\n    Exception: If the underlying data source\
      \ read operation fails.\n\"\"\""
  classes: []
- file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.__init__
    name: __init__
    signature: 'def __init__(self, database: str)'
    docstring: "Initialize the BlobDatasource with the given database identifier.\n\
      \nArgs:\n    database: The database identifier used to access the blob storage.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
    name: _get_container
    signature: 'def _get_container(account_name: str, container_name: str) -> ContainerClient'
    docstring: "\"\"\"Return a ContainerClient for the specified Azure Blob Storage\
      \ container.\n\nArgs:\n    account_name: The Azure storage account name.\n \
      \   container_name: The name of the blob container.\n\nReturns:\n    ContainerClient:\
      \ The container client for the specified container.\n\nRaises:\n    Exception:\
      \ If authentication, network, or other Azure Blob Storage errors occur.\n\"\"\
      \""
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_prompt_config
    name: load_blob_prompt_config
    signature: "def load_blob_prompt_config(\n    dataset: str,\n    account_name:\
      \ str | None = blob_account_name,\n    container_name: str | None = blob_container_name,\n\
      ) -> dict[str, str]"
    docstring: "\"\"\"Load blob prompt configuration for a dataset from Azure Blob\
      \ Storage.\n\nArgs:\n    dataset: The dataset name to load prompts for.\n  \
      \  account_name: The Azure storage account name. If None, no prompts are loaded.\n\
      \    container_name: The blob container name. If None, no prompts are loaded.\n\
      \nReturns:\n    dict[str, str]: A mapping from prompt map name to its content\
      \ loaded from the blob storage.\n\nRaises:\n    Exception: Propagated exceptions\
      \ from underlying Azure Blob Storage operations.\n\"\"\""
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
    name: load_blob_file
    signature: "def load_blob_file(\n    dataset: str | None,\n    file: str | None,\n\
      \    account_name: str | None = blob_account_name,\n    container_name: str\
      \ | None = blob_container_name,\n) -> BytesIO"
    docstring: "Load blob file from container.\n\nArgs:\n    dataset: The dataset\
      \ prefix to use when constructing the blob path. If None, only the file name\
      \ is used.\n    file: The blob file name to load. If dataset is provided, the\
      \ blob path will be \"<dataset>/<file>\".\n    account_name: The Azure storage\
      \ account name. Defaults to blob_account_name.\n    container_name: The Azure\
      \ Blob container name. Defaults to blob_container_name.\n\nReturns:\n    BytesIO:\
      \ An in-memory binary stream containing the blob data read into the stream.\
      \ If account_name or container_name is None, an empty BytesIO is returned.\n\
      \nRaises:\n    Exception: May raise exceptions from Azure Blob Storage operations\
      \ (authentication, network, or other errors) during container retrieval or blob\
      \ download."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "\"\"\"Read parquet file for a given table from blob storage.\n\nArgs:\n\
      \    table: The table name to read (without the .parquet extension).\n    throw_on_missing:\
      \ If True, raise FileNotFoundError when the table file does not exist.\n   \
      \ columns: Optional list of column names to read from the parquet file. If None,\
      \ all columns are read.\n\nReturns:\n    pd.DataFrame: A DataFrame containing\
      \ the data from the parquet file. If the table file does not exist and throw_on_missing\
      \ is False, an empty DataFrame is returned. If columns are provided, the empty\
      \ DataFrame will have those columns.\n\nRaises:\n    FileNotFoundError: If the\
      \ table does not exist and throw_on_missing is True.\n\"\"\""
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
    name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    docstring: "Read settings from container.\n\nArgs:\n    self: The BlobDatasource\
      \ instance.\n    file: The blob file name containing the settings.\n    throw_on_missing:\
      \ If True, raise FileNotFoundError when the file does not exist.\n\nReturns:\n\
      \    GraphRagConfig | None: The graphrag configuration loaded from the settings\
      \ file, or None if not found.\n\nRaises:\n    FileNotFoundError: If the file\
      \ does not exist and throw_on_missing is True."
  classes:
  - class_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource
    name: BlobDatasource
    docstring: "BlobDatasource provides access to knowledge data stored in Azure Blob\
      \ Storage, enabling reading Parquet tables and graphrag configurations used\
      \ by the knowledge loader. It connects to a configured Azure Blob container\
      \ using the provided database identifier to locate data and settings.\n\nArgs:\n\
      \    database (str): The database identifier used to access the blob storage.\
      \ This maps to a logical namespace within the configured container and determines\
      \ where data and settings for this knowledge domain are stored.\n\nAttributes:\n\
      \    database (str): The database identifier used to access the blob storage.\n\
      \nMethods:\n    read(table, throw_on_missing=False, columns=None) -> pd.DataFrame\n\
      \        Read a Parquet table from blob storage.\n\n        Args:\n        \
      \    table (str): The table name to read (without the .parquet extension).\n\
      \            throw_on_missing (bool): If True, raise FileNotFoundError when\
      \ the blob does not exist.\n            columns (list[str] | None): Optional\
      \ subset of columns to load; if None, load all columns.\n\n        Returns:\n\
      \            pd.DataFrame: A DataFrame containing the data from the Parquet\
      \ file.\n\n        Raises:\n            FileNotFoundError: If the table file\
      \ does not exist and throw_on_missing is True.\n            Azure-related errors:\
      \ Authentication/permission/network-related errors may be raised.\n\n    read_settings(file,\
      \ throw_on_missing=False) -> GraphRagConfig | None\n        Read graphrag configuration\
      \ settings from a blob container.\n\n        Args:\n            file (str):\
      \ The blob file name containing the graphrag settings.\n            throw_on_missing\
      \ (bool): If True, raise FileNotFoundError when the settings file does not exist.\n\
      \n        Returns:\n            GraphRagConfig | None: The graphrag configuration\
      \ loaded from the file, or None if not found.\n\n        Raises:\n         \
      \   FileNotFoundError: If the file is missing and throw_on_missing is True.\n\
      \            YAML parsing errors or Azure-related errors may occur.\n\nNotes:\n\
      \    - Requires Azure credentials with access to the storage account and container.\
      \ Uses DefaultAzureCredential to obtain credentials and connects via BlobServiceClient/ContainerClient."
    methods:
    - name: __init__
      signature: 'def __init__(self, database: str)'
    - name: read
      signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
        \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    - name: read_settings
      signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
        \ bool = False,\n    ) -> GraphRagConfig | None"
- file: unified-search-app/app/knowledge_loader/data_sources/loader.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
    name: _get_base_path
    signature: "def _get_base_path(\n    dataset: str | None, root: str | None, extra_path:\
      \ str | None = None\n) -> str"
    docstring: "\"\"\"Construct and return the base path for the given dataset and\
      \ extra path.\n\nArgs:\n    dataset (str | None): The dataset folder name, or\
      \ None to omit.\n    root (str | None): The root path segment, or None to omit.\n\
      \    extra_path (str | None): Additional path segments separated by '/' (if\
      \ provided).\n\nReturns:\n    str: The constructed base path as a string.\n\"\
      \"\""
  - node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::create_datasource
    name: create_datasource
    signature: 'def create_datasource(dataset_folder: str) -> Datasource'
    docstring: "Return a datasource that reads from a local or blob storage parquet\
      \ file.\n\nArgs:\n    dataset_folder: Path to the dataset folder to load data\
      \ from.\n\nReturns:\n    Datasource: A datasource instance. If blob_account_name\
      \ is set, a BlobDatasource is returned;\n        otherwise, a LocalDatasource\
      \ configured with the base path derived from dataset_folder and local_data_root.\n\
      \nRaises:\n    Exceptions that may be raised by the underlying BlobDatasource\
      \ or LocalDatasource constructors."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_dataset_listing
    name: load_dataset_listing
    signature: def load_dataset_listing() -> list[DatasetConfig]
    docstring: "Load dataset listing file from blob storage or local data.\n\nThis\
      \ function takes no parameters and returns a list of DatasetConfig instances\
      \ parsed from the listing file. When blob storage is configured (blob_account_name\
      \ is set), the function loads the listing from blob storage and, on error, prints\
      \ the issue and returns an empty list (no exception is raised).\n\nWhen blob\
      \ storage is not configured, the function loads the listing from the local filesystem\
      \ using the path derived from local_data_root and LISTING_FILE, parses the JSON\
      \ content, and converts each listing item into a DatasetConfig instance. Errors\
      \ during local loading may propagate to the caller.\n\nReturns:\n    list[DatasetConfig]:\
      \ A list of DatasetConfig instances created from the listing entries. May be\
      \ empty if loading from blob storage fails or no data is found.\n\nRaises:\n\
      \    FileNotFoundError: If the local listing file cannot be found when blob\
      \ storage is not used.\n    json.JSONDecodeError: If the listing JSON content\
      \ is invalid when loaded from local path.\n    TypeError: If a listing item\
      \ does not provide the required fields for DatasetConfig."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_prompts
    name: load_prompts
    signature: 'def load_prompts(dataset: str) -> dict[str, str]'
    docstring: "\"\"\"Return the prompts configuration for a specific dataset.\n\n\
      If a blob account name is configured, the prompts are loaded from blob storage;\
      \ otherwise\nthey are loaded from local storage.\n\nArgs:\n    dataset (str):\
      \ The dataset name to load prompts for.\n\nReturns:\n    dict[str, str]: The\
      \ prompts configuration for the specified dataset.\n\nRaises:\n    Exception:\
      \ Propagated exceptions from underlying loading functions (load_blob_prompt_config\n\
      \        or load_local_prompt_config) may be raised.\n\"\"\""
  classes: []
- file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::load_local_prompt_config
    name: load_local_prompt_config
    signature: def load_local_prompt_config(base_path="") -> dict[str, str]
    docstring: "Load local prompt configuration.\n\nArgs:\n    base_path: Path to\
      \ the folder containing prompt files.\n\nReturns:\n    dict[str, str]: Mapping\
      \ from the prompt name (filename without extension) to the file contents as\
      \ a string.\n\nRaises:\n    FileNotFoundError: If base_path does not exist.\n\
      \    OSError: If an OS error occurs while listing or reading files."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "Read file from local source.\n\nArgs:\n    table: The table name to\
      \ read (without the .parquet extension).\n    throw_on_missing: If True, raise\
      \ FileNotFoundError when the table file does not exist.\n    columns: Optional\
      \ list of column names to read from the parquet file. If None, all columns are\
      \ read.\n\nReturns:\n    A pandas DataFrame containing the data from the parquet\
      \ file. If the table file does not exist\n    and throw_on_missing is False,\
      \ returns an empty DataFrame; if columns is provided, the DataFrame\n    will\
      \ contain those columns.\n\nRaises:\n    FileNotFoundError: If the table file\
      \ does not exist and throw_on_missing is True."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.__init__
    name: __init__
    signature: 'def __init__(self, base_path: str)'
    docstring: "Initialize LocalDatasource with the provided base path.\n\nArgs:\n\
      \    base_path: The base directory path for local data sources. Type: str.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
    name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    docstring: "Read settings file from local source.\n\nNote: The 'file' parameter\
      \ is unused. Settings are loaded by invoking load_config with root_dir derived\
      \ from the datasource's base_path.\n\nArgs:\n    self: The LocalDatasource instance.\n\
      \    file: str. Unused. Path to the settings file; present for API compatibility.\n\
      \    throw_on_missing: bool. Ignored by this implementation.\n\nReturns:\n \
      \   GraphRagConfig | None: The GraphRagConfig produced by load_config, or None\
      \ if no configuration could be loaded.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; any loading errors originate from load_config."
  classes:
  - class_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource
    name: LocalDatasource
    docstring: "LocalDatasource provides access to Parquet data stored on the local\
      \ filesystem and loads Graph Rag configuration using the provided base_path\
      \ as the root of the data source.\n\nArgs:\n- base_path: The base directory\
      \ path for local data sources. Type: str.\n\nAttributes:\n- base_path (str):\
      \ The base directory path for local data sources.\n\nMethods:\n- read(table:\
      \ str, throw_on_missing: bool = False, columns: list[str] | None = None) ->\
      \ pd.DataFrame\n  Read a parquet file named \"<table>.parquet\" located under\
      \ base_path.\n  - Parameters:\n    - table: The table name to read (without\
      \ the .parquet extension).\n    - throw_on_missing: If True, raise FileNotFoundError\
      \ when the file does not exist.\n    - columns: Optional list of column names\
      \ to read from the parquet file; if None, all columns are loaded.\n  - Returns:\n\
      \    - A pandas DataFrame containing the data from the parquet file.\n  - Raises:\n\
      \    - FileNotFoundError: if the file is missing and throw_on_missing is True.\n\
      \    - OSError / IOError: underlying I/O errors may propagate.\n\n- __init__(base_path:\
      \ str) -> None\n  Initialize the instance with the provided base_path and store\
      \ internal state for data access and settings loading.\n\n- read_settings(file:\
      \ str, throw_on_missing: bool = False) -> GraphRagConfig | None\n  Read settings\
      \ from the local source. Note: The file parameter is unused. Settings are loaded\
      \ by invoking load_config with a root_dir derived from base_path.\n  - Parameters:\n\
      \    - file: str. Unused; present for API compatibility.\n    - throw_on_missing:\
      \ bool. Ignored by this implementation.\n  - Returns:\n    - GraphRagConfig\
      \ | None: The loaded configuration, or None if not found.\n  - Raises:\n   \
      \ - Exceptions raised by load_config may propagate."
    methods:
    - name: read
      signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
        \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    - name: __init__
      signature: 'def __init__(self, base_path: str)'
    - name: read_settings
      signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
        \ bool = False,\n    ) -> GraphRagConfig | None"
- file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read_settings
    name: read_settings
    signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
    docstring: "Read settings for a datasource from a file.\n\nArgs:\n    file: Path\
      \ to the settings file.\n\nReturns:\n    GraphRagConfig | None: The GraphRagConfig\
      \ read from the file, or None if no settings could be read.\n\nRaises:\n   \
      \ NotImplementedError: If the method is not implemented by a subclass."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "Read method for a datasource.\n\nArgs:\n    table: str - The name\
      \ of the table to read from.\n    throw_on_missing: bool - If True, raise an\
      \ error when the table is missing; otherwise, handle gracefully.\n    columns:\
      \ list[str] | None - Specific columns to read from the table, or None to read\
      \ all columns.\n\nReturns:\n    pd.DataFrame - The data read from the specified\
      \ table.\n\nRaises:\n    NotImplementedError - If the method is not implemented."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.__call__
    name: __call__
    signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
    docstring: "Call method definition for a datasource to retrieve a DataFrame for\
      \ the given table and optional columns.\n\nArgs:\n  table: name of the table\
      \ to query\n  columns: optional list of column names to include; if None, all\
      \ columns are returned\n\nReturns:\n  pd.DataFrame: the DataFrame resulting\
      \ from the call\n\nRaises:\n  NotImplementedError"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.write
    name: write
    signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
      \ | None = None\n    ) -> None"
    docstring: "Write data to a table.\n\nArgs:\n    table (str): The name of the\
      \ target table.\n    df (pd.DataFrame): The data to write to the table.\n  \
      \  mode (WriteMode | None): The write mode to use; None indicates default behavior.\n\
      \nReturns:\n    None\n\nRaises:\n    NotImplementedError"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.has_table
    name: has_table
    signature: 'def has_table(self, table: str) -> bool'
    docstring: "\"\"\"Check if table exists.\n\nArgs:\n    table: The name of the\
      \ table to check for existence.\n\nReturns:\n    bool: True if the table exists,\
      \ otherwise False.\n\nRaises:\n    NotImplementedError: If the method is not\
      \ implemented.\n\"\"\""
  classes:
  - class_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource
    name: Datasource
    docstring: 'Datasource interface for knowledge loader data sources.


      Purpose

      Abstract base class that defines the common interface for data sources used
      by the knowledge loader. Concrete implementations provide reading from and writing
      to their underlying storage, loading configuration, and checking the presence
      of tables.


      Key attributes

      - GraphRagConfig: The configuration type used by read_settings to represent
      datasource settings.

      - Overwrite, Append: Module-level constants representing write modes defined
      for write operations.


      Summary

      Subclasses must implement the following methods to interact with their data
      backends:

      - read_settings(file: str) -> GraphRagConfig | None

      - read(table: str, throw_on_missing: bool = False, columns: list[str] | None
      = None) -> pd.DataFrame

      - __call__(table: str, columns: list[str] | None) -> pd.DataFrame

      - write(table: str, df: pd.DataFrame, mode: WriteMode | None = None) -> None

      - has_table(table: str) -> bool'
    methods:
    - name: read_settings
      signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
    - name: read
      signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
        \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    - name: __call__
      signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
    - name: write
      signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
        \ | None = None\n    ) -> None"
    - name: has_table
      signature: 'def has_table(self, table: str) -> bool'
- file: unified-search-app/app/knowledge_loader/model.py
  functions:
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_entities
    name: load_entities
    signature: "def load_entities(\n    dataset: str,\n    _datasource: Datasource,\n\
      ) -> pd.DataFrame"
    docstring: "Return a DataFrame of Entity data loaded from the given dataset and\
      \ datasource.\n\nArgs:\n    dataset: The dataset identifier to load entities\
      \ from.\n    _datasource: The Datasource descriptor used to access the data.\n\
      \nReturns:\n    pd.DataFrame: DataFrame containing the loaded Entity data.\n\
      \nRaises:\n    Exception: Propagates any exceptions raised by get_entity_data."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_entity_relationships
    name: load_entity_relationships
    signature: "def load_entity_relationships(\n    dataset: str,\n    _datasource:\
      \ Datasource,\n) -> pd.DataFrame"
    docstring: "Return a DataFrame containing the entity-relationship data loaded\
      \ from the given dataset and datasource.\n\nArgs:\n  dataset: str \u2014 The\
      \ dataset identifier to load the entity-relationship data from.\n  _datasource:\
      \ Datasource \u2014 The Datasource descriptor used to access the data.\n\nReturns:\n\
      \  pd.DataFrame \u2014 DataFrame containing the relationship data as produced\
      \ by get_relationship_data. The specific columns depend on the underlying data_prep\
      \ implementation.\n\nRaises:\n  Exception \u2014 Propagates any exceptions raised\
      \ by get_relationship_data."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_communities
    name: load_communities
    signature: "def load_communities(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    docstring: "Return a dataframe with communities data from the indexed-data.\n\n\
      Args:\n    _datasource: Datasource to read the communities data from the indexed-data.\n\
      \nReturns:\n    A dataframe with communities data loaded from the indexed-data.\n\
      \nRaises:\n    Exception: If the underlying data source read operation fails."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_covariates
    name: load_covariates
    signature: 'def load_covariates(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    docstring: "Load covariate data as a DataFrame for the given dataset and datasource.\n\
      \nArgs:\n  dataset: str - The dataset identifier to load covariates for.\n \
      \ _datasource: Datasource - The data source to query for covariates.\n\nReturns:\n\
      \  pd.DataFrame - A DataFrame containing covariate data loaded for the specified\
      \ dataset.\n\nRaises:\n  Propagates any exceptions raised by get_covariate_data."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_community_reports
    name: load_community_reports
    signature: "def load_community_reports(\n    _datasource: Datasource,\n) -> pd.DataFrame"
    docstring: "Load community report data from the indexed data source.\n\nThis function\
      \ delegates to get_community_report_data to obtain a DataFrame of\ncommunity\
      \ reports using the provided Datasource and returns the result.\n\nArgs:\n \
      \   _datasource (Datasource): Datasource to read the community report data from\
      \ the indexed data.\n\nReturns:\n    pd.DataFrame: DataFrame containing community\
      \ report data loaded from the indexed data.\n\nRaises:\n    Exception: If the\
      \ underlying data source read operation fails."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_text_units
    name: load_text_units
    signature: 'def load_text_units(dataset: str, _datasource: Datasource) -> pd.DataFrame'
    docstring: "Load text units from the specified dataset and data source.\n\nThis\
      \ function delegates to get_text_unit_data to retrieve a DataFrame containing\
      \ text unit records. It returns a DataFrame, not a list of objects.\n\nArgs:\n\
      \    dataset: str The dataset identifier.\n    _datasource: Datasource The data\
      \ source to read text units from.\n\nReturns:\n    pd.DataFrame: A DataFrame\
      \ containing the text unit records from the indexed data.\n    The exact columns\
      \ reflect the text unit schema defined by get_text_unit_data.\n\nRaises:\n \
      \   Exception: If reading from the data source or processing fails."
  - node_id: unified-search-app/app/knowledge_loader/model.py::load_model
    name: load_model
    signature: "def load_model(\n    dataset: str,\n    datasource: Datasource,\n)"
    docstring: "Load all relevant graph-indexed data into collections of knowledge\
      \ model objects and store the model collections in the session variables.\n\n\
      This is a one-time data retrieval and preparation per session.\n\nArgs:\n  \
      \  dataset (str): The dataset identifier to load from.\n    datasource (Datasource):\
      \ The Datasource descriptor used to access the data.\n\nReturns:\n    KnowledgeModel:\
      \ A KnowledgeModel containing the loaded DataFrames for entities, relationships,\
      \ community_reports, communities, text_units, and covariates (covariates will\
      \ be None if empty).\n\nRaises:\n    Exception: Propagates any exceptions raised\
      \ by the underlying data-loading helpers (e.g., get_entity_data, get_relationship_data,\
      \ get_covariate_data, get_community_report_data, get_communities_data, get_text_unit_data)."
  classes: []
- file: unified-search-app/app/state/query_variable.py
  functions:
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.__init__
    name: __init__
    signature: 'def __init__(self, key: str, default: Any | None)'
    docstring: "Initialize a QueryVariable to manage a single URL query parameter\
      \ and its corresponding session_state entry.\n\nThis constructor reads the value\
      \ for the given key from the URL query parameters when available; if the key\
      \ is not present, it uses the provided default. When reading from the query\
      \ string, the value is normalized to lowercase to support case-insensitive URLs.\
      \ If the resulting value equals \"true\" or \"false\" (after normalization),\
      \ it is converted to the corresponding Python boolean True or False. If the\
      \ key is not already present in Streamlit's session_state, the derived value\
      \ is written to session_state under that key. If the key already exists in session_state,\
      \ its existing value is preserved and not overwritten during initialization.\n\
      \nArgs:\n    key (str): The key of the query parameter to manage.\n    default\
      \ (Any | None): The default value to use if the key is not present in the query\
      \ parameters.\n\nReturns:\n    None"
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.key
    name: key
    signature: def key(self) -> str
    docstring: "Key property that returns the session_state key for this variable.\n\
      \nArgs:\n    self (QueryVariable): The instance of QueryVariable.\n\nReturns:\n\
      \    str: The key used to access the session_state dictionary for this variable."
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.value
    name: value
    signature: 'def value(self, value: Any) -> None'
    docstring: "Value setter for the QueryVariable. Sets the session state value and\
      \ updates the corresponding URL query parameter with the lowercase string representation\
      \ of the value.\n\nArgs:\n    value: The new value to assign to the session\
      \ variable. Can be of any type.\n\nReturns:\n    None"
  classes:
  - class_id: unified-search-app/app/state/query_variable.py::QueryVariable
    name: QueryVariable
    docstring: "Manages a single URL query parameter and its corresponding Streamlit\
      \ session_state entry.\n\nPurpose:\n  Maintain a two-way linkage between a URL\
      \ query parameter and a Streamlit session_state entry. It reads the initial\
      \ value from the URL query string when available; if the key is not present,\
      \ it uses the provided default. When reading from the query string, the value\
      \ is normalized to lowercase to support case-insensitive URLs.\n\nKey attributes:\n\
      \  key: The session_state key associated with this variable (derived from the\
      \ provided key parameter).\n\nArgs:\n  key: The URL query parameter key and\
      \ the session_state key for this variable.\n  default: The default value to\
      \ use if the key is not present in the URL. Can be None.\n\nReturns:\n  None\n\
      \nRaises:\n  None"
    methods:
    - name: __init__
      signature: 'def __init__(self, key: str, default: Any | None)'
    - name: key
      signature: def key(self) -> str
    - name: value
      signature: 'def value(self, value: Any) -> None'
- file: unified-search-app/app/state/session_variable.py
  functions:
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__init__
    name: __init__
    signature: 'def __init__(self, default: Any = "", prefix: str = "")'
    docstring: "Create a managed session variable with a default value and an optional\
      \ prefix.\n\nThe prefix is used to avoid collisions between variables with the\
      \ same name.\n\nArgs:\n    default: The initial/default value for the session\
      \ variable.\n    prefix: Optional prefix to prepend to the key to differentiate\
      \ this variable from others with the same name.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.value
    name: value
    signature: 'def value(self, value: Any) -> None'
    docstring: "Set the session variable value.\n\nArgs:\n    value: The new value\
      \ to assign to the session variable. Can be of any type.\n\nReturns:\n    None"
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__repr__
    name: __repr__
    signature: def __repr__(self) -> Any
    docstring: "Return a string representation of the managed session variable value.\n\
      \nArgs:\n  self: The instance of the class containing the key used to index\
      \ session_state.\n\nReturns:\n  str: The string representation of the value\
      \ stored in st.session_state for this key.\n\nRaises:\n  KeyError: If the key\
      \ is not present in st.session_state."
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.key
    name: key
    signature: def key(self) -> str
    docstring: "Key property that returns the session_state key for this variable.\n\
      \nReturns:\n    str: The key used to access the session_state dictionary for\
      \ this variable."
  classes:
  - class_id: unified-search-app/app/state/session_variable.py::SessionVariable
    name: SessionVariable
    docstring: "SessionVariable provides a small wrapper around Streamlit's session_state\
      \ to manage a single variable with optional collision avoidance via a prefix.\n\
      \nPurpose:\n- To store and retrieve a value associated with a unique key in\
      \ st.session_state, with a configurable prefix to prevent collisions when multiple\
      \ variables share the same base name.\n\nArgs:\n  default: The initial/default\
      \ value for the session variable.\n  prefix: Optional prefix to prepend to the\
      \ key to differentiate this variable from others with the same name.\n\nReturns:\n\
      \  None\n  Note: __init__ initializes the instance and does not return a value.\n\
      \nRaises:\n  None\n\nAttributes:\n  key: str. The session_state key used to\
      \ access this variable. It is derived from the provided prefix and the internal\
      \ variable name.\n  value: Any. The current value stored in session_state for\
      \ this variable. This is readable and writable; assigning a new value updates\
      \ st.session_state accordingly.\n\n__repr__:\n  Returns: str. A string representation\
      \ that includes both the key and the current value to aid debugging."
    methods:
    - name: __init__
      signature: 'def __init__(self, default: Any = "", prefix: str = "")'
    - name: value
      signature: 'def value(self, value: Any) -> None'
    - name: __repr__
      signature: def __repr__(self) -> Any
    - name: key
      signature: def key(self) -> str
- file: unified-search-app/app/state/session_variables.py
  functions:
  - node_id: unified-search-app/app/state/session_variables.py::SessionVariables.__init__
    name: __init__
    signature: def __init__(self)
    docstring: 'Initialize all SessionVariables for the unified search app.


      SessionVariables.__init__ creates and initializes every session attribute used
      to track the

      state of a unified search session. Each attribute is assigned a default value
      to ensure a

      consistent, predictable initial state.


      Attributes initialized (with defaults):

      - dataset: QueryVariable("dataset", "")

      - datasets: SessionVariable([])

      - dataset_config: SessionVariable()

      - datasource: SessionVariable()

      - graphrag_config: SessionVariable()

      - question: QueryVariable("question", "")

      - suggested_questions: SessionVariable(default_suggested_questions)

      - entities: SessionVariable([])

      - relationships: SessionVariable([])

      - covariates: SessionVariable({})

      - communities: SessionVariable([])

      - community_reports: SessionVariable([])

      - text_units: SessionVariable([])

      - question_in_progress: SessionVariable("")

      - include_global_search: QueryVariable("include_global_search", True)

      - include_local_search: QueryVariable("include_local_search", True)

      - include_drift_search: QueryVariable("include_drift_search", False)

      - include_basic_rag: QueryVariable("include_basic_rag", False)

      - selected_report: SessionVariable()

      - graph_community_level: SessionVariable(0)

      - selected_question: SessionVariable("")

      - generated_questions: SessionVariable([])

      - show_text_input: SessionVariable(True)


      Returns:

      None'
  classes:
  - class_id: unified-search-app/app/state/session_variables.py::SessionVariables
    name: SessionVariables
    docstring: "SessionVariables stores and initializes per-session state for the\
      \ unified search application.\n\nPurpose:\n    Manage session-scoped attributes\
      \ used to track the user's search state across the app.\n\nAttributes:\n   \
      \ dataset (QueryVariable): The currently selected dataset, initialized as QueryVariable(\"\
      dataset\", \"\").\n    datasets (SessionVariable): The collection/state of datasets\
      \ for the session, initialized to an empty list.\n\nConstructor:\n    __init__(self)\n\
      \        Creates and initializes the session attributes to a consistent default\
      \ state. Initializes dataset and datasets with their defaults. Seeds an initial\
      \ set of suggested questions from default_suggested_questions when available.\n\
      \nReturns:\n    None\n\nRaises:\n    Propagates exceptions raised by the initialization\
      \ of the contained attributes (QueryVariable, SessionVariable)."
    methods:
    - name: __init__
      signature: def __init__(self)
- file: unified-search-app/app/ui/full_graph.py
  functions:
  - node_id: unified-search-app/app/ui/full_graph.py::create_full_graph_ui
    name: create_full_graph_ui
    signature: 'def create_full_graph_ui(sv: SessionVariables)'
    docstring: "Create and render the full graph UI from the provided session variables.\n\
      \nArgs:\n    sv (SessionVariables): Container with entities, communities, and\
      \ graph_community_level used to construct and filter the graph.\n\nReturns:\n\
      \    alt.Chart: The Altair chart object representing the full graph UI. The\
      \ function also renders the chart via Streamlit."
  classes: []
- file: unified-search-app/app/ui/questions_list.py
  functions:
  - node_id: unified-search-app/app/ui/questions_list.py::create_questions_list_ui
    name: create_questions_list_ui
    signature: 'def create_questions_list_ui(sv: SessionVariables)'
    docstring: "Create and render a Streamlit UI component to display a list of generated\
      \ questions and update the selected question when a row is selected.\n\nArgs:\n\
      \    sv (SessionVariables): The session state object containing generated_questions\
      \ and selected_question attributes used to render the UI and handle selection\
      \ updates.\n\nReturns:\n    None: This function does not return a value."
  classes: []
- file: unified-search-app/app/ui/report_details.py
  functions:
  - node_id: unified-search-app/app/ui/report_details.py::create_report_details_ui
    name: create_report_details_ui
    signature: 'def create_report_details_ui(sv: SessionVariables)'
    docstring: 'Render the report details UI for the currently selected report using
      Streamlit; this function does not return a value.


      It loads the selected report JSON from sv.selected_report.value.full_content_json
      and renders the report title, summary, priority, and explanation, collecting
      citations for entities and relationships to highlight in the graph.


      If no report is selected, it writes No report selected to the UI.


      Notes:

      - JSONDecodeError is caught locally; in case of invalid JSON, error messages
      and the raw JSON content are written to the UI.

      - Missing keys in the loaded JSON may raise KeyError since the code directly
      accesses required fields such as title, summary, rating, rating explanation,
      and findings.

      - The function handles findings as a list or as a string; it gathers citations
      and renders hyperlinks accordingly.

      - The UI text is post-processed to replace internal tokens for display friendliness
      and then rendered via Markdown; finally, a graph citation visualization is shown
      for the selected entities and relationships.'
  classes: []
- file: unified-search-app/app/ui/report_list.py
  functions:
  - node_id: unified-search-app/app/ui/report_list.py::create_report_list_ui
    name: create_report_list_ui
    signature: 'def create_report_list_ui(sv: SessionVariables)'
    docstring: "Render the report list UI and update the selected report in the session\
      \ state based on user selection.\n\nArgs:\n    sv (SessionVariables): The session\
      \ state object containing community_reports and selected_report used to render\
      \ the UI and handle selection updates.\n\nReturns:\n    None: This function\
      \ does not return a value."
  classes: []
- file: unified-search-app/app/ui/search.py
  functions:
  - node_id: unified-search-app/app/ui/search.py::init_search_ui
    name: init_search_ui
    signature: "def init_search_ui(\n    container: DeltaGenerator, search_type: SearchType,\
      \ title: str, caption: str\n)"
    docstring: "Initialize search UI component in the specified container for the\
      \ given search type.\n\nArgs:\n    container: DeltaGenerator\n        The DeltaGenerator\
      \ container to render the UI into.\n    search_type: SearchType\n        The\
      \ type of search UI to configure.\n    title: str\n        The title text to\
      \ display in the UI.\n    caption: str\n        The caption text to display\
      \ in the UI.\n\nReturns:\n    None"
  - node_id: unified-search-app/app/ui/search.py::render_html_table
    name: render_html_table
    signature: 'def render_html_table(df: pd.DataFrame, search_type: str, key: str)'
    docstring: 'Render HTML table into the UI.


      Builds an HTML fragment representing the given DataFrame as a table with a header
      and body. It applies per-cell formatting: strings may be truncated for display,
      and if a string value starts with a JSON-like object (for example, a dictionary
      in string form), the code attempts to extract the "summary" field from that
      JSON. It also generates per-row HTML ids to enable UI interactions; ids are
      constructed from the lowercased and stripped search_type and key with the row''s
      id when available, otherwise using the row index. The function returns the HTML
      string suitable for insertion into the UI (for example via Streamlit''s st.markdown
      with unsafe_allow_html=True).


      Args:

      - df (pd.DataFrame): DataFrame to render as HTML table for UI display.

      - search_type (str): Type of search; used to generate the per-row HTML id (lowercased
      and stripped).

      - key (str): Key associated with the search type; used in ID generation when
      an id column exists.


      Returns:

      - str: HTML string representing the rendered table.


      Raises:

      - json.JSONDecodeError: If a string value that begins with ''{'' cannot be parsed
      as JSON to extract a summary.

      - AttributeError or TypeError: If a row contains an id value that is not a string
      or otherwise cannot be stripped, or if input types are incompatible.

      - ValueError: If inputs are of an unexpected type or contain invalid data for
      rendering.'
  - node_id: unified-search-app/app/ui/search.py::convert_numbered_list_to_array
    name: convert_numbered_list_to_array
    signature: def convert_numbered_list_to_array(numbered_list_str)
    docstring: "Convert a numbered-list string into an array of extracted items.\n\
      \nArgs:\n    numbered_list_str: str-like\n        A string-like object containing\
      \ a numbered list. Each line that matches a numeric dot pattern (one or more\
      \ digits followed by a dot and optional whitespace) will have the text after\
      \ the marker extracted as an item. Non-matching lines are ignored. The order\
      \ of extracted items matches their appearance in the input.\n\nReturns:\n  \
      \  list[str]\n        A list of extracted items in the input order.\n\nRaises:\n\
      \    AttributeError\n        If numbered_list_str does not support strip or\
      \ split (i.e., is not a string-like object)."
  - node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks_by_key
    name: format_response_hyperlinks_by_key
    signature: "def format_response_hyperlinks_by_key(\n    str_response: str, key:\
      \ str, anchor: str, search_type: str = \"\"\n)"
    docstring: "Format response to show hyperlinks inside the response UI by key.\n\
      \nArgs:\n    str_response: The response string to process.\n    key: The key\
      \ label in the response to locate citation patterns (e.g., \"Entities\").\n\
      \    anchor: The anchor component used to construct hyperlink targets.\n   \
      \ search_type: The search type value; used to build the hyperlink href. Optional.\n\
      \nReturns:\n    str: The response string with the matched citation numbers converted\
      \ to hyperlinks."
  - node_id: unified-search-app/app/ui/search.py::get_ids_per_key
    name: get_ids_per_key
    signature: 'def get_ids_per_key(str_response: str, key: str)'
    docstring: "Get IDs associated with a given key from a string response.\n\nArgs:\n\
      \  str_response (str): The string to search for occurrences of the pattern 'key\
      \ (<numbers>)'.\n  key (str): The prefix text preceding the parenthesized list\
      \ of IDs.\n\nReturns:\n  List[str]: The IDs extracted from the parentheses after\
      \ the last matching occurrence of the key. If no occurrences are found, returns\
      \ an empty list.\n\nNotes:\n  - If multiple matches are found, only the IDs\
      \ from the last match are returned.\n  - IDs are strings and may contain whitespace;\
      \ you may trim each element if needed. The IDs are extracted from within the\
      \ first pair of parentheses following the key and are split on commas.\n  -\
      \ The function does not perform type checking and assumes inputs are strings;\
      \ it does not raise a TypeError on invalid types.\n\nPattern:\n  A regular expression\
      \ that matches a parenthesized list of digits separated by commas, optionally\
      \ followed by , +more."
  - node_id: unified-search-app/app/ui/search.py::format_suggested_questions
    name: format_suggested_questions
    signature: 'def format_suggested_questions(questions: str)'
    docstring: "Format suggested questions to the UI.\n\nArgs:\n    questions: str\n\
      \        A string containing suggested questions. The function removes square-bracketed\
      \ citations (patterns like [ ... ]) and then converts the remaining text into\
      \ an array by parsing a numbered list.\n\nReturns:\n    list[str]\n        A\
      \ list of extracted questions obtained from the remaining numbered-list text."
  - node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks
    name: format_response_hyperlinks
    signature: 'def format_response_hyperlinks(str_response: str, search_type: str
      = "")'
    docstring: "Format response to show hyperlinks inside the response UI.\n\nArgs:\n\
      \    str_response (str): The response string to process.\n    search_type (str):\
      \ The search type value; used to build the hyperlink href. Optional.\nReturns:\n\
      \    str: The response string with the matched citation numbers converted to\
      \ hyperlinks."
  - node_id: unified-search-app/app/ui/search.py::display_citations
    name: display_citations
    signature: "def display_citations(\n    container: DeltaGenerator | None = None,\
      \ result: SearchResult | None = None\n)"
    docstring: "Display citations into the UI.\n\nArgs:\n  container: DeltaGenerator\
      \ | None = None \u2014 The UI container to render citations into. If None, citations\
      \ will not be rendered.\n  result: SearchResult | None = None \u2014 The SearchResult\
      \ containing the context data to display as citations. If provided, the context\
      \ data will be processed and displayed.\n\nReturns:\n  None."
  - node_id: unified-search-app/app/ui/search.py::display_graph_citations
    name: display_graph_citations
    signature: "def display_graph_citations(\n    entities: pd.DataFrame, relationships:\
      \ pd.DataFrame, citation_type: str\n)"
    docstring: "Display graph citations into the UI.\n\nArgs:\n  entities: pd.DataFrame\
      \ \u2014 AI-extracted entities to render in the UI.\n  relationships: pd.DataFrame\
      \ \u2014 AI-extracted relationships to render in the UI.\n  citation_type: str\
      \ \u2014 The type used when rendering the HTML tables (passed to render_html_table).\n\
      \nReturns:\n  None \u2014 This function does not return a value."
  - node_id: unified-search-app/app/ui/search.py::display_search_result
    name: display_search_result
    signature: "def display_search_result(\n    container: DeltaGenerator, result:\
      \ SearchResult, stats: SearchStats | None = None\n)"
    docstring: "Display search results in the UI and update the corresponding placeholder.\n\
      \nThis function formats the search result response with hyperlinks via format_response_hyperlinks,\
      \ renders it as HTML in the provided Streamlit container, and stores the rendered\
      \ content in a session_state placeholder derived from the result's search_type\
      \ (for example, a placeholder named \"<search_type>_response_placeholder\").\
      \ If stats are provided and completion_time is available, it also renders a\
      \ summary line showing tokens used, LLM calls, and elapsed time.\n\nArgs:\n\
      \    container (DeltaGenerator): The Streamlit container to render the search\
      \ result into.\n    result (SearchResult): The search result data to display,\
      \ including response and the\n        search_type used to derive UI keys and\
      \ the HTML element id.\n    stats (SearchStats | None): Optional statistics\
      \ about the search operation. When provided\n        and completion_time is\
      \ not None, a formatted stats line is shown.\n\nReturns:\n    None"
  classes: []
- file: unified-search-app/app/ui/sidebar.py
  functions:
  - node_id: unified-search-app/app/ui/sidebar.py::update_basic_rag
    name: update_basic_rag
    signature: 'def update_basic_rag(sv: SessionVariables)'
    docstring: "Update basic rag state.\n\nArgs:\n    sv: SessionVariables\n     \
      \   The container of session variables; used to read and update the include_basic_rag\
      \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The\
      \ function does not return a value.\n\nRaises:\n    KeyError\n        If the\
      \ expected key sv.include_basic_rag.key is not found in st.session_state."
  - node_id: unified-search-app/app/ui/sidebar.py::reset_app
    name: reset_app
    signature: def reset_app()
    docstring: "Reset app to its original state.\n\nClears the Streamlit data cache\
      \ and the session state, then reruns the app to restore its initial state.\n\
      \nReturns:\n    None"
  - node_id: unified-search-app/app/ui/sidebar.py::update_global_search
    name: update_global_search
    signature: 'def update_global_search(sv: SessionVariables)'
    docstring: "Update global rag state.\n\nArgs:\n    sv: SessionVariables\n    \
      \    The container of session variables; used to read and update the include_global_search\
      \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The\
      \ function does not return a value.\n\nRaises:\n    KeyError\n        If the\
      \ expected key sv.include_global_search.key is not found in st.session_state."
  - node_id: unified-search-app/app/ui/sidebar.py::lookup_label
    name: lookup_label
    signature: 'def lookup_label(key: str)'
    docstring: "Return the display label for the given dataset key.\n\nArgs:\n   \
      \ key: The dataset key to lookup the label for.\n\nReturns:\n    The label corresponding\
      \ to the dataset key, as determined by dataset_name(key, sv).\n\nRaises:\n \
      \   Exception: Exceptions raised by dataset_name may be propagated."
  - node_id: unified-search-app/app/ui/sidebar.py::update_drift_search
    name: update_drift_search
    signature: 'def update_drift_search(sv: SessionVariables)'
    docstring: "Update drift rag state.\n\nArgs:\n    sv: SessionVariables\n     \
      \   The container of session variables; used to read and update the include_drift_search\
      \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The\
      \ function does not return a value.\n\nRaises:\n    KeyError\n        If the\
      \ expected key sv.include_drift_search.key is not found in st.session_state."
  - node_id: unified-search-app/app/ui/sidebar.py::update_local_search
    name: update_local_search
    signature: 'def update_local_search(sv: SessionVariables)'
    docstring: "Update local rag state.\n\nArgs:\n    sv: SessionVariables\n     \
      \   The container of session variables; used to read and update the include_local_search\
      \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The\
      \ function does not return a value.\n\nRaises:\n    KeyError\n        If the\
      \ expected key sv.include_local_search.key is not found in st.session_state...."
  - node_id: unified-search-app/app/ui/sidebar.py::create_side_bar
    name: create_side_bar
    signature: 'def create_side_bar(sv: SessionVariables)'
    docstring: "Create a Streamlit sidebar panel in the app to configure dataset selection,\
      \ the number of suggested questions, and search options.\n\nThis function renders\
      \ the following UI components inside the Streamlit sidebar:\n- a selectbox labeled\
      \ \"Dataset\" to choose a dataset (options derived from sv.datasets.value and\
      \ displayed using dataset_name as the label)\n- a number input labeled \"Number\
      \ of suggested questions\" for the count\n- a subheader \"Search options:\"\
      \ followed by four toggles:\n  - \"Include basic RAG\"\n  - \"Include local\
      \ search\"\n  - \"Include global search\"\n  - \"Include drift search\" \n\n\
      Behavior notes:\n- Uses sv as the source of keys and current values, and registers\
      \ callbacks (on_change) to update state when widgets change.\n- The function\
      \ does not return a value (returns None) and renders directly to the UI.\n\n\
      Assumptions:\n- sv implements the following attributes and structure:\n  - sv.datasets.value\
      \ is iterable of items with .key\n  - sv.dataset.key is the widget key for the\
      \ dataset selectbox\n  - sv.suggested_questions.key is the key for the number\
      \ input\n  - sv.include_basic_rag.key, sv.include_local_search.key, sv.include_global_search.key,\
      \ sv.include_drift_search.key are keys for the toggles\n  - update_dataset,\
      \ update_basic_rag, update_local_search, update_global_search, update_drift_search\
      \ are defined to handle changes\n\nRaises:\n- AttributeError if any required\
      \ sv attribute is missing."
  - node_id: unified-search-app/app/ui/sidebar.py::update_dataset
    name: update_dataset
    signature: 'def update_dataset(sv: SessionVariables)'
    docstring: "Update dataset from the dropdown and reinitialize related UI state.\n\
      \nArgs:\n    sv: SessionVariables\n        Container holding session-related\
      \ configuration, including dataset metadata and the keys used to read UI state\
      \ from st.session_state. In particular, sv.dataset.key is used to retrieve the\
      \ selected dataset value.\n\nReturns:\n    None\n\nSide effects:\n- Clears the\
      \ Streamlit cache using st.cache_data.clear().\n- Ensures st.session_state.response_lengths\
      \ exists; resets it to an empty list.\n- Loads the selected dataset via load_dataset(value,\
      \ sv), where value is obtained as value = st.session_state[sv.dataset.key].\n\
      \nNotes:\n- Exceptions are not handled within this function; they propagate\
      \ to the caller."
  classes: []
