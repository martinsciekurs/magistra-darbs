- class_id: tests/integration/storage/test_factory.py::CustomStorage
  file: tests/integration/storage/test_factory.py
  name: CustomStorage
  docstring: "CustomStorage: Test double implementing the PipelineStorage interface\
    \ for integration tests.\n\nThis class provides a minimal storage backend used\
    \ in tests to validate StorageFactory behavior by implementing the methods defined\
    \ in PipelineStorage: get, get_creation_date, delete, find, keys, child, has,\
    \ clear, and set. The constructor accepts arbitrary keyword arguments which are\
    \ ignored.\n\nArgs:\n    kwargs (dict[str, Any]): Keyword arguments passed to\
    \ the initializer. They are ignored.\n\nReturns:\n    None\n\nRaises:\n    None"
  methods:
  - name: get
    signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
      \ encoding: str | None = None\n        ) -> Any"
  - name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
  - name: delete
    signature: 'def delete(self, key: str) -> None'
  - name: find
    signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
      \            base_dir: str | None = None,\n            file_filter: dict[str,\
      \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]"
  - name: keys
    signature: def keys(self) -> list[str]
  - name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: clear
    signature: def clear(self) -> None
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: __init__
    signature: def __init__(self, **kwargs)
- class_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry
  file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  name: RandomWaitRetry
  docstring: "Retry policy that retries both asynchronous and synchronous functions\
    \ using a random delay between attempts, up to a configurable maximum number of\
    \ retries.\n\nArgs:\n    max_retry_wait: The maximum delay, in seconds, between\
    \ retries. The actual delay is drawn uniformly from [0, max_retry_wait].\n   \
    \ max_retries: The maximum number of retry attempts. Must be greater than 0.\n\
    \    kwargs: Additional keyword arguments accepted by the constructor (for extensibility).\n\
    \nReturns:\n    None\n\nRaises:\n    ValueError: If max_retries <= 0 or max_retry_wait\
    \ <= 0.\n\nAttributes:\n    max_retry_wait: float - maximum delay (seconds) between\
    \ retries.\n    max_retries: int - maximum number of retry attempts.\n    logger:\
    \ logging.Logger - internal logger for debug messages (if initialized).\n\nSummary:\n\
    \    - aretry retries an asynchronous function by awaiting func(**kwargs) until\
    \ it succeeds or max_retries is reached; returns the result on success.\n    -\
    \ retry retries a synchronous function by invoking func(**kwargs) until it succeeds\
    \ or max_retries is reached; returns the result on success.\n    - The delay before\
    \ each retry is a random value drawn uniformly from [0, max_retry_wait]. For the\
    \ async path this is implemented via await asyncio.sleep(delay); for the sync\
    \ path this uses time.sleep(delay).\n    - If all attempts fail, the last raised\
    \ exception is propagated to the caller."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
  - name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
  - name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
- class_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: CosmosDBPipelineStorage
  docstring: 'CosmosDBPipelineStorage is a Cosmos DB-backed storage backend that stores
    and retrieves data in a Cosmos DB container. It implements the storage interface
    used by Graphrag to manage databases, containers, and items, including creation/deletion
    of databases and containers, insertion of file contents, retrieval, and query-based
    operations. It maintains an internal reference to the active container via _container_client,
    and this reference is cleared when the database is deleted or when no container
    exists, reflecting the lifecycle of the underlying Cosmos DB resources.


    Summary:

    - Provides a persistent storage layer backed by Azure Cosmos DB for file-based
    data.

    - Manages the lifecycle of a database and container and exposes item-level operations
    consistent with the PipelineStorage API.

    - Keeps an internal _container_client to interact with the current container;
    this is cleared when the database/container is removed.


    Args:

    - cosmosdb_account_url: The URL of the Cosmos DB account. Used to initialize CosmosClient
    when a connection string is not provided.

    - connection_string: The Cosmos DB connection string. Used to initialize CosmosClient
    when provided.

    - base_dir: The database name to create/use.

    - container_name: The container name to create/use.

    - encoding: Encoding to use for serialization/deserialization (e.g., utf-8).


    Returns:

    - None: This constructor does not return a value.


    Raises:

    - CosmosHttpResponseError: If the Cosmos DB service returns an HTTP error during
    initialization or resource creation.'
  methods:
  - name: _delete_database
    signature: def _delete_database(self) -> None
  - name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
  - name: clear
    signature: def clear(self) -> None
  - name: keys
    signature: def keys(self) -> list[str]
  - name: _delete_container
    signature: def _delete_container(self) -> None
  - name: child
    signature: 'def child(self, name: str | None) -> PipelineStorage'
  - name: _create_container
    signature: def _create_container(self) -> None
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: _create_database
    signature: def _create_database(self) -> None
  - name: delete
    signature: 'def delete(self, key: str) -> None'
  - name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
  - name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
  - name: _get_prefix
    signature: 'def _get_prefix(self, key: str) -> str'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
  - name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
- class_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks
  file: graphrag/callbacks/workflow_callbacks.py
  name: WorkflowCallbacks
  docstring: "WorkflowCallbacks defines a Protocol for observers of workflow and pipeline\
    \ lifecycle events. Implementations act as stateless observers that react to progress\
    \ updates and the start/end signals emitted by the orchestration layer.\n\nMethods\n\
    \nprogress(self, progress: Progress) -> None\n    Called when a progress update\
    \ occurs.\n    progress: Progress object representing the current progress event.\n\
    \    Returns: None\n\nworkflow_start(self, name: str, instance: object) -> None\n\
    \    Invoked when a workflow starts.\n    name: The name of the workflow starting.\n\
    \    instance: The workflow instance object associated with the start event.\n\
    \    Returns: None\n\npipeline_start(self, names: list[str]) -> None\n    Invoked\
    \ to signal that the entire pipeline starts.\n    names: The names of the pipelines\
    \ that started.\n    Returns: None\n\npipeline_end(self, results: list[PipelineRunResult])\
    \ -> None\n    Invoked to signal that the entire pipeline ends.\n    results:\
    \ A list of PipelineRunResult objects representing the pipeline results.\n   \
    \ Returns: None\n\nworkflow_end(self, name: str, instance: object) -> None\n \
    \   Invoked when a workflow ends.\n    name: The name of the workflow.\n    instance:\
    \ The workflow instance object.\n    Returns: None\n\nUsage\nObservers implement\
    \ this Protocol and register with the orchestration system to receive these callbacks\
    \ during workflow and pipeline lifecycle events."
  methods:
  - name: progress
    signature: 'def progress(self, progress: Progress) -> None'
  - name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
  - name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
  - name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  - name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
- class_id: graphrag/query/structured_search/drift_search/action.py::DriftAction
  file: graphrag/query/structured_search/drift_search/action.py
  name: DriftAction
  docstring: "DriftAction represents a single drift action within a structured drift\
    \ search workflow. It encapsulates a query, an optional answer, and an optional\
    \ list of follow-up actions, and it carries a score that may be computed later\
    \ by a scorer.\n\nArgs:\n  query (str): The query for the action.\n  answer (Optional[str]):\
    \ The answer to the query, if available.\n  follow_ups (Optional[List['DriftAction']]):\
    \ A list of follow-up actions.\n\nAttributes:\n  query (str): The action's query\
    \ string.\n  answer (Optional[str]): The answer to the query, if available.\n\
    \  follow_ups (Optional[List['DriftAction']]): Nested follow-up actions.\n  score\
    \ (Optional[float]): The computed score for this action. May be None before scoring.\n\
    \nNotes:\n  score is initialized to None. Use compute_score(scorer) to assign\
    \ a numeric score based on the query and answer; if the scorer returns None, score\
    \ remains None.\n\nNotable methods:\n  compute_score(scorer): Compute and assign\
    \ the action's score using the provided scorer.\n  __hash__(self) -> int: Return\
    \ a hash based on the query to enable hashing in graphs. Assumes queries are unique.\n\
    \  __init__(self, query: str, answer: Optional[str] = None, follow_ups: Optional[List['DriftAction']]\
    \ = None) -> None: Initialize the action with a query, optional answer, and optional\
    \ follow-ups.\n  serialize(self, include_follow_ups: bool = True) -> dict[str,\
    \ Any]: Serialize the action to a dict; optionally include serialized follow-ups.\n\
    \  is_complete(self) -> bool: Return True if an answer is present.\n  from_primer_response(cls,\
    \ query: str, response: str | dict[str, Any] | list[dict[str, Any]]) -> \"DriftAction\"\
    : Create a DriftAction from a DRIFTPrimer response.\n  deserialize(cls, data:\
    \ dict[str, Any]) -> \"DriftAction\": Deserialize an action from a dict.\n  __eq__(self,\
    \ other: object) -> bool: Equality based on the query.\n  search(self, search_engine:\
    \ Any, global_query: str, scorer: Any = None) -> \"DriftAction\": Execute an asynchronous\
    \ search and update the action; if a scorer is provided, compute the score."
  methods:
  - name: compute_score
    signature: 'def compute_score(self, scorer: Any)'
  - name: __hash__
    signature: def __hash__(self) -> int
  - name: __init__
    signature: "def __init__(\n        self,\n        query: str,\n        answer:\
      \ str | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
      \    )"
  - name: serialize
    signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str,
      Any]'
  - name: is_complete
    signature: def is_complete(self) -> bool
  - name: from_primer_response
    signature: "def from_primer_response(\n        cls, query: str, response: str\
      \ | dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
  - name: deserialize
    signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
  - name: __eq__
    signature: 'def __eq__(self, other: object) -> bool'
  - name: search
    signature: 'def search(self, search_engine: Any, global_query: str, scorer: Any
      = None)'
- class_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: TestStableLCC
  docstring: 'TestStableLCC is a unit test suite for validating the stability and
    correctness of the stable_largest_connected_component function across both undirected
    and directed graphs.


    Purpose:

    - Verify determinism: ensure stable_largest_connected_component returns identical
    graphs on repeated runs, even when input edges are flipped.

    - Preserve directed relationships: ensure that, for DiGraph inputs, the source
    and target directions are preserved in the resulting components.

    - Validate consistency across graph types: cover both undirected and directed
    graphs.


    Key helpers:

    - _create_strongly_connected_graph(digraph=False): helper to construct a representative
    linear chain graph with node attributes; by default undirected Graph, DiGraph
    when digraph=True.

    - _create_strongly_connected_graph_with_edges_flipped(digraph=False): helper to
    construct a five-node graph used for stability tests; flips edges scenario; supports
    Graph or DiGraph.


    Tests provided:

    - test_undirected_graph_run_twice_produces_same_graph

    - test_directed_graph_keeps_source_target_intact

    - test_directed_graph_run_twice_produces_same_graph


    Note: This docstring focuses on high-level intent and test coverage; it omits
    implementation details.'
  methods:
  - name: _create_strongly_connected_graph
    signature: def _create_strongly_connected_graph(self, digraph=False)
  - name: _create_strongly_connected_graph_with_edges_flipped
    signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
  - name: test_undirected_graph_run_twice_produces_same_graph
    signature: def test_undirected_graph_run_twice_produces_same_graph(self)
  - name: test_directed_graph_keeps_source_target_intact
    signature: def test_directed_graph_keeps_source_target_intact(self)
  - name: test_directed_graph_run_twice_produces_same_graph
    signature: def test_directed_graph_run_twice_produces_same_graph(self)
- class_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
  file: graphrag/vector_stores/azure_ai_search.py
  name: AzureAISearchVectorStore
  docstring: "Azure AI Search vector store integrating Azure Cognitive Search to provide\
    \ vector-based retrieval.\n\nThis class provides vector similarity search, text-based\
    \ search, and document load/retrieval backed by Azure AI Search. It delegates\
    \ initialization to the base vector store class and is configured via VectorStoreSchemaConfig.\n\
    \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema configuration\
    \ for the vector store.\n  **kwargs: Any - Additional keyword arguments forwarded\
    \ to the base class initializer.\n\nReturns:\n  None\n\nRaises:\n  Exceptions\
    \ raised by the base class __init__ are propagated."
  methods:
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
- class_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  file: graphrag/callbacks/noop_query_callbacks.py
  name: NoopQueryCallbacks
  docstring: "NoopQueryCallbacks is a no-op implementation of the QueryCallbacks interface\
    \ used for query callback events. It deliberately performs no actions and maintains\
    \ no internal state.\n\nArgs:\n  None: This class has no constructor parameters.\n\
    \nReturns:\n  None: The class does not return a value.\n\nRaises:\n  None: This\
    \ class does not raise exceptions by itself.\n\nAttributes:\n  None: This class\
    \ maintains no internal state."
  methods:
  - name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
  - name: on_llm_new_token
    signature: def on_llm_new_token(self, token)
  - name: on_context
    signature: 'def on_context(self, context: Any) -> None'
  - name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
  - name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
  - name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
- class_id: graphrag/query/structured_search/drift_search/state.py::QueryState
  file: graphrag/query/structured_search/drift_search/state.py
  name: QueryState
  docstring: "Represents the state of a drift search query as a graph of DriftAction\
    \ nodes.\n\nArgs:\n    self: The instance being initialized. This class is instantiated\
    \ without external parameters.\n\nReturns:\n    None\n\nRaises:\n    None\n\n\
    Purpose/responsibility:\n    Manage a directed graph of DriftAction nodes, enabling\
    \ addition of actions, linking follow-ups, serialization/deserialization, and\
    \ ranking of incomplete actions during structured drift search.\n\nKey attributes:\n\
    \    graph (nx.DiGraph): The directed graph storing DriftAction nodes and their\
    \ relationships. Each node carries metadata with keys 'llm_calls', 'prompt_tokens',\
    \ and 'output_tokens'."
  methods:
  - name: action_token_ct
    signature: def action_token_ct(self) -> dict[str, int]
  - name: __init__
    signature: def __init__(self)
  - name: serialize
    signature: "def serialize(\n        self, include_context: bool = True\n    )\
      \ -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
  - name: find_incomplete_actions
    signature: def find_incomplete_actions(self) -> list[DriftAction]
  - name: add_action
    signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any]
      | None = None)'
  - name: add_all_follow_ups
    signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
      \        follow_ups: list[DriftAction] | list[str],\n        weight: float =\
      \ 1.0,\n    )"
  - name: deserialize
    signature: 'def deserialize(self, data: dict[str, Any])'
  - name: relate_actions
    signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
      \ weight: float = 1.0\n    )"
  - name: rank_incomplete_actions
    signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
      \ float] | None = None\n    ) -> list[DriftAction]"
- class_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider
  file: graphrag/language_model/providers/fnllm/cache.py
  name: FNLLMCacheProvider
  docstring: "FNLLMCacheProvider adapts a PipelineCache to the FNLLM cache interface\
    \ by delegating all cache operations to the underlying cache instance.\n\nAttributes:\n\
    - _cache: The underlying cache instance used by this provider to perform cache\
    \ operations.\n\nArgs:\n  cache: The underlying PipelineCache instance used by\
    \ this provider.\n\nReturns:\n  None\n\nRaises:\n  Exceptions may propagate from\
    \ the underlying cache operations."
  methods:
  - name: clear
    signature: def clear(self) -> None
  - name: get
    signature: 'def get(self, key: str) -> Any | None'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: remove
    signature: 'def remove(self, key: str) -> None'
  - name: __init__
    signature: 'def __init__(self, cache: PipelineCache)'
  - name: set
    signature: "def set(\n        self, key: str, value: Any, metadata: dict[str,\
      \ Any] | None = None\n    ) -> None"
  - name: child
    signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
- class_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch
  file: graphrag/query/structured_search/drift_search/search.py
  name: DRIFTSearch
  docstring: 'DRIFTSearch orchestrates the DRIFT-style search workflow for structured
    queries by coordinating a language model, a DRIFT context builder, and local search
    components to iteratively refine results.


    Purpose:

    - Manage the end-to-end DRIFT search lifecycle, including initialization, prompt
    construction, local search steps, result aggregation, and optional streaming.


    Key responsibilities:

    - Wire together the language model interface, DRIFT context handling, tokenization,
    and the query lifecycle.

    - Expose methods to initialize local search, perform search steps, and stream
    results.

    - Coordinate reduction of model responses into a final answer and intermediate
    actions.


    Key attributes (inferred from constructor and methods):

    - model: The ChatModel used to interact with the language model.

    - context_builder: DRIFTSearchContextBuilder housing configuration and DRIFT context.

    - tokenizer: Optional Tokenizer used for token handling and prompting/streaming.

    - query_state: Optional QueryState tracking the current query status and results.

    - callbacks: Optional list[QueryCallbacks] for query lifecycle events.


    Notes:

    - This class delegates most operational logic to specialized components (e.g.,
    LocalSearch, DRIFTPrimer) to implement the DRIFT workflow.


    Raises:

    - None. This class does not raise exceptions; errors from underlying components
    may propagate to callers.'
  methods:
  - name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
      \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
      \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
      \    ) -> str"
  - name: _process_primer_results
    signature: "def _process_primer_results(\n        self, query: str, search_results:\
      \ SearchResult\n    ) -> DriftAction"
  - name: _search_step
    signature: "def _search_step(\n        self, global_query: str, search_engine:\
      \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]"
  - name: _reduce_response_streaming
    signature: "def _reduce_response_streaming(\n        self,\n        responses:\
      \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
      \ Any],\n    ) -> AsyncGenerator[str, None]"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n\
      \        query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n    )"
  - name: init_local_search
    signature: def init_local_search(self) -> LocalSearch
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
  - name: stream_search
    signature: "def stream_search(\n        self, query: str, conversation_history:\
      \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
- class_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  file: graphrag/vector_stores/lancedb.py
  name: LanceDBVectorStore
  docstring: "LanceDB-backed vector store that uses LanceDB for storing and querying\
    \ document embeddings.\n\nThis class extends BaseVectorStore to provide similarity\
    \ search by text and by vector, loading documents into LanceDB, filtering by IDs,\
    \ and connecting to a LanceDB database. Key attributes include vector_store_schema_config,\
    \ index_name (optional), and document_collection (the LanceDB table handle).\n\
    \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema configuration\
    \ for the vector store.\n  **kwargs: Any - Additional keyword arguments forwarded\
    \ to the base class initializer.\n\nReturns:\n  None\n\nRaises:\n  Exceptions\
    \ raised by the base class __init__ are propagated."
  methods:
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
      \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
- class_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: LitellmEmbeddingModel
  docstring: LitellmEmbeddingModel wraps Litellm's embedding endpoints to generate
    vector representations for text inputs, with support for batch and single-input
    embeddings, and optional request-handling wrappers for caching, logging, rate
    limiting, and retries.
  methods:
  - name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  - name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
  - name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  - name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
- class_id: tests/mock_provider.py::MockChatLLM
  file: tests/mock_provider.py
  name: MockChatLLM
  docstring: 'MockChatLLM is a configurable mock chat language model provider used
    for testing. It cycles through a predefined sequence of responses and can emit
    them synchronously via chat() or asynchronously via achat_stream(). It supports
    an optional LanguageModelConfig override and a json flag to indicate that responses
    should be treated as JSON where applicable.


    Args:

    - responses: List[str | BaseModel] | None. A list of responses to return in sequence.
    Each item can be a string or a BaseModel.

    - config: LanguageModelConfig | None. Optional configuration. If provided and
    it has a responses attribute, those will be used instead of the responses argument.

    - json: bool. If True, responses are interpreted as JSON content where applicable
    when constructing outputs.

    - kwargs: Any. Additional keyword arguments forwarded to the underlying chat handler.


    Returns:

    - None. Initializes a MockChatLLM instance.


    Raises:

    - NotImplementedError: If chat_stream is called since this streaming interface
    is not yet implemented.


    Notes:

    - If config is provided and exposes a responses attribute, those responses are
    used in place of the explicit responses argument.

    - The non-streaming chat() method returns the next response in the configured
    sequence, cycling through via modulo arithmetic. If no responses are configured,
    an empty content response is returned. The streaming interface for this mock is
    provided via achat_stream(), which yields each configured response in order and
    ignores the input prompt and history. The chat_stream() method is not implemented
    and will raise NotImplementedError if invoked.


    Examples:

    - Basic usage with string responses: create MockChatLLM(responses=["Hi","Hello"])
    and call chat() to retrieve responses in order, cycling when the end is reached.'
  methods:
  - name: achat
    signature: "def achat(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
  - name: __init__
    signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
      \ | None = None,\n        config: LanguageModelConfig | None = None,\n     \
      \   json: bool = False,\n        **kwargs: Any,\n    )"
  - name: achat_stream
    signature: "def achat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]"
  - name: chat
    signature: "def chat(\n        self,\n        prompt: str,\n        history: list\
      \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
  - name: chat_stream
    signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
- class_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: BlobDatasource
  docstring: "BlobDatasource provides access to knowledge data stored in Azure Blob\
    \ Storage, enabling reading Parquet tables and graphrag configurations used by\
    \ the knowledge loader. It connects to a configured Azure Blob container using\
    \ the provided database identifier to locate data and settings.\n\nArgs:\n   \
    \ database (str): The database identifier used to access the blob storage. This\
    \ maps to a logical namespace within the configured container and determines where\
    \ data and settings for this knowledge domain are stored.\n\nAttributes:\n   \
    \ database (str): The database identifier used to access the blob storage.\n\n\
    Methods:\n    read(table, throw_on_missing=False, columns=None) -> pd.DataFrame\n\
    \        Read a Parquet table from blob storage.\n\n        Args:\n          \
    \  table (str): The table name to read (without the .parquet extension).\n   \
    \         throw_on_missing (bool): If True, raise FileNotFoundError when the blob\
    \ does not exist.\n            columns (list[str] | None): Optional subset of\
    \ columns to load; if None, load all columns.\n\n        Returns:\n          \
    \  pd.DataFrame: A DataFrame containing the data from the Parquet file.\n\n  \
    \      Raises:\n            FileNotFoundError: If the table file does not exist\
    \ and throw_on_missing is True.\n            Azure-related errors: Authentication/permission/network-related\
    \ errors may be raised.\n\n    read_settings(file, throw_on_missing=False) ->\
    \ GraphRagConfig | None\n        Read graphrag configuration settings from a blob\
    \ container.\n\n        Args:\n            file (str): The blob file name containing\
    \ the graphrag settings.\n            throw_on_missing (bool): If True, raise\
    \ FileNotFoundError when the settings file does not exist.\n\n        Returns:\n\
    \            GraphRagConfig | None: The graphrag configuration loaded from the\
    \ file, or None if not found.\n\n        Raises:\n            FileNotFoundError:\
    \ If the file is missing and throw_on_missing is True.\n            YAML parsing\
    \ errors or Azure-related errors may occur.\n\nNotes:\n    - Requires Azure credentials\
    \ with access to the storage account and container. Uses DefaultAzureCredential\
    \ to obtain credentials and connects via BlobServiceClient/ContainerClient."
  methods:
  - name: __init__
    signature: 'def __init__(self, database: str)'
  - name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  - name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: BaseNounPhraseExtractor
  docstring: 'BaseNounPhraseExtractor is an abstract base class that defines the interface
    and shared configuration for noun phrase extraction using a SpaCy model. Concrete
    subclasses implement the actual extraction logic and provide a meaningful string
    representation.


    Attributes:

    - model_name: The name of the SpaCy model to use, or None to avoid loading a model
    at initialization time.

    - exclude_nouns: Optional list of nouns to exclude from extraction. If None, an
    empty list is used. Subclasses may normalize or store this differently.

    - max_word_length: Maximum length of a word to consider when forming noun phrases.

    - word_delimiter: Delimiter used to join words within a noun phrase.


    Args:

    - model_name: The name of the SpaCy model to use, or None.

    - exclude_nouns: List of nouns to exclude from extraction. If None, an empty list
    is used.

    - max_word_length: Maximum length of a word to consider when forming noun phrases.

    - word_delimiter: Delimiter used to join words within a noun phrase.


    Returns:

    - None


    Raises:

    - TypeError: If the abstract base class is instantiated directly (enforced by
    the abstract base class mechanism).


    Notes:

    - Instantiation of this class is prevented by the abstract base class mechanism;
    concrete subclasses must implement extract and __str__.


    Load/spacy model helper:

    - load_spacy_model model_name with optional exclude: Load a SpaCy model by name
    and optionally exclude components. Returns a SpaCy language object.


    Subclass contract:

    - Concrete subclasses must implement extract to return a list of noun phrases
    from input text and __str__ to provide a meaningful string representation. The
    base class provides a helper for loading the SpaCy model but does not dictate
    specific extraction logic.'
  methods:
  - name: load_spacy_model
    signature: "def load_spacy_model(\n        model_name: str, exclude: list[str]\
      \ | None = None\n    ) -> spacy.language.Language"
  - name: __init__
    signature: "def __init__(\n        self,\n        model_name: str | None,\n  \
      \      exclude_nouns: list[str] | None = None,\n        max_word_length: int\
      \ = 15,\n        word_delimiter: str = \" \",\n    ) -> None"
  - name: extract
    signature: 'def extract(self, text: str) -> list[str]'
  - name: __str__
    signature: def __str__(self) -> str
- class_id: graphrag/query/context_builder/conversation_history.py::QATurn
  file: graphrag/query/context_builder/conversation_history.py
  name: QATurn
  docstring: "QATurn is a dataclass that represents a single turn in a question-and-answer\
    \ conversation history, pairing a user query with optional assistant answers and\
    \ providing helpers to derive text for display.\n\nAttributes:\n  user_query:\
    \ Optional[object] - The user query for this turn. The object should have a 'content'\
    \ attribute used for display. May be None.\n  assistant_answers: Optional[List[str]]\
    \ - The assistant's answers for this turn. If None, no answer has been provided\
    \ yet.\n\nArgs:\n  user_query: Optional[object] - The user query component for\
    \ this turn. The object should have a 'content' attribute.\n  assistant_answers:\
    \ Optional[List[str]] - The assistant's answers for this turn. None indicates\
    \ no answer yet.\n\nMethods:\n  get_answer_text(self) -> Optional[str]: Returns\
    \ the assistant answers contents joined by newline characters, or None if there\
    \ are no answers.\n  __str__(self) -> str: Returns a readable representation of\
    \ the turn. If assistant answers exist, the string is \"Question: <user_query.content>\\\
    nAnswer: <concatenated answers>\"; otherwise, it's \"Question: <user_query.content>\"\
    ."
  methods:
  - name: get_answer_text
    signature: def get_answer_text(self) -> str | None
  - name: __str__
    signature: def __str__(self) -> str
- class_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
  file: graphrag/vector_stores/cosmosdb.py
  name: CosmosDBVectorStore
  docstring: "CosmosDBVectorStore is a Cosmos DB-backed vector store implementation\
    \ for GraphRAG. It manages storage, retrieval, and indexing of document vectors\
    \ and metadata in Azure Cosmos DB and supports text-based and vector-based similarity\
    \ search.\n\nArgs:\n  vector_store_schema_config (VectorStoreSchemaConfig): The\
    \ schema configuration for the vector store, including field mappings for id,\
    \ vector, text, and attributes, as well as database/container naming.\n  **kwargs:\
    \ Additional keyword arguments forwarded to the base class initializer.\n\nReturns:\n\
    \  None\n\nRaises:\n  CosmosHttpResponseError: If an HTTP error occurs while interacting\
    \ with Cosmos DB (e.g., during database/container operations or listing resources)."
  methods:
  - name: _database_exists
    signature: def _database_exists(self) -> bool
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: _delete_database
    signature: def _delete_database(self) -> None
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
  - name: cosine_similarity
    signature: def cosine_similarity(a, b)
  - name: _create_database
    signature: def _create_database(self) -> None
  - name: _create_container
    signature: def _create_container(self) -> None
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
  - name: _container_exists
    signature: def _container_exists(self) -> bool
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  - name: _delete_container
    signature: def _delete_container(self) -> None
  - name: clear
    signature: def clear(self) -> None
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
- class_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  file: graphrag/storage/blob_pipeline_storage.py
  name: BlobPipelineStorage
  docstring: "BlobPipelineStorage is an Azure Blob Storage backed implementation of\
    \ the PipelineStorage interface for caching pipeline data.\n\nSummary:\nThis class\
    \ provides a blob-based storage backend for caching results and data used by the\
    \ GraphRag pipeline. It stores dataframe exports as JSON or Parquet, supports\
    \ retrieving values, finding blobs by pattern, and basic cache management. Initialization\
    \ selects between using a connection string or a storage account URL with DefaultAzureCredential\
    \ to create the BlobServiceClient, and requires a container name.\n\nAttributes:\n\
    \  _blob_service_client (BlobServiceClient): Client used to interact with Azure\
    \ Blob Storage.\n  _container_name (str): Name of the container in which blobs\
    \ are stored.\n  _path_prefix (str): Path prefix used to scope blob names within\
    \ the container.\n\nArgs:\n  connection_string: Optional Azure Blob Storage connection\
    \ string. If provided, the BlobServiceClient is created from this string.\n  storage_account_blob_url:\
    \ Optional URL to the storage account. If provided (and connection_string is not),\
    \ the BlobServiceClient is created using DefaultAzureCredential.\n  container_name:\
    \ Name of the blob container to use. The container will be created if it does\
    \ not exist.\n\nRaises:\n  ValueError: If neither connection_string nor storage_account_blob_url\
    \ is provided."
  methods:
  - name: _abfs_url
    signature: 'def _abfs_url(self, key: str) -> str'
  - name: keys
    signature: def keys(self) -> list[str]
  - name: _set_df_json
    signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
  - name: _set_df_parquet
    signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
  - name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
  - name: clear
    signature: def clear(self) -> None
  - name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
  - name: _create_container
    signature: def _create_container(self) -> None
  - name: delete
    signature: 'def delete(self, key: str) -> None'
  - name: _container_exists
    signature: def _container_exists(self) -> bool
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: _keyname
    signature: 'def _keyname(self, key: str) -> str'
  - name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
  - name: _delete_container
    signature: def _delete_container(self) -> None
  - name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  - name: _blobname
    signature: 'def _blobname(blob_name: str) -> str'
  - name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
- class_id: graphrag/vector_stores/base.py::BaseVectorStore
  file: graphrag/vector_stores/base.py
  name: BaseVectorStore
  docstring: "Abstract base class for vector stores used by GraphRAG.\n\nThis class\
    \ defines the core interface and common lifecycle for vector-based storage and\
    \ retrieval of documents. Subclasses must implement the storage backend and search\
    \ logic for both vector and text queries, while this class provides a consistent\
    \ initialization surface and shared attributes.\n\nArgs:\n    vector_store_schema_config\
    \ (VectorStoreSchemaConfig): The schema configuration for this vector store, including\
    \ vector dimensions and field mappings.\n    db_connection (Any, optional): Optional\
    \ database or resource handle used to connect to the underlying storage. Defaults\
    \ to None.\n    document_collection (Any, optional): Optional existing collection\
    \ of documents within the store. Defaults to None.\n    query_filter (Any, optional):\
    \ Optional default filter applied when retrieving documents. Defaults to None.\n\
    \    **kwargs (Any): Additional keyword arguments for subclass-specific behavior.\n\
    \nAttributes:\n    vector_store_schema_config: The configuration used to configure\
    \ this store's schema and vector properties.\n    db_connection: Optional store\
    \ connection/resource.\n    document_collection: Optional stored document collection.\n\
    \    query_filter: Optional default query filter.\n\nAbstract methods (must be\
    \ implemented by subclasses):\n    connect(self, **kwargs): Establish a connection\
    \ to the vector store.\n    load_documents(self, documents, overwrite=True): Load\
    \ documents into the store.\n    similarity_search_by_vector(self, query_embedding,\
    \ k=10, **kwargs): ANN search by vector.\n    similarity_search_by_text(self,\
    \ text, text_embedder, k=10, **kwargs): ANN search by text.\n    search_by_id(self,\
    \ id): Retrieve a document by its identifier.\n    filter_by_id(self, include_ids):\
    \ Build a filter to limit results by IDs.\n\nNotes:\n    This base class is not\
    \ intended to be instantiated directly. Concrete implementations should provide\
    \ the specifics of the storage backend and search mechanics."
  methods:
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
  - name: __init__
    signature: "def __init__(\n        self,\n        vector_store_schema_config:\
      \ VectorStoreSchemaConfig,\n        db_connection: Any | None = None,\n    \
      \    document_collection: Any | None = None,\n        query_filter: Any | None\
      \ = None,\n        **kwargs: Any,\n    )"
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
- class_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer
  file: graphrag/tokenizer/tiktoken_tokenizer.py
  name: TiktokenTokenizer
  docstring: "TiktokenTokenizer is a Tokenizer implementation that uses the tiktoken\
    \ library to encode and decode text using a specified encoding.\n\nParameters:\n\
    \  encoding_name (str): The name of the Tiktoken encoding to use for tokenization.\n\
    \nAttributes:\n  encoding_name (str): The name of the Tiktoken encoding used for\
    \ tokenization.\n  encoding: The tiktoken encoding object created from encoding_name.\n\
    \nNotes:\n  - Encoding and decoding are performed via the underlying tiktoken\
    \ encoding.\n  - encode(text) returns a list of token IDs for the provided text.\n\
    \  - decode(tokens) returns the string corresponding to the given list of token\
    \ IDs."
  methods:
  - name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
  - name: __init__
    signature: 'def __init__(self, encoding_name: str) -> None'
  - name: encode
    signature: 'def encode(self, text: str) -> list[int]'
- class_id: graphrag/query/structured_search/local_search/search.py::LocalSearch
  file: graphrag/query/structured_search/local_search/search.py
  name: LocalSearch
  docstring: "LocalSearch orchestrates local search operations by building a compact\
    \ context and querying a language model to generate an answer for a user query.\
    \ It coordinates the context builder, optional tokenizer, system prompt, and callbacks\
    \ to produce a structured search result or a streaming output.\n\nPurpose:\n-\
    \ Build a local search context that fits a single context window and generates\
    \ an answer for the user query.\n\nArgs:\n    model: The language model interface\
    \ used for this local search.\n    context_builder: The builder that constructs\
    \ the context for the local search.\n    tokenizer: Optional tokenizer to use.\n\
    \    system_prompt: System prompt for the local search. If None, uses the default\
    \ system prompt.\n    response_type: The format of the response produced by the\
    \ search.\n    callbacks: The query callbacks to be invoked during search.\n \
    \   model_params: Parameters for the underlying language model.\n    context_builder_params:\
    \ Parameters for the context builder.\n\nReturns:\n- LocalSearch: An initialized\
    \ LocalSearch instance.\n\nRaises:\n- Exceptions from the underlying components\
    \ (model, tokenizer, context builder) may propagate to the caller."
  methods:
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
  - name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator"
- class_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry
  file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  name: IncrementalWaitRetry
  docstring: "IncrementalWaitRetry provides a retry strategy that inserts incremental\
    \ delays between attempts for both asynchronous and synchronous callables.\n\n\
    Attributes:\n  max_retry_wait (float): The maximum delay between retries in seconds.\n\
    \  max_retries (int): The maximum number of retry attempts.\n  base_delay (float,\
    \ optional): Optional initial delay used in the incremental computation.\n  delay_increment\
    \ (float, optional): Optional per-retry increment for the incremental delay.\n\
    \nDelay calculation:\n  The delay before the nth retry is computed as:\n  delay_n\
    \ = min(max_retry_wait, base_delay + (n - 1) * delay_increment)\n  where n starts\
    \ at 1 for the first retry. If base_delay or delay_increment are not provided,\
    \ the implementation uses sensible defaults.\n\naretry:\n  Retry an asynchronous\
    \ callable until success or the maximum number of retries is reached. The function\
    \ is invoked as await func(**kwargs). The computed incremental delay is applied\
    \ between attempts. Returns the result of the awaited function on success; if\
    \ all retries fail, the last raised exception is propagated.\n\nretry:\n  Retry\
    \ a synchronous callable until success or the maximum number of retries is reached.\
    \ The function is invoked as func(**kwargs). The computed incremental delay is\
    \ applied between attempts. Returns the value returned by the wrapped function\
    \ on success; if all retries fail, the last raised exception is propagated.\n\n\
    Raises:\n  ValueError: max_retries must be greater than 0.\n  ValueError: max_retry_wait\
    \ must be greater than 0."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
  - name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
  - name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
- class_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks
  file: graphrag/callbacks/query_callbacks.py
  name: QueryCallbacks
  docstring: "QueryCallbacks is a base class that defines callback hooks used during\
    \ a query processing workflow involving map and reduce operations and interactions\
    \ with a language model.\n\nPurpose:\n    Provide default, overridable callback\
    \ methods for lifecycle events such as starting and ending map and reduce operations,\
    \ handling new tokens from the LLM, and processing context data.\n\nKey attributes:\n\
    \    None documented. This class does not define persistent state in the provided\
    \ data.\n\nSummary:\n    The class declares the following callback methods:\n\
    \    - on_reduce_response_start(reduce_response_context: str | dict[str, Any])\
    \ -> None\n    - on_llm_new_token(token) -> None\n    - on_map_response_end(map_response_outputs:\
    \ list[SearchResult]) -> None\n    - on_map_response_start(map_response_contexts:\
    \ list[str]) -> None\n    - on_context(context: Any) -> None\n    - on_reduce_response_end(reduce_response_output:\
    \ str) -> None\n\n    Subclasses may override these methods to implement custom\
    \ side effects. In particular, on_map_response_end and on_context describe no-op\
    \ default behavior."
  methods:
  - name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
  - name: on_llm_new_token
    signature: def on_llm_new_token(self, token) -> None
  - name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
  - name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
  - name: on_context
    signature: 'def on_context(self, context: Any) -> None'
  - name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
- class_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore
  file: tests/integration/vector_stores/test_factory.py
  name: CustomVectorStore
  docstring: "Test utility vector store used in tests that forwards initialization\
    \ to the base vector store.\n\nThis class provides a minimal implementation of\
    \ the vector store interface to support tests by forwarding keyword arguments\
    \ to BaseVectorStore.__init__.\n\nArgs:\n  kwargs: dict of keyword arguments forwarded\
    \ to BaseVectorStore.__init__\n\nReturns:\n  None\n\nRaises:\n  Propagates exceptions\
    \ raised by BaseVectorStore.__init__"
  methods:
  - name: similarity_search_by_text
    signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
  - name: filter_by_id
    signature: def filter_by_id(self, include_ids)
  - name: connect
    signature: def connect(self, **kwargs)
  - name: similarity_search_by_vector
    signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
  - name: __init__
    signature: def __init__(self, **kwargs)
  - name: load_documents
    signature: def load_documents(self, documents, overwrite=True)
  - name: search_by_id
    signature: def search_by_id(self, id)
- class_id: graphrag/config/enums.py::ModelType
  file: graphrag/config/enums.py
  name: ModelType
  docstring: 'ModelType is an enumeration of string-based model type identifiers used
    in graphrag''s configuration.


    Purpose:

    Provide a centralized, type-safe collection of ModelType Members that map to backend
    and component identifiers such as lancedb, azure_ai_search, cosmosdb, embedding,
    chat, and related utilities. This helps ensure consistency across configuration
    and usage wherever model types are referenced.


    Enum Members:

    - LanceDB: "lancedb"

    - AzureAISearch: "azure_ai_search"

    - CosmosDB: "cosmosdb"

    - OpenAIEmbedding: "openai_embedding"

    - AzureOpenAIEmbedding: "azure_openai_embedding"

    - Embedding: "embedding"

    - OpenAIChat: "openai_chat"

    - AzureOpenAIChat: "azure_openai_chat"

    - Chat: "chat"

    - MockChat: "mock_chat"

    - MockEmbedding: "mock_embedding"

    - APIKey: "api_key"

    - AzureManagedIdentity: "azure_managed_identity"

    - AsyncIO: "asyncio"

    - Threaded: "threaded"

    - LOCAL: "local"

    - GLOBAL: "global"

    - DRIFT: "drift"

    - BASIC: "basic"

    - Standard: "standard"

    - Fast: "fast"

    - StandardUpdate: "standard-update"

    - FastUpdate: "fast-update"

    - RegexEnglish: "regex_english"

    - Syntactic: "syntactic_parser"

    - CFG: "cfg"

    - Graph: "graph"

    - LCC: "lcc"

    - WeightedComponents: "weighted_components"


    Args:

    None: This Enum does not require constructor arguments.


    Returns:

    There is no return value for a class. To obtain the underlying string, use member.value.


    Raises:

    None


    Notes:

    - Access the string value via member.value; the member object itself is an Enum
    member.

    - __repr__ and __str__ representations of Enum members are not guaranteed; do
    not rely on a specific format.'
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: TokenTextSplitter
  docstring: "TokenTextSplitter splits input text into chunks using a token-based\
    \ tokenizer.\n\nArgs:\n    tokenizer: Tokenizer | None\n        Tokenizer to use\
    \ for tokenization. If None, a default tokenizer is obtained via get_tokenizer().\n\
    \    **kwargs: Any\n        Additional keyword arguments forwarded to the base\
    \ class initializer via super().__init__(**kwargs).\n\nAttributes:\n    tokenizer:\
    \ Tokenizer | None\n        Tokenizer used for tokenization. If None, a default\
    \ tokenizer is obtained via get_tokenizer().\n\nReturns:\n    None\n        The\
    \ constructor does not return a value; it initializes the instance.\n\nRaises:\n\
    \    Exception: If get_tokenizer() or the base class initializer raise an exception."
  methods:
  - name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
  - name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> list[str]'
  - name: __init__
    signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None\
      \ = None,\n        **kwargs: Any,\n    )"
- class_id: graphrag/query/structured_search/base.py::BaseSearch
  file: graphrag/query/structured_search/base.py
  name: BaseSearch
  docstring: "Abstract base class for structured searches using a language model and\
    \ contextual builders.\n\nThis ABC defines the interface and provides common configuration\
    \ for search implementations that operate with a ChatModel, a context builder,\
    \ and an optional tokenizer. It also holds optional parameter dictionaries for\
    \ both the model and the context builder.\n\nKey attributes:\n  model: The language\
    \ model interface used for this base search.\n  context_builder: The builder that\
    \ constructs the context for the search (generic type T).\n  tokenizer: Optional\
    \ tokenizer to use. If None, a tokenizer is selected via get_tokenizer().\n  model_params:\
    \ Optional dictionary of parameters to configure the language model.\n  context_builder_params:\
    \ Optional dictionary of parameters to configure the context builder.\n\nArgs:\n\
    \  model: The language model interface used for this base search.\n  context_builder:\
    \ The builder that constructs the context for the search.\n  tokenizer: Optional\
    \ tokenizer to use. If None, a tokenizer is selected via get_tokenizer().\n  model_params:\
    \ Optional dictionary of parameters to configure the language model.\n  context_builder_params:\
    \ Optional dictionary of parameters to configure the context builder.\n\nRaises:\n\
    \  NotImplementedError: Subclasses must implement the abstract methods search\
    \ and stream_search."
  methods:
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  - name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
      \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None\
      \ = None,\n    )"
- class_id: graphrag/utils/api.py::MultiVectorStore
  file: graphrag/utils/api.py
  name: MultiVectorStore
  docstring: Unified interface that combines multiple vector stores into a single
    multi-store for cross-store search and retrieval.
  methods:
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  - name: __init__
    signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
      \        index_names: list[str],\n    )"
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
- class_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage
  file: graphrag/storage/memory_pipeline_storage.py
  name: MemoryPipelineStorage
  docstring: "Memory-based storage backend for pipeline data.\n\nThis class implements\
    \ an in-memory storage backend that stores key-value pairs in a\nPython dictionary\
    \ in memory and conforms to the PipelineStorage interface. It provides\nfast,\
    \ non-persistent storage for pipeline data and supports creating child storages\
    \ for\nisolated namespaces.\n\nAttributes:\n- _storage: dict[str, Any] storing\
    \ the in-memory mapping of keys to values.\n\nArgs:\n  self (MemoryPipelineStorage):\
    \ The instance being initialized. No additional parameters.\n\nReturns:\n  None"
  methods:
  - name: clear
    signature: def clear(self) -> None
  - name: keys
    signature: def keys(self) -> list[str]
  - name: delete
    signature: 'def delete(self, key: str) -> None'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: __init__
    signature: def __init__(self)
  - name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
  - name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
- class_id: graphrag/tokenizer/tokenizer.py::Tokenizer
  file: graphrag/tokenizer/tokenizer.py
  name: Tokenizer
  docstring: 'Abstract interface for tokenization operations.


    This abstract base class defines the contract for encoding text into a sequence
    of token identifiers, determining how many tokens a given text would yield, and
    decoding a list of tokens back into text. Subclasses must implement encode, num_tokens,
    and decode.


    Key attributes:

    None defined at this level. Concrete implementations may expose internal state
    such as a vocabulary or encoding rules.


    Returns:

    This class does not produce values by itself. The concrete methods return the
    types described by their signatures when implemented in subclasses (list[int]
    for encode, int for num_tokens, and str for decode).


    Raises:

    NotImplementedError: The encode, num_tokens, and decode methods must be implemented
    by subclasses.'
  methods:
  - name: encode
    signature: 'def encode(self, text: str) -> list[int]'
  - name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
  - name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
- class_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch
  file: graphrag/query/structured_search/global_search/search.py
  name: GlobalSearch
  docstring: 'GlobalSearch orchestrates a multi-step global search by querying multiple
    batches of community reports in parallel and reducing the results into a final
    answer.


    Purpose:

    Perform a structured global search by running parallel language model calls on
    batches of short summaries and then combining those results to produce a final
    answer.


    Key attributes:

    - model: ChatModel - The language model interface used for this global search.

    - context_builder: GlobalContextBuilder - The builder that constructs the context
    for the search.

    - tokenizer: Tokenizer | None - Optional tokenizer to use; if None, a default
    tokenizer will be used.

    - map_system_prompt: str | None - System prompt for the mapping stage.

    - reduce_system_prompt: str | None - System prompt for the reducing stage.

    - response_type: str - The response format; default "multiple paragraphs".

    - allow_general_knowledge: bool - Whether general knowledge is allowed during
    responses.

    - general_knowledge_inclusion_prompt: str | None - Prompt to include general knowledge
    in responses.

    - json_mode: bool - If True, use JSON-based parsing for responses.

    - callbacks: list[QueryCallbacks] | None - Callbacks invoked during processing.

    - max_data_tokens: int - Maximum tokens used for data pieces.

    - map_llm_params: dict[str, Any] | None - Parameters for map phase LLM calls.

    - reduce_llm_params: dict[str, Any] | None - Parameters for reduce phase LLM calls.

    - map_max_length: int - Maximum length for map outputs.

    - reduce_max_length: int - Maximum length for reduce outputs.

    - context_builder_params: dict[str, Any] | None - Parameters for the context builder.

    - concurrent_coroutines: int - Number of concurrent coroutines to run.


    Args:

    - model: ChatModel - The language model interface used for this global search.

    - context_builder: GlobalContextBuilder - The builder that constructs the context
    for the search.

    - tokenizer: Tokenizer | None - Optional tokenizer to use; if None, a default
    tokenizer will be used.

    - map_system_prompt: str | None - System prompt for the mapping stage.

    - reduce_system_prompt: str | None - System prompt for the reducing stage.

    - response_type: str - The response format; default "multiple paragraphs".

    - allow_general_knowledge: bool - Whether general knowledge is allowed during
    responses.

    - general_knowledge_inclusion_prompt: str | None - Prompt to include general knowledge
    in responses.

    - json_mode: bool - Whether to parse responses as JSON.

    - callbacks: list[QueryCallbacks] | None - Callbacks invoked during processing.

    - max_data_tokens: int - Maximum tokens used for data pieces.

    - map_llm_params: dict[str, Any] | None - Parameters for map phase LLM calls.

    - reduce_llm_params: dict[str, Any] | None - Parameters for reduce phase LLM calls.

    - map_max_length: int - Maximum length for map outputs.

    - reduce_max_length: int - Maximum length for reduce outputs.

    - context_builder_params: dict[str, Any] | None - Parameters for the context builder.

    - concurrent_coroutines: int - Number of concurrent coroutines to run.


    Returns:

    None.


    Raises:

    Exceptions raised by underlying components (e.g., ChatModel, GlobalContextBuilder,
    Tokenizer) may propagate.'
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n     \
      \   map_system_prompt: str | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str = \"multiple paragraphs\",\n  \
      \      allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
      \ str | None = None,\n        json_mode: bool = True,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n        max_data_tokens: int = 8000,\n        map_llm_params:\
      \ dict[str, Any] | None = None,\n        reduce_llm_params: dict[str, Any] |\
      \ None = None,\n        map_max_length: int = 1000,\n        reduce_max_length:\
      \ int = 2000,\n        context_builder_params: dict[str, Any] | None = None,\n\
      \        concurrent_coroutines: int = 32,\n    )"
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
  - name: _map_response_single_batch
    signature: "def _map_response_single_batch(\n        self,\n        context_data:\
      \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
      \    ) -> SearchResult"
  - name: _stream_reduce_response
    signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
      \ list[SearchResult],\n        query: str,\n        max_length: int,\n     \
      \   **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
  - name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  - name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
      \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
  - name: _parse_search_response
    signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
      Any]]'
- class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: OpenAIEmbeddingFNLLM
  docstring: "OpenAI Embedding FNLLM provider that generates text embeddings using\
    \ the FNLLM OpenAI embeddings LLM.\n\nArgs:\n  name: The name to assign to the\
    \ internal cache provider and model instance.\n  config: LanguageModelConfig used\
    \ to derive the OpenAI configuration.\n  callbacks: Optional WorkflowCallbacks;\
    \ if provided, used for workflow event handling.\n  cache: Optional PipelineCache;\
    \ if provided, used to cache results.\n\nReturns:\n  OpenAIEmbeddingFNLLM: An\
    \ initialized instance of the provider.\n\nRaises:\n  Exception: If initialization\
    \ fails due to configuration issues or internal errors."
  methods:
  - name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  - name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
- class_id: graphrag/config/errors.py::AzureApiBaseMissingError
  file: graphrag/config/errors.py
  name: AzureApiBaseMissingError
  docstring: "AzureApiBaseMissingError is an internal exception raised when the Azure\
    \ API Base configuration is missing for the specified LLM type.\n\nThis exception\
    \ signals that the required Azure API Base is not configured for the given LLM\
    \ type.\n\nArgs:\n    llm_type (str): The LLM type for which the API Base is required.\
    \ The value is stored on the instance as llm_type and used to construct the error\
    \ message.\n\nAttributes:\n    llm_type (str): The LLM type for which the API\
    \ Base is required. Used to customize the error message.\n\nNotes:\n    The __init__\
    \ method stores llm_type and creates a descriptive message, typically \"Azure\
    \ API Base missing for LLM type: {llm_type}\".\n\nExamples:\n    raise AzureApiBaseMissingError('text-davinci-003')\
    \ results in an exception with a message indicating the missing API Base for that\
    \ LLM type."
  methods:
  - name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationHistory
  docstring: "ConversationHistory is a helper for storing and manipulating a sequence\
    \ of conversation turns used for prompt construction and QA-turn generation.\n\
    \nThe history is stored as an ordered list of turns, where each turn is represented\
    \ as a dictionary with keys \"role\" and \"content\". Roles typically correspond\
    \ to the constants SYSTEM, USER, and ASSISTANT.\n\nAttributes:\n    turns: list[dict[str,\
    \ str]]\n        The conversation turns in chronological order. Each dictionary\
    \ has keys \"role\" and \"content\".\n\nThis class provides methods to initialize\
    \ from an existing list, add new turns, retrieve user turns, convert the history\
    \ to QA-turns, and build context data for prompts."
  methods:
  - name: to_qa_turns
    signature: def to_qa_turns(self) -> list[QATurn]
  - name: from_list
    signature: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
      \    ) -> \"ConversationHistory\""
  - name: __init__
    signature: def __init__(self)
  - name: add_turn
    signature: 'def add_turn(self, role: ConversationRole, content: str)'
  - name: get_user_turns
    signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
  - name: build_context
    signature: "def build_context(\n        self,\n        tokenizer: Tokenizer |\
      \ None = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
      \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
      \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
- class_id: graphrag/language_model/protocol/base.py::ChatModel
  file: graphrag/language_model/protocol/base.py
  name: ChatModel
  docstring: "ChatModel protocol for chat-based language model interfaces.\n\nPurpose\n\
    \    Abstract protocol that defines how chat-based language models should generate\
    \ responses\n    from prompts, with optional history, and support both synchronous\
    \ and streaming interfaces\n    via four methods: achat, chat, chat_stream, and\
    \ achat_stream.\n\nAttributes\n    config: LanguageModelConfig\n        The configuration\
    \ for the language model, including model name and generation options\n      \
    \  that implementations may use to control behavior.\n\nMethods\n    achat(self,\
    \ prompt: str, history: list | None = None, **kwargs: Any) -> ModelResponse\n\
    \        Generate a synchronous, non-streaming response for the given prompt.\n\
    \        Args:\n            prompt: The text to generate a response for.\n   \
    \         history: Optional list of prior messages in the conversation.\n    \
    \        **kwargs: Additional keyword arguments (e.g., model parameters, generation\
    \ controls).\n        Returns:\n            ModelResponse: The generated response\
    \ wrapped in a ModelResponse.\n        Raises:\n            Exception: If an error\
    \ occurs during generation.\n\n    chat(self, prompt: str, history: list | None\
    \ = None, **kwargs: Any) -> ModelResponse\n        Generate a synchronous, non-streaming\
    \ response using the standard chat interface.\n        Args:\n            prompt:\
    \ The text to generate a response for.\n            history: Optional list of\
    \ prior messages in the conversation.\n            **kwargs: Additional keyword\
    \ arguments (e.g., model parameters, generation controls).\n        Returns:\n\
    \            ModelResponse: The generated response.\n        Raises:\n       \
    \     Exception: If an error occurs during generation.\n\n    chat_stream(self,\
    \ prompt: str, history: list | None = None, **kwargs: Any) -> Generator[str, None]\n\
    \        Streaming interface that yields partial strings as the model generates\
    \ a response.\n        Args:\n            prompt: The text to generate a response\
    \ for.\n            history: Optional list of prior messages in the conversation.\n\
    \            **kwargs: Additional keyword arguments (e.g., model parameters, streaming\
    \ controls).\n        Returns:\n            Generator[str, None]: A generator\
    \ that yields strings composing the final response.\n        Raises:\n       \
    \     Exception: If an error occurs during generation.\n\n    achat_stream(self,\
    \ prompt: str, history: list | None = None, **kwargs: Any) -> AsyncGenerator[str,\
    \ None]\n        Asynchronous streaming interface that yields partial strings\
    \ over time as the model progresses.\n        Args:\n            prompt: The text\
    \ to generate a response for.\n            history: Optional list of prior messages\
    \ in the conversation.\n            **kwargs: Additional keyword arguments (e.g.,\
    \ model parameters, streaming controls).\n        Returns:\n            AsyncGenerator[str,\
    \ None]: An async generator yielding strings for the final response.\n       \
    \ Raises:\n            Exception: If an error occurs during generation."
  methods:
  - name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
  - name: chat
    signature: "def chat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
  - name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
  - name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
- class_id: graphrag/storage/factory.py::StorageFactory
  file: graphrag/storage/factory.py
  name: StorageFactory
  docstring: "StorageFactory: Registry-based factory for pipeline storage backends.\n\
    \nPurpose:\nProvides a centralized registry that maps storage type identifiers\
    \ to creator callables for concrete PipelineStorage implementations (BlobPipelineStorage,\
    \ CosmosDBPipelineStorage, FilePipelineStorage, MemoryPipelineStorage). It enables\
    \ checking supported types, creating storage instances, registering new types,\
    \ and listing available storage types.\n\nAttributes:\n    _registry: ClassVar[dict[str,\
    \ Callable[..., PipelineStorage]]]\n        Registry mapping storage type keys\
    \ to their creator callables.\n\nMethods:\n    is_supported_type(cls, storage_type:\
    \ str) -> bool:\n        Check if the given storage_type is registered.\n\n  \
    \  create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage:\n\
    \        Create a storage object from the provided type.\n\n    register(cls,\
    \ storage_type: str, creator: Callable[..., PipelineStorage]) -> None:\n     \
    \   Register a custom storage implementation.\n\n    get_storage_types(cls) ->\
    \ list[str]:\n        Get the registered storage implementations.\n\nReturns:\n\
    \    For is_supported_type: bool indicating support.\n    For create_storage:\
    \ a PipelineStorage instance.\n    For register: None (side effect: registry updated).\n\
    \    For get_storage_types: list of registered storage type keys.\n\nRaises:\n\
    \    ValueError: If the storage type is not registered when attempting to create\
    \ storage."
  methods:
  - name: is_supported_type
    signature: 'def is_supported_type(cls, storage_type: str) -> bool'
  - name: create_storage
    signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
  - name: register
    signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
      \ PipelineStorage]\n    ) -> None"
  - name: get_storage_types
    signature: def get_storage_types(cls) -> list[str]
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: CFGNounPhraseExtractor
  docstring: CFGNounPhraseExtractor is a fast noun phrase extractor that combines
    CFG-based noun-chunk matching with optional named-entity recognition (NER) to
    support noun-phrase extraction for graph-building. It processes text with a SpaCy
    model, applies configured CFG grammars to identify candidate noun phrases, and
    then filters and merges results according to the configured rules. The extractor
    is configurable via instance attributes and aims to be robust across languages
    with grammar-driven matching and optional NER enrichment.
  methods:
  - name: __str__
    signature: def __str__(self) -> str
  - name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
      \        noun_phrase_tags: list[str],\n    )"
  - name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  - name: extract_cfg_matches
    signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
  - name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str],\
      \ entities: set[str] | None = None\n    ) -> dict[str, Any]"
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: RegexENNounPhraseExtractor
  docstring: "Regular-expression-based English noun phrase extractor used for building\
    \ noun graphs.\n\nNOTE: This extractor was used in the first benchmarking of LazyGraphRAG\
    \ and only works for English. It is much faster but likely less accurate than\
    \ the syntactic parser-based extractor. TODO: Reimplement this using SpaCy to\
    \ remove TextBlob dependency.\n\nArgs:\n  exclude_nouns: Nouns to exclude from\
    \ extraction.\n  max_word_length: Maximum length of words to consider when forming\
    \ noun phrases.\n  word_delimiter: Delimiter used to join tokens within noun phrases."
  methods:
  - name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  - name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
      \ list[str] | None = None\n    ) -> dict[str, Any]"
  - name: __str__
    signature: def __str__(self) -> str
  - name: __init__
    signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n\
      \        max_word_length: int,\n        word_delimiter: str,\n    )"
- class_id: graphrag/config/environment_reader.py::EnvironmentReader
  file: graphrag/config/environment_reader.py
  name: EnvironmentReader
  docstring: "Reads configuration values by combining a per-context stack of configuration\
    \ sections with environment variables. The EnvironmentReader uses an Env instance\
    \ to access environment values and maintains a private _config_stack to support\
    \ context-based reads. Reads resolve keys against the current top-most section\
    \ on the stack, and if not found, fall back to environment values via an internal\
    \ helper. Context managers can push a new section onto the stack for the duration\
    \ of a with-block to scope reads.\n\nArgs:\n    env (Env): Environment instance\
    \ used to read configuration values.\n\nAttributes:\n    env (Env): The environment\
    \ instance used to read configuration values.\n    _config_stack (list): Internal\
    \ stack that stores configuration sections for the duration of a read context.\n\
    \nNotes:\n    - The private helper _read_env is used to fetch values from the\
    \ environment.\n    - Keys are normalized to strings via internal mechanisms before\
    \ lookup.\n    - Context management affects subsequent reads within the with-block\
    \ by extending the active configuration.\n    - Typical return values are strings,\
    \ integers, booleans, floats, lists, or None when a value cannot be found.\n\n\
    Raises:\n    - TypeError, ValueError: if arguments have invalid types or the read\
    \ context is misused.\n    - Exceptions from the underlying Env reads may be raised\
    \ as encountered.\n\nPublic API overview:\n    _read_env(env_key, default_value,\
    \ read) -> T | None\n    __init__(self, env) -> None\n    config_context() ->\
    \ contextmanager\n    section() -> dict\n    use(value) -> contextmanager\n  \
    \  env() -> Env\n    envvar_prefix(prefix) -> None\n    str(key, env_key=None,\
    \ default_value=None) -> str | None\n    int(key, env_key=None, default_value=None)\
    \ -> int | None\n    bool(key, env_key=None, default_value=None) -> bool | None\n\
    \    float(key, env_key=None, default_value=None) -> float | None\n    list(key,\
    \ env_key=None, default_value=None) -> list | None"
  methods:
  - name: _read_env
    signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
      \ T, read: Callable[[str, T], T]\n    ) -> T | None"
  - name: __init__
    signature: 'def __init__(self, env: Env)'
  - name: config_context
    signature: def config_context()
  - name: section
    signature: def section(self) -> dict
  - name: use
    signature: 'def use(self, value: Any | None)'
  - name: env
    signature: def env(self)
  - name: envvar_prefix
    signature: 'def envvar_prefix(self, prefix: KeyValue)'
  - name: str
    signature: "def str(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: str | None = None,\n    )\
      \ -> str | None"
  - name: int
    signature: "def int(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: int | None = None,\n    )\
      \ -> int | None"
  - name: bool
    signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: bool | None = None,\n   \
      \ ) -> bool | None"
  - name: float
    signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: float | None = None,\n  \
      \  ) -> float | None"
  - name: list
    signature: "def list(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: list | None = None,\n   \
      \ ) -> list | None"
- class_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
  file: graphrag/storage/file_pipeline_storage.py
  name: FilePipelineStorage
  docstring: 'File-based storage backend for a pipeline that stores items as individual
    files under a root directory. This class implements the PipelineStorage interface
    and provides filesystem-backed operations to read, write, delete, list keys, clear
    storage, and find files by pattern.


    Attributes:

    - _root_dir: str - Root directory for storage; created if missing.

    - _encoding: str - Text encoding used for file I/O.


    Args:

    - base_dir: Directory path where files are stored. Defaults to the empty string
    (uses the current working directory).

    - encoding: Text encoding for file operations. Defaults to "utf-8".


    Returns:

    - None


    Raises:

    - OSError: If a filesystem operation fails during initialization or storage operations.

    - FileNotFoundError: If a file is not found when querying creation date or reading
    a specific item.'
  methods:
  - name: clear
    signature: def clear(self) -> None
  - name: keys
    signature: def keys(self) -> list[str]
  - name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  - name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
  - name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
  - name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
  - name: _read_file
    signature: "def _read_file(\n        self,\n        path: str | Path,\n      \
      \  as_bytes: bool | None = False,\n        encoding: str | None = None,\n  \
      \  ) -> Any"
  - name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: delete
    signature: 'def delete(self, key: str) -> None'
  - name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
- class_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig
  file: graphrag/config/models/graph_rag_config.py
  name: GraphRagConfig
  docstring: 'GraphRagConfig is a Pydantic-based configuration model that aggregates
    and validates GraphRag''s diverse configuration options.


    Args:

    - None: This model is constructed via Pydantic from internal fields representing
    sub-configs; there are no explicit __init__ parameters exposed.


    Returns:

    - GraphRagConfig: The same instance after initialization and internal validation.


    Raises:

    - ValueError: If a configuration value is invalid or required paths are missing.

    - FileNotFoundError: If a required root_dir or other directory does not exist.

    - LanguageModelConfigMissingError: If a default language model is not configured
    when required.


    Key attributes:

    - Sub-config models such as BasicSearchConfig, CacheConfig, ChunkingConfig, ClusterGraphConfig,
    CommunityReportsConfig, DRIFTSearchConfig, EmbedGraphConfig, ClaimExtractionConfig,
    ExtractGraphConfig, ExtractGraphNLPConfig, GlobalSearchConfig, InputConfig, LanguageModelConfig,
    LocalSearchConfig, PruneGraphConfig, ReportingConfig, SnapshotsConfig, StorageConfig,
    SummarizeDescriptionsConfig, TextEmbeddingConfig, UmapConfig, VectorStoreConfig.

    - Factories RateLimiterFactory and RetryFactory used for validating rate limiting
    and retry strategies.


    Summary:

    GraphRagConfig acts as the centralized, validated container for the GraphRag configuration,
    coordinating multiple sub-configs and providing utilities for path normalization,
    model validation, and access to vector store and language model configurations.'
  methods:
  - name: _validate_input_base_dir
    signature: def _validate_input_base_dir(self) -> None
  - name: _validate_rate_limiter_services
    signature: def _validate_rate_limiter_services(self) -> None
  - name: _validate_reporting_base_dir
    signature: def _validate_reporting_base_dir(self) -> None
  - name: _validate_factories
    signature: def _validate_factories(self) -> None
  - name: _validate_model
    signature: def _validate_model(self)
  - name: _validate_output_base_dir
    signature: def _validate_output_base_dir(self) -> None
  - name: _validate_retry_services
    signature: def _validate_retry_services(self) -> None
  - name: __str__
    signature: def __str__(self)
  - name: get_vector_store_config
    signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
  - name: _validate_vector_store_db_uri
    signature: def _validate_vector_store_db_uri(self) -> None
  - name: _validate_input_pattern
    signature: def _validate_input_pattern(self) -> None
  - name: _validate_multi_output_base_dirs
    signature: def _validate_multi_output_base_dirs(self) -> None
  - name: get_language_model_config
    signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
  - name: __repr__
    signature: def __repr__(self) -> str
  - name: _validate_models
    signature: def _validate_models(self) -> None
  - name: _validate_update_index_output_base_dir
    signature: def _validate_update_index_output_base_dir(self) -> None
  - name: _validate_root_dir
    signature: def _validate_root_dir(self) -> None
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: SyntacticNounPhraseExtractor
  docstring: "SyntacticNounPhraseExtractor extracts noun phrases from text using syntactic\
    \ parsing with SpaCy, with configurable filters and optional named-entity integration.\n\
    \nArgs:\n    model_name: The name of the NLP model used by the underlying SpaCy\
    \ pipeline.\n    max_word_length: Maximum number of words allowed in a noun phrase.\n\
    \    include_named_entities: Whether to include named entities as noun phrases.\n\
    \    exclude_entity_tags: List of entity tags to exclude from consideration.\n\
    \    exclude_pos_tags: List of part-of-speech tags to exclude from noun phrases.\n\
    \    exclude_nouns: List of noun strings to exclude from results.\n    word_delimiter:\
    \ Delimiter used to join tokens into a noun phrase.\n\nReturns:\n    None\n\n\
    Attributes:\n    model_name: The NLP model identifier used for parsing.\n    max_word_length:\
    \ Maximum length of a noun phrase by word count.\n    include_named_entities:\
    \ Flag indicating whether named entities are included.\n    exclude_entity_tags:\
    \ Entity tags to skip during extraction.\n    exclude_pos_tags: POS tags to skip\
    \ when forming noun phrases.\n    exclude_nouns: Specific nouns to exclude from\
    \ results.\n    word_delimiter: Delimiter used to join tokens inside a noun phrase."
  methods:
  - name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  - name: __str__
    signature: def __str__(self) -> str
  - name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n    )"
  - name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities:\
      \ list[Span]\n    ) -> dict[str, Any]"
- class_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger
  file: graphrag/logger/blob_workflow_logger.py
  name: BlobWorkflowLogger
  docstring: "Blob-based workflow logger that persists log records to Azure Blob storage\
    \ as JSON lines.\n\nSummary:\nThe BlobWorkflowLogger is a logging handler that\
    \ formats records into JSON payloads and appends them as lines to a blob in an\
    \ Azure Storage container. It supports categorizing log entries by type (log,\
    \ warning, error) and reinitializes its internal client when the accumulation\
    \ reaches a configured maximum.\n\nArgs:\n    connection_string: Connection string\
    \ for the blob storage, or None\n    container_name: Name of the blob container\n\
    \    blob_name: Name of the blob to create; if empty, a timestamped default will\
    \ be used\n    base_dir: Base directory to prepend to the blob name, or None\n\
    \    storage_account_blob_url: URL of the storage account blob service, or None\n\
    \    level: Logging level threshold (default NOTSET)\n\nReturns:\n    None\n\n\
    Raises:\n    OSError: If an I/O error occurs during blob operations or persistence"
  methods:
  - name: _write_log
    signature: 'def _write_log(self, log: dict[str, Any])'
  - name: _get_log_type
    signature: 'def _get_log_type(self, level: int) -> str'
  - name: __init__
    signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
      \        container_name: str | None,\n        blob_name: str = \"\",\n     \
      \   base_dir: str | None = None,\n        storage_account_blob_url: str | None\
      \ = None,\n        level: int = logging.NOTSET,\n    )"
  - name: emit
    signature: def emit(self, record) -> None
- class_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: ConsoleWorkflowCallbacks
  docstring: "Console-based callback implementation that prints workflow and pipeline\
    \ events to the console.\n\nThis class inherits NoopWorkflowCallbacks and provides\
    \ a concrete, console-oriented implementation of the workflow callback interface.\
    \ It prints status messages for pipeline and workflow lifecycle events and renders\
    \ a live progress bar to stdout as progress updates are received. When verbose\
    \ mode is enabled, additional diagnostic information may be emitted to aid debugging.\n\
    \nArgs:\n    verbose (bool): Enable verbose logging to the console.\n\nAttributes:\n\
    \    _verbose (bool): Internal flag controlling verbose output.\n\nReturns:\n\
    \    None\n\nRaises:\n    May raise OSError or other I/O-related exceptions raised\
    \ by writing to stdout.\n\nMethods:\n    __init__(verbose: bool = False)\n   \
    \ pipeline_start(names: list[str]) -> None\n    pipeline_end(results: list[PipelineRunResult])\
    \ -> None\n    workflow_start(name: str, instance: object) -> None\n    progress(progress:\
    \ Progress) -> None\n    workflow_end(name: str, instance: object) -> None"
  methods:
  - name: __init__
    signature: def __init__(self, verbose=False)
  - name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  - name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
  - name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
  - name: progress
    signature: 'def progress(self, progress: Progress) -> None'
  - name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
- class_id: graphrag/config/models/language_model_config.py::LanguageModelConfig
  file: graphrag/config/models/language_model_config.py
  name: LanguageModelConfig
  docstring: "LanguageModelConfig encapsulates configuration for language model integration\
    \ and performs validation for API keys, Azure AOI settings, rate limits, and model\
    \ selection.\n\nArgs:\n  model_type: The type of language model to configure,\
    \ as defined by ModelType.\n  model_provider: The provider of the model (e.g.,\
    \ \"azure\").\n  encoding_model: The encoding model name used for tokenization.\
    \ If omitted, it may be derived from the configured model.\n  deployment_name:\
    \ Azure OpenAI deployment name when using AOI; may be optional for non-AOI configurations.\n\
    \  api_base: Base URL for API requests when AOI or OpenAI endpoints are used.\n\
    \  api_version: API version for AOI usage.\n  rate_limit_strategy: Strategy for\
    \ rate limiting (e.g., None or \"auto\").\n  requests_per_minute: Allowed requests\
    \ per minute; integer >= 1, \"auto\", or None.\n  max_retries: Maximum number\
    \ of retry attempts.\n  tokens_per_minute: Allowed tokens per minute; integer\
    \ >= 1, \"auto\", or None.\n  auth_type: Authentication type for API access (e.g.,\
    \ ApiKey, AzureManagedIdentity).\n\nReturns:\n  LanguageModelConfig: The same\
    \ instance after validation.\n\nRaises:\n  ApiKeyMissingError: If an API key is\
    \ required but not provided.\n  AzureApiBaseMissingError: If the API base is required\
    \ but missing.\n  AzureApiVersionMissingError: If the API version is required\
    \ but missing.\n  ConflictingSettingsError: If settings conflict (e.g., auth/type\
    \ mismatch or incompatible provider)."
  methods:
  - name: _validate_api_key
    signature: def _validate_api_key(self) -> None
  - name: _validate_encoding_model
    signature: def _validate_encoding_model(self) -> None
  - name: _validate_deployment_name
    signature: def _validate_deployment_name(self) -> None
  - name: _validate_azure_settings
    signature: def _validate_azure_settings(self) -> None
  - name: _validate_model_provider
    signature: def _validate_model_provider(self) -> None
  - name: _validate_requests_per_minute
    signature: def _validate_requests_per_minute(self) -> None
  - name: _validate_max_retries
    signature: def _validate_max_retries(self) -> None
  - name: _validate_tokens_per_minute
    signature: def _validate_tokens_per_minute(self) -> None
  - name: _validate_api_base
    signature: def _validate_api_base(self) -> None
  - name: _validate_type
    signature: def _validate_type(self) -> None
  - name: _validate_auth_type
    signature: def _validate_auth_type(self) -> None
  - name: _validate_model
    signature: def _validate_model(self)
  - name: _validate_api_version
    signature: def _validate_api_version(self) -> None
- class_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter
  file: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  name: RateLimiter
  docstring: "RateLimiter is an abstract base class that defines the interface for\
    \ rate limiting strategies. Concrete subclasses must implement their own initialization\
    \ logic and provide a concrete acquire method that returns a context manager guarding\
    \ a request.\n\nArgs:\n  kwargs: Additional keyword arguments passed to initialization.\
    \ Subclasses may use them to configure the limiter; the base class does not perform\
    \ concrete initialization.\nReturns:\n  None\nNotes:\n  This class is abstract\
    \ and cannot be instantiated. Subclasses must implement acquire to provide a concrete\
    \ rate-limiting context manager.\n\nAcquire:\n  token_count (int): The estimated\
    \ number of tokens for the current request.\n  Returns:\n    ContextManager[None]:\
    \ A context manager that yields None and guards the request."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n\
      \    ) -> None"
  - name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
- class_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: TestAzureAISearchVectorStore
  docstring: 'TestAzureAISearchVectorStore: Test suite for AzureAISearchVectorStore
    integration with Azure Cognitive Search.


    This class provides integration-style tests for the AzureAISearchVectorStore using
    mocked Azure Cognitive Search clients. It validates interaction with the search
    and indexing layers, embedding handling, and basic vector-store operations without
    requiring live services. The tests are driven by fixtures that configure a vector
    store instance, supply embedder behavior, and provide sample documents, while
    patching the SearchClient and SearchIndexClient to simulate Azure AI Search behavior.


    Key fixtures and test coverage:

    - vector_store: a configured AzureAISearchVectorStore instance bound to mocked
    search and index clients.

    - none_embedder: a placeholder embedder that returns None to exercise edge cases.

    - vector_store_custom: a vector store configured with custom field mappings.

    - mock_embedder: a simple embedder returning a fixed embedding vector.

    - mock_search_client: a mocked Azure AI Search SearchClient used in tests.

    - mock_index_client: a mocked Azure AI Search SearchIndexClient used in tests.

    - sample_documents: a small list of VectorStoreDocument objects used for load/search
    scenarios.

    - test_empty_embedding: tests behavior when embedding yields an empty or None
    vector.

    - test_vector_store_customization: tests that custom field mappings are honored
    during indexing/search.

    - test_vector_store_operations: tests basic vector store operations (add/load/search)
    with mocks.'
  methods:
  - name: vector_store
    signature: def vector_store(self, mock_search_client, mock_index_client)
  - name: none_embedder
    signature: 'def none_embedder(text: str) -> None'
  - name: vector_store_custom
    signature: def vector_store_custom(self, mock_search_client, mock_index_client)
  - name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
  - name: test_vector_store_customization
    signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
      \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
      \    )"
  - name: test_vector_store_operations
    signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
      \ mock_search_client, mock_index_client\n    )"
  - name: mock_search_client
    signature: def mock_search_client(self)
  - name: sample_documents
    signature: def sample_documents(self)
  - name: test_empty_embedding
    signature: def test_empty_embedding(self, vector_store, mock_search_client)
  - name: mock_index_client
    signature: def mock_index_client(self)
- class_id: graphrag/storage/pipeline_storage.py::PipelineStorage
  file: graphrag/storage/pipeline_storage.py
  name: PipelineStorage
  docstring: "Abstract base class for storage backends used by the pipeline.\n\nDefines\
    \ the interface for a key-value style storage with support for existence checks,\n\
    pattern-based file discovery, retrieval with optional byte/encoding handling,\
    \ listing\nkeys, creation date retrieval, and deletion. Implementations may back\
    \ the storage with\nin-memory structures, filesystem, database, or remote services.\n\
    \nAttributes:\n  Not defined at this level. Concrete implementations may define\
    \ internal state and\n  configuration."
  methods:
  - name: has
    signature: 'def has(self, key: str) -> bool'
  - name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
  - name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
  - name: clear
    signature: def clear(self) -> None
  - name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
  - name: keys
    signature: def keys(self) -> list[str]
  - name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
  - name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  - name: delete
    signature: 'def delete(self, key: str) -> None'
- class_id: graphrag/factory/factory.py::Factory
  file: graphrag/factory/factory.py
  name: Factory
  docstring: "Factory is a generic, per-subclass singleton that registers and creates\
    \ service instances by strategy name.\n\nThe Factory maintains a registry of strategy\
    \ names to callables that return instances of T. Strategies can be registered\
    \ with register, queried with __contains__, listed with keys, and used to create\
    \ instances with create. Each subclass uses its own singleton instance.\n\nAttributes:\n\
    \    _services: dict[str, Callable[..., T]] \u2014 registry mapping strategy names\
    \ to callables that produce T instances."
  methods:
  - name: create
    signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
  - name: __contains__
    signature: 'def __contains__(self, strategy: str) -> bool'
  - name: __new__
    signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
  - name: register
    signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
      T]) -> None'
  - name: __init__
    signature: def __init__(self)
  - name: keys
    signature: def keys(self) -> list[str]
- class_id: graphrag/language_model/manager.py::ModelManager
  file: graphrag/language_model/manager.py
  name: ModelManager
  docstring: "Singleton manager for chat and embedding language models.\n\nOverview:\n\
    ModelManager is a singleton responsible for creating, registering, retrieving,\
    \ and listing ChatModel and EmbeddingModel instances. It delegates on-demand instantiation\
    \ to ModelFactory and stores instances in internal registries for reuse. Access\
    \ to the singleton is provided via __new__ or get_instance.\n\nAttributes:\n \
    \   chat_models (dict[str, ChatModel]): Registry of registered chat models keyed\
    \ by name.\n    embedding_models (dict[str, EmbeddingModel]): Registry of registered\
    \ embedding models keyed by name.\n    _initialized (bool): Initialization flag\
    \ to prevent reinitialization.\n\nRaises:\n    ValueError: If attempting to retrieve\
    \ a non-registered chat or embedding model."
  methods:
  - name: get_or_create_chat_model
    signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
      \ str, **chat_kwargs: Any\n    ) -> ChatModel"
  - name: list_chat_models
    signature: def list_chat_models(self) -> dict[str, ChatModel]
  - name: remove_chat
    signature: 'def remove_chat(self, name: str) -> None'
  - name: list_embedding_models
    signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
  - name: get_chat_model
    signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
  - name: get_or_create_embedding_model
    signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
      \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
  - name: get_instance
    signature: def get_instance(cls) -> ModelManager
  - name: register_embedding
    signature: "def register_embedding(\n        self, name: str, model_type: str,\
      \ **embedding_kwargs: Any\n    ) -> EmbeddingModel"
  - name: __new__
    signature: def __new__(cls) -> Self
  - name: register_chat
    signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
      \ Any\n    ) -> ChatModel"
  - name: __init__
    signature: def __init__(self) -> None
  - name: remove_embedding
    signature: 'def remove_embedding(self, name: str) -> None'
  - name: get_embedding_model
    signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
- class_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: ClaimExtractor
  docstring: "ClaimExtractor\n\nClass responsible for orchestrating claim extraction\
    \ from input texts using configurable prompts and parsing the results into structured\
    \ dictionaries. It updates claim objects with resolved entity identifiers and\
    \ supports customizable delimiters and error handling.\n\nArgs:\n  model_invoker:\
    \ ChatModel\n    The model invoker used to run prompts.\n  extraction_prompt:\
    \ str | None\n    Custom prompt for extraction. If None, defaults to EXTRACT_CLAIMS_PROMPT.\n\
    \  input_text_key: str | None\n    Key in the inputs for the input text.\n  input_entity_spec_key:\
    \ str | None\n    Key for the entity specifications.\n  input_claim_description_key:\
    \ str | None\n    Key for the claim description.\n  input_resolved_entities_key:\
    \ str | None\n    Key for the resolved entities used to update claims.\n  tuple_delimiter_key:\
    \ str | None\n    Key for the tuple delimiter used when parsing claims.\n  record_delimiter_key:\
    \ str | None\n    Key for the record delimiter used when parsing claims.\n  completion_delimiter_key:\
    \ str | None\n    Key for the completion delimiter used to signal end of extraction.\n\
    \  max_gleanings: int | None\n    Maximum number of gleanings (iterations) to\
    \ perform during extraction.\n  on_error: ErrorHandlerFn | None\n    Optional\
    \ error handler callback invoked on errors.\n\nReturns:\n  None\n    Initializes\
    \ the instance; no return value is produced.\n\nRaises:\n  Exception\n    Exceptions\
    \ raised during initialization or by underlying components may be propagated unless\
    \ handled by on_error."
  methods:
  - name: _clean_claim
    signature: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
      \ dict\n    ) -> dict"
  - name: _process_document
    signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
      \ int\n    ) -> list[dict]"
  - name: _parse_claim_tuples
    signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
      \ dict\n    ) -> list[dict[str, Any]]"
  - name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        input_text_key: str\
      \ | None = None,\n        input_entity_spec_key: str | None = None,\n      \
      \  input_claim_description_key: str | None = None,\n        input_resolved_entities_key:\
      \ str | None = None,\n        tuple_delimiter_key: str | None = None,\n    \
      \    record_delimiter_key: str | None = None,\n        completion_delimiter_key:\
      \ str | None = None,\n        max_gleanings: int | None = None,\n        on_error:\
      \ ErrorHandlerFn | None = None,\n    )"
  - name: pull_field
    signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
  - name: __call__
    signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
      \ dict | None = None\n    ) -> ClaimExtractorResult"
- class_id: graphrag/language_model/response/base.pyi::ModelOutput
  file: graphrag/language_model/response/base.pyi
  name: ModelOutput
  docstring: "ModelOutput: Represents the outcome produced by a language model, providing\
    \ access to the textual content and the complete raw payload when available.\n\
    \nPurpose:\n  Encapsulates model output data and provides convenient accessors\
    \ to the core content and the full payload.\n\nMethods:\n  content() -> str\n\
    \    Returns the textual content of the model output as a string.\n\n  full_response()\
    \ -> dict[str, Any] | None\n    Returns the full raw response payload as a dictionary,\
    \ or None if available.\n\nNotes:\n  These are methods (not attributes). Access\
    \ data by calling content() and full_response() on the instance."
  methods:
  - name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
  - name: content
    signature: def content(self) -> str
- class_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: MockBaseVectorStore
  docstring: "MockBaseVectorStore is a test helper that provides a lightweight, in-memory\
    \ vector store for unit tests. It is designed to mirror essential aspects of a\
    \ real vector store without performing real embeddings or persisting data, offering\
    \ deterministic, mock search and retrieval behavior.\n\nArgs:\n    documents (list[VectorStoreDocument]):\
    \ Documents to initialize the mock vector store with for testing.\n\nAttributes:\n\
    \    documents (list[VectorStoreDocument]): Documents stored in the mock vector\
    \ store used for retrieval and search in tests.\n\nNotes:\n    - Deterministic\
    \ results: search and retrieval behavior is fixed and repeatable across runs.\n\
    \    - No persistence: data exists only for the lifetime of the test process.\n\
    \    - This class provides a test-facing implementation that mirrors the BaseVectorStore\
    \ interface as needed by tests, without embedding calculations.\n\nMethods:\n\
    \    connect(**kwargs: Any) -> None\n        No-op connection helper used in tests.\
    \ Accepts arbitrary keyword arguments and performs no action.\n\n    __init__(self,\
    \ documents: list[VectorStoreDocument]) -> None\n        Initialize the mock vector\
    \ store with the provided documents for testing.\n\n    filter_by_id(include_ids:\
    \ list[str] | list[int]) -> list[VectorStoreDocument]\n        Return the documents\
    \ whose id is in include_ids.\n\n    similarity_search_by_text(\n        self,\
    \ text: str, text_embedder: TextEmbedder, k: int = 10, **kwargs: Any\n    ) ->\
    \ list[VectorStoreSearchResult]\n        Perform a deterministic, length-based\
    \ search ignoring embeddings. Returns up to k results based on stored documents\
    \ with a fixed score.\n\n    load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None\n        Load documents into the vector\
    \ store; if overwrite is True, replace existing data; otherwise, append.\n\n \
    \   similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]\n       \
    \ Return the top-k documents using a deterministic mock search that ignores the\
    \ query embedding; returns the first k documents with a fixed score of 1.\n\n\
    \    search_by_id(self, id: str) -> VectorStoreDocument\n        Return the first\
    \ stored document with its id set to the provided id."
  methods:
  - name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
  - name: __init__
    signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
  - name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  - name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
  - name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  - name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
- class_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: Datasource
  docstring: 'Datasource interface for knowledge loader data sources.


    Purpose

    Abstract base class that defines the common interface for data sources used by
    the knowledge loader. Concrete implementations provide reading from and writing
    to their underlying storage, loading configuration, and checking the presence
    of tables.


    Key attributes

    - GraphRagConfig: The configuration type used by read_settings to represent datasource
    settings.

    - Overwrite, Append: Module-level constants representing write modes defined for
    write operations.


    Summary

    Subclasses must implement the following methods to interact with their data backends:

    - read_settings(file: str) -> GraphRagConfig | None

    - read(table: str, throw_on_missing: bool = False, columns: list[str] | None =
    None) -> pd.DataFrame

    - __call__(table: str, columns: list[str] | None) -> pd.DataFrame

    - write(table: str, df: pd.DataFrame, mode: WriteMode | None = None) -> None

    - has_table(table: str) -> bool'
  methods:
  - name: read_settings
    signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
  - name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  - name: __call__
    signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
  - name: write
    signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
      \ | None = None\n    ) -> None"
  - name: has_table
    signature: 'def has_table(self, table: str) -> bool'
- class_id: graphrag/language_model/factory.py::ModelFactory
  file: graphrag/language_model/factory.py
  name: ModelFactory
  docstring: 'ModelFactory is a registry-based factory that creates chat and embedding
    language model backends.


    Purpose:

    - Maintain registries for embedding and chat model implementations.

    - Provide a uniform API to register model backends and instantiate models by type.

    - Offer utilities to query supported model types.


    Attributes:

    - _embedding_registry: ClassVar mapping[str, Callable[..., EmbeddingModel]] of
    model_type to a constructor for EmbeddingModel.

    - _chat_registry: ClassVar mapping[str, Callable[..., ChatModel]] of model_type
    to a constructor for ChatModel.


    Summary:

    - Coordinates model backends from different providers (e.g., FNLLM, Litellm) by
    model type.


    Returns:

    - None


    Raises:

    - ValueError: If attempting to create a model for an unregistered model_type via
    create_chat_model or create_embedding_model.'
  methods:
  - name: register_embedding
    signature: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
      \ EmbeddingModel]\n    ) -> None"
  - name: get_embedding_models
    signature: def get_embedding_models(cls) -> list[str]
  - name: create_chat_model
    signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
  - name: is_supported_model
    signature: 'def is_supported_model(cls, model_type: str) -> bool'
  - name: is_supported_chat_model
    signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
  - name: create_embedding_model
    signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any) ->
      EmbeddingModel'
  - name: is_supported_embedding_model
    signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
  - name: get_chat_models
    signature: def get_chat_models(cls) -> list[str]
  - name: register_chat
    signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
      -> None'
- class_id: tests/integration/language_model/test_factory.py::CustomChatModel
  file: tests/integration/language_model/test_factory.py
  name: CustomChatModel
  docstring: "Lightweight test-oriented chat model that provides synchronous, streaming,\
    \ and asynchronous chat interfaces.\n\nPurpose:\n    Simulate basic chat interactions\
    \ for tests by offering predictable responses across methods. It does not maintain\
    \ internal state between calls and ignores any initialization keyword arguments.\n\
    \nAttributes:\n    None - the class does not store internal state.\n\nArgs:\n\
    \    kwargs: dict[str, Any] - Optional keyword arguments provided to the initializer.\
    \ They are ignored.\n\nReturns:\n    None"
  methods:
  - name: achat
    signature: "def achat(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> ModelResponse"
  - name: chat
    signature: "def chat(\n            self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n        ) -> ModelResponse"
  - name: chat_stream
    signature: "def chat_stream(\n            self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n        ) -> Generator[str, None]"
  - name: achat_stream
    signature: "def achat_stream(\n            self, prompt: str, history: list |\
      \ None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
  - name: __init__
    signature: def __init__(self, **kwargs)
- class_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: WorkflowCallbacksManager
  docstring: "Manager that holds registered WorkflowCallbacks instances and dispatches\
    \ lifecycle events to them.\n\nPurpose:\nTo centralize the registration and dispatch\
    \ of workflow and pipeline lifecycle events to all registered callbacks.\n\nKey\
    \ attributes:\n- _callbacks: List[WorkflowCallbacks]\n    Internal registry of\
    \ callbacks to which events are forwarded.\n\nBrief summary:\nThe manager maintains\
    \ an internal registry of WorkflowCallbacks implementations and forwards events\
    \ such as pipeline_start, pipeline_end, workflow_start, workflow_end, and progress\
    \ to each registered callback that implements the corresponding method. There\
    \ is no deduplication when registering callbacks; the same instance can be added\
    \ multiple times.\n\nArgs:\n- self: The WorkflowCallbacksManager instance. The\
    \ constructor takes no external parameters.\n\nReturns:\nNone\n\nRaises:\nNone"
  methods:
  - name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
  - name: __init__
    signature: def __init__(self)
  - name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
  - name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
  - name: progress
    signature: 'def progress(self, progress: Progress) -> None'
  - name: register
    signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
  - name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
- class_id: graphrag/index/operations/layout_graph/typing.py::NodePosition
  file: graphrag/index/operations/layout_graph/typing.py
  name: NodePosition
  docstring: "NodePosition represents the position and rendering properties of a node\
    \ in the layout graph.\n\nAttributes:\n  label: str\n  x: float\n  y: float\n\
    \  cluster: int | str\n  size: float\n\nArgs:\n  label: The node's display label.\n\
    \  x: The x-coordinate of the node in the layout.\n  y: The y-coordinate of the\
    \ node in the layout.\n  cluster: The cluster identifier (int or str) to which\
    \ the node belongs.\n  size: The visual size of the node.\n\nReturns:\n  None:\
    \ The dataclass constructor initializes a NodePosition instance and does not return\
    \ a value.\n\nto_pandas:\n  def to_pandas(self) -> tuple[str, float, float, str,\
    \ float]:\n    Converts this NodePosition to a pandas-friendly 5-tuple in the\
    \ order: (label, x, y, cluster, size).\n    The 4th element (cluster) is represented\
    \ as a string to maintain consistency in pandas DataFrames.\n    Returns: A 5-tuple\
    \ containing (label, x, y, cluster, size)."
  methods:
  - name: to_pandas
    signature: def to_pandas(self) -> tuple[str, float, float, str, float]
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationTurn
  docstring: "Represents a single turn in a conversation history.\n\nArgs:\n    role:\
    \ The role associated with this turn (for example, SYSTEM, USER, or ASSISTANT).\n\
    \    content: The textual content of the turn.\n\nReturns:\n    str: The string\
    \ representation of the turn, provided by __str__, in the format \"<role>: <content>\"\
    .\n\nRaises:\n    None"
  methods:
  - name: __str__
    signature: def __str__(self) -> str
- class_id: tests/mock_provider.py::MockEmbeddingLLM
  file: tests/mock_provider.py
  name: MockEmbeddingLLM
  docstring: '# TODO: Document this class'
  methods:
  - name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
  - name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  - name: __init__
    signature: 'def __init__(self, **kwargs: Any)'
- class_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext
  file: graphrag/query/structured_search/basic_search/basic_context.py
  name: BasicSearchContext
  docstring: 'Builds and manages the context used by the basic search mode in graphrag.


    Purpose:

    The BasicSearchContext encapsulates the logic to construct a coherent, compact
    context for a user query by leveraging an embedding model and a vector store of
    text units. It can incorporate optional conversation history and token constraints
    when building the context via build_context, coordinating with the BasicContextBuilder
    to produce a ContextBuilderResult.


    Attributes:

    text_embedder: EmbeddingModel

    text_unit_embeddings: BaseVectorStore

    text_units: list[TextUnit] | None

    tokenizer: Tokenizer | None

    embedding_vectorstore_key: str


    Args:

    text_embedder (EmbeddingModel): The embedding model used to embed text for similarity
    search

    text_unit_embeddings (BaseVectorStore): The vector store containing embeddings
    for text units

    text_units (list[TextUnit] | None): Optional list of text units to consider

    tokenizer (Tokenizer | None): Optional tokenizer to use

    embedding_vectorstore_key (str): The key in the vector store that identifies items
    (default "id")


    Returns:

    None


    Raises:

    AttributeError: If any text unit is missing ''id'' or ''short_id'' attributes
    when mapping ids via _map_ids'
  methods:
  - name: _map_ids
    signature: def _map_ids(self) -> dict[str, str]
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
      \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
      \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col:\
      \ str = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult"
  - name: __init__
    signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
      \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
      \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = \"id\",\n    )"
- class_id: graphrag/logger/factory.py::LoggerFactory
  file: graphrag/logger/factory.py
  name: LoggerFactory
  docstring: "LoggerFactory is a registry-based factory for creating logging.Handler\
    \ instances for various reporting types.\n\nPurpose:\n- Maintain an internal registry\
    \ mapping reporting_type identifiers to creator callables.\n- Provide a classmethod-based\
    \ interface to register new loggers, check supported types, create loggers for\
    \ a given type, and retrieve the set of available types.\n\nAttributes:\n- _registry:\
    \ ClassVar[dict[str, Callable[..., logging.Handler]]]\n    Internal registry that\
    \ maps a reporting_type string to a creator callable that returns a logging.Handler\
    \ instance when invoked with appropriate keyword arguments.\n\nSummary:\nThe class\
    \ acts as a centralized factory and registry for logger handlers. Client code\
    \ can register new logger implementations, query supported types, and request\
    \ a handler for a specific reporting type. All operations are performed at the\
    \ class level."
  methods:
  - name: create_logger
    signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
  - name: is_supported_type
    signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
  - name: register
    signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
      \ logging.Handler]\n    ) -> None"
  - name: get_logger_types
    signature: def get_logger_types(cls) -> list[str]
- class_id: graphrag/query/question_gen/base.py::BaseQuestionGen
  file: graphrag/query/question_gen/base.py
  name: BaseQuestionGen
  docstring: "BaseQuestionGen is a base class for generating questions using a language\
    \ model and context builders.\n\nPurpose:\n    Provide a common interface and\
    \ shared setup for question generation by coordinating a ChatModel with a context_builder\
    \ (GlobalContextBuilder or LocalContextBuilder) and an optional Tokenizer. Subclasses\
    \ implement the concrete generation logic.\n\nArgs:\n    model (ChatModel): The\
    \ language model interface used for this base question generator.\n    context_builder\
    \ (GlobalContextBuilder | LocalContextBuilder): The builder that constructs the\
    \ context for the questions.\n    tokenizer (Tokenizer | None): Optional tokenizer\
    \ to use. If None, a tokenizer appropriate for the model will be used.\n    model_params\
    \ (dict[str, Any] | None): Optional parameters for configuring the underlying\
    \ model.\n    context_builder_params (dict[str, Any] | None): Optional parameters\
    \ for configuring the context builder.\n\nAttributes:\n    model: The language\
    \ model interface used for generation.\n    context_builder: The context builder\
    \ instance used to assemble context data.\n    tokenizer: Optional tokenizer instance\
    \ used to tokenize prompts and outputs.\n    model_params: Optional parameters\
    \ for the model configuration.\n    context_builder_params: Optional parameters\
    \ for the context builder configuration.\n\nMethods:\n    generate(question_history:\
    \ list[str], context_data: str | None, question_count: int, **kwargs) -> QuestionResult:\n\
    \        Generate questions synchronously.\n    agenerate(question_history: list[str],\
    \ context_data: str | None, question_count: int, **kwargs) -> QuestionResult:\n\
    \        Generate questions asynchronously."
  methods:
  - name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
  - name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
- class_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel
  file: tests/integration/language_model/test_factory.py
  name: CustomEmbeddingModel
  docstring: "CustomEmbeddingModel provides synchronous and asynchronous embeddings\
    \ for text inputs, including batch processing. It is stateless and accepts arbitrary\
    \ keyword arguments at initialization, which are ignored.\n\nArgs:\n  kwargs:\
    \ dict[str, Any] - keyword arguments provided to the initializer. They are ignored.\n\
    \nReturns:\n  None\n\nMethods:\n  aembed_batch(self, text_list: list[str], **kwargs)\
    \ -> list[list[float]]: Asynchronously compute embeddings for a batch of input\
    \ texts.\n    Returns: A list of embeddings, where each embedding is a list of\
    \ floats corresponding to the input texts.\n    Raises: Exceptions from the underlying\
    \ embedding process may propagate.\n  aembed(self, text: str, **kwargs) -> list[float]:\
    \ Asynchronously generate an embedding for the input text.\n    Returns: A list\
    \ of floats representing the embedding.\n    Raises: Exceptions from the underlying\
    \ embedding process may propagate.\n  embed_batch(self, text_list: list[str],\
    \ **kwargs) -> list[list[float]]: Compute embeddings for a batch of input texts.\n\
    \    Returns: A list of embeddings corresponding to the input texts.\n    Raises:\
    \ Exceptions from the underlying embedding process may propagate.\n  embed(self,\
    \ text: str, **kwargs) -> list[float]: Generate an embedding for the input text.\n\
    \    Returns: A list of floats representing the embedding.\n    Raises: Exceptions\
    \ from the underlying embedding process may propagate.\n  __init__(self, **kwargs):\
    \ Initialize the instance with arbitrary keyword arguments; no initialization\
    \ is performed.\n    Returns: None\n    Raises: None"
  methods:
  - name: aembed_batch
    signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
      \        ) -> list[list[float]]"
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  - name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  - name: __init__
    signature: def __init__(self, **kwargs)
- class_id: graphrag/config/models/storage_config.py::StorageConfig
  file: graphrag/config/models/storage_config.py
  name: StorageConfig
  docstring: "StorageConfig manages storage-related configuration for GraphRAG backends.\n\
    \nPurpose:\n    Encapsulates settings for storage backends, including the storage\
    \ type and\n    the base directory for local storage. It reads defaults from graphrag_config_defaults\n\
    \    and uses StorageType to determine behavior. It provides a validator to normalize\n\
    \    the base_dir when using local storage.\n\nAttributes:\n    base_dir: The\
    \ filesystem path for local storage base directory. Used when storage_type\n \
    \       indicates local file storage.\n    storage_type: The StorageType enum\
    \ value that selects the storage backend.\n\nBrief summary:\n    Centralizes storage\
    \ configuration for GraphRAG, ensuring consistent handling of\n    base directories\
    \ for local storage and pass-through for non-local storage types.\n\nMethods:\n\
    \    validate_base_dir(cls, value, info):\n        Normalize base_dir to a filesystem\
    \ path string for local storage. This validator\n        does not verify that\
    \ the path exists or is valid beyond conversion to a string.\n        It only\
    \ performs normalization when the storage type is local (StorageType.file);\n\
    \        for all other storage types, the input value is returned unchanged.\n\
    \nArgs:\n    cls (type): The class that defines the validator.\n    value (Any):\
    \ The input value to be validated and normalized.\n    info (dict): Validation\
    \ information provided by Pydantic.\n\nReturns:\n    str: The normalized base_dir\
    \ path when storage_type is local; otherwise, the input value\n        unchanged.\n\
    \nRaises:\n    None"
  methods:
  - name: validate_base_dir
    signature: def validate_base_dir(cls, value, info)
- class_id: unified-search-app/app/state/session_variable.py::SessionVariable
  file: unified-search-app/app/state/session_variable.py
  name: SessionVariable
  docstring: "SessionVariable provides a small wrapper around Streamlit's session_state\
    \ to manage a single variable with optional collision avoidance via a prefix.\n\
    \nPurpose:\n- To store and retrieve a value associated with a unique key in st.session_state,\
    \ with a configurable prefix to prevent collisions when multiple variables share\
    \ the same base name.\n\nArgs:\n  default: The initial/default value for the session\
    \ variable.\n  prefix: Optional prefix to prepend to the key to differentiate\
    \ this variable from others with the same name.\n\nReturns:\n  None\n  Note: __init__\
    \ initializes the instance and does not return a value.\n\nRaises:\n  None\n\n\
    Attributes:\n  key: str. The session_state key used to access this variable. It\
    \ is derived from the provided prefix and the internal variable name.\n  value:\
    \ Any. The current value stored in session_state for this variable. This is readable\
    \ and writable; assigning a new value updates st.session_state accordingly.\n\n\
    __repr__:\n  Returns: str. A string representation that includes both the key\
    \ and the current value to aid debugging."
  methods:
  - name: __init__
    signature: 'def __init__(self, default: Any = "", prefix: str = "")'
  - name: value
    signature: 'def value(self, value: Any) -> None'
  - name: __repr__
    signature: def __repr__(self) -> Any
  - name: key
    signature: def key(self) -> str
- class_id: graphrag/data_model/text_unit.py::TextUnit
  file: graphrag/data_model/text_unit.py
  name: TextUnit
  docstring: 'TextUnit is a data model that encapsulates a unit of text and its metadata
    for graph-based data handling.


    Purpose:

    - Store the text content together with identifiers linking it to entities, relationships,
    covariates, and related documents.


    Inherits:

    - Identified to provide a stable, unique identifier for the text unit.


    Key attributes:

    - id: Text unit identifier.

    - human_readable_id: Optional short identifier.

    - text: The actual text content.

    - entity_ids: Identifiers of entities present in the text.

    - relationship_ids: Identifiers of relationships associated with the text.

    - covariate_ids: Identifiers of covariates linked to the text.

    - n_tokens: Number of tokens in the text.

    - document_ids: Identifiers of documents containing the text unit.

    - attributes: Additional arbitrary attributes.


    From dictionary construction:

    - This class provides a from_dict classmethod to construct a TextUnit from a dictionary,
    mapping configured keys to the corresponding fields.


    Args:

    - cls: The class.

    - d (dict[str, Any]): The source dictionary containing the values for the TextUnit
    fields.

    - id_key (str): Key in d for the text unit''s identifier. Defaults to "id".

    - short_id_key (str): Key in d for the optional short identifier. Defaults to
    "human_readable_id".

    - text_key (str): Key in d for the text content. Defaults to "text".

    - entities_key (str): Key in d for the entity identifiers. Defaults to "entity_ids".

    - relationships_key (str): Key in d for the relationship identifiers. Defaults
    to "relationship_ids".

    - covariates_key (str): Key in d for the covariate identifiers. Defaults to "covariate_ids".

    - n_tokens_key (str): Key in d for the token count. Defaults to "n_tokens".

    - document_ids_key (str): Key in d for the document identifiers. Defaults to "document_ids".

    - attributes_key (str): Key in d for the attributes. Defaults to "attributes".


    Returns:

    - TextUnit: A new TextUnit instance constructed from the provided dictionary data.


    Raises:

    - May raise exceptions stemming from dictionary access or type mismatches depending
    on input data.'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        text_key: str = \"text\",\n        entities_key: str = \"entity_ids\"\
      ,\n        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
      \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n    \
      \    document_ids_key: str = \"document_ids\",\n        attributes_key: str\
      \ = \"attributes\",\n    ) -> \"TextUnit\""
- class_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: MockTokenizer
  docstring: "MockTokenizer is a simple, stateless tokenizer used in unit tests to\
    \ deterministically encode and decode text by Unicode code points. It simulates\
    \ a Unicode code point based tokenizer without relying on a real tokenizer implementation.\n\
    \nAttributes:\n- Stateless; no persistent state or instance attributes are required\
    \ or maintained.\n\nDeterminism:\n- All operations are deterministic: encode uses\
    \ ord() per character; decode reconstructs the string exactly from code points.\n\
    \nUsage:\nmock = MockTokenizer()\ntext = 'Test'\ntokens = mock.encode(text)\n\
    # tokens would be [84, 101, 115, 116]\ndecoded = mock.decode(tokens)\n# decoded\
    \ would be 'Test'\n\nMethods:\nencode(text)\n  Description: Encode the input text\
    \ as a list of Unicode code points.\n  Args:\n    text: str. The input text to\
    \ encode as Unicode code points.\n  Returns:\n    list[int]. A list of integers\
    \ where each integer is the Unicode code point of the corresponding character\
    \ in text.\n  Raises:\n    TypeError: If text is None or not a string (or not\
    \ iterable).\n\ndecode(token_ids)\n  Description: Decode token ids to string by\
    \ converting each integer to a character and concatenating.\n  Args:\n    token_ids:\
    \ An iterable of integers representing token IDs to decode into a string.\n  Returns:\n\
    \    str: The decoded string.\n  Raises:\n    TypeError: If token_ids contains\
    \ non-integer elements or elements cannot be processed by chr.\n    ValueError:\
    \ If any token_id is outside the valid Unicode range for chr (e.g., not in 0 <=\
    \ id <= 0x10FFFF)."
  methods:
  - name: encode
    signature: def encode(self, text)
  - name: decode
    signature: def decode(self, token_ids)
- class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: OpenAIChatFNLLM
  docstring: "OpenAIChatFNLLM provider that integrates FNLLM's OpenAI chat LLM with\
    \ Graphrag's language-model framework to support chat-based interactions within\
    \ Graphrag's LLM workflows. It wires FNLLM's OpenAI chat client to Graphrag's\
    \ API surface, deriving its configuration from a LanguageModelConfig and optionally\
    \ enabling event callbacks and caching.\n\nArgs:\n  name (str): The name assigned\
    \ to this provider instance, used for internal identification and cache naming.\n\
    \  config (LanguageModelConfig): The configuration used to derive the OpenAI client\
    \ and related settings (e.g., model, prompts, etc.).\n  callbacks (WorkflowCallbacks\
    \ | None): Optional WorkflowCallbacks for propagating lifecycle events and errors\
    \ through Graphrag.\n  cache (PipelineCache | None): Optional cache to persist\
    \ responses; when provided, the provider reads from and writes to this cache as\
    \ part of operation.\n\nReturns:\n  None\n\nRaises:\n  Exception: Exceptions raised\
    \ by the underlying FNLLM/OpenAI integrations are propagated to the caller.\n\n\
    Attributes:\n  name: Identifier for the provider instance.\n  config: The LanguageModelConfig\
    \ used to configure the OpenAI components.\n  callbacks: Optional WorkflowCallbacks\
    \ for event propagation.\n  cache: Optional PipelineCache used for caching model\
    \ outputs.\n\nNotes:\n  If a cache is provided, responses may be cached and reused\
    \ for repeated prompts; the cache lifecycle and eviction are governed by the PipelineCache\
    \ implementation."
  methods:
  - name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
  - name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
  - name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
  - name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
- class_id: graphrag/config/errors.py::ApiKeyMissingError
  file: graphrag/config/errors.py
  name: ApiKeyMissingError
  docstring: "Exception raised when an API key is missing for a specific LLM type;\
    \ this internal error is meant to be raised, not returned. The error message constructed\
    \ for this exception includes the llm_type and, if provided, the azure_auth_type\
    \ to aid diagnosis.\n\nAttributes:\n    llm_type: The LLM type for which the API\
    \ Key is required.\n    azure_auth_type: Optional Azure authentication type; included\
    \ in the message if provided.\n\nArgs:\n    llm_type: The LLM type for which the\
    \ API Key is required.\n    azure_auth_type: Optional Azure authentication type;\
    \ if provided, included in the message."
  methods:
  - name: __init__
    signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None = None)
      -> None'
- class_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry
  file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  name: ExponentialRetry
  docstring: "ExponentialRetry provides exponential backoff retry logic for both synchronous\
    \ and asynchronous callables, with optional jitter.\n\nArgs:\n  max_retries: int.\
    \ Maximum number of retry attempts. Must be greater than 0. Default: 5.\n  base_delay:\
    \ float. Base delay between retries in seconds. Must be greater than 0.0. Default:\
    \ 2.0.\n  jitter: bool. If True, apply a small random jitter to each delay. Default:\
    \ True.\n  kwargs: Any. Additional keyword arguments accepted for forward compatibility;\
    \ not used by the retry logic.\n\nAttributes:\n  max_retries: int\n  base_delay:\
    \ float\n  jitter: bool\n  _logger: logging.Logger (internal). Diagnostic logger\
    \ for retry events.\n\nNotes:\n  - Jitter, when enabled, adds randomness to delays\
    \ to reduce thundering herd problems and applies to both retry and aretry paths.\n\
    \  - The retry behavior is determined by the configured max_retries, base_delay,\
    \ and jitter; delays are computed according to an exponential backoff scheme.\n\
    \  - This class is intended to be instantiated with the given configuration and\
    \ used to invoke retry on synchronous functions or aretry on asynchronous functions.\n\
    \nRaises:\n  ValueError: If max_retries <= 0 or base_delay <= 0.0."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n   \
      \     **kwargs: Any,\n    )"
  - name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  - name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
- class_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey
  file: graphrag/query/context_builder/entity_extraction.py
  name: EntityVectorStoreKey
  docstring: "Enum that defines the keys used to locate and identify Entity vectors\
    \ in a vector store.\n\nPurpose:\n    Provides the valid keys for identifying\
    \ an Entity vector, typically by id or by title.\n\nAttributes:\n    ID: The enum\
    \ member representing the string key \"id\".\n    TITLE: The enum member representing\
    \ the string key \"title\".\n\nMethods:\n    from_string(value: str) -> EntityVectorStoreKey:\n\
    \        Convert a string key to the corresponding EntityVectorStoreKey enum member.\n\
    \        Returns: The corresponding enum member (EntityVectorStoreKey.ID for \"\
    id\",\n                 EntityVectorStoreKey.TITLE for \"title\").\n        Raises:\
    \ ValueError if value is not a valid key (\"id\" or \"title\")."
  methods:
  - name: from_string
    signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
- class_id: graphrag/vector_stores/factory.py::VectorStoreFactory
  file: graphrag/vector_stores/factory.py
  name: VectorStoreFactory
  docstring: "VectorStoreFactory is a registry-based factory for constructing vector\
    \ store instances from a registry of concrete implementations.\n\nPurpose\nThis\
    \ class maintains a registry that maps vector_store_type keys (strings) to creator\
    \ callables that return BaseVectorStore instances. It exposes classmethods to\
    \ instantiate vector stores by type, list supported types, verify support for\
    \ a type, and register new implementations.\n\nClass Attributes\n  _registry (ClassVar[dict[str,\
    \ Callable[..., BaseVectorStore]]]): Internal registry mapping vector_store_type\
    \ keys to creator callables. Each creator should return a BaseVectorStore when\
    \ invoked with a VectorStoreSchemaConfig and any forwarded keyword arguments.\n\
    \nMethods\n  create_vector_store(\n      cls,\n      vector_store_type: str,\n\
    \      vector_store_schema_config: VectorStoreSchemaConfig,\n      **kwargs: dict\n\
    \  ) -> BaseVectorStore\n  Create a vector store instance by looking up the registered\
    \ creator for the given vector_store_type and invoking it with vector_store_schema_config\
    \ and any additional keyword arguments. If the type is not registered, raises\
    \ an error (e.g., KeyError or ValueError). Exceptions raised by the concrete vector\
    \ store constructor are propagated.\n\n  get_vector_store_types(cls) -> list[str]\n\
    \  Return a list of registered vector_store_type keys.\n\n  is_supported_type(cls,\
    \ vector_store_type: str) -> bool\n  Return True if vector_store_type is registered;\
    \ otherwise False.\n\n  register(cls, vector_store_type: str, creator: Callable[...,\
    \ BaseVectorStore]) -> None\n  Register a new vector store implementation under\
    \ the given vector_store_type. The provided creator is stored and invoked by create_vector_store.\
    \ Raises ValueError if vector_store_type is already registered.\n\nNotes\n The\
    \ factory delegates all construction details to the registered creators. The concrete\
    \ creators may require or accept additional kwargs, which are forwarded by create_vector_store."
  methods:
  - name: create_vector_store
    signature: "def create_vector_store(\n        cls,\n        vector_store_type:\
      \ str,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n     \
      \   **kwargs: dict,\n    ) -> BaseVectorStore"
  - name: get_vector_store_types
    signature: def get_vector_store_types(cls) -> list[str]
  - name: is_supported_type
    signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
  - name: register
    signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]\n    ) -> None"
- class_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: DRIFTSearchContextBuilder
  docstring: "DRIFTSearchContextBuilder wires together core DRIFT components to assemble\
    \ a coherent DRIFT search context used for retrieval and reasoning over community\
    \ reports, entities, covariates, and relationships. It serves as the central integration\
    \ point that binds the language model interface, embedding model, entity context,\
    \ prompts, and optional metadata into a single context object consumed by DRIFT-style\
    \ structured search workflows.\n\nKey attributes:\n- model: The chat-based language\
    \ model interface used for conversational reasoning.\n- text_embedder: The embedding\
    \ model used to encode text for similarity search.\n- entities: A list of Entity\
    \ objects representing known entities in reports or queries.\n- entity_text_embeddings:\
    \ A vector store containing embeddings for entity mentions.\n- text_units: Optional\
    \ list of TextUnit describing units of text content.\n- reports: Optional pre-loaded\
    \ list of CommunityReport objects to consider.\n- relationships: Optional relationships\
    \ between entities.\n- covariates: Optional mapping of covariates, keyed by domain,\
    \ to lists of Covariate.\n- tokenizer: Optional Tokenizer used to tokenize prompts/text.\n\
    - embedding_vectorstore_key: Key to locate entity embeddings in the vector store;\
    \ defaults to EntityVectorStoreKey.ID.\n- config: Optional DRIFTSearchConfig configuring\
    \ DRIFT behavior.\n- local_system_prompt: Optional custom system prompt for local\
    \ DRIFT search.\n- local_mixed_context: Optional pre-built LocalSearchMixedContext;\
    \ if omitted, one will be constructed as needed.\n- reduce_system_prompt: Optional\
    \ prompt used during reduction steps.\n- response_type: Optional hint for response\
    \ formatting.\n\nArgs:\n- model (ChatModel): The chat-based language model interface.\n\
    - text_embedder (EmbeddingModel): The embedding model used to encode text for\
    \ retrieval.\n- entities (list[Entity]): Entities referenced in reports and queries.\n\
    - entity_text_embeddings (BaseVectorStore): Vector store containing embeddings\
    \ for entity mentions.\n- text_units (list[TextUnit] | None): Optional list of\
    \ text units; defaults to None.\n- reports (list[CommunityReport] | None): Optional\
    \ pre-loaded reports; defaults to None.\n- relationships (list[Relationship] |\
    \ None): Optional relationships between entities; defaults to None.\n- covariates\
    \ (dict[str, list[Covariate]] | None): Optional covariates by domain; defaults\
    \ to None.\n- tokenizer (Tokenizer | None): Optional tokenizer; if None, a tokenizer\
    \ will be created as needed.\n- embedding_vectorstore_key (str): Key for locating\
    \ embeddings in the vector store; defaults to EntityVectorStoreKey.ID.\n- config\
    \ (DRIFTSearchConfig | None): Optional DRIFT configuration; defaults to None.\n\
    - local_system_prompt (str | None): Optional local system prompt; defaults to\
    \ None (uses DRIFT_LOCAL_SYSTEM_PROMPT).\n- local_mixed_context (LocalSearchMixedContext\
    \ | None): Optional pre-built local mixed context; defaults to None.\n- reduce_system_prompt\
    \ (str | None): Optional reduction prompt; defaults to None (uses DRIFT_REDUCE_PROMPT).\n\
    - response_type (str | None): Optional response formatting hint; defaults to None.\n\
    \nReturns:\n- None: This is a constructor that initializes and wires the components\
    \ into the DRIFT search context.\n\nRaises:\n- ValueError: If required inputs\
    \ are missing or embeddings are incompatible for downstream comparison.\n\nNotes:\n\
    - When optional inputs are omitted, sensible defaults are created to enable DRIFT-style\
    \ search operations.\n\nExamples:\n- Basic usage:\n  builder = DRIFTSearchContextBuilder(\n\
    \      model=my_chat_model,\n      text_embedder=my_embedder,\n      entities=entities_list,\n\
    \      entity_text_embeddings=entity_embedding_store\n  )\n  local = builder.init_local_context_builder()\n\
    \  df, stats = builder.build_context(\"find reports mentioning CityA\")"
  methods:
  - name: init_local_context_builder
    signature: def init_local_context_builder(self) -> LocalSearchMixedContext
  - name: build_context
    signature: "def build_context(\n        self, query: str, **kwargs\n    ) -> tuple[pd.DataFrame,\
      \ dict[str, int]]"
  - name: check_query_doc_encodings
    signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
      -> bool'
  - name: convert_reports_to_df
    signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
      \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n    \
      \    reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None =\
      \ None,\n        local_system_prompt: str | None = None,\n        local_mixed_context:\
      \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str | None = None,\n    )"
- class_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  file: graphrag/config/models/vector_store_schema_config.py
  name: VectorStoreSchemaConfig
  docstring: 'VectorStoreSchemaConfig defines and validates the mapping of schema
    field names used by the vector store.


    Purpose: Centralizes the configuration of field names for id, vector, text, and
    attributes and provides validation to ensure field names are safe and valid.


    Key attributes:

    - id_field: The field name used for the unique identifier in the schema.

    - vector_field: The field name containing the vector data.

    - text_field: The field name containing associated text data.

    - attributes_field: The field name containing additional attributes.


    Args:

    - id_field: The field name used as the unique identifier (must be a valid identifier).

    - vector_field: The field name for the vector data (must be a valid identifier).

    - text_field: The field name for the associated text (must be a valid identifier).

    - attributes_field: The field name for additional attributes (must be a valid
    identifier).


    Returns:

    - None


    Raises:

    - ValueError: If an unsafe or invalid field name is encountered during validation.'
  methods:
  - name: _validate_model
    signature: def _validate_model(self)
  - name: _validate_schema
    signature: def _validate_schema(self) -> None
- class_id: graphrag/language_model/response/base.py::ModelResponse
  file: graphrag/language_model/response/base.py
  name: ModelResponse
  docstring: "ModelResponse is a generic container for responses from an LLM provider.\
    \ It encapsulates the textual output, the provider's full JSON response, and an\
    \ optional parsed model instance along with a history of responses. The class\
    \ is parameterized by T, a subclass of BaseModel, representing a typed interpretation\
    \ of the response.\n\nAttributes:\n  output (ModelOutput): The output associated\
    \ with this response. Access the textual content via output.content and the complete\
    \ provider JSON via output.full_response (dict[str, Any] or None).\n  history\
    \ (list[Any]): The history of this response as a list of entries.\n  parsed_response\
    \ (T | None): The parsed model instance of type T, or None if not available.\n\
    \nNotes:\n- ModelOutput is a separate type that wraps the textual content and\
    \ the full provider JSON.\n- This docstring describes class-level behavior and\
    \ fields; initialization parameters, if any, are defined in the implementation."
  methods:
  - name: output
    signature: def output(self) -> ModelOutput
  - name: history
    signature: def history(self) -> list
  - name: parsed_response
    signature: def parsed_response(self) -> T | None
- class_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: SummarizeExtractor
  docstring: "SummarizeExtractor orchestrates the summarization of a list of description\
    \ strings into a single concise description for a target entity by invoking a\
    \ chat-based language model with a summarization prompt. The class is initialized\
    \ with a model invoker and configuration and is intended to be called to process\
    \ descriptions for a given identifier (or pair of identifiers).\n\nThis class\
    \ does not return a value from initialization. __init__ initializes the instance\
    \ and returns None implicitly.\n\nArgs\n    model_invoker: The ChatModel used\
    \ to run prompts.\n    max_summary_length: Maximum length of the summary to produce.\n\
    \    max_input_tokens: Maximum number of input tokens to consider for summarization.\n\
    \    summarization_prompt: Optional custom prompt to use for summarization.\n\
    \    on_error: Optional error handler invoked on errors.\n\nReturns\n    None\n\
    \nRaises\n    Exception: If initialization fails or the underlying LLM call fails\
    \ or processing encounters an error.\n\nAttributes\n    model_invoker: The ChatModel\
    \ used to run prompts.\n    max_summary_length: Maximum length of the summary\
    \ to produce.\n    max_input_tokens: Maximum number of input tokens to consider\
    \ for summarization.\n    summarization_prompt: Optional custom prompt to use\
    \ for summarization.\n    on_error: Optional error handler invoked on errors.\n\
    \nNotes\n    The class relies on the top-level constants ENTITY_NAME_KEY, DESCRIPTION_LIST_KEY,\
    \ and MAX_LENGTH_KEY to structure input payloads for the summarization process.\
    \ These keys influence how the input data is organized before being sent to the\
    \ language model but are not modified by SummarizeExtractor."
  methods:
  - name: _summarize_descriptions_with_llm
    signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
      \ str] | list[str], descriptions: list[str]\n    )"
  - name: _summarize_descriptions
    signature: "def _summarize_descriptions(\n        self, id: str | tuple[str, str],\
      \ descriptions: list[str]\n    ) -> str"
  - name: __call__
    signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n    ) -> SummarizationResult"
  - name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
      \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
- class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: AzureOpenAIEmbeddingFNLLM
  docstring: "Azure OpenAI Embedding FNLLM provider for generating text embeddings.\n\
    \nThis class provides embedding capabilities using an Azure OpenAI embedding model\
    \ via FNLLM and exposes methods to embed single texts and batches, with both asynchronous\
    \ and synchronous variants. It is initialized with a name, a LanguageModelConfig\
    \ used to derive the OpenAI configuration, and optional components such as callbacks\
    \ and a cache.\n\nArgs:\n  name: The name to assign to the internal cache provider\
    \ and model instance.\n  config: The LanguageModelConfig used to derive the OpenAI\
    \ configuration.\n  callbacks: Optional WorkflowCallbacks; if provided, an error\
    \ handler will be used.\n  cache: Optional PipelineCache.\n\nReturns:\n  None\n\
    \nRaises:\n  ValueError: If no embeddings are found in the response."
  methods:
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  - name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  - name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
- class_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: DRIFTContextBuilder
  docstring: "DRIFTContextBuilder is an abstract base class that defines the contract\
    \ for constructing the DRIFT context used to prime subsequent search actions for\
    \ a given query. It specifies an asynchronous interface to build the context,\
    \ exposed via the build_context(query, **kwargs) method, which returns a tuple\
    \ containing a DataFrame of contextual items and a metrics dictionary used to\
    \ warm up or seed downstream DRIFT search processes.\n\nArgs:\n    self: The instance\
    \ of the class.\n    query (str): The search query for which to build the context.\n\
    \    **kwargs: Additional keyword arguments to customize or influence context\
    \ construction.\n\nReturns:\n    tuple[pd.DataFrame, dict[str, int]]: A pair consisting\
    \ of a DataFrame of contextual items and a metrics dictionary mapping metric names\
    \ to integer counts.\n\nRaises:\n    NotImplementedError: If the subclass does\
    \ not implement build_context."
  methods:
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
      \    ) -> tuple[pd.DataFrame, dict[str, int]]"
- class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion
  file: graphrag/language_model/providers/litellm/types.py
  name: AFixedModelCompletion
  docstring: "Async fixed-model chat completion interface for litellm integration.\
    \ This class exposes a callable, asynchronous surface to perform chat completions\
    \ against an OpenAI compatible API using a fixed model, with optional streaming\
    \ support via litellm. It is designed for straightforward, open-ended chat interactions\
    \ and can be combined with function calling and tools when provided.\n\nArgs:\n\
    \  messages: list. Chat messages to include in the request. Defaults to an empty\
    \ list. Optional.\n  stream: bool | None. If True, stream partial responses as\
    \ they arrive. Defaults to None (non-streaming).\n  stream_options: dict | None.\
    \ Options controlling streaming behavior. Defaults to None.\n  stop: Any. Stop\
    \ sequence or sequences. Optional.\n  max_completion_tokens: int | None. Maximum\
    \ tokens allowed in the completion. Optional.\n  max_tokens: int | None. Maximum\
    \ tokens allowed for the response. Optional.\n  modalities: list of ChatCompletionModality\
    \ | None. Modality hints to apply. Optional.\n  prediction: ChatCompletionPredictionContentParam\
    \ | None. Prediction content to request. Optional.\n  audio: ChatCompletionAudioParam\
    \ | None. Audio-related parameters for the request. Optional.\n  logit_bias: dict\
    \ | None. Biases for logits of the model. Optional.\n  user: str | None. User\
    \ identifier for the request. Optional.\n  response_format: dict | type BaseModel\
    \ | None. Response format specification. Optional.\n  seed: int | None. Random\
    \ seed for reproducibility. Optional.\n  tools: list | None. Tools to be used\
    \ in processing the request. Optional.\n  tool_choice: str | dict | None. Tool\
    \ selection information. Optional.\n  logprobs: bool | None. Whether to include\
    \ log probabilities in the result. Optional.\n  top_logprobs: int | None. Number\
    \ of top log probabilities to return. Optional.\n  parallel_tool_calls: bool |\
    \ None. Allow parallel tool invocations. Optional.\n  web_search_options: OpenAIWebSearchOptions\
    \ | None. Options for web search augmentation. Optional.\n  deployment_id: any.\
    \ Deployment identifier. Optional.\n  extra_headers: dict | None. Additional HTTP\
    \ headers to include. Optional.\n  functions: list | None. Deprecated parameter.\
    \ Use function_call instead. Optional and deprecated.\n  function_call: str |\
    \ None. Behavior for function calling (eg, auto, none, or function name). Optional.\n\
    \  thinking: AnthropicThinkingParam | None. Thinking constraints for the underlying\
    \ model. Optional.\n  kwargs: Any. Additional keyword arguments forwarded to the\
    \ API. Optional.\n\nReturns:\n  ModelResponse | CustomStreamWrapper. The chat\
    \ completion result or a streaming wrapper for partial results.\n\nRaises:\n \
    \ Exceptions raised by the underlying litellm/OpenAI API clients during request\
    \ handling.\n\nExample:\n  # Non-streaming call\n  ac = AFixedModelCompletion(...)\n\
    \  result = await ac(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n\
    \n  # Streaming call\n  stream = ac(messages=[{\"role\": \"user\", \"content\"\
    : \"Tell me a story.\"}], stream=True)\n  async for chunk in stream:\n      #\
    \ process streaming chunks\n      pass"
  methods:
  - name: __call__
    signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
      \ params: see https://platform.openai.com/docs/api-reference/chat/create\n \
      \       messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
      \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
      \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
      \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
      \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
      \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n \
      \       logit_bias: dict | None = None,  # type: ignore\n        user: str |\
      \ None = None,\n        # openai v1.0+ new params\n        response_format:\
      \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int |\
      \ None = None,\n        tools: list | None = None,  # type: ignore\n       \
      \ tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
      \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
- class_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer
  file: graphrag/query/structured_search/drift_search/primer.py
  name: DRIFTPrimer
  docstring: "DRIFTPrimer coordinates the DRIFT drift-search workflow for structured\
    \ query processing over community reports. It decomposes queries using global\
    \ guidance, splits the input reports into folds for parallel processing, and executes\
    \ asynchronous searches against a language model, using a tokenizer to manage\
    \ tokens.\n\nAttributes:\n    config (DRIFTSearchConfig): Configuration settings\
    \ for DRIFT search.\n    chat_model (ChatModel): The language model used for searching.\n\
    \    tokenizer (Tokenizer): Tokenizer used to manage tokens during processing.\n\
    \nArgs:\n    config (DRIFTSearchConfig): Configuration settings for DRIFT search.\n\
    \    chat_model (ChatModel): The language model used for searching.\n    tokenizer\
    \ (Tokenizer, optional): Tokenizer for managing tokens. If not provided, a default\
    \ tokenizer is obtained.\n\nReturns:\n    None\n\nRaises:\n    None"
  methods:
  - name: split_reports
    signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
      \ pd.DataFrame,\n    ) -> SearchResult"
  - name: decompose_query
    signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
      \    ) -> tuple[dict, dict[str, int]]"
  - name: __init__
    signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n\
      \        chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
      \    )"
- class_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType
  file: graphrag/index/operations/summarize_communities/typing.py
  name: CreateCommunityReportsStrategyType
  docstring: 'Enum describing the strategies for creating community reports in the
    summarize_communities operation.


    Description:

    CreateCommunityReportsStrategyType is an enumeration of the available strategies
    used to create community reports in the summarize_communities workflow. Each member
    represents a concrete strategy and exposes its identity via the member''s name
    and its associated representation via the member''s value. The exact type of member.value
    is defined by the enum''s members and may be a string, a callable, or another
    object that encodes the strategy.


    Notes:

    - Access member.name for the programmer-friendly identifier and member.value for
    the underlying representation.

    - This docstring follows Python Enum semantics: unless overridden, __repr__ and
    __str__ reflect Enum behavior, and the value attached to each member is whatever
    was assigned.

    - If you need a human-facing description of a member, consider maintaining a separate
    mapping or documentation since Enum members themselves typically only provide
    name and value.


    Examples:

    - Access attributes of a member: strategy.name and strategy.value

    - Iterate over all members: for s in CreateCommunityReportsStrategyType: ...'
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig
  file: graphrag/config/models/vector_store_config.py
  name: VectorStoreConfig
  docstring: "Configuration model for vector store settings used by graphrag.\n\n\
    It centralizes validation and handling of vector store configuration, including\
    \ the store type, connection details (such as db_uri and url), embeddings schema\
    \ validation, and the vector store schema configuration. This ensures consistent,\
    \ validated settings are available for downstream operations.\n\nKey attributes:\n\
    \  vector_store: The vector store configuration block that includes the store\
    \ type and connection details.\n  db_uri: The database URI for the vector store;\
    \ may be set to a default when the type is LanceDB.\n  url: The URL for the vector\
    \ store; required when vector_store.type == azure_ai_search.\n  embeddings_schema:\
    \ A collection of embedding schema names to use; validated against all_embeddings.\n\
    \  vector_store_schema_config: The schema configuration for the vector store,\
    \ typically an instance of VectorStoreSchemaConfig.\n\nArgs:\n  vector_store:\
    \ The vector store configuration block containing the store type and connection\
    \ details.\n  db_uri: The database URI for the vector store; may be defaulted\
    \ for LanceDB.\n  url: The connection URL for the vector store.\n  embeddings_schema:\
    \ The list of embedding schema names to validate.\n  vector_store_schema_config:\
    \ The VectorStoreSchemaConfig object for schema validation.\n\nReturns:\n  None\n\
    \nRaises:\n  ValueError: If vector_store.type is not LanceDB and a non-empty db_uri\
    \ is provided.\n  ValueError: If any entry in embeddings_schema is not a known\
    \ embedding schema name present in all_embeddings.\n  ValueError: If vector_store.type\
    \ == azure_ai_search and vector_store.url is missing or empty.\n  ValueError:\
    \ If an invalid database URI, URL, or embeddings schema is encountered during\
    \ validation."
  methods:
  - name: _validate_db_uri
    signature: def _validate_db_uri(self) -> None
  - name: _validate_embeddings_schema
    signature: def _validate_embeddings_schema(self) -> None
  - name: _validate_url
    signature: def _validate_url(self) -> None
  - name: _validate_model
    signature: def _validate_model(self)
- class_id: graphrag/language_model/response/base.pyi::ModelResponse
  file: graphrag/language_model/response/base.pyi
  name: ModelResponse
  docstring: "Protocol describing a model response produced by the GraphRAG language\
    \ model integration. This Protocol exposes three properties: parsed_response,\
    \ history, and output, and is generic over _T, the type of the parsed response.\n\
    \nType parameters:\n- _T: The type of the parsed_response value.\n\nAttributes:\n\
    - parsed_response: _T | None \u2014 The parsed response, or None if not available.\n\
    - history: list[Any] \u2014 The history of the model responses as a list of items.\n\
    - output: ModelOutput \u2014 The structured output for this response. ModelOutput\
    \ is defined elsewhere and typically exposes:\n  - content: str \u2014 The textual\
    \ content of the output.\n  - full_response: dict[str, Any] | None \u2014 The\
    \ full JSON response from the LLM provider, or None if not available."
  methods:
  - name: parsed_response
    signature: def parsed_response(self) -> _T | None
  - name: history
    signature: def history(self) -> list[Any]
  - name: output
    signature: def output(self) -> ModelOutput
- class_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: LitellmChatModel
  docstring: "LitellmChatModel is a Graphrag wrapper around a Litellm chat model with\
    \ streaming, caching, and resilience features.\n\nArgs:\n  name: The name of the\
    \ model instance.\n  config: LanguageModelConfig containing configuration for\
    \ the language model.\n  cache: Optional PipelineCache to use for responses; if\
    \ provided, a child cache scoped to this model's name is created.\n  kwargs: Arbitrary\
    \ keyword arguments forwarded to the underlying Litellm client.\n\nReturns:\n\
    \  LitellmChatModel: The initialized model instance.\n\nRaises:\n  May raise exceptions\
    \ from underlying libraries (e.g., authentication or network errors) during initialization\
    \ or operation."
  methods:
  - name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
  - name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
  - name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs:
      Any) -> "MR"'
  - name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
  - name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> \"MR\""
  - name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
- class_id: graphrag/config/enums.py::SearchMethod
  file: graphrag/config/enums.py
  name: SearchMethod
  docstring: 'SearchMethod enumeration of the available search methods.


    Purpose:

    Represents the set of string-backed identifiers used to select among different
    search backends or strategies in Graphrag''s configuration and runtime logic.


    Attributes:

    - value (str): The string identifier associated with the enum member (the enum''s
    underlying value).

    - name (str): The member''s name.


    Notes:

    The __str__ method returns the string representation of the enum value.'
  methods:
  - name: __str__
    signature: def __str__(self)
- class_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore
  file: tests/integration/vector_stores/test_lancedb.py
  name: TestLanceDBVectorStore
  docstring: 'Integration tests for LanceDBVectorStore integration.


    Purpose:

    Test the LanceDB-backed vector store implementation (LanceDBVectorStore) by exercising
    common operations such as creating and deleting collections, loading documents,
    performing vector similarity searches, applying filters, and ensuring basic vector
    store functionality works as expected in an integration test context.


    Summary:

    This test class uses sample_documents and sample_documents_categories helpers
    to generate VectorStoreDocument instances and relies on a simple mock_embedder
    that returns a fixed embedding [0.1, 0.2, 0.3, 0.4, 0.5]. It includes tests for:

    - test_empty_collection

    - test_vector_store_customization

    - test_filter_search

    - test_vector_store_operations


    Inferred key attributes:

    No explicit instance attributes are documented; the tests rely on the imports
    of VectorStoreDocument and LanceDBVectorStore, as well as the helper methods.'
  methods:
  - name: sample_documents_categories
    signature: def sample_documents_categories(self)
  - name: sample_documents
    signature: def sample_documents(self)
  - name: test_empty_collection
    signature: def test_empty_collection(self)
  - name: test_vector_store_customization
    signature: def test_vector_store_customization(self, sample_documents)
  - name: test_filter_search
    signature: def test_filter_search(self, sample_documents_categories)
  - name: test_vector_store_operations
    signature: def test_vector_store_operations(self, sample_documents)
  - name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
- class_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch
  file: graphrag/query/structured_search/basic_search/search.py
  name: BasicSearch
  docstring: 'BasicSearch orchestrates the construction of a single-context-window
    search context and the generation (or streaming) of an answer for a user query.


    Purpose and responsibility:

    - Build a single-context-window search context

    - Orchestrate a language model (ChatModel) with a BasicContextBuilder to produce
    results

    - Support both full result generation via search and streaming output via stream_search


    Key attributes inferred from initialization:

    - model: The language model interface used for this basic search.

    - context_builder: The builder that constructs the context for the search.

    - tokenizer: Optional tokenizer to use.

    - system_prompt: System prompt for the search. If None, a default system prompt
    is used.

    - response_type: The type/format of the response. Default is "multiple paragraphs".

    - callbacks: Optional list of QueryCallbacks to handle events during search.

    - model_params: Additional parameters passed to the model.

    - context_builder_params: Additional parameters passed to the context builder.


    Args:

    model: The language model interface used for this basic search.

    context_builder: The builder that constructs the context for the search.

    tokenizer: Optional tokenizer to use.

    system_prompt: System prompt for the search. If None, the default system prompt
    is used.

    response_type: The type of response formatting. Default is "multiple paragraphs".

    callbacks: Optional list of QueryCallbacks to be invoked during search.

    model_params: Additional parameters passed to the model.

    context_builder_params: Additional parameters passed to the context builder.


    Returns:

    None: This initializer does not return a value.


    Raises:

    Exceptions raised by the underlying components (e.g., ChatModel, BasicContextBuilder)
    may be propagated to the caller.'
  methods:
  - name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  - name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
- class_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig
  file: graphrag/config/models/text_embedding_config.py
  name: TextEmbeddingConfig
  docstring: 'TextEmbeddingConfig: Configuration holder for the text embedding strategy
    used to embed text in Graphrag. It centralizes the selection between a user-provided
    custom strategy and a default strategy resolved from LanguageModelConfig and graphrag_config_defaults.


    Initialization: __init__(strategy: dict | None = None) initializes the configuration
    with an optional custom strategy. If a strategy is provided, resolved_strategy(model_config)
    will return this value; otherwise, a default strategy is resolved based on the
    given LanguageModelConfig and the configured defaults.


    Attributes:

    - strategy: dict | None - Custom text embedding strategy to use when provided;
    otherwise None.


    Methods:

    - resolved_strategy(model_config: LanguageModelConfig) -> dict - Returns the resolved
    text embedding strategy. If a custom strategy is provided via self.strategy, that
    value is returned; otherwise, a default strategy dictionary is produced. The default
    includes at least the key ''type'' set to TextEmbedStrategyType.openai and other
    keys derived from the language model configuration and graphrag_config_defaults.


    Raises:

    - pydantic.ValidationError: if initialization receives an invalid strategy type
    or shape.

    - Exception during resolution if required configuration is missing or invalid.'
  methods:
  - name: resolved_strategy
    signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) ->
      dict'
- class_id: tests/smoke/test_fixtures.py::TestIndexer
  file: tests/smoke/test_fixtures.py
  name: TestIndexer
  docstring: 'TestIndexer is a test helper that coordinates setup, execution, and
    validation of smoke-test fixtures for the indexer and its queries. It encapsulates
    helpers to run the indexer, issue queries, verify indexer outputs against a workflow
    configuration, and prepare blob-storage backed test data for fixtures used by
    the test suite. The class orchestrates test preparation, execution, and validation
    within the tests/smoke/test_fixtures.py module.


    Attributes:

    - The class does not expose explicit attributes in the excerpt; it relies on internal
    helpers and runtime parameters to perform operations.


    Methods:

    __assert_indexer_outputs(self, root: Path, workflow_config: dict[str, dict[str,
    Any]]) -> None

    - Asserts that the indexer outputs under root conform to the provided workflow_config.

    - Returns: None

    - Raises: AssertionError if outputs do not match; ValueError for invalid inputs.


    __run_indexer(self, root: Path, input_file_type: str) -> subprocess.CompletedProcess

    - Runs the indexer command for the given root and input_file_type and returns
    the subprocess result.

    - Returns: CompletedProcess representing the executed command.

    - Raises: subprocess.CalledProcessError if the command fails.


    __run_query(self, root: Path, query_config: dict[str, str]) -> subprocess.CompletedProcess

    - Runs a query against the indexer using the provided root and query_config and
    returns the subprocess result.

    - Returns: CompletedProcess of the query command.

    - Raises: subprocess.CalledProcessError on failure.


    test_fixture(self, input_path: str, input_file_type: str, workflow_config: dict[str,
    dict[str, Any]], query_config: list[dict[str, str]]) -> Callable[[], None]

    - Prepares test fixture data in the blob-storage-like backend used by the tests,
    uploading data from the specified input_path and configuring the test run as described
    by workflow_config and query_config.

    - Returns: A no-argument callable that cleans up the created resources when invoked.

    - Raises: ValueError if inputs are invalid; RuntimeError for storage-related failures.'
  methods:
  - name: __assert_indexer_outputs
    signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
      \ dict[str, dict[str, Any]]\n    )"
  - name: __run_indexer
    signature: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
      \ str,\n    )"
  - name: __run_query
    signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
  - name: test_fixture
    signature: "def test_fixture(\n        self,\n        input_path: str,\n     \
      \   input_file_type: str,\n        workflow_config: dict[str, dict[str, Any]],\n\
      \        query_config: list[dict[str, str]],\n    )"
- class_id: graphrag/logger/progress.py::ProgressTicker
  file: graphrag/logger/progress.py
  name: ProgressTicker
  docstring: 'ProgressTicker tracks progress toward a known total and optionally notifies
    a callback with progress updates.


    This class maintains an internal counter of completed items out of a known total
    and, on demand, emits progress events to an optional callback. A progress event
    is represented as an object with at least total_items and completed_items fields,
    and may also include the description provided at construction.


    Args:

    - callback: ProgressHandler | None - A function to handle progress reports, or
    None to disable updates. Default: None. The callback receives a progress-like
    object describing current progress.

    - num_total: int - Total number of items to track.

    - description: str - Optional description for the progress updates. Default: "".


    Returns:

    - None


    Raises:

    - None


    Attributes:

    - _callback: ProgressHandler | None - The function to call with progress updates,
    or None to suppress updates.

    - _num_total: int - Total number of items to track.

    - _description: str - Optional description for display.

    - _completed_items: int - Count of completed ticks.


    Behavior:

    - __call__(self, num_ticks: int = 1) - Advances the internal counter by num_ticks.
    If a callback is set, invokes it with a progress-like object containing total_items
    (equal to _num_total) and completed_items (updated value).

    - done(self) - Marks the progress as done. If a callback is provided, invokes
    it with a progress-like object whose total_items and completed_items both equal
    _num_total, preserving the description from initialization.


    Example:

    A minimal usage example:

    Define a callback function that prints the progress, create a ProgressTicker with
    a total, and advance it by calling the instance. Call done() to report completion.'
  methods:
  - name: __call__
    signature: 'def __call__(self, num_ticks: int = 1) -> None'
  - name: __init__
    signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n    )"
  - name: done
    signature: def done(self) -> None
- class_id: graphrag/data_model/document.py::Document
  file: graphrag/data_model/document.py
  name: Document
  docstring: 'Document data model representing a document in the GraphRag data model.


    Purpose:

    Encapsulates identifiers, metadata, and content for a document, supporting construction
    from a dictionary.


    Key attributes:

    - id: The document''s identifier.

    - human_readable_id: Optional short identifier.

    - title: The document title.

    - type: The document type.

    - text: The document text content.

    - text_units: Units describing text content.

    - attributes: Additional attributes associated with the document.


    From dictionary construction:

    The class provides a from_dict classmethod to create a Document instance from
    a dictionary. The method accepts keys for each field, with sensible defaults:

    - id_key: Key in d for the document''s identifier. Defaults to "id".

    - short_id_key: Key in d for the optional short identifier. Defaults to "human_readable_id".

    - title_key: Key in d for the title. Defaults to "title".

    - type_key: Key in d for the document''s type. Defaults to "type".

    - text_key: Key in d for the document''s text. Defaults to "text".

    - text_units_key: Key in d for the text units. Defaults to "text_units".

    - attributes_key: Key in d for the document''s attributes. Defaults to "attributes".


    Returns:

    Document: The constructed Document instance.


    Raises:

    None documented.'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      text_key: str = \"text\",\n        text_units_key: str = \"text_units\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Document\""
- class_id: graphrag/index/workflows/factory.py::PipelineFactory
  file: graphrag/index/workflows/factory.py
  name: PipelineFactory
  docstring: "PipelineFactory coordinates registration and construction of pipelines\
    \ composed of workflow functions. It maintains a class-level registry of named\
    \ WorkflowFunction callables and can assemble these into reusable Pipeline objects\
    \ for GraphRag-based workflows. A Pipeline is a sequence of WorkflowFunction objects\
    \ executed in order to process GraphRag data.\n\nAttributes:\n  registry: ClassVar[dict[str,\
    \ WorkflowFunction]] - class-level mapping of names to workflow callables used\
    \ to build pipelines and validate references.\n\nMethods:\n  register(cls, name:\
    \ str, workflow: WorkflowFunction)\n    Register a custom workflow function.\n\
    \    Args:\n      cls: The class that provides access to the registry (PipelineFactory).\n\
    \      name: The name under which the workflow will be registered.\n      workflow:\
    \ The workflow function to register for the given name.\n    Returns:\n      None\n\
    \    Raises:\n      TypeError: If the provided name or workflow are of incorrect\
    \ types.\n\n  register_all(cls, workflows: dict[str, WorkflowFunction])\n    Register\
    \ a dict of custom workflow functions.\n    Args:\n      cls: The class that provides\
    \ access to the registry (PipelineFactory).\n      workflows: A dictionary mapping\
    \ workflow names to workflow functions.\n    Returns:\n      None\n    Raises:\n\
    \      TypeError: If the mapping is not of the expected type or contains invalid\
    \ entries.\n\n  create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
    \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline\n\
    \    Create a pipeline for executing a sequence of workflows.\n    Args:\n   \
    \   cls: The class reference (provided automatically for classmethod).\n     \
    \ config: GraphRagConfig describing the graph/rag indexing setup.\n      method:\
    \ The indexing method or key to select a predefined pipeline. Defaults to IndexingMethod.Standard.\n\
    \    Returns:\n      Pipeline: The constructed Pipeline object.\n    Raises:\n\
    \      KeyError: If any workflow name in the selected workflows is not registered.\n\
    \      TypeError: If the provided config or method have invalid types.\n     \
    \ ValueError: If the resolved workflow list is empty or otherwise invalid.\n\n\
    \  register_pipeline(cls, name: str, workflows: list[str])\n    Register a new\
    \ pipeline method as a list of workflow names.\n    Args:\n      cls: The class\
    \ reference (PipelineFactory).\n      name: The name of the pipeline to register.\n\
    \      workflows: A list of workflow names that constitute the pipeline.\n   \
    \ Returns:\n      None\n    Raises:\n      TypeError: If inputs have incorrect\
    \ types.\n      KeyError: If any referenced workflow name is not registered.\n\
    \      ValueError: If the workflows list is empty."
  methods:
  - name: register
    signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
  - name: register_all
    signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
  - name: create_pipeline
    signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
      \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline"
  - name: register_pipeline
    signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
- class_id: graphrag/data_model/relationship.py::Relationship
  file: graphrag/data_model/relationship.py
  name: Relationship
  docstring: "Represents a relationship between two entities in the graph data model.\n\
    \nThis dataclass captures metadata about a relationship, including identifiers,\
    \ source\nand target references, descriptive text, ranking and weight, related\
    \ text units, and\narbitrary attributes.\n\nArgs:\n    id: Unique identifier for\
    \ the relationship.\n    short_id: Optional short identifier (human_readable_id).\n\
    \    source: Reference to the source entity in the relationship.\n    target:\
    \ Reference to the target entity in the relationship.\n    description: Description\
    \ of the relationship.\n    rank: Ranking value for ordering or prioritization.\n\
    \    weight: Weight or strength of the relationship.\n    text_unit_ids: Identifiers\
    \ of related text units.\n    attributes: Arbitrary additional attributes associated\
    \ with the relationship.\n\nReturns:\n    Relationship: A Relationship instance\
    \ created from dictionary data via from_dict.\n\nRaises:\n    KeyError: If required\
    \ keys are missing from the input dictionary during construction.\n    TypeError:\
    \ If input data types are not as expected."
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        source_key: str = \"source\",\n        target_key: str = \"target\"\
      ,\n        description_key: str = \"description\",\n        rank_key: str =\
      \ \"rank\",\n        weight_key: str = \"weight\",\n        text_unit_ids_key:\
      \ str = \"text_unit_ids\",\n        attributes_key: str = \"attributes\",\n\
      \    ) -> \"Relationship\""
- class_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError
  file: graphrag/index/utils/derive_from_rows.py
  name: ParallelizationError
  docstring: 'ParallelizationError stores information about errors that occurred during
    parallel transformation.


    Attributes:

    - num_errors (int): The number of errors that occurred during the parallel transformation.

    - example (str | None): Optional example error string to include in messages or
    logs. Defaults to None.


    Initialization:

    __init__(self, num_errors: int, example: str | None = None)


    Initializes the instance with the given error details and stores them on the object
    for later access (e.g., for error reporting or messaging).'
  methods:
  - name: __init__
    signature: 'def __init__(self, num_errors: int, example: str | None = None)'
- class_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig
  file: graphrag/config/models/community_reports_config.py
  name: CommunityReportsConfig
  docstring: "Configuration model for extracting and summarizing community reports\
    \ in GraphRag.\n\nOverview:\n- This Pydantic BaseModel stores the configuration\
    \ that governs how community reports are produced and prepared for downstream\
    \ processing. It supports resolving a concrete strategy to be used by other components,\
    \ either by returning a provided strategy or by constructing a default one from\
    \ configured defaults and the supplied language model configuration.\n\nAttributes:\n\
    - strategy: Optional[CreateCommunityReportsStrategyType]. The optional strategy\
    \ configuration for community report extraction. If provided, the resolved_strategy\
    \ method will return this strategy as-is. If omitted, a default strategy is constructed\
    \ from graphrag_config_defaults and later augmented with the llm from a LanguageModelConfig\
    \ when resolved.\n\nResolved strategy behavior:\n- If strategy is provided, resolved_strategy\
    \ returns the provided strategy unchanged.\n- If strategy is None, resolved_strategy\
    \ builds a default strategy from graphrag_config_defaults and populates its llm\
    \ field with the serialized model configuration obtained from model_config.model_dump().\
    \ This enables downstream components to know which language model to use during\
    \ execution.\n\nMethods:\n- resolved_strategy(root_dir: str, model_config: LanguageModelConfig)\
    \ -> dict\n  Returns: A dictionary describing the resolved community report extraction\
    \ strategy, ready for downstream processing.\n\nRaises:\n- ValidationError: If\
    \ invalid data is provided to initialize the model (e.g., missing required fields\
    \ or inappropriate types).\n\nExamples:\n- Provided strategy:\n  config = CommunityReportsConfig(strategy={\"\
    type\": \"custom\", \"params\": {\"foo\": 1}})\n  resolved = config.resolved_strategy(\"\
    /root\", some_model_config)\n  # resolved is exactly config.strategy\n\n- No strategy\
    \ provided:\n  config = CommunityReportsConfig(strategy=None)\n  resolved = config.resolved_strategy(\"\
    /root\", some_model_config)\n  # resolved is a dict built from graphrag_config_defaults\
    \ with llm populated from some_model_config.model_dump()."
  methods:
  - name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
- class_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: GraphExtractor
  docstring: "GraphExtractor orchestrates graph extraction from text using a language\
    \ model and structured prompts, accumulating results into graphs and supporting\
    \ iterative gleaning via continuation prompts when configured.\n\nKey attributes\
    \ include: model_invoker (the ChatModel used for prompt execution) and configuration\
    \ for delimiter/prompt keys (tuple_delimiter_key, record_delimiter_key, input_text_key,\
    \ entity_types_key, completion_delimiter_key), as well as optional prompt, join_descriptions,\
    \ max_gleanings, and on_error.\n\nArgs:\n  model_invoker: The model invoker used\
    \ to run the extraction prompts against the language model.\n  tuple_delimiter_key:\
    \ Prompt variable key for the delimiter between tuples.\n  record_delimiter_key:\
    \ Prompt variable key for the delimiter between records.\n  input_text_key: Prompt\
    \ variable key for the input text content.\n  entity_types_key: Prompt variable\
    \ key for the entity types to extract.\n  completion_delimiter_key: Prompt variable\
    \ key for the delimiter that marks completion of an extraction loop.\n  prompt:\
    \ Optional prompt template to override defaults.\n  join_descriptions: Flag indicating\
    \ whether to join entity descriptions in the results.\n  max_gleanings: Maximum\
    \ number of gleaning iterations allowed.\n  on_error: Error handler to be invoked\
    \ on errors during processing.\n\nReturns:\n  GraphExtractionResult: The result\
    \ of running graph extraction on the input texts.\n\nRaises:\n  Exception: Exceptions\
    \ may be raised by the underlying model invocation or processing, which may be\
    \ handled by the on_error callback."
  methods:
  - name: __call__
    signature: "def __call__(\n        self, texts: list[str], prompt_variables: dict[str,\
      \ Any] | None = None\n    ) -> GraphExtractionResult"
  - name: _process_document
    signature: "def _process_document(\n        self, text: str, prompt_variables:\
      \ dict[str, str]\n    ) -> str"
  - name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
      \ str | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        prompt: str | None = None,\n        join_descriptions=True,\n     \
      \   max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn | None\
      \ = None,\n    )"
  - name: _process_results
    signature: "def _process_results(\n        self,\n        results: dict[int, str],\n\
      \        tuple_delimiter: str,\n        record_delimiter: str,\n    ) -> nx.Graph"
- class_id: graphrag/language_model/protocol/base.py::EmbeddingModel
  file: graphrag/language_model/protocol/base.py
  name: EmbeddingModel
  docstring: "EmbeddingModel protocol for generating text embeddings.\n\nPurpose:\n\
    \    Defines the interface for producing embedding vectors from text, including\
    \ asynchronous and synchronous single-item and batch operations.\n\nArgs:\n  \
    \  text_list (list[str]): The list of strings to generate embeddings for. Used\
    \ by aembed_batch and embed_batch.\n    text (str): The text to generate an embedding\
    \ for. Used by aembed and embed.\n    kwargs (Any): Additional keyword arguments\
    \ (e.g., model parameters) supplied to embedding methods.\n\nReturns:\n    For\
    \ aembed_batch and embed_batch: list[list[float]] (a batch of embedding vectors).\n\
    \    For aembed and embed: list[float] (a single embedding vector).\n\nRaises:\n\
    \    Exception: If an error occurs during embedding generation."
  methods:
  - name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
  - name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  - name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  - name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
- class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: AzureOpenAIChatFNLLM
  docstring: "Azure OpenAI Chat LLM provider using FNLLM.\n\nResponsible for interfacing\
    \ with Azure OpenAI's chat endpoints through FNLLM wrappers, exposing both synchronous\
    \ and streaming chat interfaces, supporting optional conversation history, and\
    \ integrating caching and error handling via optional callbacks.\n\nArgs:\n  \
    \  name (str): The name to assign to the internal cache provider and model instance.\n\
    \    config (LanguageModelConfig): The configuration used to derive the OpenAI\
    \ configuration.\n    callbacks (WorkflowCallbacks | None): Optional WorkflowCallbacks;\
    \ if provided, an error handler will be created to log issues.\n    cache (PipelineCache\
    \ | None): Optional PipelineCache used for caching responses and model state.\n\
    \nReturns:\n    None\n\nRaises:\n    Exception: Exceptions raised by the underlying\
    \ model initialization or by auxiliary utilities may propagate."
  methods:
  - name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
  - name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
  - name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
  - name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
- class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding
  file: graphrag/language_model/providers/litellm/types.py
  name: AFixedModelEmbedding
  docstring: "Callable interface to obtain embeddings via a fixed Litellm model.\n\
    \nThis class stores the configuration for a pre-selected model and exposes a callable\
    \ interface (__call__) that returns an EmbeddingResponse from the underlying Litellm/OpenAI\
    \ service. Key configuration attributes include the selected model parameters\
    \ and per-request settings (timeouts, API credentials, and caching) used when\
    \ requesting embeddings.\n\nArgs:\n  request_id: str | None (default: None)\n\
    \      Optional request identifier.\n  input: list (default: [])\n      List input\
    \ to embed.\n  dimensions: int | None (default: None)\n      Optional embedding\
    \ dimensions.\n  encoding_format: str | None (default: None)\n      Optional encoding\
    \ format.\n  timeout: int (default: 600)\n      Timeout in seconds for the request.\n\
    \  api_base: str | None (default: None)\n      Optional API base.\n  api_version:\
    \ str | None (default: None)\n      Optional API version.\n  api_key: str | None\
    \ (default: None)\n      Optional API key.\n  api_type: str | None (default: None)\n\
    \      Optional API type.\n  caching: bool (default: False)\n      Whether to\
    \ enable caching.\n  user: str | None (default: None)\n      Optional user.\n\
    \  **kwargs: Any\n      Additional keyword arguments forwarded to the embedding\
    \ service.\n\nReturns:\n  EmbeddingResponse\n      The embedding response as produced\
    \ by the underlying service (type alias to CreateEmbeddingResponse in litellm).\n\
    \nRaises:\n  Exception\n      Exceptions raised by the embedding service or underlying\
    \ libraries."
  methods:
  - name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
- class_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: NoopWorkflowCallbacks
  docstring: 'Noop implementation of the WorkflowCallbacks interface that performs
    no operations.


    Purpose:

    Provide a safe, no-op callback implementation that conforms to the WorkflowCallbacks

    interface, suitable for tests or scenarios where callbacks are required but should

    not alter behavior.


    Attributes:

    - Stateless: This class does not maintain internal state between calls.


    Summary:

    All callback methods implemented by this class return None and perform no side
    effects.'
  methods:
  - name: progress
    signature: 'def progress(self, progress: Progress) -> None'
  - name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  - name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
  - name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
  - name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
- class_id: graphrag/language_model/events/base.py::ModelEventHandler
  file: graphrag/language_model/events/base.py
  name: ModelEventHandler
  docstring: 'ModelEventHandler Protocol for handling model-related events, with a
    focus on error handling within the language model system.


    Purpose:

    Define the contract that concrete event handlers must follow to process and respond
    to errors raised by model operations.


    Key attributes:

    - on_error: The error-handling contract that implementations must provide.


    on_error signature:

    def on_error(self, error: BaseException | None, traceback: str | None = None,
    arguments: dict[str, Any] | None = None) -> None


    Notes:

    - The Args described below correspond to the on_error method parameters; there
    are no separate class-level parameters.

    - All parameters are optional to allow graceful handling when error information
    is incomplete.


    Args:

    - error: The error that occurred, or None if no error is provided.

    - traceback: The traceback string associated with the error, or None if not available.

    - arguments: Additional contextual arguments related to the error, or None.


    Returns:

    - None: The function does not return a value.


    Raises:

    - None'
  methods:
  - name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory
  file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  name: NounPhraseExtractorFactory
  docstring: "Factory for selecting and instantiating noun phrase extractors based\
    \ on configuration.\n\nThis class provides a registry of available noun phrase\
    \ extractors and a single entry point to instantiate the appropriate extractor\
    \ according to a TextAnalyzerConfig. It relies on a class-level mapping from extractor\
    \ type identifiers to extractor classes to resolve and instantiate the requested\
    \ extractor.\n\nKey attributes\n- np_extractor_types: ClassVar mapping from extractor\
    \ type identifiers (str) to extractor classes (the registry).\n\nMethods\n- get_np_extractor(cls,\
    \ config: TextAnalyzerConfig) -> BaseNounPhraseExtractor: Get the noun phrase\
    \ extractor instance based on the configured type.\n  Args: cls: The class (used\
    \ as a classmethod parameter). config: TextAnalyzerConfig containing extractor_type\
    \ and related options such as model_name, max_word_length, include_named_entities,\
    \ exclude_entity_tags, exclude_pos_tags, exclude_nouns, word_delimiter, noun_phrase_grammars,\
    \ and noun_phrase_tags.\n  Returns: BaseNounPhraseExtractor\n  Raises: Exceptions\
    \ raised by underlying extractors or by configuration issues may propagate.\n\n\
    - register(cls, np_extractor_type: str, np_extractor: type): Register a noun phrase\
    \ extractor in NounPhraseExtractorFactory by adding it to the class-level registry\
    \ np_extractor_types.\n  Args: cls: The class on which this classmethod is invoked.\
    \ np_extractor_type: The string identifier for the extractor type. np_extractor:\
    \ The extractor class to register.\n  Returns: None\n  Raises: Exceptions raised\
    \ during registration may propagate."
  methods:
  - name: get_np_extractor
    signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
  - name: register
    signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
- class_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType
  file: graphrag/index/operations/extract_graph/typing.py
  name: ExtractEntityStrategyType
  docstring: 'ExtractEntityStrategyType is an Enum subclass that defines the available
    strategies for extracting entities in the graphrag index''s extraction workflow.


    Purpose:

    This enumeration provides a type-safe way to refer to a specific extraction strategy.
    Each member represents one strategy and has an associated value defined by the
    enum. The class itself does not implement behavior; it serves as a collection
    of named constants used by the extraction logic.


    Members:

    The enum''s members are defined in the source file. Each member has a unique name
    and value. The exact set of members may evolve as features are added, but all
    members share the Enum semantics.


    Usage:

    - You can enumerate or inspect the defined members at runtime via ExtractEntityStrategyType.__members__.keys().

    - To use a strategy, reference a concrete member in your code (for example, strategy
    == ExtractEntityStrategyType.SOME_MEMBER). Replace SOME_MEMBER with the actual
    member defined in your project.


    Notes:

    - __repr__ and __str__ representations follow standard Python Enum behavior unless
    overridden in the enum definition.

    - This docstring intentionally avoids asserting specific values or representations,
    since those depend on the actual member definitions in the codebase.'
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/query/context_builder/builders.py::BasicContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: BasicContextBuilder
  docstring: 'BasicContextBuilder is a concrete implementation of a context builder
    that constructs the minimal context required for the basic search mode by combining
    the user query with optional conversation history.


    Args:

    - None: The constructor takes no public parameters.


    Returns:

    - ContextBuilderResult: The result type produced when build_context is called,
    representing the assembled context for a basic search.


    Raises:

    - TypeError: If the provided inputs do not match expected types when building
    the context.

    - ValueError: If inputs are invalid (e.g., non-string query).


    Attributes:

    - No public attributes are declared for this class. Internal state, if any, is
    encapsulated.


    Summary:

    - This class participates in Graphrag''s query context builder system as the basic-mode
    context creator, supplying the minimal context necessary to perform basic search.'
  methods:
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
- class_id: graphrag/data_model/community_report.py::CommunityReport
  file: graphrag/data_model/community_report.py
  name: CommunityReport
  docstring: 'Dataclass-based model representing a community report, storing identifiers,
    metadata, and content for a specific community. The class inherits from Named
    and acts as a lightweight data container with a convenient from_dict constructor.


    Args:

    - id (str): The report''s identifier.

    - title (str): The report title.

    - community_id (str): The associated community''s id.

    - human_readable_id (Optional[str]): A short, human-friendly identifier.

    - summary (Optional[str]): A brief summary of the report.

    - full_content (Optional[str]): The full content of the report.

    - rank (Optional[int]): The report''s ranking or order.

    - attributes (Optional[dict[str, Any]]): Additional arbitrary attributes.

    - size (Optional[int]): A size-related metric.

    - period (Optional[str]): The time period the report covers.


    Returns:

    - CommunityReport: A new CommunityReport instance.


    Raises:

    - ValueError: If required fields are missing or have invalid types.


    From_dict:

    - cls (type): The class used for construction (typically CommunityReport).

    - d (dict[str, Any]): Source dictionary containing field values.

    - id_key (str): Key in d for the report''s identifier. Defaults to ''id''.

    - title_key (str): Key in d for the report title. Defaults to ''title''.

    - community_id_key (str): Key in d for the associated community''s id. Defaults
    to ''community''.

    - short_id_key (str): Key in d for the human-readable id. Defaults to ''human_readable_id''.

    - summary_key (str): Key in d for the summary. Defaults to ''summary''.

    - full_content_key (str): Key in d for the full content. Defaults to ''full_content''.

    - rank_key (str): Key in d for the rank. Defaults to ''rank''.

    - attributes_key (str): Key in d for additional attributes. Defaults to ''attributes''.

    - size_key (str): Key in d for the size. Defaults to ''size''.

    - period_key (str): Key in d for the period. Defaults to ''period''.


    Returns:

    - CommunityReport: A new instance constructed from d.


    Raises:

    - KeyError or ValueError: If required keys are missing or values are invalid.'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
      \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        summary_key: str = \"summary\",\n        full_content_key: str = \"\
      full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"CommunityReport\""
- class_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig
  file: graphrag/config/models/extract_claims_config.py
  name: ClaimExtractionConfig
  docstring: 'Configuration container for the claim extraction strategy used during
    claim extraction.


    This class stores the claim extraction strategy and exposes resolution logic to
    produce a concrete strategy at runtime via the resolved_strategy method.


    Attributes:

    - strategy: The configured claim extraction strategy. If provided, resolved_strategy
    returns it unchanged.


    Methods:

    - resolved_strategy(root_dir: str, model_config: LanguageModelConfig) -> dict:
    Get the resolved claim extraction strategy. Args: root_dir: The root directory
    used to resolve the graph and text prompt file paths. model_config: The LanguageModelConfig
    instance containing the model configuration; its model_dump() result is included
    in the strategy as llm. Returns: dict: The resolved strategy. If self.strategy
    is provided, it is returned as-is; otherwise, a dict representing the resolved
    strategy is constructed, including the model_config dump as llm.'
  methods:
  - name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
- class_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry
  file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  name: NativeRetry
  docstring: "NativeRetry provides retry logic for both asynchronous and synchronous\
    \ callables with a configurable maximum number of retries.\n\nArgs:\n    max_retries:\
    \ The maximum number of retry attempts. Must be greater than 0.\n    kwargs: Additional\
    \ keyword arguments accepted by the initializer.\n\nAttributes:\n    max_retries:\
    \ The maximum number of retry attempts.\n\nSummary:\n    The class is initialized\
    \ with max_retries (default 5) and exposes two methods:\n    aretry(func, **kwargs):\
    \ Retry an asynchronous function until it succeeds or the maximum number of retries\
    \ is reached. Returns the result of the awaited function.\n    retry(func, **kwargs):\
    \ Retry a synchronous function until it succeeds or the maximum number of retries\
    \ is reached. Returns the value returned by the function on success.\n\nNotes:\n\
    \    Both aretry and retry return the wrapped function's result (not None) when\
    \ successful. They retry on exceptions raised by the wrapped function. If the\
    \ maximum number of retries is exceeded, the last exception raised by the wrapped\
    \ function is propagated.\n\nReturns:\n    None\n\nRaises:\n    ValueError: max_retries\
    \ must be greater than 0.\n    Exception: If the wrapped function keeps raising\
    \ and the maximum number of retries is exceeded."
  methods:
  - name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
  - name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        **kwargs: Any,\n    )"
- class_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: GlobalContextBuilder
  docstring: "GlobalContextBuilder builds the context used for the global search mode.\n\
    \nPurpose:\nA specialized builder responsible for constructing the context used\
    \ when performing a global search. It considers the user query and may incorporate\
    \ optional conversation history and additional keyword arguments to assemble a\
    \ ContextBuilderResult that downstream components can use to execute or facilitate\
    \ the global search.\n\nAttributes:\n- No explicit instance attributes are defined\
    \ in the provided interface.\n\nMethods:\n- build_context(self, query: str, conversation_history:\
    \ ConversationHistory | None = None, **kwargs) -> ContextBuilderResult\n  Build\
    \ the context for the global search mode.\n\nArgs (for build_context):\n- query:\
    \ The user query to build context for.\n- conversation_history: Optional conversation\
    \ history to consider while constructing the context.\n- kwargs: Additional keyword\
    \ arguments that may influence how the context is built.\n\nReturns (for build_context):\n\
    - ContextBuilderResult: The result containing the built context for the global\
    \ search operation. The exact contents depend on downstream usage and typically\
    \ include the constructed context data and any necessary metadata.\n\nRaises:\n\
    - NotImplementedError: This class declares build_context as an abstract method.\
    \ Concrete subclasses must provide an implementation."
  methods:
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
- class_id: graphrag/prompt_tune/types.py::DocSelectionType
  file: graphrag/prompt_tune/types.py
  name: DocSelectionType
  docstring: "DocSelectionType is an enumeration of strategies for selecting documents\
    \ in the prompt tuning workflow.\n\nIt defines four strategies, each associated\
    \ with a string value:\n- ALL -> \"all\"\n- RANDOM -> \"random\"\n- TOP -> \"\
    top\"\n- AUTO -> \"auto\"\n\nAttributes:\n    ALL (str): The \"all\" selection\
    \ strategy.\n    RANDOM (str): The \"random\" selection strategy.\n    TOP (str):\
    \ The \"top\" selection strategy.\n    AUTO (str): The \"auto\" selection strategy.\n\
    \nMethods:\n    __str__(self) -> str:\n        Returns: str\n            The string\
    \ representation of the enum value."
  methods:
  - name: __str__
    signature: def __str__(self)
- class_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback
  file: graphrag/callbacks/llm_callbacks.py
  name: BaseLLMCallback
  docstring: 'BaseLLMCallback is a base interface for callbacks that respond to token
    generation events in an LLM pipeline.


    Purpose:

    Define the contract for handling LLM events, specifically when new tokens are
    produced. Subclasses should override on_llm_new_token(token) to implement custom
    behavior.


    Attributes:

    - None defined at the base level. Concrete implementations may add state as needed.


    Summary:

    This class provides a lightweight, extendable hook for observing or processing
    tokens emitted by the language model.


    Args:

    - token: str The new token generated by the LLM. This value is passed to on_llm_new_token
    when a token is produced.


    Returns:

    - None


    Raises:

    - None'
  methods:
  - name: on_llm_new_token
    signature: 'def on_llm_new_token(self, token: str)'
- class_id: graphrag/config/enums.py::ChunkStrategyType
  file: graphrag/config/enums.py
  name: ChunkStrategyType
  docstring: 'ChunkStrategyType is an Enum subclass that defines the available chunking
    strategies used by Graphrag configuration.


    This enum maps each strategy to a string value that appears in configuration and
    is intended to be used to configure chunking behavior.


    Enum members:

    - BASIC: basic

    - Standard: standard

    - Fast: fast

    - StandardUpdate: standard-update

    - FastUpdate: fast-update


    Accessing values:

    The literal string for a member is accessible via the value attribute. For example,
    ChunkStrategyType.BASIC.value yields basic.


    Inheritance:

    This class inherits from Enum; members are enumeration members rather than plain
    strings.


    Usage notes:

    Do not rely on the string representations of the members; use .value to obtain
    the underlying string used in configuration.'
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/data_model/entity.py::Entity
  file: graphrag/data_model/entity.py
  name: Entity
  docstring: 'Entity represents a graph entity with identifying information and metadata
    in the GraphRag data model.


    This class encapsulates identifiers, descriptive metadata, embeddings, and relationships
    to related data such as text units and communities. It can be constructed from
    a dictionary via the from_dict class method, which supports configurable keys
    for mapping.


    Key attributes:

    - id: The unique identifier for the entity.

    - human_readable_id: Optional short identifier for human-friendly display.

    - title: Title of the entity.

    - type: Classification type of the entity.

    - description: Textual description of the entity.

    - description_embedding: Embedding representation of the description.

    - name_embedding: Embedding representation of the name.

    - community: Community the entity belongs to.

    - text_unit_ids: Identifiers of related text units.

    - degree: Rank or degree of the entity.

    - attributes: Additional attributes associated with the entity.


    From dictionary construction:

    - from_dict(cls, d, id_key="id", short_id_key="human_readable_id", title_key="title",
    type_key="type", description_key="description", description_embedding_key="description_embedding",
    name_embedding_key="name_embedding", community_key="community", text_unit_ids_key="text_unit_ids",
    rank_key="degree", attributes_key="attributes") creates an Entity from the provided
    dictionary using the given key mappings and defaults.


    Returns: An Entity instance created from the input dictionary.


    Raises: Exceptions may be raised by from_dict if the input data are invalid or
    required fields are missing.'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      description_key: str = \"description\",\n        description_embedding_key:\
      \ str = \"description_embedding\",\n        name_embedding_key: str = \"name_embedding\"\
      ,\n        community_key: str = \"community\",\n        text_unit_ids_key: str\
      \ = \"text_unit_ids\",\n        rank_key: str = \"degree\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Entity\""
- class_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  file: graphrag/language_model/providers/fnllm/events.py
  name: FNLLMEvents
  docstring: "FNLLMEvents handles FNLLM-specific events and delegates error processing\
    \ to a provided error handler.\n\nArgs:\n    on_error: ErrorHandlerFn to be invoked\
    \ on errors.\n\nReturns:\n    None\n\nRaises:\n    Exception: If the configured\
    \ error handler raises an exception."
  methods:
  - name: __init__
    signature: 'def __init__(self, on_error: ErrorHandlerFn)'
  - name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
- class_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer
  file: graphrag/tokenizer/litellm_tokenizer.py
  name: LitellmTokenizer
  docstring: "LitellmTokenizer provides text-to-token and token-to-text conversions\
    \ using a Litellm model.\n\nIt wraps the Litellm encode and decode functions to\
    \ operate on a single model identified by model_name.\n\nArgs:\n    model_name\
    \ (str): The name of the Litellm model to use for tokenization.\n\nReturns:\n\
    \    None: This initializer does not return a value.\n\nRaises:\n    Exception:\
    \ If initialization fails due to an underlying error.\n\nAttributes:\n    model_name\
    \ (str): The name of the Litellm model used for tokenization."
  methods:
  - name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
  - name: __init__
    signature: 'def __init__(self, model_name: str) -> None'
  - name: encode
    signature: 'def encode(self, text: str) -> list[int]'
- class_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor
  file: graphrag/query/structured_search/drift_search/primer.py
  name: PrimerQueryProcessor
  docstring: "PrimerQueryProcessor expands a user query using a randomly selected\
    \ community report template and computes its embedding with the provided models.\
    \ It processes a single query per call.\n\nArgs:\n    chat_model (ChatModel):\
    \ The language model used to expand the query into an augmented form via a randomized\
    \ template.\n    text_embedder (EmbeddingModel): The embedding model used to compute\
    \ the dense vector for the expanded query.\n    reports (list[CommunityReport]):\
    \ A list of CommunityReport instances used as templates for expansion.\n    tokenizer\
    \ (Tokenizer | None): Optional tokenizer to count tokens during expansion and\
    \ embedding.\n\nReturns:\n    None\n\nRaises:\n    ValueError, TypeError, RuntimeError:\
    \ If inputs are invalid or underlying models fail.\n\nMethods:\n    __call__(self,\
    \ query: str) -> tuple[list[float], dict[str, int]]\n        Process a single\
    \ query by expanding it (via expand_query) and then computing its embedding.\n\
    \        Returns:\n            embedding (list[float]): The embedding vector produced\
    \ by text_embedder.\n            token_usage (dict[str, int]): Token usage details\
    \ from the expansion/embedding process (e.g., llm_calls, prompt_tokens, output_tokens).\n\
    \n    expand_query(self, query: str) -> tuple[str, dict[str, int]]\n        Expand\
    \ the input query using a randomly selected community report template.\n     \
    \   Returns:\n            expanded_query (str): The expanded query text.\n   \
    \         token_usage (dict[str, int]): Token usage details from the expansion\
    \ step (e.g., llm_calls, prompt_tokens, output_tokens)."
  methods:
  - name: __call__
    signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
  - name: expand_query
    signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
  - name: __init__
    signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n   \
      \     text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
      \        tokenizer: Tokenizer | None = None,\n    )"
- class_id: graphrag/index/typing/pipeline.py::Pipeline
  file: graphrag/index/typing/pipeline.py
  name: Pipeline
  docstring: "Pipeline stores and manages an ordered sequence of named Workflow objects,\
    \ exposing operations to inspect and mutate the pipeline.\n\nPurpose\n- Provide\
    \ a lightweight container for (name, Workflow) pairs that preserves order.\n-\
    \ Enable removal of all entries with a given name, iteration over the current\
    \ entries, and retrieval of names.\n\nAttributes\n- workflows: list[tuple[str,\
    \ Workflow]]\n  Internal storage of the pipeline as (name, Workflow) pairs.\n\n\
    Methods\n- __init__(self, workflows: list[tuple[str, Workflow]])\n  Initialize\
    \ the Pipeline with the provided (name, Workflow) pairs. Returns None.\n\n- remove(self,\
    \ name: str) -> None\n  Remove all workflows from the pipeline that have the given\
    \ name. This mutates the internal state\n  by removing every workflow whose first\
    \ element (the name) equals the provided value. All\n  matching entries are removed;\
    \ not just the first match.\n  Complexity: O(n) where n is the number of entries.\
    \ If no entries match, the pipeline remains unchanged.\n\n- run(self) -> Generator[tuple[str,\
    \ Workflow], None, None]\n  Yield a generator of (name, workflow) pairs from the\
    \ pipeline. The items yielded come from\n  self.workflows and are tuples of (name,\
    \ Workflow), i.e., each yield is a pair containing the\n  workflow's name (str)\
    \ and the corresponding Workflow object.\n\n- names(self) -> list[str]\n  Return\
    \ the names of the workflows in the pipeline, in the same order as stored in\n\
    \  self.workflows.\n\nNotes\n- Iteration reflects the current state of the pipeline;\
    \ modifications after obtaining a generator\n  may not affect items that have\
    \ already been yielded.\n- Edge cases: multiple entries with the same name are\
    \ all removed by remove."
  methods:
  - name: remove
    signature: 'def remove(self, name: str) -> None'
  - name: __init__
    signature: 'def __init__(self, workflows: list[Workflow])'
  - name: run
    signature: def run(self) -> Generator[Workflow]
  - name: names
    signature: def names(self) -> list[str]
- class_id: graphrag/config/errors.py::LanguageModelConfigMissingError
  file: graphrag/config/errors.py
  name: LanguageModelConfigMissingError
  docstring: "LanguageModelConfigMissingError is raised when a required language model\
    \ configuration key is missing from the configuration.\n\nArgs:\n    key: The\
    \ key of the missing model configuration. Used to customize the error message\
    \ in settings.yaml.\n\nReturns:\n    None\n\nRaises:\n    None\n\nAttributes:\n\
    \    key: The key of the missing model configuration. Stored on the instance to\
    \ provide context for the error message."
  methods:
  - name: __init__
    signature: 'def __init__(self, key: str = "") -> None'
- class_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen
  file: graphrag/query/question_gen/local_gen.py
  name: LocalQuestionGen
  docstring: "Generates questions using a local context builder and a language model.\n\
    \nThis class coordinates a LocalContextBuilder and a ChatModel to generate questions\
    \ based on a history of questions and optional context data. It provides asynchronous\
    \ and synchronous generation methods that return a QuestionResult, and it uses\
    \ a configurable system prompt.\n\nAttributes:\n    model: The language model\
    \ interface to use (ChatModel).\n    context_builder: The builder that constructs\
    \ context for local question generation (LocalContextBuilder).\n    tokenizer:\
    \ Optional tokenizer to use (Tokenizer | None).\n    system_prompt: System prompt\
    \ for question generation; defaults to QUESTION_SYSTEM_PROMPT (str).\n    callbacks:\
    \ Optional list of callbacks for model events (list[BaseLLMCallback] | None).\n\
    \    model_params: Optional parameters for the model (dict[str, Any] | None).\n\
    \    context_builder_params: Optional parameters for the context builder (dict[str,\
    \ Any] | None).\n\nArgs:\n    model: ChatModel - The language model interface\
    \ to use.\n    context_builder: LocalContextBuilder - The builder that constructs\
    \ the context for local question generation.\n    tokenizer: Tokenizer | None\
    \ - Optional tokenizer to use.\n    system_prompt: str - System prompt for question\
    \ generation. Defaults to QUESTION_SYSTEM_PROMPT.\n    callbacks: list[BaseLLMCallback]\
    \ | None - Optional callbacks for model events.\n    model_params: dict[str, Any]\
    \ | None - Optional parameters for the model.\n    context_builder_params: dict[str,\
    \ Any] | None - Optional parameters for the context builder.\n\nReturns:\n   \
    \ QuestionResult - The generated QuestionResult from agenerate or generate.\n\n\
    Raises:\n    Exception - If underlying components fail during generation."
  methods:
  - name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
  - name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
  - name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback]\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
- class_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry
  file: graphrag/language_model/providers/litellm/services/retry/retry.py
  name: Retry
  docstring: "Retry is an abstract base class that defines the interface for applying\
    \ a configurable retry policy to operations.\n\nPurpose\n  Provide a pluggable\
    \ retry mechanism by specifying concrete strategies for retrying both synchronous\
    \ and asynchronous callables.\n\nAttributes\n  This base class does not define\
    \ concrete state. Subclasses may store configuration (e.g., max_retries, backoff,\
    \ or jitter) in their own __init__.\n\nAbstract interface\n  Concrete subclasses\
    \ must implement:\n    - retry(func: Callable[..., Any], **kwargs: Any) -> Any\n\
    \      Retry a synchronous function and return the final result of the successful\
    \ invocation.\n    - aretry(func: Callable[..., Awaitable[Any]], **kwargs: Any)\
    \ -> Any\n      Retry an asynchronous function and return the final awaited result.\n\
    \nInitialization\n  __init__(self, /, **kwargs: Any)\n    Initialize a Retry subclass\
    \ with arbitrary keyword arguments used to configure the strategy.\n\nNotes\n\
    \  This class is abstract. Attempting to instantiate it directly or instantiate\
    \ a subclass that leaves abstract methods unimplemented will raise TypeError (not\
    \ NotImplementedError).\n\nArgs\n  kwargs: Arbitrary keyword arguments for subclass\
    \ initialization.\n\nRaises\n  TypeError: If attempting to instantiate this abstract\
    \ class or a subclass with unimplemented abstract methods."
  methods:
  - name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  - name: __init__
    signature: 'def __init__(self, /, **kwargs: Any)'
  - name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
- class_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection
  file: graphrag/query/context_builder/dynamic_community_selection.py
  name: DynamicCommunitySelection
  docstring: "DynamicCommunitySelection orchestrates dynamic selection of relevant\
    \ communities for a given query using a language model and relevancy scoring.\n\
    \nArgs:\n  community_reports (list[CommunityReport]): Reports for communities\
    \ to consider, mapped by community_id.\n  communities (list[Community]): Community\
    \ objects used to build the hierarchy and starting points.\n  model (ChatModel):\
    \ Language model instance used to rate relevancy of communities to the query.\n\
    \  tokenizer (Tokenizer): Tokenizer instance used to prepare text for prompting\
    \ and evaluation.\n  rate_query (str): Rate query string used to guide the relevancy\
    \ assessment. Default is RATE_QUERY.\n  use_summary (bool): If True, summaries\
    \ are incorporated into the evaluation context.\n  threshold (int): Threshold\
    \ determining the cutoff for considering a community relevant.\n  keep_parent\
    \ (bool): If True, keep parent relationships during hierarchical selection.\n\
    \  num_repeats (int): Number of times to repeat the evaluation steps.\n  max_level\
    \ (int): Maximum depth level to traverse in the hierarchy.\n  concurrent_coroutines\
    \ (int): Maximum number of concurrent coroutines for asynchronous processing.\n\
    \  model_params (dict[str, Any] | None): Optional dictionary of additional parameters\
    \ for the model.\n\nReturns:\n  tuple[list[CommunityReport], dict[str, Any]]:\
    \ A tuple containing:\n    - A list of CommunityReport objects representing the\
    \ relevant communities.\n    - A dictionary with additional information, including\
    \ llm usage metrics (llm_calls, prompt_tokens, output_tokens) and rating results.\n\
    \nRaises:\n  Exception: If an error occurs during selection processing.\n\nAttributes:\n\
    \  community_reports: The provided community reports used for consideration.\n\
    \  communities: The provided Community objects used for hierarchy and starting\
    \ points.\n  model: Language model used for prompting/rating.\n  tokenizer: Tokenizer\
    \ used for text processing.\n  rate_query: The rate query string.\n  use_summary:\
    \ Whether summaries are used in evaluation.\n  threshold: Selection threshold.\n\
    \  keep_parent: Whether to retain parent relationships.\n  num_repeats: Number\
    \ of repeats in evaluation.\n  max_level: Maximum hierarchy depth.\n  concurrent_coroutines:\
    \ Concurrency limit for asynchronous tasks.\n  model_params: Optional model configuration\
    \ parameters."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
      \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
      \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
      \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
      \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    )"
  - name: select
    signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
      Any]]'
- class_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: TextSplitter
  docstring: "Abstract base class for splitting input text into chunks for downstream\
    \ processing (e.g., embeddings).\n\nThe class provides shared configuration for\
    \ text chunking and defines the abstract interface split_text, which concrete\
    \ subclasses must implement to perform the actual splitting.\n\nArgs:\n    chunk_size\
    \ (int): Maximum length of a chunk as measured by length_function. This follows\
    \ the OpenAI embedding model's max input buffer length guidance. Default: 8191.\n\
    \    chunk_overlap (int): Overlap between consecutive chunks, measured using length_function.\
    \ Default: 100.\n    length_function (LengthFn): Function to compute the length\
    \ of a string. Default: len.\n    keep_separator (bool): Whether to keep the separator\
    \ between chunks. Default: False.\n    add_start_index (bool): Whether to prefix\
    \ each chunk with its start index. Default: False.\n    strip_whitespace (bool):\
    \ Whether to strip leading/trailing whitespace when forming chunks. Default: True.\n\
    \nAttributes:\n    chunk_size\n    chunk_overlap\n    length_function\n    keep_separator\n\
    \    add_start_index\n    strip_whitespace\n\nMethods:\n    split_text(text: str\
    \ | list[str]) -> Iterable[str]\n        Abstract method. Split text into chunks\
    \ according to the concrete implementation. Accepts a single string or a list\
    \ of strings as input and yields an iterable of text chunks.\n\nNotes:\n    This\
    \ is an abstract base class. split_text is not implemented here and must be provided\
    \ by subclasses."
  methods:
  - name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
  - name: __init__
    signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
      \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
      \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n      \
      \  length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
      \        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n\
      \    )"
- class_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor
  file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  name: CommunityReportsExtractor
  docstring: "CommunityReportsExtractor orchestrates generation of a markdown-formatted\
    \ community report from input text by invoking a chat model with a configurable\
    \ extraction prompt, producing both a structured report and a human-readable markdown\
    \ output.\n\nArgs:\n    model_invoker: ChatModel - The model invoker used to run\
    \ prompts.\n    extraction_prompt: str | None - Custom prompt to use for extraction.\
    \ If None, defaults to COMMUNITY_REPORT_PROMPT.\n    on_error: ErrorHandlerFn\
    \ | None - Function to handle errors. If None, a no-op is used.\n    max_report_length:\
    \ int | None - Maximum length for the generated report.\n\nReturns:\n    CommunityReportsResult\
    \ - The result containing:\n      structured_output: CommunityReportResponse |\
    \ None - The parsed structured report from the model.\n      output: str - The\
    \ human-readable markdown text report.\n\nRaises:\n    Exceptions raised by the\
    \ underlying model invocation or error handler may propagate.\n\nAttributes:\n\
    \    model_invoker: The model invoker used to run prompts (ChatModel).\n    extraction_prompt:\
    \ str | None - Custom prompt to use for extraction. If None, defaults to COMMUNITY_REPORT_PROMPT.\n\
    \    on_error: ErrorHandlerFn | None - Function to handle errors. If None, a no-op\
    \ is used.\n    max_report_length: int | None - Maximum report length."
  methods:
  - name: __call__
    signature: 'def __call__(self, input_text: str)'
  - name: _get_text_output
    signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
  - name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n        max_report_length: int | None = None,\n    )"
- class_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter
  file: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  name: StaticRateLimiter
  docstring: 'StaticRateLimiter enforces fixed-per-period limits on requests per minute
    (RPM) and tokens per minute (TPM) with an optional default stagger between requests.
    It tracks usage within a configurable period and resets counts at period boundaries.
    Limits can be disabled individually by setting rpm or tpm to None. The acquire()
    method is a context manager used to guard a block of code; pass token_count as
    the estimated number of tokens for the current request. The context manager yields
    None and blocks as needed to ensure the limits are not exceeded. If both RPM and
    TPM are disabled, acquire() yields immediately. A non-negative default_stagger
    is applied between requests when appropriate.


    Key attributes

    - rpm: RPM limit; positive integer or None to disable.

    - tpm: TPM limit; positive integer or None to disable.

    - default_stagger: Non-negative delay between requests, in seconds.

    - period_in_seconds: Length of the monitoring period in seconds; must be > 0.


    Constructor and usage notes

    - rpm and tpm must be None or a positive integer; invalid values raise ValueError.

    - default_stagger must be >= 0; invalid values raise ValueError.

    - period_in_seconds must be > 0; invalid values raise ValueError.

    - token_count should be a positive integer; misuse may raise ValueError.


    Thread-safety

    - The limiter is designed for multi-threaded use; internal state is synchronized
    to support concurrent acquire() calls.


    Behavior overview

    - Limits are enforced within each period window and reset when a new period begins.
    The limiter blocks (and optionally staggers) to ensure that usage never exceeds
    the configured RPM/TPM within a period.'
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        *,\n        rpm: int | None\
      \ = None,\n        tpm: int | None = None,\n        default_stagger: float =\
      \ 0.0,\n        period_in_seconds: int = 60,\n        **kwargs: Any,\n    )"
  - name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
- class_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion
  file: graphrag/language_model/providers/litellm/types.py
  name: FixedModelCompletion
  docstring: "FixedModelCompletion is a fixed-model chat completion provider backed\
    \ by Litellm. It exposes a callable interface that forwards a set of chat completion\
    \ parameters to the underlying service and returns either the model's final response\
    \ or a streaming wrapper when streaming is requested.\n\nPurpose\n- Provide a\
    \ lightweight, fixed-model completion integration built on Litellm for generating\
    \ chat completions against a predetermined model.\n- Offer a simple __call__ interface\
    \ that mirrors the underlying client while surfacing the fixed-model semantics.\n\
    \nKey behavior\n- Returns either a ModelResponse when streaming is not enabled,\
    \ or a CustomStreamWrapper when stream is True, allowing incremental consumption\
    \ of results.\n- Delegates all additional keyword arguments to the underlying\
    \ client via kwargs, enabling access to the full range of Litellm/OpenAI options.\n\
    \nArgs\n- messages: List of ChatCompletion messages to include in the request.\
    \ Defaults to an empty list.\n- stream: If True, stream partial responses as they\
    \ arrive. Defaults to None (non-streaming).\n- stream_options: Options for streaming.\n\
    - stop: Stop sequence or token.\n- max_completion_tokens: Maximum number of tokens\
    \ in the completion.\n- max_tokens: Maximum tokens for the response.\n- modalities:\
    \ Modality to use for the completion.\n- prediction: Prediction content parameter.\n\
    - audio: Audio parameter for audio-enabled modes.\n- logit_bias: Token bias mapping.\n\
    - user: User identifier for the request.\n- response_format: OpenAI v1.0+ response\
    \ format parameter; may be a dict or a model type.\n- seed: Random seed for deterministic\
    \ behavior.\n- tools: Tools to be used during the completion.\n- tool_choice:\
    \ Specific tool choice or mapping.\n- logprobs: Whether to return log probabilities.\n\
    - top_logprobs: Number of top logprobs to return.\n- parallel_tool_calls: Whether\
    \ to perform tool calls in parallel.\n- web_search_options: Web search options\
    \ for information retrieval.\n- deployment_id: Deployment identifier for the model.\n\
    - extra_headers: Additional headers to include in the request.\n- functions: OpenAI\
    \ function definitions (legacy).\n- function_call: Function call type or directive.\n\
    - thinking: AnthropicThinkingParam to influence model thinking behavior.\n- kwargs:\
    \ Additional keyword arguments passed through to the underlying client.\n\nReturns\n\
    - ModelResponse: The model's response when streaming is not enabled.\n- CustomStreamWrapper:\
    \ A streaming wrapper that yields results when streaming is enabled.\n\nRaises\n\
    - Propagates exceptions raised by the underlying Litellm/OpenAI clients (e.g.,\
    \ network errors, validation errors) to surface service failures.\n\nExamples\n\
    - Non-streaming use:\n  FixedModelCompletion(...)(messages=[...])\n- Streaming\
    \ use:\n  stream = FixedModelCompletion(...)(messages=[...], stream=True)\n  for\
    \ chunk in stream:\n      ..."
  methods:
  - name: __call__
    signature: "def __call__(\n        self,\n        *,\n        messages: list =\
      \ [],  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n \
      \       stream_options: dict | None = None,  # type: ignore\n        stop=None,\
      \  # type: ignore\n        max_completion_tokens: int | None = None,\n     \
      \   max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
      \ | None = None,\n        prediction: ChatCompletionPredictionContentParam |\
      \ None = None,\n        audio: ChatCompletionAudioParam | None = None,\n   \
      \     logit_bias: dict | None = None,  # type: ignore\n        user: str | None\
      \ = None,\n        # openai v1.0+ new params\n        response_format: dict\
      \ | type[BaseModel] | None = None,  # type: ignore\n        seed: int | None\
      \ = None,\n        tools: list | None = None,  # type: ignore\n        tool_choice:\
      \ str | dict | None = None,  # type: ignore\n        logprobs: bool | None =\
      \ None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
- class_id: graphrag/language_model/response/base.py::ModelOutput
  file: graphrag/language_model/response/base.py
  name: ModelOutput
  docstring: "ModelOutput encapsulates the textual content of a language model's output\
    \ along with the complete raw JSON response returned by the LLM provider.\n\n\
    Purpose:\n    Provide convenient access to both the human-readable content and\
    \ the full provider response for debugging and downstream processing.\n\nReturns:\n\
    \    content() -> str: The textual content of the output.\n    full_response()\
    \ -> dict[str, Any] | None: The complete JSON response returned by the model."
  methods:
  - name: content
    signature: def content(self) -> str
  - name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
- class_id: graphrag/data_model/covariate.py::Covariate
  file: graphrag/data_model/covariate.py
  name: Covariate
  docstring: 'Covariate model representing a covariate linked to a subject in the
    graph-based data model.


    This class encapsulates a covariate''s identity, its association with a subject,
    its covariate_type, a human_readable_id, related text_unit_ids, and additional
    attributes. The id attribute is inherited from the Identified base class.


    Key attributes

    - id: Unique identifier for the covariate (inherited from Identified)

    - subject_id: Identifier of the subject this covariate pertains to

    - covariate_type: Type or category of the covariate

    - human_readable_id: Short, human-readable identifier

    - text_unit_ids: Collection of text unit identifiers related to this covariate

    - attributes: Additional arbitrary attributes for the covariate


    Brief summary

    Describes a covariate and its relationships to subjects and text units, with a
    utility to construct instances from a dictionary via from_dict.


    From_dict

    This classmethod constructs a Covariate from a dictionary. It reads and uses the
    following keys by default: id ("id"), subject_id ("subject_id"), covariate_type
    ("covariate_type"), human_readable_id ("human_readable_id"), text_unit_ids ("text_unit_ids"),
    attributes ("attributes"). The keys can be customized via id_key, subject_id_key,
    covariate_type_key, short_id_key, text_unit_ids_key, and attributes_key.


    Args

    - cls (type Covariate): The Covariate class; this is a classmethod.

    - d (dict[str, Any]): The dictionary containing covariate fields.

    - id_key (str): The key in d that corresponds to the covariate id (default "id")

    - subject_id_key (str): The key in d for the subject identifier (default "subject_id")

    - covariate_type_key (str): The key in d for the covariate type (default "covariate_type")

    - short_id_key (str): The key in d for the human-readable id (default "human_readable_id")

    - text_unit_ids_key (str): The key in d for the text unit identifiers (default
    "text_unit_ids")

    - attributes_key (str): The key in d for the attributes dictionary (default "attributes")


    Returns

    - Covariate: An instance populated from the provided data


    Raises

    - KeyError: If required keys are missing from the input dictionary

    - TypeError: If any field has an inappropriate type'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        subject_id_key: str = \"subject_id\",\n   \
      \     covariate_type_key: str = \"covariate_type\",\n        short_id_key: str\
      \ = \"human_readable_id\",\n        text_unit_ids_key: str = \"text_unit_ids\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Covariate\""
- class_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: NoopTextSplitter
  docstring: "NoopTextSplitter is a no-op text splitter that returns the input text\
    \ unchanged as a sequence of strings. It is stateless and serves as a minimal\
    \ default splitter when no actual splitting is required.\n\nsplit_text(self, text:\
    \ str | list[str]) -> Iterable[str]:\n    Split text into chunks without modification.\
    \ If text is a string, returns a single-element list containing that string; if\
    \ text is a list of strings, returns that list as-is.\n\nArgs:\n    text: The\
    \ input text to split. A single string or a list of strings.\n\nReturns:\n   \
    \ Iterable[str]: The input text as an iterable of strings. Behavior is:\n    -\
    \ string input -> [string]\n    - list[str] input -> that same list\n\nRaises:\n\
    \    None"
  methods:
  - name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
- class_id: graphrag/config/enums.py::InputFileType
  file: graphrag/config/enums.py
  name: InputFileType
  docstring: "InputFileType is an enumeration of input file types used by the configuration.\n\
    \nThis Enum defines string-valued members that identify various input sources.\
    \ Each member maps to a string key defined elsewhere in the configuration.\n\n\
    Enum members:\n    LanceDB\n    AzureAISearch\n    CosmosDB\n    OpenAIEmbedding\n\
    \    AzureOpenAIEmbedding\n    Embedding\n    OpenAIChat\n    AzureOpenAIChat\n\
    \    Chat\n    MockChat\n    MockEmbedding\n    APIKey\n    AzureManagedIdentity\n\
    \    AsyncIO\n    Threaded\n    LOCAL\n    GLOBAL\n    DRIFT\n    BASIC\n    Standard\n\
    \    Fast\n    StandardUpdate\n    FastUpdate\n    RegexEnglish\n    Syntactic\n\
    \    CFG\n    Graph\n    LCC\n    WeightedComponents\n\nUsage:\n  - Access a member\
    \ directly: InputFileType.LanceDB\n  - Construct by value: InputFileType(value)\n\
    \nRaises:\n  ValueError: If value is not a valid member value."
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationRole
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationRole
  docstring: "ConversationRole is an enumeration that represents the role of a participant\
    \ in a conversation (system, user, or assistant). It defines three members: SYSTEM,\
    \ USER, and ASSISTANT, each with an underlying string value: \"system\", \"user\"\
    , and \"assistant\". The underlying string value is accessible via the .value\
    \ attribute. Note that __str__(self) may not return the underlying value by default;\
    \ depending on the implementation, str(role) can yield the enum member name (for\
    \ example, ConversationRole.SYSTEM). If you need the string form, use role.value.\n\
    \nFrom_string(value: str) converts a string to the corresponding ConversationRole.\
    \ Args: value: The role as a string. Expected values are \"system\", \"user\"\
    , or \"assistant\". Returns: The corresponding ConversationRole enum member. Raises:\
    \ ValueError if value is not one of the supported roles.\n\nAttributes:\n  SYSTEM:\
    \ The member representing the system role with value \"system\".\n  USER: The\
    \ member representing the user role with value \"user\".\n  ASSISTANT: The member\
    \ representing the assistant role with value \"assistant\".\n\nExamples:\n  role\
    \ = ConversationRole.from_string(\"system\")\n  assert role is ConversationRole.SYSTEM\n\
    \  print(role.value)  # \"system\""
  methods:
  - name: __str__
    signature: def __str__(self) -> str
  - name: from_string
    signature: 'def from_string(value: str) -> "ConversationRole"'
- class_id: graphrag/config/enums.py::CacheType
  file: graphrag/config/enums.py
  name: CacheType
  docstring: "\"CacheType\"  \nEnumeration of cache backends used by graphrag configuration.\n\
    \nPurpose:\nThis Enum provides a typed collection of string-valued identifiers\
    \ that correspond to the top-level configuration constants defined in graphrag.config.\
    \ Each member's value is the backend identifier string used in configuration.\
    \ The constants include LanceDB, AzureAISearch, CosmosDB, OpenAIEmbedding, AzureOpenAIEmbedding,\
    \ Embedding, OpenAIChat, AzureOpenAIChat, Chat, MockChat, MockEmbedding, APIKey,\
    \ AzureManagedIdentity, AsyncIO, Threaded, LOCAL, GLOBAL, DRIFT, BASIC, Standard,\
    \ Fast, StandardUpdate, FastUpdate, RegexEnglish, Syntactic, CFG, Graph, LCC,\
    \ WeightedComponents. The actual code uses AzureAISearch as the constant name\
    \ (not the spaced human-friendly form).\n\nAttributes:\n- Each member has a string\
    \ value that corresponds to the backend identifier used in configuration. Examples:\n\
    \  \"lancedb\" (LanceDB), \"azure_ai_search\" (AzureAISearch), \"cosmosdb\" (CosmosDB),\
    \ \"openai_embedding\" (OpenAIEmbedding), \"azure_openai_embedding\" (AzureOpenAIEmbedding),\
    \ \"embedding\" (Embedding), \"openai_chat\" (OpenAIChat), \"azure_openai_chat\"\
    \ (AzureOpenAIChat), \"chat\" (Chat), \"mock_chat\" (MockChat), \"mock_embedding\"\
    \ (MockEmbedding), \"api_key\" (APIKey), \"azure_managed_identity\" (AzureManagedIdentity),\
    \ \"asyncio\" (AsyncIO), \"threaded\" (Threaded), \"local\" (LOCAL), \"global\"\
    \ (GLOBAL), \"drift\" (DRIFT), \"basic\" (BASIC), \"standard\" (Standard), \"\
    fast\" (Fast), \"standard-update\" (StandardUpdate), \"fast-update\" (FastUpdate),\
    \ \"regex_english\" (RegexEnglish), \"syntactic_parser\" (Syntactic), \"cfg\"\
    \ (CFG), \"graph\" (Graph), \"lcc\" (LCC), \"weighted_components\" (WeightedComponents).\n\
    \nRepresentation:\n- __repr__(self) -> str: Returns a string representation of\
    \ the enumeration member. Following standard Enum semantics, this typically includes\
    \ the member name and its value."
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/config/enums.py::ReportingType
  file: graphrag/config/enums.py
  name: ReportingType
  docstring: "\"\"\"ReportingType is an enumeration of the available reporting configurations\
    \ used by the graphrag configuration system.\n\nSummary:\nProvides a type-safe\
    \ collection of reporter configuration options, each with a string value that\
    \ can be consumed by the codebase or serialized.\n\nAttributes:\n    value: The\
    \ string value associated with the enumeration member.\n\nMethods:\n    __repr__(self):\n\
    \        Return a string representation of the enumeration member. The current\
    \ implementation returns the member's value wrapped in double quotes (e.g., '\"\
    standard\"'), which is convenient for certain serialization scenarios but differs\
    \ from Python's standard Enum representation. For conventional display, consider\
    \ implementing __str__ or using the .value attribute.\n\nExamples:\n    from graphrag.config.enums\
    \ import ReportingType\n    t = ReportingType.Standard\n    s = t.value      \
    \     # \"standard\"\n    r = repr(t)           # '\"standard\"' (based on current\
    \ __repr__)\n\"\"\""
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/language_model/response/base.pyi::BaseModelOutput
  file: graphrag/language_model/response/base.pyi
  name: BaseModelOutput
  docstring: "BaseModelOutput stores the result produced by a language model, including\
    \ the main content and an optional full_response payload. Key attributes: content,\
    \ full_response.\n\nArgs:\n    content: The output content as a string.\n    full_response:\
    \ Optional dict[str, Any] representing the full response; defaults to None.\n\n\
    Returns:\n    None"
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
      \ dict[str, Any] | None = None,\n    ) -> None"
- class_id: graphrag/config/errors.py::ConflictingSettingsError
  file: graphrag/config/errors.py
  name: ConflictingSettingsError
  docstring: "Exception raised when configuration contains conflicting or incompatible\
    \ settings.\n\nRepresents an error condition raised during GraphRAG configuration\
    \ when two or more settings conflict with each other. The error message provided\
    \ at initialization describes the specific conflict and is passed to the base\
    \ ValueError constructor.\n\nArgs:\n    msg: The error message to pass to the\
    \ base ValueError constructor.\n\nReturns:\n    None\n\nRaises:\n    None"
  methods:
  - name: __init__
    signature: 'def __init__(self, msg: str) -> None'
- class_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: LocalSearchMixedContext
  docstring: "Local search context builder that mixes community data with local entity/relationship/covariate\
    \ context for structured searches.\n\nThis class aggregates data from community\
    \ reports, entity/relationship/covariate tables, and text units to create a comprehensive\
    \ context for local search prompts. It relies on a vector store of entity text\
    \ embeddings and a text embedding model to rank and assemble relevant content.\n\
    \nArgs:\n  entities: The list of entities to include.\n  entity_text_embeddings:\
    \ The vector store containing embeddings for entity text.\n  text_embedder: The\
    \ embedding model used to embed text for similarity search.\n  text_units: Optional\
    \ list of TextUnit.\n  community_reports: Optional list of CommunityReport.\n\
    \  relationships: Optional list of Relationship.\n  covariates: Optional dict[str,\
    \ list[Covariate]]; covariate data grouped by key.\n  tokenizer: Optional Tokenizer.\n\
    \  embedding_vectorstore_key: Key to select the embedding store (default EntityVectorStoreKey.ID).\n\
    \nReturns:\n  An initialized LocalSearchMixedContext instance.\n\nRaises:\n  Not\
    \ specified in the provided information."
  methods:
  - name: filter_by_entity_keys
    signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
  - name: __init__
    signature: "def __init__(\n        self,\n        entities: list[Entity],\n  \
      \      entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
      \        text_units: list[TextUnit] | None = None,\n        community_reports:\
      \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n    )"
  - name: _build_text_unit_context
    signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
      \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n \
      \       conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
      \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
      \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
      \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
      \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
      \ str = \"number of relationships\",\n        include_relationship_weight: bool\
      \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n      \
      \  return_candidate_context: bool = False,\n        use_community_summary: bool\
      \ = False,\n        min_community_rank: int = 0,\n        community_context_name:\
      \ str = \"Reports\",\n        column_delimiter: str = \"|\",\n        **kwargs:\
      \ dict[str, Any],\n    ) -> ContextBuilderResult"
  - name: _build_community_context
    signature: "def _build_community_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
      \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
      \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
      \ dict[str, pd.DataFrame]]"
  - name: _build_local_context
    signature: "def _build_local_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
      \ bool = False,\n        rank_description: str = \"relationship count\",\n \
      \       include_relationship_weight: bool = False,\n        top_k_relationships:\
      \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n     \
      \   return_candidate_context: bool = False,\n        column_delimiter: str =\
      \ \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
- class_id: unified-search-app/app/state/session_variables.py::SessionVariables
  file: unified-search-app/app/state/session_variables.py
  name: SessionVariables
  docstring: "SessionVariables stores and initializes per-session state for the unified\
    \ search application.\n\nPurpose:\n    Manage session-scoped attributes used to\
    \ track the user's search state across the app.\n\nAttributes:\n    dataset (QueryVariable):\
    \ The currently selected dataset, initialized as QueryVariable(\"dataset\", \"\
    \").\n    datasets (SessionVariable): The collection/state of datasets for the\
    \ session, initialized to an empty list.\n\nConstructor:\n    __init__(self)\n\
    \        Creates and initializes the session attributes to a consistent default\
    \ state. Initializes dataset and datasets with their defaults. Seeds an initial\
    \ set of suggested questions from default_suggested_questions when available.\n\
    \nReturns:\n    None\n\nRaises:\n    Propagates exceptions raised by the initialization\
    \ of the contained attributes (QueryVariable, SessionVariable)."
  methods:
  - name: __init__
    signature: def __init__(self)
- class_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: LocalDatasource
  docstring: "LocalDatasource provides access to Parquet data stored on the local\
    \ filesystem and loads Graph Rag configuration using the provided base_path as\
    \ the root of the data source.\n\nArgs:\n- base_path: The base directory path\
    \ for local data sources. Type: str.\n\nAttributes:\n- base_path (str): The base\
    \ directory path for local data sources.\n\nMethods:\n- read(table: str, throw_on_missing:\
    \ bool = False, columns: list[str] | None = None) -> pd.DataFrame\n  Read a parquet\
    \ file named \"<table>.parquet\" located under base_path.\n  - Parameters:\n \
    \   - table: The table name to read (without the .parquet extension).\n    - throw_on_missing:\
    \ If True, raise FileNotFoundError when the file does not exist.\n    - columns:\
    \ Optional list of column names to read from the parquet file; if None, all columns\
    \ are loaded.\n  - Returns:\n    - A pandas DataFrame containing the data from\
    \ the parquet file.\n  - Raises:\n    - FileNotFoundError: if the file is missing\
    \ and throw_on_missing is True.\n    - OSError / IOError: underlying I/O errors\
    \ may propagate.\n\n- __init__(base_path: str) -> None\n  Initialize the instance\
    \ with the provided base_path and store internal state for data access and settings\
    \ loading.\n\n- read_settings(file: str, throw_on_missing: bool = False) -> GraphRagConfig\
    \ | None\n  Read settings from the local source. Note: The file parameter is unused.\
    \ Settings are loaded by invoking load_config with a root_dir derived from base_path.\n\
    \  - Parameters:\n    - file: str. Unused; present for API compatibility.\n  \
    \  - throw_on_missing: bool. Ignored by this implementation.\n  - Returns:\n \
    \   - GraphRagConfig | None: The loaded configuration, or None if not found.\n\
    \  - Raises:\n    - Exceptions raised by load_config may propagate."
  methods:
  - name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  - name: __init__
    signature: 'def __init__(self, base_path: str)'
  - name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
- class_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig
  file: graphrag/config/models/extract_graph_config.py
  name: ExtractGraphConfig
  docstring: 'ExtractGraphConfig is a configuration container for graph extraction
    settings and the resolution of the final entity extraction strategy.


    Purpose:

    This class stores an optional strategy configuration and provides a method to
    resolve the active strategy using a root directory and a LanguageModelConfig.
    The resolved strategy is returned as a dict and may incorporate the LanguageModelConfig
    data via its model_dump() as llm.


    Attributes:

    strategy: Optional dict representing the base configuration for the extraction
    strategy. If provided, it participates in determining the final resolved strategy.


    Args:

    strategy: Optional dict. Base configuration for the extraction strategy used during
    resolution.


    Returns:

    dict: The final, resolved extraction strategy produced by resolving the configuration
    against the given root_dir and LanguageModelConfig. If a base strategy was provided,
    the result is derived from that configuration; otherwise a default strategy is
    constructed using graphrag_config_defaults and the provided inputs.


    Raises:

    ValueError: If root_dir is not a valid non-empty string or the model_config is
    invalid.

    TypeError: If model_config is not an instance of LanguageModelConfig.

    NotImplementedError: If strategy resolution is not implemented for the given inputs.


    Example:

    config = ExtractGraphConfig(strategy={''type'': ''entity'', ''params'': {}})

    resolved = config.resolved_strategy(''/path/to/root'', LanguageModelConfig(...))'
  methods:
  - name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
- class_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig
  file: graphrag/config/models/summarize_descriptions_config.py
  name: SummarizeDescriptionsConfig
  docstring: "Config model that controls how descriptions are summarized and resolves\
    \ the concrete summarization strategy at runtime.\n\nAttributes:\n  strategy:\
    \ Optional[SummarizeStrategyType]. Custom strategy to override the default description\
    \ summarization strategy. If provided, this strategy will be used during strategy\
    \ resolution by resolved_strategy(). Default: None.\n\nMethods:\n  resolved_strategy(self,\
    \ root_dir: str, model_config: LanguageModelConfig) -> Any\n    Get the resolved\
    \ description summarization strategy.\n    root_dir: The root directory used to\
    \ resolve graph and text prompt file paths.\n    model_config: The LanguageModelConfig\
    \ instance; its model_dump() result is included in the strategy under the key\
    \ llm.\n    Returns: The resolved strategy representation. The exact type depends\
    \ on the underlying components; it could be a dict or another strategy object.\
    \ If a custom strategy was provided via self.strategy, that strategy participates\
    \ in resolution and may appear in the final result.\n    Raises: Exceptions raised\
    \ by underlying components during strategy resolution may propagate to callers."
  methods:
  - name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
- class_id: unified-search-app/app/state/query_variable.py::QueryVariable
  file: unified-search-app/app/state/query_variable.py
  name: QueryVariable
  docstring: "Manages a single URL query parameter and its corresponding Streamlit\
    \ session_state entry.\n\nPurpose:\n  Maintain a two-way linkage between a URL\
    \ query parameter and a Streamlit session_state entry. It reads the initial value\
    \ from the URL query string when available; if the key is not present, it uses\
    \ the provided default. When reading from the query string, the value is normalized\
    \ to lowercase to support case-insensitive URLs.\n\nKey attributes:\n  key: The\
    \ session_state key associated with this variable (derived from the provided key\
    \ parameter).\n\nArgs:\n  key: The URL query parameter key and the session_state\
    \ key for this variable.\n  default: The default value to use if the key is not\
    \ present in the URL. Can be None.\n\nReturns:\n  None\n\nRaises:\n  None"
  methods:
  - name: __init__
    signature: 'def __init__(self, key: str, default: Any | None)'
  - name: key
    signature: def key(self) -> str
  - name: value
    signature: 'def value(self, value: Any) -> None'
- class_id: graphrag/config/enums.py::StorageType
  file: graphrag/config/enums.py
  name: StorageType
  docstring: "Enumeration of storage and configuration key types used by graphrag.\n\
    \nRepresents the various storage backends and related configuration keys that\
    \ the project may utilize. Each member corresponds to a string key used in the\
    \ configuration system.\n\nNote: The __repr__ method returns the member's value\
    \ wrapped in double quotes.\n\nArgs:\n    None: The StorageType Enum does not\
    \ accept initialization parameters; members are defined on the class.\n\nReturns:\n\
    \    StorageType: The enumeration type itself; members represent specific storage/config\
    \ keys.\n\nRaises:\n    None: This class does not raise exceptions during normal\
    \ usage."
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/query/context_builder/builders.py::LocalContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: LocalContextBuilder
  docstring: "Abstract base class for building the local-context used in local search\
    \ mode.\n\nThis builder defines the contract for assembling the user query and\
    \ optional conversation history into a ContextBuilderResult that downstream components\
    \ can use to perform a local (on-device) search.\n\nAttributes:\n    No explicit\
    \ data attributes are defined on this base class. Concrete implementations may\
    \ define configuration parameters, caches, or data sources as needed.\n\nArgs:\n\
    \    query (str): The user query to build context for.\n    conversation_history\
    \ (ConversationHistory | None): Optional conversation history to consider while\
    \ constructing the context.\n    **kwargs: Additional keyword arguments that may\
    \ influence how the context is built. Implementations may interpret or ignore\
    \ these as needed.\n\nReturns:\n    ContextBuilderResult: The result containing\
    \ the built context for downstream processing.\n\nRaises:\n    NotImplementedError:\
    \ If invoked on the abstract base class.\n    ValueError: If inputs are invalid\
    \ or inconsistent (implementation-specific)."
  methods:
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
- class_id: graphrag/data_model/community.py::Community
  file: graphrag/data_model/community.py
  name: Community
  docstring: 'Dataclass representing a Community in the graph data model, extending
    Named with community-specific metadata and relationships. It encapsulates identity,
    hierarchical placement, and associations to entities, relationships, text units,
    and covariates.


    Inherits from Named to provide consistent identity and naming semantics across
    the graph model. This class is a dataclass and is intended to be instantiated
    directly or via the from_dict factory method.


    Attributes:

    - id (str): Unique identifier of the community.

    - title (str): Human-readable title of the community.

    - human_readable_id (Optional[str]): Optional short identifier for human readability
    (default: None).

    - level (int): Hierarchical level of the community within the graph.

    - entity_ids (List[str]): Identifiers of entities belonging to the community.

    - relationship_ids (List[str]): Identifiers of relationships associated with the
    community.

    - text_unit_ids (List[str]): Identifiers of text units in the community.

    - covariate_ids (List[str]): Identifiers of covariates associated with the community.

    - parent (Optional[str]): Identifier of the parent community, if any (default:
    None).

    - children (List[str]): Identifiers of child communities.

    - attributes (Dict[str, Any]): Additional attributes describing the community
    (default: empty dict).

    - size (Optional[int]): Size metric for the community (default: None).

    - period (Optional[str]): Time period associated with the community (default:
    None).


    Factory methods:

    - from_dict: Classmethod that constructs a Community from a dictionary using configurable
    keys. It maps dict keys to the corresponding fields (with defaults for key names)
    and returns a Community instance populated from the provided data.'
  methods:
  - name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
      \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n      \
      \  entities_key: str = \"entity_ids\",\n        relationships_key: str = \"\
      relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n    \
      \    covariates_key: str = \"covariate_ids\",\n        parent_key: str = \"\
      parent\",\n        children_key: str = \"children\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"Community\""
- class_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc
  file: graphrag/language_model/providers/litellm/types.py
  name: AsyncLitellmRequestFunc
  docstring: "AsyncLitellmRequestFunc is a callable wrapper around an asynchronous\
    \ Litellm request function used for chat completions or embeddings. It forwards\
    \ all provided keyword arguments to the underlying request function, enabling\
    \ flexible use with different backends.\n\nArgs:\n    kwargs: Arbitrary keyword\
    \ arguments forwarded to the underlying request function. Specific accepted keys\
    \ depend on the concrete implementation.\n\nReturns:\n    The result produced\
    \ by the underlying request function.\n\nRaises:\n    Exception: Exceptions raised\
    \ by the underlying request function."
  methods:
  - name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
- class_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType
  file: graphrag/index/operations/summarize_descriptions/typing.py
  name: SummarizeStrategyType
  docstring: "Enumeration of strategies used to summarize descriptions within the\
    \ graphrag project.\n\nArgs:\n    There are no initialization parameters for this\
    \ enum.\n\nReturns:\n    SummarizeStrategyType: The enum member representing a\
    \ specific summarization strategy.\n\nRaises:\n    This class does not raise exceptions\
    \ during normal usage.\nNote:\n    The __repr__ method returns the enum member's\
    \ value enclosed in double quotes."
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType
  file: graphrag/index/operations/embed_text/embed_text.py
  name: TextEmbedStrategyType
  docstring: 'Enum describing the available text embedding strategies used by the
    embedding operation.


    Purpose:

    Represent the different strategies that can generate text embeddings, enabling
    the embedding workflow to select and apply the appropriate strategy implementation
    (for example, OpenAI-based or mock strategies as evidenced by related modules).


    Key attributes:

    Enum members correspond to specific embedding strategies used by the system, as
    seen in the openai and mock strategy modules.'
  methods:
  - name: __repr__
    signature: def __repr__(self)
- class_id: graphrag/language_model/response/base.pyi::BaseModelResponse
  file: graphrag/language_model/response/base.pyi
  name: BaseModelResponse
  docstring: "BaseModelResponse is a generic container for the response produced by\
    \ a base language model. It pairs the raw model output with optional parsed content\
    \ and related metadata, and is parameterized by the type _T of the parsed response.\
    \ The actual raw content is held in output (BaseModelOutput), while parsed_response\
    \ holds a typed interpretation if available.\n\nArgs:\n    output: BaseModelOutput\n\
    \        BaseModelOutput instance containing the content and full_response produced\
    \ by the base model.\n    parsed_response: _T | None\n        The parsed response\
    \ of type _T, or None if no parsing was performed.\n    history: list[Any]\n \
    \       History list related to the interaction; defaults are provided by the\
    \ Pydantic framework.\n    tool_calls: list[Any]\n        Tool calls associated\
    \ with the response; defaults are provided by the Pydantic framework.\n    metrics:\
    \ Any | None\n        Optional metrics about the response (e.g., latency, resource\
    \ usage).\n    cache_hit: bool | None\n        Indicates whether a cached response\
    \ was used, if applicable.\n\nAttributes:\n    output: BaseModelOutput\n     \
    \   Raw model output content and full_response.\n    parsed_response: _T | None\n\
    \        Parsed content of type _T, if available.\n    history: list[Any]\n  \
    \      Interaction history;\n    tool_calls: list[Any]\n        Tool calls associated\
    \ with the response.\n    metrics: Any | None\n        Optional metrics about\
    \ the response.\n    cache_hit: bool | None\n        Whether a cached response\
    \ was used.\n\nReturns:\n    None. The constructor is provided by the Pydantic\
    \ BaseModel superclass; this class does not implement a custom __init__.\n\nRaises:\n\
    \    None"
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n \
      \       parsed_response: _T | None = None,\n        history: list[Any] = ...,\
      \  # default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
      \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
      \ bool | None = None,\n    ) -> None"
- class_id: graphrag/config/errors.py::AzureApiVersionMissingError
  file: graphrag/config/errors.py
  name: AzureApiVersionMissingError
  docstring: "AzureApiVersionMissingError is a specialized ValueError indicating that\
    \ an API version is required for a given LLM type.\n\nThe constructor formats\
    \ the error message using the provided llm_type as: \"API Version is required\
    \ for {llm_type}. Please rerun graphrag init and set the api_version.\" It initializes\
    \ the base ValueError with that message.\n\nArgs:\n  llm_type: The LLM type for\
    \ which the API version is required.\n\nReturns:\n  None. The __init__ method\
    \ returns None and initializes the base ValueError with the constructed message.\n\
    \nRaises:\n  None"
  methods:
  - name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
- class_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding
  file: graphrag/language_model/providers/litellm/types.py
  name: FixedModelEmbedding
  docstring: "FixedModelEmbedding: Synchronous embedding function for a pre-configured\
    \ model.\n\nThis class provides an embedding interface that uses a pre-configured\
    \ model. It computes embeddings for a batch of inputs using the configured model\
    \ and does not require a model parameter. It mirrors litellm.embedding but omits\
    \ the model argument, relying on the model configuration.\n\nArgs:\n  request_id:\
    \ Optional request identifier.\n  input: List of inputs to embed.\n  dimensions:\
    \ Embedding dimensionality to request (optional).\n  encoding_format: Encoding\
    \ format to return (optional).\n  timeout: Timeout for the embedding request in\
    \ seconds (default to 600, i.e., 10 minutes).\n  api_base: API base URL override\
    \ (optional).\n  api_version: API version override (optional).\n  api_key: API\
    \ key override (optional).\n  api_type: API type override (optional).\n  caching:\
    \ Enable/disable caching.\n  user: User identifier (optional).\n  **kwargs: Additional\
    \ keyword arguments.\n\nReturns:\n  EmbeddingResponse: The embedding response\
    \ containing the computed embeddings.\n\nRaises:\n  Exception: If an error occurs\
    \ during embedding computation or API calls (propagates from underlying libraries)."
  methods:
  - name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
- class_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc
  file: graphrag/language_model/providers/litellm/types.py
  name: LitellmRequestFunc
  docstring: "Synchronous request function for Litellm that forwards calls to the\
    \ underlying request function, capable of handling either chat completion or embedding.\n\
    \nArgs:\n    kwargs: Arbitrary keyword arguments forwarded to the underlying request\
    \ function. Specific accepted keys depend on the concrete implementation.\n\n\
    Returns:\n    Any: The result of the underlying request function.\n\nRaises:\n\
    \    Exception: Exceptions raised by the underlying request function."
  methods:
  - name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
- class_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: TestRunChain
  docstring: 'TestRunChain is a unittest.TestCase subclass that validates the graph
    intelligence entity extraction workflow exercised by run_extract_graph. The tests
    focus on ensuring that, given a set of input Document objects and a mocked LLM,
    the resulting graph contains the expected entities and edges, and that entity/edge
    source_ids are consistently mapped across multiple documents. The class provides
    a high-level overview of the testing goal and the scope of what is being verified,
    without enumerating individual test methods.


    Purpose

    - Verify that the graph produced by run_extract_graph correctly captures entities
    and relationships for single- and multi-document inputs.

    - Ensure that source_id mappings for both entities and edges are propagated across
    documents as expected.


    Scope and responsibilities

    - Uses a controlled mock LLM to simulate responses and drive deterministic results.

    - Exercises the graph-building behavior of the Graph Intelligence strategy, not
    the LLM implementation itself.


    Key attributes

    - mock_llm: a mock language model used to supply deterministic responses during
    tests.

    - documents: input Document objects used to drive run_extract_graph.

    - expected_graph: abstracted expectations used to verify correctness of the produced
    graph.


    Usage

    - Part of the unit test suite, discovered and run via standard unittest discovery.

    - Requires no external resources beyond the provided mock LLM.


    Note

    - This class documents the overall testing goal and scope rather than listing
    individual test methods.'
  methods:
  - name: test_run_extract_graph_single_document_correct_entities_returned
    signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
  - name: test_run_extract_graph_multiple_documents_correct_entities_returned
    signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
      \        self,\n    )"
  - name: test_run_extract_graph_multiple_documents_correct_edges_returned
    signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
  - name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
      \        self,\n    )"
  - name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
      \        self,\n    )"
- class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: TestRunSentences
  docstring: 'TestRunSentences validates the sentence-splitting behavior of the chunk_text
    strategies used by the indexing module. The tests focus on observable outcomes:
    input documents are split into sentences, each sentence is emitted as a TextChunk
    with the exact sentence text, and every chunk carries a reference to its originating
    document via source_doc_indices. A progress reporter is exercised to indicate
    per-document processing progress without asserting a specific invocation count.
    Setup initializes required resources by calling bootstrap before tests run. The
    suite examines basic functionality with a single document, handling of multiple
    documents, and mixed/edge whitespace scenarios.'
  methods:
  - name: setup_method
    signature: def setup_method(self, method)
  - name: test_basic_functionality
    signature: def test_basic_functionality(self)
  - name: test_multiple_documents
    signature: def test_multiple_documents(self)
  - name: test_mixed_whitespace_handling
    signature: def test_mixed_whitespace_handling(self)
- class_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext
  file: graphrag/query/structured_search/global_search/community_context.py
  name: GlobalCommunityContext
  docstring: "GlobalCommunityContext is a global context builder for structured search\
    \ across multiple communities. It extends GlobalContextBuilder and coordinates\
    \ community reports, communities, optional entities, and tokenizer-driven text\
    \ processing to assemble a unified context used by the global search workflow.\
    \ It also supports optional dynamic community selection to tailor context content\
    \ to the query.\n\nArgs:\n  community_reports (list[CommunityReport]): Reports\
    \ for communities to consider.\n  communities (list[Community]): Community objects\
    \ used to build the hierarchy and starting points.\n  entities (list[Entity] |\
    \ None, optional): Optional list of Entity objects to include in the context.\n\
    \  tokenizer (Tokenizer | None, optional): Tokenizer to use; if None, a default\
    \ tokenizer is obtained via get_tokenizer.\n  dynamic_community_selection (bool):\
    \ Enable dynamic community selection.\n  dynamic_community_selection_kwargs (dict[str,\
    \ Any] | None, optional): Optional kwargs for dynamic selection.\n  random_state\
    \ (int): Random seed for reproducibility.\n\nAttributes:\n  community_reports:\
    \ The provided CommunityReport objects.\n  communities: The provided Community\
    \ objects.\n  entities: Optional list of Entity objects included in the context.\n\
    \  tokenizer: The Tokenizer instance in use.\n  dynamic_community_selection: Flag\
    \ indicating if dynamic selection is enabled.\n  dynamic_community_selection_kwargs:\
    \ Additional kwargs for dynamic selection.\n  random_state: Random seed for reproducibility.\n\
    \nInitialization:\n  This constructor initializes the instance with the provided\
    \ data and configuration. It does not return a value. If tokenizer is None, a\
    \ default tokenizer is obtained via get_tokenizer. If dynamic_community_selection\
    \ is True, dynamic selection will be configured using dynamic_community_selection_kwargs.\
    \ The class relies on the base GlobalContextBuilder for shared behavior and may\
    \ raise TypeError or ValueError for invalid inputs.\n\nSee Also:\n  GlobalContextBuilder\n\
    \nMethods:\n  build_context: Prepare batches of community report data as context\
    \ data for global search. Returns a ContextBuilderResult."
  methods:
  - name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        entities: list[Entity] | None\
      \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
      \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
      \ | None = None,\n        random_state: int = 86,\n    )"
  - name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        use_community_summary: bool =\
      \ True,\n        column_delimiter: str = \"|\",\n        shuffle_data: bool\
      \ = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
      \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
      \ bool = True,\n        community_weight_name: str = \"occurrence\",\n     \
      \   normalize_community_weight: bool = True,\n        max_context_tokens: int\
      \ = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
      \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n  \
      \      **kwargs: Any,\n    ) -> ContextBuilderResult"
- class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: TestRunTokens
  docstring: "Unit tests for the RunTokens tokenization strategy used to chunk text\
    \ into token-based chunks for indexing.\n\nArgs:\n  self: The test case instance.\n\
    \nReturns:\n  None. The test class contains test methods that perform assertions\
    \ and do not return a value.\n\nRaises:\n  AssertionError: If any test assertion\
    \ fails during execution."
  methods:
  - name: test_basic_functionality
    signature: def test_basic_functionality(self, mock_get_encoding)
  - name: test_non_string_input
    signature: def test_non_string_input(self, mock_get_encoding)
