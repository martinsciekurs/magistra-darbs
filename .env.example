# LLM Provider Configuration
# Options: openai, ollama, gemini, deepseek
LLM_PROVIDER=ollama
DEBUG=true
LOG_LEVEL=INFO # DEBUG, INFO, WARNING, ERROR

# LLM Output Configuration
# Maximum tokens for LLM output (set based on your model's capabilities)
# Recommended: 4096 for smaller models, 8192+ for larger models
# Higher values help prevent truncation for complex documentation generation
LLM_MAX_TOKENS=8192

# OpenAI Configuration
OPENAI_API_KEY=your-api-key-here
OPENAI_MODEL=gpt-5-nano
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# Google Gemini Configuration
# Option 1: API Key authentication
GOOGLE_API_KEY=your-api-key-here
# Option 2: Vertex AI with gcloud CLI (ADC) - leave GOOGLE_API_KEY empty
# GOOGLE_PROJECT_ID=your-gcp-project-id
# GOOGLE_LOCATION=us-central1
GOOGLE_MODEL=gemini-3-flash
GOOGLE_EMBEDDING_MODEL=text-embedding-004

# DeepSeek Configuration
# Note: DeepSeek doesn't provide embedding models. When using DeepSeek, either:
#   - Set OPENAI_API_KEY for embeddings, OR
#   - Set ENABLE_EMBEDDINGS=false
# Note: deepseek-reasoner (R1) doesn't support function calling, use deepseek-chat instead
DEEPSEEK_API_KEY=your-api-key-here
DEEPSEEK_MODEL=deepseek-chat
# Optional: Override base URL (defaults to https://api.deepseek.com)
# DEEPSEEK_BASE_URL=https://api.deepseek.com

# Langfuse Configuration (Optional - for LLM observability)
# Get your keys from https://cloud.langfuse.com or your self-hosted instance
LANGFUSE_PUBLIC_KEY=your-public-key-here
LANGFUSE_SECRET_KEY=your-secret-key-here
# Optional: Defaults to https://cloud.langfuse.com if not set
# LANGFUSE_HOST=https://cloud.langfuse.com

LANGFUSE_FLUSH_EVERY=10

# Embeddings Configuration
# Set to "false" to disable embeddings (faster, but no similar function suggestions)
ENABLE_EMBEDDINGS=true
# HITL (Human-In-The-Loop) Configuration
# Confidence threshold (1-100) for repository analysis
# If confidence score is below this threshold, user will be prompted for feedback
HITL_CONFIDENCE_THRESHOLD=50
# Enable TOC HITL: If true, user will be prompted to confirm/modify the proposed documentation TOC
# If false, LLM-generated TOC will be used automatically
ENABLE_TOC_HITL=false
# Reflection Loop Configuration
# Minimum confidence score to exit reflection loop early (default: 70)
# Should be >= HITL_CONFIDENCE_THRESHOLD
REFLECTION_CONFIDENCE_THRESHOLD=70
# Maximum reflection iterations before falling back to HITL (default: 3)
MAX_REFLECTION_ITERATIONS=3

# Code Example Generation Configuration
# Enable code example generation from tests or function code
ENABLE_CODE_EXAMPLES=true
# Maximum retries for code example generation (1 = no retry, 2 = one retry)
CODE_EXAMPLE_MAX_RETRIES=2
