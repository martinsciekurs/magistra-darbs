- node_id: unified-search-app/app/ui/sidebar.py::update_basic_rag
  file: unified-search-app/app/ui/sidebar.py
  name: update_basic_rag
  signature: 'def update_basic_rag(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Update basic rag state.\n\nArgs:\n    sv: SessionVariables\n       \
    \ The container of session variables; used to read and update the include_basic_rag\
    \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The function\
    \ does not return a value.\n\nRaises:\n    KeyError\n        If the expected key\
    \ sv.include_basic_rag.key is not found in st.session_state."
  code_example: 'from module import update_basic_rag

    import streamlit as st

    sv = type(''S'', (), {})()

    sv.include_basic_rag = type(''V'', (), {})()

    sv.include_basic_rag.key = ''include_basic_rag''

    sv.include_basic_rag.value = False

    st.session_state[''include_basic_rag''] = True

    update_basic_rag(sv)  # expected: True'
  example_source: inferred
  line_start: 28
  line_end: 30
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.get
  file: tests/integration/storage/test_factory.py
  name: get
  signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
    \ encoding: str | None = None\n        ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous method to retrieve the value for a given key from storage.\n\
    \nThis interface method is intended for storage backends to implement. The actual\
    \ return\nvalue is implementation-dependent. In the base implementation available\
    \ here, this call\nreturns None.\n\nArgs:\n    key (str): The key to retrieve.\n\
    \    as_bytes (bool | None): If True, return the value as bytes. If None, use\
    \ the backend's\n        default behavior. The exact semantics are backend-dependent.\n\
    \    encoding (str | None): Optional encoding to apply when returning the value\
    \ as a string.\n        Its interpretation is backend-dependent.\n\nReturns:\n\
    \    None: The value retrieval is not implemented in the base class and returns\
    \ None. Concrete\n    backends may return the stored value, or may raise an error\
    \ if the key is not found.\n\nRaises:\n    Exception: Backend-specific errors\
    \ may be raised for not-found conditions or other I/O errors.\n        Implementations\
    \ define their own exception types."
  code_example: null
  example_source: inferred
  line_start: 125
  line_end: 128
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.__init__
  file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        max_retry_wait: float,\n\
    \        max_retries: int = 5,\n        **kwargs: Any,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "\"\"\"Initialize a RandomWaitRetry instance with retry configuration.\n\
    \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
    \    max_retries: The maximum number of retry attempts (int). Must be greater\
    \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n    None\n\
    \nRaises:\n    ValueError: max_retries must be greater than 0.\n    ValueError:\
    \ max_retry_wait must be greater than 0.\n\"\"\""
  code_example: null
  example_source: inferred
  line_start: 21
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_database
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _delete_database
  signature: def _delete_database(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Delete the database if it exists.\n\nDeletes the database associated\
    \ with this storage object if it exists. If the database is deleted, the internal\
    \ container reference is cleared to reflect that there is no active container\
    \ available until a new database/container is created.\n\nReturns:\n    None:\
    \ This method does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError:\
    \ If the database to delete does not exist.\n\nSide effects:\n    self._container_client\
    \ is set to None.\n    self._database_client is updated with the value returned\
    \ by delete_database."
  code_example: null
  example_source: inferred
  line_start: 94
  line_end: 100
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.progress
  file: graphrag/callbacks/workflow_callbacks.py
  name: progress
  signature: 'def progress(self, progress: Progress) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Handle when progress occurs.\n\nArgs:\n    progress: Progress\
    \ object representing the current progress event.\n\nReturns:\n    None\n\"\"\""
  code_example: 'from module import progress, Progress, Handler

    h = Handler()

    ev = Progress(50, 100)

    h.progress(ev)  # handles event

    progress(h, ev)  # unbound call alternative'
  example_source: inferred
  line_start: 35
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/config/read_dotenv.py::read_dotenv
  file: graphrag/config/read_dotenv.py
  name: read_dotenv
  signature: 'def read_dotenv(root: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Read a .env file in the given root path.\n\nLoads environment variables\
    \ from a .env file located at the specified root into the process environment\
    \ (os.environ). Only variables that are not already defined in os.environ are\
    \ set, so existing environment variables are preserved.\n\nSide effects:\n   \
    \ - Mutates os.environ by adding new variables from the .env file without overwriting\
    \ existing ones.\n\nLogging:\n    - Logs an info message when a .env file is found\
    \ and loaded.\n    - Logs an info message if no .env file is found at the given\
    \ root.\n\nArgs:\n    root: str\n        Path to the root directory containing\
    \ a .env file to load.\n\nReturns:\n    None\n        This function does not return\
    \ a value.\n\nNotes:\n    - If the .env file contains invalid lines or is unreadable,\
    \ dotenv_values will skip invalid lines or\n      otherwise perform its internal\
    \ handling; this function does not perform additional error handling."
  code_example: 'from module import read_dotenv

    import os

    os.environ.setdefault("EXIST","1")

    root = "/workspace/app"

    read_dotenv(root)  # loads .env, preserves existing vars'
  example_source: inferred
  line_start: 15
  line_end: 25
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
  file: graphrag/language_model/providers/fnllm/utils.py
  name: _create_error_handler
  signature: 'def _create_error_handler(callbacks: WorkflowCallbacks) -> ErrorHandlerFn'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create an error handler for LLM invocation errors.\n\nInternal helper\
    \ that returns an ErrorHandlerFn which logs errors using the\nconfigured logger.\
    \ The returned handler does not invoke any callbacks.\n\nArgs:\n    callbacks:\
    \ WorkflowCallbacks\n        A container of optional callbacks. This parameter\
    \ is currently unused by\n        the error handler.\n\nReturns:\n    ErrorHandlerFn\n\
    \        A function that accepts error: BaseException | None, stack: str | None,\
    \ and\n        details: dict | None, and logs an error with the message \"Error\
    \ Invoking LLM\",\n        including the exception information and any extra context.\n\
    \nRaises:\n    None"
  code_example: "from module import _create_error_handler\nclass CW: pass\ncallbacks\
    \ = CW()\nhandler = _create_error_handler(callbacks)\nhandler(Exception(\"boom\"\
    ),\n        \"trace\", {\"k\":\"v\"})  # logs \"Error Invoking LLM\""
  example_source: inferred
  line_start: 40
  line_end: 54
  dependencies: []
  called_by:
  - graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.compute_score
  file: graphrag/query/structured_search/drift_search/action.py
  name: compute_score
  signature: 'def compute_score(self, scorer: Any)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the score for the action using the provided scorer.\n\nArgs:\n\
    \    scorer (Any): The scorer to compute the score.\n\nReturns:\n    None: The\
    \ method updates self.score with the value returned by scorer.compute_score(self.query,\
    \ self.answer); if the result is None, self.score is set to -inf."
  code_example: 'from scoring import Action, SimpleScorer

    a = Action(query="What is 2+2?", answer="4")

    sc = SimpleScorer()

    a.compute_score(sc)

    print(a.score)'
  example_source: inferred
  line_start: 101
  line_end: 111
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: _create_strongly_connected_graph
  signature: def _create_strongly_connected_graph(self, digraph=False)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create and return a test graph with a linear chain of edges and node\
    \ attributes.\n\nCreates an undirected Graph by default; if digraph is True, a\
    \ DiGraph is created. Adds nodes \"1\", \"2\", \"3\", \"4\" with node_name attributes\
    \ 1, 2, 3, 4, and adds edges (\"4\",\"5\") with degree=4, (\"3\",\"4\") with degree=3,\
    \ (\"2\",\"3\") with degree=2, and (\"1\",\"2\") with degree=1.\n\nArgs:\n   \
    \ self: The instance of the test class.\n    digraph: bool, whether to create\
    \ a directed graph (DiGraph) instead of an undirected Graph.\n\nReturns:\n   \
    \ graph: networkx.Graph or networkx.DiGraph, created according to the digraph\
    \ flag."
  code_example: null
  example_source: test
  line_start: 49
  line_end: 59
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_vector
  file: graphrag/vector_stores/azure_ai_search.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a vector-based similarity search.\n\nThis method constructs\
    \ a VectorizedQuery using the provided query_embedding and k, and issues a search\
    \ via the configured database connection. It returns a list of VectorStoreSearchResult\
    \ objects, each containing a VectorStoreDocument and the corresponding similarity\
    \ score.\n\nArgs:\n  query_embedding: list[float] - Embedding vector to search\
    \ with\n  k: int - Number of nearest neighbors to retrieve\n  kwargs: Any - Additional\
    \ keyword arguments (passed through)\n\nReturns:\n  list[VectorStoreSearchResult]\
    \ - List of matching documents with scores\n\nRaises:\n  Exception - If the underlying\
    \ search operation fails or the Azure client raises an error"
  code_example: null
  example_source: test
  line_start: 168
  line_end: 193
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/base.py::VectorStoreSearchResult
  called_by: []
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_start
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_map_response_start
  signature: 'def on_map_response_start(self, map_response_contexts: list[str]) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the start of a map operation.\n\nArgs:\n    map_response_contexts:\
    \ A list of strings representing contexts for the map response operation to begin\
    \ processing.\n\nReturns:\n    None"
  code_example: "from module import on_map_response_start\nclass X:\n    def on_map_response_start(self,\
    \ map_response_contexts):\n        return on_map_response_start(\n           \
    \ self, map_response_contexts\n        )\nx = X()\nx.on_map_response_start([\"\
    init\",\"step\"])  # begins processing"
  example_source: inferred
  line_start: 18
  line_end: 19
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/create_final_text_units.py::_covariates
  file: graphrag/index/workflows/create_final_text_units.py
  name: _covariates
  signature: 'def _covariates(df: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Compute covariate IDs for each text unit from the input DataFrame.\n\
    \nArgs:\n    df: Input DataFrame containing the columns \"id\" and \"text_unit_id\"\
    .\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\" and \"covariate_ids\"\
    ; for each text_unit_id, covariate_ids is the list of unique ids associated with\
    \ that text unit.\n\nRaises:\n    KeyError: If the required columns \"id\" or\
    \ \"text_unit_id\" are missing from df."
  code_example: "from mymodule import _covariates\nimport pandas as pd\ndf = pd.DataFrame({\"\
    id\":[1,2,3,4],\n                   \"text_unit_id\":[1,1,2,2]})\nres = _covariates(df)\n\
    print(res)  # Expect covariate_ids per text_unit_id\n# id 1 -> [1,2], id 2 ->\
    \ [3,4]"
  example_source: inferred
  line_start: 110
  line_end: 118
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_final_text_units.py::create_final_text_units
- node_id: graphrag/index/operations/layout_graph/layout_graph.py::_run_layout
  file: graphrag/index/operations/layout_graph/layout_graph.py
  name: _run_layout
  signature: "def _run_layout(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
    \ NodeEmbeddings,\n) -> GraphLayout"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Run the layout algorithm on a graph and return the resulting GraphLayout.\n\
    \nArgs:\n    graph: nx.Graph\n        The graph to layout.\n    enabled: bool\n\
    \        If True, use the UMAP-based layout; otherwise fall back to the Zero layout.\n\
    \    embeddings: NodeEmbeddings\n        Embeddings for each node in the graph.\n\
    \nReturns:\n    GraphLayout\n        The resulting layout as a GraphLayout."
  code_example: "from mod import _run_layout\nimport networkx as nx\nG = nx.path_graph(4)\n\
    embeddings = {\n    0: [0.1, 0.2],\n    1: [0.2, 0.3],\n    2: [0.3, 0.4],\n \
    \   3: [0.4, 0.5],\n}\nenabled = True\nlayout = _run_layout(G, enabled, embeddings)\n\
    # layout is the resulting GraphLayout"
  example_source: inferred
  line_start: 58
  line_end: 84
  dependencies: []
  called_by:
  - graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.action_token_ct
  file: graphrag/query/structured_search/drift_search/state.py
  name: action_token_ct
  signature: def action_token_ct(self) -> dict[str, int]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the token counts across all actions in the graph.\n\nArgs:\n\
    \    self: The instance containing a graph attribute; each node in the graph has\
    \ metadata with keys 'llm_calls', 'prompt_tokens', and 'output_tokens'.\nReturns:\n\
    \    dict[str, int]: A dictionary with keys 'llm_calls', 'prompt_tokens', and\
    \ 'output_tokens' mapping to the summed counts across all nodes."
  code_example: null
  example_source: inferred
  line_start: 139
  line_end: 150
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.__init__
  file: graphrag/query/structured_search/drift_search/state.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the drift query state with an empty graph.\n\nArgs:\n   \
    \ self: The instance being initialized.\n\nReturns:\n    None\n\nRaises:\n   \
    \ None"
  code_example: null
  example_source: inferred
  line_start: 21
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.clear
  file: graphrag/language_model/providers/fnllm/cache.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Clear the cache.\n\nClears all entries from the underlying cache managed\
    \ by this provider.\n\nReturns:\n    None\nRaises:\n    Exception: if the underlying\
    \ cache clear operation fails."
  code_example: 'from graphrag.cache.memory import MemoryCache

    c = MemoryCache()

    c.set("a",1)

    c.set("b",2)

    print("before", len(c))  # 2

    c.clear()

    print("after", len(c))   # 0'
  example_source: test
  line_start: 37
  line_end: 39
  dependencies: []
  called_by: []
- node_id: graphrag/config/load_config.py::_load_dotenv
  file: graphrag/config/load_config.py
  name: _load_dotenv
  signature: 'def _load_dotenv(config_path: Path | str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Load the .env file if it exists in the same directory as the config\
    \ file.\n\nArgs:\n    config_path (Path | str): The path to the config file.\n\
    \nReturns:\n    None"
  code_example: 'from module import _load_dotenv

    from pathlib import Path

    cfg = Path("/etc/app/config.yaml")

    _load_dotenv(cfg)

    # loads .env if present'
  example_source: inferred
  line_start: 70
  line_end: 81
  dependencies: []
  called_by:
  - graphrag/config/load_config.py::load_config
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_llm_new_token
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_llm_new_token
  signature: def on_llm_new_token(self, token)
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle when a new token is generated.\n\nThis is a no-op callback in\
    \ NoopQueryCallbacks; calling this method does not modify state, perform work,\
    \ or produce side effects.\n\nArgs:\n    token: str\n        The new token generated\
    \ by the LLM.\n\nReturns:\n    None"
  code_example: 'from module import on_llm_new_token

    import types

    class _Ctx: pass

    c = _Ctx()

    bound = types.MethodType(on_llm_new_token, c)

    bound("token_42")

    # Expect: no state change; function returns None'
  example_source: inferred
  line_start: 32
  line_end: 33
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response
  file: graphrag/query/structured_search/drift_search/search.py
  name: _reduce_response
  signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
    \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
    \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
    \    ) -> str"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Reduce the response to a single comprehensive response.\n\nParameters\n\
    ----------\nresponses : str|dict[str, Any]\n    The responses to reduce. If a\
    \ string, it is treated as a single response; if a\n    dict, the function extracts\
    \ the \"answer\" field from each node in responses[\"nodes\"].\nquery : str\n\
    \    The original query.\nllm_calls : dict[str, int]\n    Counter for LLM calls\
    \ performed during reduction. This dictionary is updated in\n    place; after\
    \ execution, llm_calls[\"reduce\"] will be set to 1.\nprompt_tokens : dict[str,\
    \ int]\n    Token counts for prompts used during reduction. This dictionary is\
    \ updated in\n    place; after execution, prompt_tokens[\"reduce\"] will equal\
    \ the total number of\n    tokens in the constructed search prompt plus the encoded\
    \ query.\noutput_tokens : dict[str, int]\n    Token counts for outputs produced\
    \ during reduction. This dictionary is updated in\n    place; after execution,\
    \ output_tokens[\"reduce\"] will equal the number of tokens in\n    the reduced\
    \ response.\nllm_kwargs : dict[str, Any]\n    Additional keyword arguments to\
    \ pass to the LLM (passed to model.achat via\n    model_parameters).\n\nReturns\n\
    -------\nstr\n    The reduced (consolidated) response."
  code_example: null
  example_source: inferred
  line_start: 342
  line_end: 400
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_text
  file: graphrag/vector_stores/lancedb.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Perform a similarity search using a given input text.\n\nArgs:\n\
    \    text: The input text to search for similar documents.\n    text_embedder:\
    \ TextEmbedder used to compute an embedding for the input text.\n    k: The number\
    \ of top results to return.\n    **kwargs: Additional keyword arguments.\n\nReturns:\n\
    \    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
    \ objects.\n\nRaises:\n    Exception: If text_embedder or the underlying similarity\
    \ search raise.\n\"\"\""
  code_example: null
  example_source: test
  line_start: 153
  line_end: 160
  dependencies:
  - graphrag/vector_stores/lancedb.py::similarity_search_by_vector
  called_by: []
- node_id: graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
  file: graphrag/index/operations/compute_edge_combined_degree.py
  name: _degree_colname
  signature: 'def _degree_colname(column: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return the degree column name derived from the given column.\n\nArgs:\n\
    \    column: The original column name.\n\nReturns:\n    The degree column name\
    \ as a string, formed by appending '_degree' to the input column name."
  code_example: "from module import _degree_colname\ncolumn = \"score\" \ndegree =\
    \ _degree_colname(column)  # -> \"score_degree\" \nprint(degree)"
  example_source: inferred
  line_start: 42
  line_end: 43
  dependencies: []
  called_by:
  - graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
  - graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel._get_kwargs
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: _get_kwargs
  signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get model arguments supported by litellm.\n\nArgs:\n    kwargs: Arbitrary\
    \ keyword arguments. Only those keys in [\"name\", \"dimensions\", \"encoding_format\"\
    , \"timeout\", \"user\"] will be included in the returned dictionary.\n\nReturns:\n\
    \    dict[str, Any]: A dictionary containing the subset of keyword arguments that\
    \ litellm supports."
  code_example: 'from module import _get_kwargs

    class Dummy: pass

    d = Dummy()

    res = _get_kwargs(d, name=''gpt-4'')

    print(res)  # expected: {''name'': ''gpt-4''}'
  example_source: inferred
  line_start: 192
  line_end: 201
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_prune_graph_configs
  file: tests/unit/config/utils.py
  name: assert_prune_graph_configs
  signature: "def assert_prune_graph_configs(\n    actual: PruneGraphConfig, expected:\
    \ PruneGraphConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asserts that actual and expected PruneGraphConfig have equal values\
    \ for the prune-related fields: min_node_freq, max_node_freq_std, min_node_degree,\
    \ max_node_degree_std, min_edge_weight_pct, remove_ego_nodes, and lcc_only.\n\n\
    Args:\n    actual: PruneGraphConfig\n        The actual prune graph configuration\
    \ to validate.\n    expected: PruneGraphConfig\n        The expected prune graph\
    \ configuration to compare against.\n\nReturns:\n    None\n        This function\
    \ does not return a value. It raises AssertionError if any of the\n        prune\
    \ graph configuration fields differ between actual and expected.\n\nRaises:\n\
    \    AssertionError\n        If any of the asserted fields differ between actual\
    \ and expected."
  code_example: null
  example_source: test
  line_start: 260
  line_end: 269
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/api/query.py::on_context
  file: graphrag/api/query.py
  name: on_context
  signature: 'def on_context(context: Any) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Store the given context in the enclosing scope's context_data.\n\nArgs:\n\
    \    context (Any): The context data to store in the enclosing scope.\n\nReturns:\n\
    \    None: The function updates context_data in the outer scope and returns no\
    \ value.\n\nRaises:\n    None: This function does not raise exceptions by itself."
  code_example: null
  example_source: inferred
  line_start: 1085
  line_end: 1087
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dataframes.py::union
  file: graphrag/index/utils/dataframes.py
  name: union
  signature: 'def union(*frames: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a union operation on the given set of dataframes.\n\nArgs:\n\
    \    frames: A variable number of pandas.DataFrame objects to union.\n\nReturns:\n\
    \    pd.DataFrame: The concatenated DataFrame containing the union of all input\
    \ frames.\n\nRaises:\n    ValueError: If no frames are provided."
  code_example: 'from module import union

    import pandas as pd

    valid_context_df = pd.DataFrame({"a":[1], "b":[3]})

    invalid_context_df = pd.DataFrame({"a":[5], "b":[6]})

    result = union(valid_context_df, invalid_context_df)

    print(result)

    # expected: two rows (1,3) and (5,6)'
  example_source: inferred
  line_start: 46
  line_end: 48
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
- node_id: graphrag/cli/index.py::handle_signal
  file: graphrag/cli/index.py
  name: handle_signal
  signature: def handle_signal(signum, _)
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle a system signal by cancelling all asyncio tasks and logging exit\
    \ messages.\n\nArgs:\n    signum: The signal number received.\n    _: The current\
    \ stack frame (unused).\n\nReturns:\n    None"
  code_example: 'from app.signals import handle_signal

    import signal, types

    frame = types.SimpleNamespace()

    handle_signal(signal.SIGINT, frame)  # simulate SIGINT

    # expected: logs and task cancellation'
  example_source: inferred
  line_start: 28
  line_end: 33
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockChatLLM.achat
  file: tests/mock_provider.py
  name: achat
  signature: "def achat(\n        self,\n        prompt: str,\n        history: list\
    \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the next response in the predefined list, cycling through available\
    \ responses using modulo arithmetic. If there are no configured responses, returns\
    \ an empty content response.\n\nArgs:\n    prompt: The input prompt to process.\n\
    \    history: Optional list of previous messages for context.\n    **kwargs: Additional\
    \ keyword arguments forwarded to the underlying chat handler.\n\nReturns:\n  \
    \  ModelResponse: The next response in the predefined sequence. If the next item\
    \ is a BaseModel, it will be used as the response payload. If the item is a plain\
    \ string, it will be wrapped in a response object (the wrapper BaseModelResponse)\
    \ containing that string as content. When no responses are configured, a BaseModelResponse\
    \ with empty content is returned.\n\nRaises:\n    Propagates exceptions raised\
    \ by the underlying chat logic or input validation."
  code_example: null
  example_source: test
  line_start: 36
  line_end: 43
  dependencies:
  - tests/mock_provider.py::chat
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.__init__
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: __init__
  signature: 'def __init__(self, database: str)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the BlobDatasource with the given database identifier.\n\n\
    Args:\n    database: The database identifier used to access the blob storage.\n\
    \nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: inferred
  line_start: 84
  line_end: 86
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/search.py::init_search_ui
  file: unified-search-app/app/ui/search.py
  name: init_search_ui
  signature: "def init_search_ui(\n    container: DeltaGenerator, search_type: SearchType,\
    \ title: str, caption: str\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Initialize search UI component in the specified container for the given\
    \ search type.\n\nArgs:\n    container: DeltaGenerator\n        The DeltaGenerator\
    \ container to render the UI into.\n    search_type: SearchType\n        The type\
    \ of search UI to configure.\n    title: str\n        The title text to display\
    \ in the UI.\n    caption: str\n        The caption text to display in the UI.\n\
    \nReturns:\n    None"
  code_example: "from module import init_search_ui, SearchType\nimport streamlit as\
    \ st\ncontainer = st.container()\ninit_search_ui(\n    container, SearchType.SIMPLE,\
    \ \"File search\",\n    \"Type keywords to find files\"\n)\n# Expected: a titled\
    \ UI with a caption and session state placeholders"
  example_source: inferred
  line_start: 16
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.load_spacy_model
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: load_spacy_model
  signature: "def load_spacy_model(\n        model_name: str, exclude: list[str] |\
    \ None = None\n    ) -> spacy.language.Language"
  decorators:
  - '@staticmethod'
  raises: []
  visibility: public
  docstring: "Load a SpaCy model.\n\nArgs:\n    model_name: Name of the SpaCy model\
    \ to load.\n    exclude: Optional list of components to exclude from loading.\n\
    \nReturns:\n    spacy.language.Language: The loaded SpaCy language object.\n\n\
    Raises:\n    OSError: If the model cannot be loaded (after attempting to download\
    \ if necessary)."
  code_example: 'from module import load_spacy_model

    model = "en_core_web_sm"

    exclude = ["tagger","parser"]

    nlp = load_spacy_model(model, exclude=exclude)

    doc = nlp("Hello world.")

    print(doc.text)  # expected: Hello world.'
  example_source: inferred
  line_start: 47
  line_end: 61
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::QATurn.get_answer_text
  file: graphrag/query/context_builder/conversation_history.py
  name: get_answer_text
  signature: def get_answer_text(self) -> str | None
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the concatenated text of the assistant answers.\n\nArgs:\n  \
    \  self: The QATurn instance containing assistant answers.\n\nReturns:\n    str\
    \ | None: The assistant answers contents joined by newline characters, or None\
    \ if there are no assistant answers."
  code_example: 'from module import QATurn

    qa = QATurn()

    ans = type(''A'', (), {''content'':''Hi''})()

    qa.assistant_answers = [ans]

    print(qa.get_answer_text())  # Output: Hi'
  example_source: inferred
  line_start: 72
  line_end: 78
  dependencies: []
  called_by: []
- node_id: graphrag/index/update/entities.py::_group_and_resolve_entities
  file: graphrag/index/update/entities.py
  name: _group_and_resolve_entities
  signature: "def _group_and_resolve_entities(\n    old_entities_df: pd.DataFrame,\
    \ delta_entities_df: pd.DataFrame\n) -> tuple[pd.DataFrame, dict]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Group old and delta entity data, resolve conflicts by title, and return\
    \ a merged entities dataframe along with a mapping from delta to existing entity\
    \ IDs.\n\nThis function merges the existing entities with a delta of new or updated\
    \ entities, constructs a mapping from delta entity IDs to existing entity IDs\
    \ for overlapping titles, and returns a resolved dataframe with a consistent column\
    \ order.\n\nParameters\n----------\nold_entities_df : pd.DataFrame\n    The existing\
    \ entities dataframe containing current entities.\ndelta_entities_df : pd.DataFrame\n\
    \    The delta dataframe containing new or updated entities to be merged.\n\n\
    Returns\n-------\ntuple[pd.DataFrame, dict]\n    A pair consisting of:\n    -\
    \ The resolved entities dataframe, with columns ordered according to ENTITIES_FINAL_COLUMNS.\n\
    \    - id_mapping: A mapping from delta (B) entity ids to existing (A) entity\
    \ ids, in the form {delta_id: existing_id}. The mapping only includes titles that\
    \ exist in both dataframes. If a delta id would produce duplicate keys in the\
    \ mapping (due to duplicate delta ids for the same title), a ValueError may be\
    \ raised because dict construction is done with strict=True.\n\nRaises\n------\n\
    ValueError\n    If id_mapping cannot be constructed due to duplicate delta ids\
    \ (id_B) which would produce duplicate keys when building the mapping (strict\
    \ key enforcement).\n\nNotes\n-----\n- For overlapping titles, id_mapping records\
    \ the mapping from the delta entity id (B) to the existing entity id (A).\n- human_readable_id\
    \ in delta_entities_df is incremented to continue from the maximum value present\
    \ in old_entities_df to ensure unique identifiers.\n- The old and delta entities\
    \ are concatenated and grouped by title to resolve conflicts; for each title,\
    \ the first occurrence of fields (id, type, human_readable_id, x, y) is kept,\
    \ while description is collected as a list of strings and text_unit_ids are flattened\
    \ into a single list.\n- Frequency is recomputed as the length of the text_unit_ids\
    \ list to reflect added text units.\n- The final dataframe is explicitly ordered\
    \ to ENTITIES_FINAL_COLUMNS for consistency."
  code_example: "from my_graph_module import _group_and_resolve_entities\n# old_entities_df\
    \ and delta_entities_df\n# are predefined with required columns\nmerged_df, mapping\
    \ = _group_and_resolve_entities(\n    old_entities_df, delta_entities_df)\n# merged_df\
    \ has ENTITIES_FINAL_COLUMNS; mapping maps\n# delta ids to existing ids"
  example_source: inferred
  line_start: 14
  line_end: 78
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
- node_id: graphrag/prompt_tune/generator/community_report_summarization.py::create_community_summarization_prompt
  file: graphrag/prompt_tune/generator/community_report_summarization.py
  name: create_community_summarization_prompt
  signature: "def create_community_summarization_prompt(\n    persona: str,\n    role:\
    \ str,\n    report_rating_description: str,\n    language: str,\n    output_path:\
    \ Path | None = None,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a prompt for community summarization. If output_path is provided,\
    \ write the prompt to a file.\n\nArgs:\n    persona (str): The persona to use\
    \ for the community summarization prompt.\n    role (str): The role to use for\
    \ the community summarization prompt.\n    report_rating_description (str): Description\
    \ of the report rating to incorporate into the prompt.\n    language (str): The\
    \ language to use for the community summarization prompt.\n    output_path (Path\
    \ | None): The path to write the prompt to. If None, the prompt is not written\
    \ to a file. Defaults to None.\nReturns:\n    str: The community summarization\
    \ prompt."
  code_example: "from module import create_community_summarization_prompt\nprompt\
    \ = create_community_summarization_prompt(\n    \"community advocate\",\n    \"\
    moderator\",\n    \"rating: 4.2/5; highlights\",\n    \"en\",\n    None,\n)  #\
    \ prompt ready"
  example_source: inferred
  line_start: 15
  line_end: 50
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._database_exists
  file: graphrag/vector_stores/cosmosdb.py
  name: _database_exists
  signature: def _database_exists(self) -> bool
  decorators: []
  raises: []
  visibility: protected
  docstring: "Check if the configured Cosmos DB database exists.\n\nReturns:\n   \
    \ bool: True if the database exists, False otherwise.\n\nRaises:\n    CosmosHttpResponseError:\
    \ If there is an HTTP error while listing databases."
  code_example: null
  example_source: test
  line_start: 77
  line_end: 82
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/entities.py::to_entity_dataframe
  file: graphrag/query/input/retrieval/entities.py
  name: to_entity_dataframe
  signature: "def to_entity_dataframe(\n    entities: list[Entity],\n    include_entity_rank:\
    \ bool = True,\n    rank_description: str = \"number of relationships\",\n) ->\
    \ pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a list of entities to a pandas DataFrame.\n\nArgs:\n    entities:\
    \ list[Entity]\n        The list of Entity objects to convert to a dataframe.\n\
    \    include_entity_rank: bool\n        If True, include a column for the entity\
    \ rank. The header for this column uses rank_description.\n    rank_description:\
    \ str\n        The header name for the rank column when include_entity_rank is\
    \ True.\n\nReturns:\n    pd.DataFrame\n        A dataframe with one row per entity.\
    \ Columns start with \"id\", \"entity\", \"description\", and, if include_entity_rank\
    \ is True, a rank column with the header given by rank_description. Additional\
    \ columns are derived from the keys of the first entity's attributes (excluding\
    \ any header names). Each row contains the corresponding values as strings where\
    \ possible; empty strings are used for missing values.\n\nRaises:\n    None"
  code_example: "from module import to_entity_dataframe\nfrom types import SimpleNamespace\n\
    e1 = SimpleNamespace(short_id=\"E1\", title=\"Entity One\",\n  description=\"\
    First ent.\", attributes={\"t\":\"A\"}, rank=5)\ne2 = SimpleNamespace(short_id=\"\
    E2\", title=\"Entity Two\",\n  description=\"Second ent.\", attributes={\"t\"\
    :\"B\"}, rank=2)\nentities = [e1, e2]\ndf = to_entity_dataframe(entities, include_entity_rank=True,\
    \ rank_description=\"rank\")\nprint(df)  # id, entity, desc, rank, attrs"
  example_source: inferred
  line_start: 57
  line_end: 92
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._abfs_url
  file: graphrag/storage/blob_pipeline_storage.py
  name: _abfs_url
  signature: 'def _abfs_url(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get the ABFS URL for the given key.\n\nArgs:\n    key: The key identifying\
    \ the blob within the container and path prefix.\n\nReturns:\n    str: The ABFS\
    \ URL for the given key, formatted as abfs://{path}."
  code_example: null
  example_source: inferred
  line_start: 295
  line_end: 298
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/generator/entity_summarization_prompt.py::create_entity_summarization_prompt
  file: graphrag/prompt_tune/generator/entity_summarization_prompt.py
  name: create_entity_summarization_prompt
  signature: "def create_entity_summarization_prompt(\n    persona: str,\n    language:\
    \ str,\n    output_path: Path | None = None,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nCreate a prompt for entity summarization.\n\nThe generated prompt\
    \ is created by formatting ENTITY_SUMMARIZATION_PROMPT with the provided persona\
    \ and language. If output_path is provided, the prompt is written to a file named\
    \ summarize_descriptions.txt within output_path, creating directories as needed.\n\
    \nArgs:\n    persona (str): The persona to use for the entity summarization prompt\n\
    \    language (str): The language to use for the entity summarization prompt\n\
    \    output_path (Path | None): The path to write the prompt to. Default is None.\n\
    \nReturns:\n    str: The generated prompt.\n\nRaises:\n    OSError: If the prompt\
    \ cannot be written to output_path.\n\"\"\""
  code_example: "from module import create_entity_summarization_prompt\nfrom pathlib\
    \ import Path\npersona = \"academic researcher\"; language = \"English\"\nout\
    \ = Path(\"/tmp/prompts\")\nprompt = create_entity_summarization_prompt(\n   \
    \ persona, language, out\n)\nprint(prompt)  # shows result; writes file to out"
  example_source: inferred
  line_start: 15
  line_end: 39
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__init__
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_name: str | None,\n    \
    \    exclude_nouns: list[str] | None = None,\n        max_word_length: int = 15,\n\
    \        word_delimiter: str = \" \",\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the base noun phrase extractor.\n\nArgs:\n    model_name:\
    \ The name of the SpaCy model to use, or None.\n    exclude_nouns: List of nouns\
    \ to exclude from extraction. If None, an empty list is used. Excluded nouns are\
    \ stored in uppercase.\n    max_word_length: Maximum length of a word to consider\
    \ when forming noun phrases.\n    word_delimiter: Delimiter used to join words\
    \ within a noun phrase.\n\nReturns:\n    None"
  code_example: null
  example_source: inferred
  line_start: 17
  line_end: 29
  dependencies: []
  called_by: []
- node_id: graphrag/index/run/run_pipeline.py::_dump_json
  file: graphrag/index/run/run_pipeline.py
  name: _dump_json
  signature: 'def _dump_json(context: PipelineRunContext) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Dump the stats and context state to the storage.\n\nArgs:\n    context:\
    \ PipelineRunContext\n        The pipeline run context containing stats, state,\
    \ and output storage used for persistence.\n\nReturns:\n    None\n        The\
    \ function completes without returning a value.\n\nRaises:\n    Exception\n  \
    \      If storage operations fail or JSON serialization fails."
  code_example: "from pipeline_run import _dump_json\nimport asyncio\nclass Stats:\n\
    \    ok = True\nclass Store:\n    async def set(self, k, v):\n        pass\nctx\
    \ = type(\"Ctx\", (), {})()\nctx.stats = Stats()\nctx.state = {\"s\":\"r\"}\n\
    ctx.output_storage = Store()\nasync def _run():\n    await _dump_json(ctx)\nasyncio.run(_run())"
  example_source: inferred
  line_start: 142
  line_end: 157
  dependencies: []
  called_by:
  - graphrag/index/run/run_pipeline.py::_run_pipeline
- node_id: tests/integration/storage/test_factory.py::test_register_class_directly_works
  file: tests/integration/storage/test_factory.py
  name: test_register_class_directly_works
  signature: def test_register_class_directly_works()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that StorageFactory allows direct class registration and can instantiate
    the registered class.


    This test registers a concrete CustomStorage class directly with StorageFactory,
    verifies it is registered and reported as supported, and creates an instance to
    ensure the registration path works.


    Args:

    - None: This test has no parameters.


    Returns:

    - None: This test does not return a value; it uses assertions to verify StorageFactory
    behavior.


    Notes:

    - Scope: direct class registration via StorageFactory for StorageFactory behavior
    verification.'
  code_example: 'from module import test_register_class_directly_works

    # Import and run the test to verify StorageFactory path

    test_register_class_directly_works()  # Run; should pass'
  example_source: inferred
  line_start: 104
  line_end: 160
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/prune_graph.py::_get_upper_threshold_by_std
  file: graphrag/index/operations/prune_graph.py
  name: _get_upper_threshold_by_std
  signature: "def _get_upper_threshold_by_std(\n    data: list[float] | list[int],\
    \ std_trim: float\n) -> float"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get upper threshold by standard deviation.\n\nArgs:\n    data: list[float]\
    \ | list[int], a list of numeric values used to compute the threshold.\n    std_trim:\
    \ float, multiplier for the standard deviation to offset the mean.\n\nReturns:\n\
    \    float: The upper threshold computed as mean + std_trim * std of the data."
  code_example: "from module import _get_upper_threshold_by_std\ndata = [1.0, 2.5,\
    \ 3.0, 4.2, 5.5]\nstd_trim = 1.0\nupper_threshold = _get_upper_threshold_by_std(\n\
    \    data, std_trim)  # e.g., ~4.77"
  example_source: inferred
  line_start: 86
  line_end: 92
  dependencies: []
  called_by:
  - graphrag/index/operations/prune_graph.py::prune_graph
- node_id: graphrag/query/input/loaders/utils.py::to_optional_float
  file: graphrag/query/input/loaders/utils.py
  name: to_optional_float
  signature: 'def to_optional_float(data: Mapping[str, Any], column_name: str | None)
    -> float | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a value to an optional float.\n\nIf the specified column is\
    \ missing or its value is None, returns None. Otherwise, converts the value to\
    \ a float using Python's built-in float().\n\nArgs:\n    data (Mapping[str, Any]):\
    \ Input data mapping containing potential value\n    column_name (str | None):\
    \ Key to retrieve from data; if None or not present, returns None\n\nReturns:\n\
    \    float | None: The value converted to a float, or None if the column is missing\
    \ or its value is None\n\nRaises:\n    ValueError: If the value cannot be converted\
    \ to a float (e.g., non-numeric strings).\n    TypeError: If the value type is\
    \ not compatible with float().\n    OverflowError: If the numeric value is too\
    \ large to convert.\n\nNotes:\n    - If column_name is None or not present in\
    \ data, returns None.\n    - If the value is None, returns None.\n    - Non-numeric\
    \ inputs that cannot be parsed as float will raise ValueError."
  code_example: 'from module import to_optional_float

    row = {"weight": "72.5", "rank": 3}

    weight_col, rank_col = "weight", "rank"

    weight = to_optional_float(row, weight_col)

    rank = to_optional_float(row, rank_col)

    # expected: weight 72.5, rank 3.0'
  example_source: inferred
  line_start: 126
  line_end: 135
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_community_reports
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_vector
  file: graphrag/vector_stores/base.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Perform ANN search by vector.\n\nArgs:\n    self: The instance of the\
    \ class.\n    query_embedding: list[float] The embedding vector to search with.\n\
    \    k: int The number of top results to return.\n    **kwargs: Any Additional\
    \ keyword arguments that may influence the search.\n\nReturns:\n    list[VectorStoreSearchResult]:\
    \ The top-k search results as VectorStoreSearchResult objects.\n\nRaises:\n  \
    \  NotImplementedError: If the method is not implemented by a subclass."
  code_example: null
  example_source: test
  line_start: 73
  line_end: 76
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.decode
  file: graphrag/tokenizer/tiktoken_tokenizer.py
  name: decode
  signature: 'def decode(self, tokens: list[int]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Decode a list of tokens back into a string.\n\nArgs:\n    tokens\
    \ (list[int]): A list of tokens to decode.\n\nReturns:\n    str: The decoded string\
    \ from the list of tokens.\n\nRaises:\n    Exception: If decoding fails due to\
    \ an underlying error in the encoding.\n\"\"\""
  code_example: "from graphrag.decoder import TokenDecoder\nd = TokenDecoder()\ntokens\
    \ = [\n    72,101,108,108,111,44,32,87,111,114,108,100\n]\ntext = d.decode(tokens)\
    \  # expected 'Hello, World'\nprint(text)  # Hello, World"
  example_source: test
  line_start: 36
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/config/create_graphrag_config.py::create_graphrag_config
  file: graphrag/config/create_graphrag_config.py
  name: create_graphrag_config
  signature: "def create_graphrag_config(\n    values: dict[str, Any] | None = None,\n\
    \    root_dir: str | None = None,\n) -> GraphRagConfig"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Load Configuration Parameters from a dictionary.\n\nArgs:\n  \
    \  values: dict[str, Any] | None\n        Dictionary of configuration values to\
    \ pass into pydantic model.\n    root_dir: str | None\n        Root directory\
    \ for the project.\n\nReturns:\n    GraphRagConfig\n        The configuration\
    \ object.\n\nRaises:\n    ValidationError\n        If the configuration values\
    \ do not satisfy pydantic validation.\n\"\"\""
  code_example: null
  example_source: test
  line_start: 12
  line_end: 43
  dependencies:
  - graphrag/config/models/graph_rag_config.py::GraphRagConfig
  called_by:
  - graphrag/config/load_config.py::load_config
  - tests/unit/config/test_config.py::test_missing_openai_required_api_key
  - tests/unit/config/test_config.py::test_missing_azure_api_key
  - tests/unit/config/test_config.py::test_conflicting_auth_type
  - tests/unit/config/test_config.py::test_conflicting_azure_api_key
  - tests/unit/config/test_config.py::test_missing_azure_api_base
  - tests/unit/config/test_config.py::test_missing_azure_api_version
  - tests/unit/config/test_config.py::test_default_config
  - tests/unit/indexing/test_init_content.py::test_init_yaml
  - tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_communities.py::test_create_communities
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
  - tests/verbs/test_extract_covariates.py::test_extract_covariates
  - tests/verbs/test_extract_graph.py::test_extract_graph
  - tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
  - tests/verbs/test_finalize_graph.py::test_finalize_graph
  - tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
  - tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
  - tests/verbs/test_pipeline_state.py::test_pipeline_state
  - tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
  - tests/verbs/test_prune_graph.py::test_prune_graph
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
- node_id: unified-search-app/app/ui/report_details.py::create_report_details_ui
  file: unified-search-app/app/ui/report_details.py
  name: create_report_details_ui
  signature: 'def create_report_details_ui(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Render the report details UI for the currently selected report using
    Streamlit; this function does not return a value.


    It loads the selected report JSON from sv.selected_report.value.full_content_json
    and renders the report title, summary, priority, and explanation, collecting citations
    for entities and relationships to highlight in the graph.


    If no report is selected, it writes No report selected to the UI.


    Notes:

    - JSONDecodeError is caught locally; in case of invalid JSON, error messages and
    the raw JSON content are written to the UI.

    - Missing keys in the loaded JSON may raise KeyError since the code directly accesses
    required fields such as title, summary, rating, rating explanation, and findings.

    - The function handles findings as a list or as a string; it gathers citations
    and renders hyperlinks accordingly.

    - The UI text is post-processed to replace internal tokens for display friendliness
    and then rendered via Markdown; finally, a graph citation visualization is shown
    for the selected entities and relationships.'
  code_example: 'from ui_components import create_report_details_ui

    from types import SimpleNamespace

    sv = SimpleNamespace()

    sv.selected_report = SimpleNamespace(value=None)

    sv.entities = SimpleNamespace(value=None)

    sv.relationships = SimpleNamespace(value=None)

    create_report_details_ui(sv)  # Expected: No report selected'
  example_source: inferred
  line_start: 18
  line_end: 98
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockChatLLM.__init__
  file: tests/mock_provider.py
  name: __init__
  signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
    \ | None = None,\n        config: LanguageModelConfig | None = None,\n       \
    \ json: bool = False,\n        **kwargs: Any,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a mock chat LLM provider with optional responses and configuration.\n\
    \nArgs:\n    responses: List[str | BaseModel] | None. A list of responses to return\
    \ in sequence. Each item can be a string or a BaseModel.\n    config: LanguageModelConfig\
    \ | None. Optional configuration. If provided and it has a.responses attribute,\
    \ those will be used instead of the responses argument.\n    json: bool. JSON\
    \ serialization option (present for compatibility; not used by this initializer).\n\
    \    kwargs: Any. Additional keyword arguments passed to the initializer.\n\n\
    Returns:\n    None. This constructor initializes internal state and does not return\
    \ a value.\n\nRaises:\n    None. This initializer does not raise exceptions by\
    \ itself."
  code_example: null
  example_source: inferred
  line_start: 23
  line_end: 34
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  called_by: []
- node_id: tests/conftest.py::pytest_addoption
  file: tests/conftest.py
  name: pytest_addoption
  signature: def pytest_addoption(parser)
  decorators: []
  raises: []
  visibility: public
  docstring: "Register the pytest option --run_slow to run slow tests.\n\nParameters:\n\
    \    parser: object\n        The pytest parser object used to register command-line\
    \ options.\n\nReturns:\n    None\n        The function does not return a value."
  code_example: null
  example_source: inferred
  line_start: 5
  line_end: 8
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.search
  file: graphrag/query/structured_search/local_search/search.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Builds a local search context that fits a single context window and\
    \ generates an answer for the user query.\n\nArgs:\n    query: The user query\
    \ to process.\n    conversation_history: Optional conversation history to incorporate\
    \ into the search context.\n    **kwargs: Additional keyword arguments passed\
    \ to the context builder and the model. May include drift_query to override the\
    \ query for drift.\n\nReturns:\n    SearchResult: The constructed search result\
    \ containing the response text, context data and text, completion time, and token\
    \ usage metadata.\n\nRaises:\n    None. All exceptions are caught within the method\
    \ and result in an empty response rather than propagating errors."
  code_example: 'from graphrag.core import GraphRag

    g = GraphRag()  # minimal setup

    r = g.search("Explain drift impact", None)

    print(r.text)  # expected: drift-informed answer'
  example_source: test
  line_start: 51
  line_end: 130
  dependencies:
  - graphrag/query/structured_search/base.py::SearchResult
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.__init__
  file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        max_retry_wait: float,\n\
    \        max_retries: int = 5,\n        **kwargs: Any,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize an Incremental Wait Retry instance with retry configuration.\n\
    \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
    \    max_retries: The maximum number of retry attempts (int). Must be greater\
    \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n    None\n\
    \nRaises:\n    ValueError: max_retries must be greater than 0.\n    ValueError:\
    \ max_retry_wait must be greater than 0."
  code_example: null
  example_source: inferred
  line_start: 20
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_start
  file: graphrag/callbacks/query_callbacks.py
  name: on_reduce_response_start
  signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
    \ str | dict[str, Any]\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
    \ Context for the reduce response (str | dict[str, Any]).\nReturns:\n    None:\
    \ The function does not return a value."
  code_example: 'from module import Reducer

    r = Reducer()

    ctx = {"op":"reduce","part":"first"}

    r.on_reduce_response_start(ctx)

    # expected: None; prepares reduce state'
  example_source: inferred
  line_start: 24
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::_reconstitute_embeddings
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: _reconstitute_embeddings
  signature: "def _reconstitute_embeddings(\n    raw_embeddings: list[list[float]],\
    \ sizes: list[int]\n) -> list[list[float] | None]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Reconstitute the embeddings into the original input texts.\n\nArgs:\n\
    \    raw_embeddings: list of embeddings, where each embedding is a list of floats\n\
    \    sizes: list of ints indicating the number of embeddings that belong to each\
    \ original input text\n\nReturns:\n    list of embeddings corresponding to each\
    \ input text. Each element is either:\n    - a list of floats representing the\
    \ embedding for that input, or\n    - None if the corresponding input text had\
    \ size 0\n    For entries with size > 1, the returned embedding is the normalized\
    \ average of the associated raw embeddings.\n\nRaises:\n    None"
  code_example: 'from module import _reconstitute_embeddings

    raw = [[0.2,0.8],[0.4,0.4],[0.6,0.2],[0.5,0.5]]

    sizes = [1,2,0,1]

    result = _reconstitute_embeddings(raw, sizes)

    # expected:

    # [[0.2,0.8],[0.8575,0.5145],None,[0.5,0.5]]'
  example_source: inferred
  line_start: 158
  line_end: 177
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: tests/integration/storage/test_factory.py::CustomStorage.get_creation_date
  file: tests/integration/storage/test_factory.py
  name: get_creation_date
  signature: 'def get_creation_date(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the creation date for the given key.\n\nArgs:\n    key (str):\
    \ The key for which to retrieve the creation date.\n\nReturns:\n    str: The creation\
    \ date as a string.\n\"\"\""
  code_example: "import graphrag.storage.blob_pipeline_storage as bp\nconn = \"UseDevelopmentStorage=true\"\
    \nstorage = bp.BlobPipelineStorage(\n    connection_string=conn, container_name=\"\
    demo\",\n    path_prefix=\".\"\n)\ncreation = storage.get_creation_date(\"input/christmas.txt\"\
    )\nprint(creation)  # e.g. 2024-12-01 12:34:56 +0000"
  example_source: test
  line_start: 148
  line_end: 149
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_text
  file: tests/integration/vector_stores/test_factory.py
  name: similarity_search_by_text
  signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
  decorators: []
  raises: []
  visibility: public
  docstring: "Performs a similarity search using the provided text.\n\nArgs:\n   \
    \ text (str): The query text to search.\n    text_embedder (Any): The embedder\
    \ used to convert the text into embeddings.\n    k (int): The number of results\
    \ to return. Defaults to 10.\n    **kwargs: Additional keyword arguments passed\
    \ to the underlying search implementation.\n\nReturns:\n    list: A list of search\
    \ results. In this implementation, returns an empty list.\n\nRaises:\n    None:\
    \ This function does not raise any exceptions."
  code_example: null
  example_source: test
  line_start: 140
  line_end: 141
  dependencies: []
  called_by: []
- node_id: graphrag/config/enums.py::ModelType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of the enumeration member.\n\nArgs:\n  \
    \  self (Enum): The enumeration member.\n\nReturns:\n    str: The member's value\
    \ wrapped in double quotes."
  code_example: null
  example_source: inferred
  line_start: 100
  line_end: 102
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_report_context_text
  file: graphrag/query/context_builder/community_context.py
  name: _report_context_text
  signature: "def _report_context_text(\n        report: CommunityReport, attributes:\
    \ list[str]\n    ) -> tuple[str, list[str]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Builds a single-line context text for a CommunityReport using the given
    attributes.


    This helper relies on global flags to determine content and formatting:

    - use_community_summary: if True, include report.summary; otherwise include report.full_content.

    - include_community_rank: if True, append the report.rank to the line.

    - column_delimiter: string used to join fields into the line.


    Args:

    - report (CommunityReport): The report to extract data from.

    - attributes (list[str]): Attribute field names to include from report.attributes
    (in order).


    Returns:

    - tuple[str, list[str]]: A pair where the first element is the single-line text
    (with a trailing newline) formed by joining the context fields with column_delimiter,
    and the second element is the raw list of context fields used to build that line.


    Notes:

    - report.short_id is included as "" when missing.

    - report.title is included as a string; if it can be None, behavior is undefined.

    - For each field in attributes, the value is str(report.attributes.get(field,
    "")) if report.attributes is not None; otherwise "".

    - If include_community_rank is True, report.rank is appended as a string.'
  code_example: 'from module import _report_context_text

    import module as mod

    r = type("R", (), {})()

    r.short_id = "C-123"; r.title = "Ex"

    r.attributes = {"a":"A"}; r.summary = "S"; r.rank = 5

    mod.use_community_summary = True

    mod.include_community_rank = True

    mod.column_delimiter = " | "

    txt, ctx = _report_context_text(r, ["a"])

    print(txt)

    print(ctx)'
  example_source: inferred
  line_start: 65
  line_end: 80
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: graphrag/index/workflows/create_final_text_units.py::_entities
  file: graphrag/index/workflows/create_final_text_units.py
  name: _entities
  signature: 'def _entities(df: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Compute mapping of text units to the entity IDs that reference\
    \ them.\n\nArgs:\n    df: pd.DataFrame containing the columns \"id\" and \"text_unit_ids\"\
    .\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\" and \"entity_ids\"\
    ; for each text_unit_id, entity_ids is the list of unique ids referencing that\
    \ text unit.\n\nRaises:\n    KeyError: If the required columns \"id\" or \"text_unit_ids\"\
    \ are missing from df.\n\"\"\""
  code_example: "from module import _entities\nimport pandas as pd\ndf = pd.DataFrame([[1,[10,11]],\n\
    \                   [2,[11]],\n                   [3,[12,10]]],\n            \
    \      columns=[\"id\",\"text_unit_ids\"])\nres = _entities(df)\nprint(res)  #\
    \ Expected: id=text_unit_ids, entity_ids"
  example_source: inferred
  line_start: 86
  line_end: 95
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_final_text_units.py::create_final_text_units
- node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.num_tokens
  file: graphrag/index/text_splitting/text_splitting.py
  name: num_tokens
  signature: 'def num_tokens(self, text: str) -> int'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the number of tokens in a string.\n\nArgs:\n    text: The input\
    \ string to count tokens in.\n\nReturns:\n    int: The number of tokens in text."
  code_example: 'from graphrag.get_tokenizer import get_tokenizer

    tok = get_tokenizer()

    text = "Scrooge recalls Ali Baba as a memory"

    print(tok.num_tokens(text))  # expected token count'
  example_source: test
  line_start: 95
  line_end: 97
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/base.py::BaseSearch.search
  file: graphrag/query/structured_search/base.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Asynchronously search the given query.\n\nThis abstract method must\
    \ be implemented by subclasses. It performs an asynchronous\nsearch given a query\
    \ string and optional conversation history, returning a SearchResult.\n\nArgs:\n\
    \    query (str): The search query to execute.\n    conversation_history (ConversationHistory\
    \ | None): Optional conversation history to consider during the search. If provided,\
    \ prior turns may influence results.\n    **kwargs: Additional keyword arguments\
    \ passed to the search implementation.\n\nReturns:\n    SearchResult: The result\
    \ of the asynchronous search operation.\n\nRaises:\n    NotImplementedError: Subclasses\
    \ must implement this method."
  code_example: "from graphrag.searcher import Search  # import interface\nengine\
    \ = SomeConcreteSearcher()  # provided by runtime\nimport asyncio\nasync def main():\n\
    \    r = await engine.search(\"weather today\", None)\n    print(r)\nasyncio.run(main())"
  example_source: test
  line_start: 73
  line_end: 81
  dependencies: []
  called_by: []
- node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_text
  file: graphrag/utils/api.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a text-based similarity search.\n\nThis method computes an embedding\
    \ for the input text using the provided text_embedder. If the resulting embedding\
    \ is truthy (i.e., not None or an empty result), it delegates to similarity_search_by_vector\
    \ with that embedding and the specified k. If the embedding is falsy, it returns\
    \ an empty list, indicating no results.\n\nArgs:\n    text (str): The input text\
    \ to search for similar documents.\n    text_embedder (TextEmbedder): A callable\
    \ that returns a list of floats representing the embedding of the input text.\n\
    \    k (int): The number of top results to return. Defaults to 10.\n    **kwargs:\
    \ Additional keyword arguments passed to downstream search methods.\n\nReturns:\n\
    \    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
    \ objects, sorted by score in descending order and truncated to k results.\n\n\
    Raises:\n    Propagates exceptions raised by the text_embedder or by the underlying\
    \ similarity_search_by_vector call.\n\nNotes:\n    - If text is None or empty,\
    \ text_embedder(text) may raise or return a falsy value, in which case this method\
    \ returns [].\n    - The internal flow is: compute the embedding via text_embedder,\
    \ then perform a vector search only if the embedding is truthy; otherwise, return\
    \ an empty list."
  code_example: null
  example_source: test
  line_start: 85
  line_end: 94
  dependencies:
  - graphrag/utils/api.py::similarity_search_by_vector
  called_by: []
- node_id: graphrag/index/operations/graph_to_dataframes.py::graph_to_dataframes
  file: graphrag/index/operations/graph_to_dataframes.py
  name: graph_to_dataframes
  signature: "def graph_to_dataframes(\n    graph: nx.Graph,\n    node_columns: list[str]\
    \ | None = None,\n    edge_columns: list[str] | None = None,\n    node_id: str\
    \ = \"title\",\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: 'Deconstructs an nx.Graph into two pandas DataFrames: one for nodes and
    one for edges.


    Args:

    - graph: input graph

    - node_columns: optional list of node attribute column names to include in the
    nodes DataFrame

    - edge_columns: optional list of edge attribute column names to include in the
    edges DataFrame

    - node_id: name of the column to store node identifiers in the nodes DataFrame
    (default "title")


    Returns:

    - tuple[pd.DataFrame, pd.DataFrame]: pair of DataFrames (nodes, edges). The nodes
    DataFrame includes a column named node_id containing node identifiers; if node_columns
    is provided, only those columns are included. The edges DataFrame contains an
    undirected edge representation with columns source and target, and any edge attributes;
    if edge_columns are provided, only those columns are included.


    Raises:

    - TypeError: If inputs are not of expected types (e.g., graph is not an nx.Graph)
    or required attributes are missing.'
  code_example: 'from module import graph_to_dataframes

    import networkx as nx

    g = nx.Graph()

    g.add_node(1, kind="A")

    g.add_node(2, kind="B")

    g.add_edge(1, 2, weight=3.14)

    ndf, edf = graph_to_dataframes(g)

    print(ndf.head(), edf.head())  # shows nodes and edges'
  example_source: inferred
  line_start: 10
  line_end: 38
  dependencies: []
  called_by:
  - graphrag/index/workflows/prune_graph.py::prune_graph
- node_id: graphrag/prompt_tune/generator/community_report_rating.py::generate_community_report_rating
  file: graphrag/prompt_tune/generator/community_report_rating.py
  name: generate_community_report_rating
  signature: "def generate_community_report_rating(\n    model: ChatModel, domain:\
    \ str, persona: str, docs: str | list[str]\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a community report rating description using a language model.\n\
    \nArgs:\n    model (ChatModel): The LLM to use for generation\n    domain (str):\
    \ The domain to generate a rating for\n    persona (str): The persona to generate\
    \ a rating for\n    docs (str | list[str]): Documents used to contextualize the\
    \ rating\n\nReturns:\n    str: The generated rating description prompt response.\n\
    \nRaises:\n    Exception: If the underlying chat model call fails."
  code_example: null
  example_source: inferred
  line_start: 12
  line_end: 35
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.clear
  file: graphrag/storage/memory_pipeline_storage.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously clear all entries from the storage.\n\nReturns:\n   \
    \ None"
  code_example: 'from graphrag.storage import Storage

    storage = Storage()

    storage.clear()  # clears all entries'
  example_source: test
  line_start: 68
  line_end: 70
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.keys
  file: graphrag/storage/blob_pipeline_storage.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Return the keys in the storage.\n\nArgs:\n    self: The instance of\
    \ the BlobPipelineStorage.\n\nReturns:\n    list[str]: The keys currently stored\
    \ in the storage.\n\nRaises:\n    NotImplementedError: Blob storage does yet not\
    \ support listing keys."
  code_example: "from graphrag.storage.blob_pipeline_storage import (\n    BlobPipelineStorage\n\
    )\nS = BlobPipelineStorage(\"default\",\"input\")\ntry: print(S.keys())  # []\n\
    except NotImplementedError:\n    print(\"not implemented\")"
  example_source: test
  line_start: 286
  line_end: 289
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_get_storage_types
  file: tests/integration/storage/test_factory.py
  name: test_get_storage_types
  signature: def test_get_storage_types()
  decorators: []
  raises: []
  visibility: public
  docstring: "Verify that StorageFactory.get_storage_types returns a collection containing\
    \ the values of the built-in storage types.\n\nThe test asserts that StorageType.file.value,\
    \ StorageType.memory.value, StorageType.blob.value, and StorageType.cosmosdb.value\
    \ are present in the returned collection.\n\nReturns:\n    None"
  code_example: "from storage_tests import test_get_storage_types\ntry:\n    test_get_storage_types()\n\
    \    print(\"OK: built-ins found\")  # success\nexcept AssertionError as e:\n\
    \    print(\"FAIL:\", e)  # expected if missing"
  example_source: inferred
  line_start: 90
  line_end: 96
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.encode
  file: graphrag/tokenizer/tokenizer.py
  name: encode
  signature: 'def encode(self, text: str) -> list[int]'
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "\"\"\"Encode the given text into a list of tokens.\n\nArgs:\n    text\
    \ (str): The input text to encode.\n\nReturns:\n    list[int]: A list of tokens\
    \ representing the encoded text.\n\nRaises:\n    NotImplementedError: The encode\
    \ method must be implemented by subclasses.\n\"\"\""
  code_example: "from graphrag.encoders import BaseTextEncoder\nclass SimpleEncoder(BaseTextEncoder):\n\
    \    def encode(self, text: str) -> list[int]:\n        return [ord(c) for c in\
    \ text]\nenc = SimpleEncoder()\nprint(enc.encode(\"This is a test.\"))  # returns\
    \ codes"
  example_source: test
  line_start: 13
  line_end: 25
  dependencies: []
  called_by: []
- node_id: graphrag/utils/storage.py::write_table_to_storage
  file: graphrag/utils/storage.py
  name: write_table_to_storage
  signature: "def write_table_to_storage(\n    table: pd.DataFrame, name: str, storage:\
    \ PipelineStorage\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Write a table to storage.\n\nArgs:\n  table: pd.DataFrame\n      The\
    \ DataFrame to write to storage.\n  name: str\n      Base name for the parquet\
    \ file to be stored as name.parquet.\n  storage: PipelineStorage\n      The storage\
    \ backend to which the parquet file will be written.\n\nReturns:\n  None\n\nRaises:\n\
    \  Exception: Exceptions raised by the storage backend during the write operation\
    \ may propagate."
  code_example: null
  example_source: test
  line_start: 30
  line_end: 34
  dependencies: []
  called_by:
  - graphrag/index/run/run_pipeline.py::run_pipeline
  - graphrag/index/run/run_pipeline.py::_copy_previous_output
  - graphrag/index/update/incremental_index.py::concat_dataframes
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/index/workflows/create_communities.py::run_workflow
  - graphrag/index/workflows/create_community_reports.py::run_workflow
  - graphrag/index/workflows/create_community_reports_text.py::run_workflow
  - graphrag/index/workflows/create_final_documents.py::run_workflow
  - graphrag/index/workflows/create_final_text_units.py::run_workflow
  - graphrag/index/workflows/extract_covariates.py::run_workflow
  - graphrag/index/workflows/extract_graph.py::run_workflow
  - graphrag/index/workflows/extract_graph_nlp.py::run_workflow
  - graphrag/index/workflows/finalize_graph.py::run_workflow
  - graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  - graphrag/index/workflows/load_input_documents.py::run_workflow
  - graphrag/index/workflows/load_update_documents.py::run_workflow
  - graphrag/index/workflows/prune_graph.py::run_workflow
  - graphrag/index/workflows/update_communities.py::_update_communities
  - graphrag/index/workflows/update_community_reports.py::_update_community_reports
  - graphrag/index/workflows/update_covariates.py::_update_covariates
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
  - graphrag/index/workflows/update_text_embeddings.py::run_workflow
  - graphrag/index/workflows/update_text_units.py::_update_text_units
  - tests/verbs/test_finalize_graph.py::_prep_tables
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::update_document_metadata
- node_id: graphrag/utils/api.py::load_search_prompt
  file: graphrag/utils/api.py
  name: load_search_prompt
  signature: 'def load_search_prompt(root_dir: str, prompt_config: str | None) ->
    str | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Load the search prompt from disk if configured.\n\nIf not, leave it\
    \ empty - the search functions will load their defaults.\n\nArgs:\n    root_dir:\
    \ Root directory path as a string where the prompt_config is resolved.\n    prompt_config:\
    \ Optional path to the prompt file, relative to root_dir. If provided, the function\
    \ will attempt to load the file if it exists.\n\nReturns:\n    The contents of\
    \ the prompt file decoded as UTF-8 if found, otherwise None.\n\nRaises:\n    OSError:\
    \ If a filesystem error occurs while reading the prompt file.\n    UnicodeDecodeError:\
    \ If the prompt file contents cannot be decoded as UTF-8."
  code_example: 'from module import load_search_prompt

    root_dir = "/project"

    map_path = "config/global/map_prompt.txt"

    prompt_text = load_search_prompt(root_dir, map_path)

    # returns the prompt text or None

    local_path = "config/local_search/prompt.txt"

    local_text = load_search_prompt(root_dir, local_path)

    # holds the prompt text or None'
  example_source: inferred
  line_start: 250
  line_end: 261
  dependencies: []
  called_by:
  - graphrag/api/query.py::global_search_streaming
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/api/query.py::basic_search_streaming
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.__init__
  file: graphrag/query/structured_search/global_search/search.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n       \
    \ map_system_prompt: str | None = None,\n        reduce_system_prompt: str | None\
    \ = None,\n        response_type: str = \"multiple paragraphs\",\n        allow_general_knowledge:\
    \ bool = False,\n        general_knowledge_inclusion_prompt: str | None = None,\n\
    \        json_mode: bool = True,\n        callbacks: list[QueryCallbacks] | None\
    \ = None,\n        max_data_tokens: int = 8000,\n        map_llm_params: dict[str,\
    \ Any] | None = None,\n        reduce_llm_params: dict[str, Any] | None = None,\n\
    \        map_max_length: int = 1000,\n        reduce_max_length: int = 2000,\n\
    \        context_builder_params: dict[str, Any] | None = None,\n        concurrent_coroutines:\
    \ int = 32,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a GlobalSearch instance (internal API).\n\nArgs:\n    model:\
    \ ChatModel - The language model interface used for this global search.\n    context_builder:\
    \ GlobalContextBuilder - The builder that constructs the context for the search.\n\
    \    tokenizer: Tokenizer | None - Optional tokenizer to use; if None, a default\
    \ tokenizer will be used.\n    map_system_prompt: str | None - System prompt for\
    \ the map stage; if None, MAP_SYSTEM_PROMPT is used.\n    reduce_system_prompt:\
    \ str | None - System prompt for the reduce stage; if None, REDUCE_SYSTEM_PROMPT\
    \ is used.\n    response_type: str - How to format the response, e.g., \"multiple\
    \ paragraphs\".\n    allow_general_knowledge: bool - Whether to allow incorporating\
    \ general knowledge beyond the provided context.\n    general_knowledge_inclusion_prompt:\
    \ str | None - Prompt guiding inclusion of general knowledge; if None, GENERAL_KNOWLEDGE_INSTRUCTION\
    \ is used.\n    json_mode: bool - Whether to request responses in JSON format.\n\
    \    callbacks: list[QueryCallbacks] | None - Optional callbacks to handle search\
    \ lifecycle events.\n    max_data_tokens: int - Maximum number of tokens allocated\
    \ for data in the mapping stage.\n    map_llm_params: dict[str, Any] | None -\
    \ Parameters for the map LLM call.\n    reduce_llm_params: dict[str, Any] | None\
    \ - Parameters for the reduce LLM call.\n    map_max_length: int - Maximum token\
    \ length for map responses.\n    reduce_max_length: int - Maximum token length\
    \ for reduce responses.\n    context_builder_params: dict[str, Any] | None - Additional\
    \ parameters forwarded to the context builder.\n    concurrent_coroutines: int\
    \ - Maximum number of concurrent coroutines running during mapping.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: inferred
  line_start: 51
  line_end: 97
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed_batch
  file: graphrag/language_model/providers/fnllm/models.py
  name: aembed_batch
  signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Embed the given texts using the Model.\n\nArgs:\n    text_list: The\
    \ texts to embed.\n    kwargs: Additional arguments to pass to the LLM.\n\nReturns:\n\
    \    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n    ValueError:\
    \ If no embeddings are found in the response."
  code_example: "import asyncio\nimport graphrag.language_model.manager as mm\nasync\
    \ def main():\n    provider = \"custom\"\n    model_name = \"custom_embedding\"\
    \n    m = mm.ModelManager()\n    llm = m.get_or_create_embedding_model(provider,\
    \ model_name)\n    res = await llm.aembed_batch([\"text\"])\n    print(res)\n\
    asyncio.run(main())"
  example_source: test
  line_start: 176
  line_end: 193
  dependencies: []
  called_by: []
- node_id: graphrag/config/errors.py::AzureApiBaseMissingError.__init__
  file: graphrag/config/errors.py
  name: __init__
  signature: 'def __init__(self, llm_type: str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Init method for AzureApiBaseMissingError (internal API).\n\nArgs:\n\
    \    llm_type: The LLM type for which the API Base is required.\n\nReturns:\n\
    \    None\n\nRaises:\n    None"
  code_example: null
  example_source: inferred
  line_start: 21
  line_end: 24
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.to_qa_turns
  file: graphrag/query/context_builder/conversation_history.py
  name: to_qa_turns
  signature: def to_qa_turns(self) -> list[QATurn]
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Convert conversation history to a list of QA turns.\n\nReturns:\n\
    \    list[QATurn]: A list of QA turns created from the conversation history, where\
    \ each QA turn contains a user_query from a USER turn and a list of assistant_answers\
    \ collected from subsequent turns until the next USER turn.\n\"\"\""
  code_example: 'from module import to_qa_turns, ConversationRole

    class T: pass

    t1=T(); t1.role=ConversationRole.USER; t1.content="Hi"

    t2=T(); t2.role=ConversationRole.ASSISTANT; t2.content="Hello"

    conv = type("C",(),{"turns":[t1,t2]})()

    qa = to_qa_turns(conv)

    # 1 QA turn: Hi -> Hello'
  example_source: inferred
  line_start: 123
  line_end: 137
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.__init__
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: __init__
  signature: 'def __init__(self, **kwargs: Any) -> None'
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize a CosmosDB storage instance.\n\nArgs:\n  cosmosdb_account_url:\
    \ The URL of the Cosmos DB account. Used to initialize CosmosClient when a connection\
    \ string is not provided.\n  connection_string: The Cosmos DB connection string.\
    \ Used to initialize CosmosClient when provided.\n  base_dir: The database name\
    \ to create/use.\n  container_name: The container name to create/use.\n  encoding:\
    \ Encoding to use for data (default \"utf-8\").\nReturns:\n  None\nRaises:\n \
    \ ValueError: If no base_dir is provided for database name or if neither connection_string\
    \ nor cosmosdb_account_url is provided."
  code_example: null
  example_source: inferred
  line_start: 42
  line_end: 86
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_create_container
  - graphrag/storage/cosmosdb_pipeline_storage.py::_create_database
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::ChatModel.achat
  file: graphrag/language_model/protocol/base.py
  name: achat
  signature: "def achat(\n        self, prompt: str, history: list | None = None,\
    \ **kwargs: Any\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given text.\n\nArgs:\n    prompt: The text\
    \ to generate a response for.\n    history: The conversation history.\n    **kwargs:\
    \ Additional keyword arguments (e.g., model parameters).\n\nReturns:\n    ModelResponse:\
    \ The response for the given text."
  code_example: "from graphrag.language_model.manager import ModelManager\nimport\
    \ asyncio\nasync def main():\n    mm = ModelManager()\n    model = mm.get_or_create_chat_model('custom','custom_chat')\n\
    \    resp = await model.achat('Summarize latest project update.')\n    print(resp.output.content)\n\
    asyncio.run(main())"
  example_source: test
  line_start: 97
  line_end: 113
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter_async
  file: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py
  name: _wrapped_with_rate_limiter_async
  signature: "def _wrapped_with_rate_limiter_async(\n        **kwargs: Any,\n    )\
    \ -> Any"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronous wrapper that applies rate limiting to a request function.\n\
    \nArgs:\n    kwargs: Any\n        Arbitrary keyword arguments forwarded to the\
    \ wrapped asynchronous function.\n        The wrapper computes the rate-limiting\
    \ token count from max_tokens plus\n        a token count derived from the provided\
    \ 'messages' or 'input' in kwargs.\n\nReturns:\n    Any\n        The result of\
    \ the wrapped asynchronous function after acquiring the rate\n        limiter.\n\
    \nRaises:\n    Propagates exceptions raised by the rate limiter or by the wrapped\
    \ asynchronous\n    function."
  code_example: "from module import _wrapped_with_rate_limiter_async\nimport asyncio\n\
    async def main():\n    messages = [{\"role\":\"user\",\"content\":\"Hello\"}]\n\
    \    result = await _wrapped_with_rate_limiter_async(\n        messages=messages\n\
    \    )\n    print(result)\nasyncio.run(main())"
  example_source: inferred
  line_start: 79
  line_end: 95
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::_get_value
  file: graphrag/query/input/loaders/utils.py
  name: _get_value
  signature: "def _get_value(\n    data: Mapping[str, Any], column_name: str | None,\
    \ required: bool = True\n) -> Any"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Retrieve a value for a column from the given data mapping.\n\nIf the\
    \ column is required (required=True), raises ValueError when:\n- column_name is\
    \ None, or\n- column_name is not in data.\n\nFor optional columns (required=False),\
    \ returns None when column_name is None or when the column is missing from data.\n\
    \nArgs:\n    data: Mapping[str, Any]\n        The mapping that contains column\
    \ values.\n    column_name: str | None\n        The name of the column to retrieve,\
    \ or None.\n    required: bool\n        Whether the column must be present in\
    \ data.\n\nReturns:\n    Any\n        The value associated with column_name in\
    \ data if present; otherwise None when\n        the column is optional and missing,\
    \ or when column_name is None and required is False.\n\nRaises:\n    ValueError\n\
    \        If column_name is None and required is True, or if column_name is not\
    \ in data and\n        required is True."
  code_example: 'from mymodule import _get_value

    data = {"id": 1, "name": "Alice"}

    print(_get_value(data, ''name'', required=True))  # Alice

    data2 = {"id": 1}

    print(_get_value(data2, ''age'', required=False))  # None

    # outputs: ''Alice'' and None'
  example_source: inferred
  line_start: 12
  line_end: 34
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/utils.py::to_str
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_list
  - graphrag/query/input/loaders/utils.py::to_int
  - graphrag/query/input/loaders/utils.py::to_float
  - graphrag/query/input/loaders/utils.py::to_dict
- node_id: graphrag/storage/factory.py::StorageFactory.is_supported_type
  file: graphrag/storage/factory.py
  name: is_supported_type
  signature: 'def is_supported_type(cls, storage_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Check if the given storage type is supported.\n\nArgs:\n    storage_type\
    \ (str): The type identifier for the storage.\n\nReturns:\n    bool: True if the\
    \ storage type is registered in the registry, False otherwise."
  code_example: 'from graphrag.cache.factory import CacheFactory

    result = CacheFactory.is_supported_type("memory")

    print(result)  # True'
  example_source: test
  line_start: 74
  line_end: 76
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__str__
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "String representation of the extractor, used for cache key generation.\n\
    \nArgs:\n    self: The instance of the extractor.\n\nReturns:\n    The string\
    \ representation used for cache key generation."
  code_example: null
  example_source: inferred
  line_start: 179
  line_end: 181
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.extract
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: extract
  signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Extract noun phrases from text using TextBlob's noun phrase extractor\
    \ with post-filtering.\n\nThis English-only extractor relies on TextBlob and NLTK\
    \ data and downloads required corpora on first use\n(brown, treebank, averaged_perceptron_tagger_eng)\
    \ and tokenizers (punkt, punkt_tab). It uses instance\nproperties from the base\
    \ extractor (exclude_nouns, max_word_length, word_delimiter) to filter and format\
    \ results.\n\nThe method collects noun phrases from TextBlob and applies filtering\
    \ based on:\n- presence of a proper noun within the phrase\n- number of cleaned\
    \ tokens\n- presence of a compound word\nand ensures all tokens are valid and\
    \ within max_word_length. The resulting phrases are normalized by removing\nexcluded\
    \ tokens, joining with the configured delimiter, converting to uppercase, and\
    \ deduplicating.\n\nArgs:\n    text: The input text to extract noun phrases from.\n\
    \nReturns:\n    List[str]: A list of cleaned noun phrases. Duplicates are removed;\
    \ the order is not guaranteed.\n\nRaises:\n    May propagate exceptions from TextBlob\
    \ or NLTK if required resources cannot be downloaded or loaded."
  code_example: 'from mymodule import NounExtractor

    ex = NounExtractor()

    text = ''A quick brown fox and a lazy dog.''

    res = ex.extract(text)

    print(res)  # expected: uppercase NP deduped'
  example_source: inferred
  line_start: 60
  line_end: 87
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::_tag_noun_phrases
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.connect
  file: graphrag/vector_stores/azure_ai_search.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> Any'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: 'Connect to AI search vector storage.


    Args:

    - url: The endpoint URL for the Azure AI Search service.

    - api_key: Optional API key for authentication. If provided, AzureKeyCredential
    is used; otherwise DefaultAzureCredential is used.

    - audience: Optional audience to pass to the client.

    - vector_search_profile_name: Optional name for the vector search profile. Defaults
    to "vectorSearchProfile".


    Returns:

    - None


    Raises:

    - ValueError: Azure AI Search expects url.'
  code_example: "from graphrag.vector_stores.azure_ai_search import (\n    AzureAISearchVectorStore\
    \ as A\n)\ncfg = {\"index_name\": \"test_vectors\", \"vector_size\": 5}\nstore\
    \ = A(vector_store_schema_config=cfg)\nu = \"https://test-url.search.windows.net\"\
    \nstore.connect(url=u, api_key=\"test_api_key\")\n# Expected: None"
  example_source: test
  line_start: 48
  line_end: 77
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.__init__
  file: graphrag/tokenizer/tiktoken_tokenizer.py
  name: __init__
  signature: 'def __init__(self, encoding_name: str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the Tiktoken Tokenizer.\n\nArgs:\n    encoding_name (str):\
    \ The name of the Tiktoken encoding to use for tokenization.\n\nReturns:\n   \
    \ None"
  code_example: null
  example_source: inferred
  line_start: 14
  line_end: 21
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/covariates.py::get_candidate_covariates
  file: graphrag/query/input/retrieval/covariates.py
  name: get_candidate_covariates
  signature: "def get_candidate_covariates(\n    selected_entities: list[Entity],\n\
    \    covariates: list[Covariate],\n) -> list[Covariate]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get all covariates that are related to selected entities.\n\nArgs:\n\
    \    selected_entities: The list of Entity objects representing the selected entities.\n\
    \    covariates: The list of Covariate objects to filter.\n\nReturns:\n    list[Covariate]:\
    \ Covariates related to the selected entities."
  code_example: 'from covariate_utils import get_candidate_covariates

    from types import SimpleNamespace

    sel = [SimpleNamespace(title="Entity A")]

    covA = SimpleNamespace(subject_id="Entity A")

    covC = SimpleNamespace(subject_id="Entity C")

    covs = [covA, covC]

    res = get_candidate_covariates(sel, covs)

    print(res)  # covariates for Entity A'
  example_source: inferred
  line_start: 14
  line_end: 24
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: graphrag/config/environment_reader.py::EnvironmentReader._read_env
  file: graphrag/config/environment_reader.py
  name: _read_env
  signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
    \ T, read: Callable[[str, T], T]\n    ) -> T | None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Read environment value(s) using a reader function and return the first\
    \ non-default result.\n\nArgs:\n    env_key: str | list[str]. Environment key\
    \ or keys to look up. If a single string is provided, it will be treated as a\
    \ one-element list. Keys are checked in order and converted to upper-case before\
    \ reading.\n    default_value: T. The default value to return if no key yields\
    \ a non-default result.\n    read: Callable[[str, T], T]. A function that takes\
    \ an environment key (uppercase) and a default value, and returns a value of type\
    \ T.\n\nReturns:\n    T | None. The value returned by read for the first key that\
    \ yields a value different from default_value; otherwise returns default_value.\n\
    \nRaises:\n    Any exception raised by the read callable is propagated to the\
    \ caller."
  code_example: "from module import _read_env\nenv = {\"FOO\": \"VALUE1\", \"BAR\"\
    : \"DEFAULT\"}\ndef read(k, d):\n    return env.get(k, d)\nres = _read_env(None,\
    \ [\"FOO\",\"BAR\"], \"DEFAULT\", read)\nprint(res)  # Expected VALUE1"
  example_source: inferred
  line_start: 41
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.clear
  file: graphrag/storage/file_pipeline_storage.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Clear all entries under the root directory.\n\nRemoves directories\
    \ recursively and unlinks files directly under the root directory, effectively\
    \ clearing the storage contents.\n\nArgs:\n    self: The FilePipelineStorage instance\
    \ to operate on.\n\nReturns:\n    None\n\nRaises:\n    OSError: If a filesystem\
    \ operation fails during removal of files or directories.\n\"\"\""
  code_example: "from graphrag.storage.file_pipeline_storage import (\n    FilePipelineStorage\n\
    )\nimport tempfile\nwith tempfile.TemporaryDirectory() as r:\n    FilePipelineStorage(r).clear()\n\
    \    # after clear, root is empty"
  example_source: test
  line_start: 140
  line_end: 146
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_base_dir
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_input_base_dir
  signature: def _validate_input_base_dir(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the input base directory.\n\nArgs:\n    self: The instance\
    \ of the configuration model containing input configuration and root_dir.\n\n\
    Returns:\n    None. This method updates input.storage.base_dir to an absolute\
    \ path derived from root_dir joined with the provided base_dir.\n\nRaises:\n \
    \   ValueError: If the input storage type is file and the input storage base_dir\
    \ is empty."
  code_example: 'from graphrag.config import Config

    import graphrag.defs as defs

    cfg = Config(); cfg.root_dir = ''/work''

    cfg.input = type(''I'',(),{})()

    cfg.input.storage = type(''S'',(),{})()

    cfg.input.storage.type = defs.StorageType.file

    cfg.input.storage.base_dir = ''data/in''

    cfg._validate_input_base_dir()  # base_dir -> /work/data/in'
  example_source: inferred
  line_start: 152
  line_end: 160
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.search
  file: graphrag/query/structured_search/global_search/search.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a global search.\n\nGlobal search mode includes two steps:\n\
    \n- Step 1: Run parallel LLM calls on communities' short summaries to generate\
    \ answer for each batch\n- Step 2: Combine the answers from step 2 to generate\
    \ the final answer\n\nArgs:\n    query: The search query.\n    conversation_history:\
    \ Optional conversation history to provide context for the search.\n    kwargs:\
    \ Additional keyword arguments for the search.\n\nReturns:\n    GlobalSearchResult:\
    \ The result of the global search."
  code_example: 'from graphrag.search import Graphrag

    g = Graphrag()

    res = g.search("search graphrag", conversation_history=None)

    print(res)  # GlobalSearchResult'
  example_source: test
  line_start: 135
  line_end: 207
  dependencies:
  - graphrag/query/structured_search/global_search/search.py::_map_response_single_batch
  - graphrag/query/structured_search/global_search/search.py::_reduce_response
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.extract
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: extract
  signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Extract noun phrases from text. Noun phrases may include named entities\
    \ and noun chunks, which are filtered based on some heuristics.\n\nArgs:\n   \
    \ text: Text.\n\nReturns:\n    List of noun phrases."
  code_example: 'from nlp_module import NPExtractor

    ex = NPExtractor()

    txt = "Apple Inc. announced a new iPhone in California."

    res = ex.extract(txt)

    print(res)  # [''Apple Inc.'', ''iPhone'', ''California'']'
  example_source: inferred
  line_start: 63
  line_end: 121
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::_tag_noun_phrases
  called_by: []
- node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._write_log
  file: graphrag/logger/blob_workflow_logger.py
  name: _write_log
  signature: 'def _write_log(self, log: dict[str, Any])'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Write log data to blob storage.\n\nThis method appends the provided\
    \ log data as a JSON-formatted line to the blob, and reinitializes the internal\
    \ client when the accumulated block count reaches the configured maximum.\n\n\
    Args:\n    log: dict[str, Any] Log data to serialize as JSON and append as a line\
    \ in the blob.\n\nReturns:\n    None\n\nRaises:\n    OSError: If an I/O error\
    \ occurs during blob operations or during client reinitialization.\n    ValueError:\
    \ If the log data cannot be serialized to JSON."
  code_example: 'from module import BlobLogger

    conn = "DefaultEndpointsProtocol=https;AccountName=a"

    cont = "logs"

    url = "https://acct.blob.core.windows.net"

    logger = BlobLogger(conn, cont, url)

    log = {"level":"INFO","msg":"start"}

    logger._write_log(log)  # appends JSON line'
  example_source: inferred
  line_start: 101
  line_end: 119
  dependencies:
  - graphrag/logger/blob_workflow_logger.py::__init__
  called_by: []
- node_id: graphrag/utils/storage.py::storage_has_table
  file: graphrag/utils/storage.py
  name: storage_has_table
  signature: 'def storage_has_table(name: str, storage: PipelineStorage) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Check if a table exists in storage.\n\nArgs:\n    name: The name of\
    \ the table to check (without the extension).\n    storage: The storage backend\
    \ implementing PipelineStorage.\n\nReturns:\n    bool: True if a file named \"\
    <name>.parquet\" exists in storage, False otherwise.\n\nRaises:\n    Propagates\
    \ exceptions raised by storage.has."
  code_example: "from storage_utils import storage_has_table\nname = \"optional_file\"\
    \nclass DummyStorage:\n    def has(self, path: str) -> bool:\n        return path\
    \ == \"optional_file.parquet\"\nstorage = DummyStorage()\nresult = storage_has_table(name,\
    \ storage)\nprint(result)  # True if exists"
  example_source: inferred
  line_start: 42
  line_end: 44
  dependencies: []
  called_by:
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/index/workflows/create_community_reports.py::run_workflow
  - graphrag/index/workflows/create_final_text_units.py::run_workflow
  - graphrag/index/workflows/update_covariates.py::run_workflow
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.__init__
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: __init__
  signature: def __init__(self, verbose=False)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize ConsoleWorkflowCallbacks with an optional verbose mode.\n\
    \nArgs:\n  verbose: Enable verbose logging to the console.\n\nReturns:\n  None"
  code_example: null
  example_source: inferred
  line_start: 18
  line_end: 19
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_key
  file: graphrag/config/models/language_model_config.py
  name: _validate_api_key
  signature: def _validate_api_key(self) -> None
  decorators: []
  raises:
  - ApiKeyMissingError
  - ConflictingSettingsError
  visibility: protected
  docstring: "Validate the API key.\n\nAPI Key is required when using OpenAI API or\
    \ when using Azure API with API Key authentication.\nFor the time being, this\
    \ check is extra verbose for clarity.\nIt will also raise an exception if an API\
    \ Key is provided when one is not expected such as the case of using Azure Managed\
    \ Identity.\n\nArgs:\n    self: The LanguageModelConfig instance.\n\nReturns:\n\
    \    None\n\nRaises:\n    ApiKeyMissingError: If the API key is missing and is\
    \ required.\n    ConflictingSettingsError: If an API Key is provided when using\
    \ Azure Managed Identity."
  code_example: null
  example_source: inferred
  line_start: 33
  line_end: 60
  dependencies:
  - graphrag/config/errors.py::ApiKeyMissingError
  - graphrag/config/errors.py::ConflictingSettingsError
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.__init__
  file: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  name: __init__
  signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n \
    \   ) -> None"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: protected
  docstring: "Abstract initializer for rate limiters. Subclasses must implement their\
    \ own initialization logic; this method should not perform concrete initialization.\n\
    \nArgs:\n    kwargs: Additional keyword arguments passed to initialization.\n\n\
    Returns:\n    None"
  code_example: "from module import __init__ as init\nclass A:\n    def __init__(self,\
    \ **kwargs):\n        init(self, **kwargs)\ninst = A(limit=100, win=\"1s\")  #\
    \ base init no-op"
  example_source: inferred
  line_start: 16
  line_end: 20
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_text
  file: graphrag/vector_stores/azure_ai_search.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Perform a text-based similarity search.\n\nArgs:\n    text (str):\
    \ The input text to search for similar documents.\n    text_embedder (TextEmbedder):\
    \ The callable used to compute an embedding for the input text.\n    k (int):\
    \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
    \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
    \ VectorStoreSearchResult objects.\n\nRaises:\n    None\n\"\"\""
  code_example: null
  example_source: test
  line_start: 195
  line_end: 204
  dependencies:
  - graphrag/vector_stores/azure_ai_search.py::similarity_search_by_vector
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: vector_store
  signature: def vector_store(self, mock_search_client, mock_index_client)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create an Azure AI Search vector store fixture.\n\nArgs:\n    self:\
    \ The test class instance.\n    mock_search_client: Mock Azure AI Search client\
    \ to be assigned to vector_store.db_connection.\n    mock_index_client: Mock Azure\
    \ AI Search index client to be assigned to vector_store.index_client.\n\nReturns:\n\
    \    AzureAISearchVectorStore: The configured Azure AI Search vector store instance."
  code_example: null
  example_source: inferred
  line_start: 41
  line_end: 57
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.has
  file: graphrag/storage/pipeline_storage.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Return True if the given key exists in the storage.\n\nArgs:\n    key:\
    \ The key to check for.\n\nReturns:\n    output - True if the key exists in the\
    \ storage, False otherwise."
  code_example: "import asyncio\nfrom storage import Storage\nasync def main():\n\
    \    s = Storage({'a':1, 'b':2})\n    return await s.has('a')\nres = asyncio.run(main())\n\
    print(res)  # True"
  example_source: inferred
  line_start: 51
  line_end: 60
  dependencies: []
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
  file: graphrag/storage/pipeline_storage.py
  name: get_timestamp_formatted_with_local_tz
  signature: 'def get_timestamp_formatted_with_local_tz(timestamp: datetime) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the formatted timestamp with the local time zone.\n\nArgs:\n\
    \    timestamp (datetime): The timestamp to format in the local time zone.\n\n\
    Returns:\n    str: The timestamp represented as 'YYYY-MM-DD HH:MM:SS \xB1HHMM'\
    \ in the local time zone.\n\nRaises:\n    None: This function does not raise any\
    \ exceptions.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 95
  line_end: 99
  dependencies: []
  called_by:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
- node_id: graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
  file: graphrag/index/operations/chunk_text/bootstrap.py
  name: bootstrap
  signature: def bootstrap()
  decorators: []
  raises: []
  visibility: public
  docstring: "Bootstrap initialization for NLTK resources.\n\nDownloads and prepares\
    \ the required NLTK data on the first call, and sets the module-level flag initialized_nltk\
    \ to True to prevent repeated work.\n\nThis function downloads the following resources\
    \ and ensures WordNet is loaded: punkt, punkt_tab, averaged_perceptron_tagger,\
    \ averaged_perceptron_tagger_eng, maxent_ne_chunker, maxent_ne_chunker_tab, words,\
    \ and wordnet; it also calls wn.ensure_loaded().\n\nReturns:\n    None\n\nRaises:\n\
    \    ImportError: If the nltk package or required submodules are unavailable."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 31
  dependencies: []
  called_by:
  - graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
- node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.aretry
  file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  name: aretry
  signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
    \        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry an asynchronous function with a random delay between retries until\
    \ it succeeds or the maximum number of retries is reached.\n\nArgs:\n    func:\
    \ The asynchronous function to retry.\n    kwargs: Additional keyword arguments\
    \ to pass to the function.\n\nReturns:\n    The result of the awaited function.\n\
    \nRaises:\n    Exception: If the wrapped function keeps raising and the maximum\
    \ number of retries is exceeded."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 79
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_rate_limiter_services
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_rate_limiter_services
  signature: def _validate_rate_limiter_services(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the rate limiter services configuration.\n\nThis method checks\
    \ each model's rate_limit_strategy. For each model with a configured strategy,\
    \ it verifies the strategy is registered with RateLimiterFactory and, if rpm or\
    \ tpm values are provided, creates a corresponding rate limiter instance to validate\
    \ configuration.\n\nArgs:\n  self: The instance containing the models configuration\
    \ to validate.\n\nReturns:\n  None: This method does not return a value.\n\nRaises:\n\
    \  ValueError: If a rate limiter strategy for a model is not registered."
  code_example: null
  example_source: null
  line_start: 114
  line_end: 137
  dependencies:
  - graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py::RateLimiterFactory
  called_by: []
- node_id: graphrag/factory/factory.py::Factory.create
  file: graphrag/factory/factory.py
  name: create
  signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Create a service instance based on the strategy.\n\nArgs:\n    strategy\
    \ (str): The name of the strategy.\n    kwargs (Any): Additional arguments to\
    \ pass to the service initializer.\n\nReturns:\n    T: An instance of T.\n\nRaises:\n\
    \    ValueError: If the strategy is not registered."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 68
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_customization
  file: tests/integration/vector_stores/test_cosmosdb.py
  name: test_vector_store_customization
  signature: def test_vector_store_customization()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test vector store customization with CosmosDB.\n\nArgs:\n    None: The\
    \ function does not accept any parameters.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 108
  line_end: 166
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.none_embedder
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: none_embedder
  signature: 'def none_embedder(text: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "A placeholder embedder function used in tests that accepts a text string\
    \ and returns None.\n\nArgs:\n    text: str\n        The input text to be embedded.\
    \ The function does not perform embedding and returns None.\n\nReturns:\n    None\n\
    \        The function returns no value."
  code_example: null
  example_source: null
  line_start: 167
  line_end: 168
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__hash__
  file: graphrag/query/structured_search/drift_search/action.py
  name: __hash__
  signature: def __hash__(self) -> int
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return a hash value for the DriftAction object to enable hashing in\
    \ networkx.MultiDiGraph.\n\nAssumes queries are unique.\n\nArgs:\n    self: The\
    \ DriftAction instance.\n\nReturns:\n    int: Hash based on the query."
  code_example: null
  example_source: null
  line_start: 210
  line_end: 221
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_chat_model
  file: graphrag/language_model/manager.py
  name: get_or_create_chat_model
  signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
    \ str, **chat_kwargs: Any\n    ) -> ChatModel"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get or create the ChatLLM instance registered under the given name.\n\
    \nIf the ChatLLM does not exist, it is created and registered.\n\nArgs:\n    name:\
    \ Unique identifier for the ChatLLM instance.\n    model_type: Key for the ChatModel\
    \ implementation in LLMFactory.\n    chat_kwargs: Additional keyword arguments\
    \ for instantiation.\n\nReturns:\n    ChatModel: The ChatLLM instance associated\
    \ with the given name.\n\nRaises:\n    Exception: Any error raised during creation\
    \ via register_chat or the underlying factory."
  code_example: null
  example_source: null
  line_start: 105
  line_end: 120
  dependencies:
  - graphrag/language_model/manager.py::register_chat
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_create_unknown_vector_store
  file: tests/integration/vector_stores/test_factory.py
  name: test_create_unknown_vector_store
  signature: def test_create_unknown_vector_store()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that creating an unknown vector store type raises a ValueError.\n\
    \nReturns:\n    None\n\nRaises:\n    ValueError: Unknown vector store type: unknown"
  code_example: null
  example_source: null
  line_start: 105
  line_end: 110
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/query/llm/text_utils.py::batched
  file: graphrag/query/llm/text_utils.py
  name: batched
  signature: 'def batched(iterable: Iterator, n: int)'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"\nBatch data into tuples of length n. The last batch may be shorter.\n\
    \nArgs:\n    iterable (Iterator): The input iterable to batch.\n    n (int): The\
    \ batch size (must be at least 1).\n\nReturns:\n    Iterator[tuple]: An iterator\
    \ that yields batches as tuples of length n (the last batch may be shorter).\n\
    \nRaises:\n    ValueError: If n < 1.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 21
  line_end: 33
  dependencies: []
  called_by:
  - graphrag/query/llm/text_utils.py::chunk_text
- node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_vector
  file: graphrag/utils/api.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a vector-based similarity search across all configured embedding\
    \ stores and merge results.\n\nArgs:\n  query_embedding: list[float] - Embedding\
    \ vector to search with\n  k: int - Number of top results to return\n  kwargs:\
    \ Any - Additional keyword arguments for compatibility; not used directly by this\
    \ method\n\nReturns:\n  list[VectorStoreSearchResult] - Top-k results merged from\
    \ all stores, sorted by score in descending order\n\nRaises:\n  Exception - Exceptions\
    \ raised by the underlying embedding stores may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 83
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed_batch
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
    \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
    \ (e.g., model parameters).\n\nReturns:\n    A list of embeddings, where each\
    \ embedding is a list of floats."
  code_example: null
  example_source: null
  line_start: 244
  line_end: 259
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.from_list
  file: graphrag/query/context_builder/conversation_history.py
  name: from_list
  signature: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
    \    ) -> \"ConversationHistory\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a ConversationHistory from a list of conversation turns.\n\n\
    Each turn is a dictionary in the form of {\"role\": \"<conversation_role>\", \"\
    content\": \"<turn content>\"}.\n\nArgs:\n    cls: The class object, used to instantiate\
    \ a new ConversationHistory.\n    conversation_turns: A list of dictionaries representing\
    \ turns. Each dictionary has keys \"role\" and \"content\".\n\nReturns:\n    ConversationHistory:\
    \ A new ConversationHistory containing the parsed turns."
  code_example: null
  example_source: null
  line_start: 99
  line_end: 117
  dependencies: []
  called_by: []
- node_id: graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
  file: graphrag/index/text_splitting/text_splitting.py
  name: split_multiple_texts_on_tokens
  signature: "def split_multiple_texts_on_tokens(\n    texts: list[str], tokenizer:\
    \ TokenChunkerOptions, tick: ProgressTicker\n) -> list[TextChunk]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Split multiple texts and return chunks with metadata using the tokenizer.\n\
    \nArgs:\n    texts: list[str] The texts to split into chunks.\n    tokenizer:\
    \ TokenChunkerOptions The tokenizer configuration used to encode texts into tokens\
    \ and decode chunks.\n    tick: ProgressTicker A callback function to track progress.\
    \ If provided, tick(1) is called for each processed text.\n\nReturns:\n    list[TextChunk]\
    \ A list of TextChunk objects. Each TextChunk contains the chunk_text, the indices\
    \ of source documents contributing to the chunk, and the number of tokens in the\
    \ chunk."
  code_example: null
  example_source: null
  line_start: 142
  line_end: 173
  dependencies:
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by:
  - graphrag/index/operations/chunk_text/strategies.py::run_tokens
  - tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_multiple_texts_on_tokens
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.clear
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Clear all contents from storage.\n\nThis currently deletes the database,\
    \ including all containers and data within it.\nTODO: We should decide what granularity\
    \ of deletion is the ideal behavior (e.g. delete all items within a container,\
    \ delete the current container, delete the current database)\n\nReturns:\n   \
    \ None: The function does not return a value.\n\nRaises:\n    Exception: If deletion\
    \ fails."
  code_example: null
  example_source: null
  line_start: 320
  line_end: 326
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_delete_database
  called_by: []
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._clean_claim
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: _clean_claim
  signature: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
    \ dict\n    ) -> dict"
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Update a claim''s object and subject identifiers in place using a resolved_entities
    mapping. This function does not filter by status and does not remove claims with
    status = False.


    Args:

    - claim (dict): The claim dictionary to update. The function reads the object_id
    (or object) and subject_id (or subject), substitutes them using resolved_entities,
    and writes the resulting values back to object_id and subject_id in the input
    dict. If keys are missing, existing values are preserved.

    - document_id (str): Identifier of the document containing the claim. This parameter
    is unused by this function.

    - resolved_entities (dict): Mapping of original entity identifiers to resolved
    identifiers; used to substitute the object and subject when present.


    Returns:

    - dict: The updated claim dictionary (the same object, mutated in place).


    Notes:

    - The function updates only the object_id and subject_id keys.

    - No status filtering is performed; claims with status = False are not removed.

    - When reading, object is considered as a fallback for object_id and subject as
    a fallback for subject_id.'
  code_example: null
  example_source: null
  line_start: 135
  line_end: 147
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::ModelOutput.full_response
  file: graphrag/language_model/response/base.pyi
  name: full_response
  signature: def full_response(self) -> dict[str, Any] | None
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the full response payload as a dictionary, or None if not available.\n\
    \nArgs:\n    self: The instance of the class.\n\nReturns:\n    dict[str, Any]\
    \ | None: The full response payload as a dictionary, or None if not present."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 14
  dependencies: []
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.connect
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> None'
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Connect to the vector store.\n\nArgs:\n    kwargs: Arbitrary keyword\
    \ arguments.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: Raised\
    \ to indicate the method is not implemented."
  code_example: null
  example_source: null
  line_start: 28
  line_end: 29
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_json
  file: graphrag/storage/blob_pipeline_storage.py
  name: _set_df_json
  signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Store a dataframe as JSON at the specified key in storage.\n\nThe dataframe\
    \ is serialized to JSON in records format with one JSON object per line and written\
    \ to the path derived from the provided key. Depending on configuration, the write\
    \ uses either a storage account with DefaultAzureCredential or a connection string.\n\
    \nArgs:\n    key: The key under which to store the JSON export of the dataframe.\n\
    \    dataframe: The dataframe to serialize to JSON.\n\nReturns:\n    None\n\n\
    Raises:\n    Exception: If an error occurs during serialization or storage write."
  code_example: null
  example_source: null
  line_start: 214
  line_end: 234
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_abfs_url
  called_by: []
- node_id: graphrag/cli/query.py::on_context
  file: graphrag/cli/query.py
  name: on_context
  signature: 'def on_context(context: Any) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Stores the given context in the enclosing scope's nonlocal variable\
    \ context_data.\n\nThis function uses a nonlocal binding to the variable context_data\
    \ defined in the outer scope and assigns the provided context to it. As a result,\
    \ the outer scope's context_data is updated. This function returns None.\n\nArgs:\n\
    \    context (Any): The context data to store in the enclosing scope.\n\nReturns:\n\
    \    None: The function updates the outer scope's context_data and returns no\
    \ value.\n\nRaises:\n    NameError: If the enclosing scope does not define context_data\
    \ (nonlocal binding cannot be resolved)."
  code_example: null
  example_source: null
  line_start: 442
  line_end: 444
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store_custom
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: vector_store_custom
  signature: def vector_store_custom(self, mock_search_client, mock_index_client)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create an Azure AI Search vector store fixture with custom field mappings.\n\
    \nArgs:\n    self: The test class instance.\n    mock_search_client: Mock Azure\
    \ AI Search client to be assigned to vector_store.db_connection.\n    mock_index_client:\
    \ Mock Azure AI Search index client to be assigned to vector_store.index_client.\n\
    \nReturns:\n    AzureAISearchVectorStore: The configured Azure AI Search vector\
    \ store instance."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 81
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_encoding_model
  file: graphrag/config/models/language_model_config.py
  name: _validate_encoding_model
  signature: def _validate_encoding_model(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Validate the encoding model.


    If encoding_model is not provided (empty or whitespace) and the type is not Chat
    or Embedding, derive

    the encoding model name for the configured LLM model using tiktoken.encoding_name_for_model
    and assign it

    to self.encoding_model. This updates the attribute in place and returns None.'
  code_example: null
  example_source: null
  line_start: 137
  line_end: 161
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_extract_claims_configs
  file: tests/unit/config/utils.py
  name: assert_extract_claims_configs
  signature: "def assert_extract_claims_configs(\n    actual: ClaimExtractionConfig,\
    \ expected: ClaimExtractionConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Assert that the actual and expected ClaimExtractionConfig instances\
    \ are equal.\n\nThis helper asserts that the fields enabled, prompt, description,\
    \ max_gleanings, strategy, and model_id on actual match those on expected.\n\n\
    Args:\n    actual: The actual ClaimExtractionConfig instance produced by the code\
    \ under test.\n    expected: The expected ClaimExtractionConfig instance to compare\
    \ against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If any of the\
    \ compared fields differ between actual and expected.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 292
  line_end: 300
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.keys
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "\"\"\"Keys listing is not supported for CosmosDB storage.\n\nArgs:\n\
    \    self: The instance of the storage.\n\nReturns:\n    None: This method does\
    \ not return any keys; it raises NotImplementedError.\n\nRaises:\n    NotImplementedError:\
    \ CosmosDB storage does not support listing keys.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 328
  line_end: 331
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.filter_by_id
  file: graphrag/vector_stores/azure_ai_search.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Builds a query filter to filter documents by a list of IDs.\n\nArgs:\n\
    \  include_ids: list[str] | list[int] The IDs to include in the filter. If None\
    \ or an empty\n    list is provided, the filter is cleared (set to None) and None\
    \ is returned.\n\nReturns:\n  Any The constructed query filter string to filter\
    \ documents by the provided IDs, or None if no IDs are provided."
  code_example: null
  example_source: null
  line_start: 152
  line_end: 166
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging
  file: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py
  name: _wrapped_with_logging
  signature: 'def _wrapped_with_logging(**kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Wraps the synchronous request function with logging.\n\nArgs:\n\
    \    kwargs: Keyword arguments passed to the underlying synchronous request function.\n\
    \nReturns:\n    Any: The value returned by the underlying sync_fn when called\
    \ with the provided kwargs.\n\nRaises:\n    Exception: Re-raised after logging\
    \ the exception encountered during the call.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 36
  line_end: 43
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed_batch
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: aembed_batch
  signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
    \    ) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
    \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
    \ (e.g., model parameters).\n\nReturns:\n    list[list[float]]: A batch of embeddings."
  code_example: null
  example_source: null
  line_start: 203
  line_end: 221
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read_settings
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: read_settings
  signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Read settings for a datasource from a file.\n\nArgs:\n    file: Path\
    \ to the settings file.\n\nReturns:\n    GraphRagConfig | None: The GraphRagConfig\
    \ read from the file, or None if no settings could be read.\n\nRaises:\n    NotImplementedError:\
    \ If the method is not implemented by a subclass."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 45
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.register_embedding
  file: graphrag/language_model/factory.py
  name: register_embedding
  signature: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
    \ EmbeddingModel]\n    ) -> None"
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register an EmbeddingModel implementation.\n\nStores the given creator\
    \ in the internal _embedding_registry mapping for the specified model_type.\n\n\
    Args:\n    model_type: The type identifier for the EmbeddingModel to register.\n\
    \    creator: A callable that returns an EmbeddingModel instance when invoked.\n\
    \nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 35
  line_end: 39
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat
  file: tests/integration/language_model/test_factory.py
  name: achat
  signature: "def achat(\n            self, prompt: str, history: list | None = None,\
    \ **kwargs: Any\n        ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous achat that returns a fixed content response for the given\
    \ prompt.\n\nArgs:\n    prompt (str): The input prompt. The content is fixed and\
    \ does not depend on the prompt.\n    history (list | None): Optional conversation\
    \ history. May be unused by this placeholder implementation.\n    **kwargs (Any):\
    \ Additional keyword arguments. May be ignored.\n\nReturns:\n    ModelResponse:\
    \ The response for the prompt. The BaseModelResponse contains output with content=\"\
    content\". The output.full_response is None.\n\nRaises:\n    None documented for\
    \ this placeholder implementation.\n\nNotes:\n    This is a placeholder implementation.\
    \ It does not engage a real chat model; it always returns the fixed content string."
  code_example: null
  example_source: null
  line_start: 28
  line_end: 31
  dependencies:
  - graphrag/language_model/response/base.py::BaseModelOutput
  - graphrag/language_model/response/base.py::BaseModelResponse
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._map_response_single_batch
  file: graphrag/query/structured_search/global_search/search.py
  name: _map_response_single_batch
  signature: "def _map_response_single_batch(\n        self,\n        context_data:\
    \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
    \    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Generate an answer for a single chunk of community reports.\n\nArgs:\n\
    \    context_data (str): Contextual data for the current chunk of community reports.\n\
    \    query (str): The query to be answered based on the provided context.\n  \
    \  max_length (int): Maximum length allowed for the generated response.\n    llm_kwargs\
    \ (Any): Additional keyword arguments forwarded to the language model (model_parameters).\n\
    \nReturns:\n    SearchResult: The processed response for this batch, including\
    \ the parsed response, context data, timing, and token usage.\n\nRaises:\n   \
    \ None: This function handles exceptions internally and does not propagate exceptions\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 209
  line_end: 264
  dependencies:
  - graphrag/query/structured_search/base.py::SearchResult
  - graphrag/query/structured_search/global_search/search.py::_parse_search_response
  called_by: []
- node_id: unified-search-app/app/app_logic.py::load_knowledge_model
  file: unified-search-app/app/app_logic.py
  name: load_knowledge_model
  signature: 'def load_knowledge_model(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Load knowledge model from the datasource and populate the session\
    \ variables with the loaded model data.\n\nArgs:\n    sv (SessionVariables): The\
    \ session variables container to be updated with the loaded knowledge model\n\
    \        data, including entities, relationships, covariates, community reports,\
    \ communities, and text units.\n        The function also resets generated_questions\
    \ and selected_question.\n\nReturns:\n    SessionVariables: The same sv object\
    \ after it has been populated with the knowledge model data.\n\nRaises:\n    Propagates\
    \ exceptions raised by load_model or datasource access as encountered.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 354
  line_end: 368
  dependencies: []
  called_by:
  - unified-search-app/app/app_logic.py::load_dataset
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.__init__
  file: graphrag/query/context_builder/conversation_history.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a ConversationHistory with an empty list of turns.\n\nReturns:\n\
    \    None\n        This initializer does not return a value. It initializes the\
    \ turns attribute to an empty list."
  code_example: null
  example_source: null
  line_start: 95
  line_end: 96
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__init__
  file: graphrag/query/structured_search/drift_search/action.py
  name: __init__
  signature: "def __init__(\n        self,\n        query: str,\n        answer: str\
    \ | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n \
    \   )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the DriftAction with a query, optional answer, and follow-up\
    \ actions.\n\nArgs:\n    query (str): The query for the action.\n    answer (Optional[str]):\
    \ The answer to the query, if available.\n    follow_ups (Optional[list[DriftAction]]):\
    \ A list of follow-up actions.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 22
  line_end: 46
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_community_report_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_community_report_data
  signature: "def get_community_report_data(\n    _datasource: Datasource,\n) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "\"\"\"Return a dataframe with community report data from the indexed-data.\n\
    \nArgs:\n    _datasource: Datasource to read the community report data from the\
    \ indexed-data.\n\nReturns:\n    A dataframe with community report data loaded\
    \ from the indexed-data.\n\nRaises:\n    Exception: If the underlying data source\
    \ read operation fails.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 60
  line_end: 67
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  file: graphrag/tokenizer/get_tokenizer.py
  name: get_tokenizer
  signature: "def get_tokenizer(\n    model_config: LanguageModelConfig | None = None,\n\
    \    encoding_model: str = ENCODING_MODEL,\n) -> Tokenizer"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the tokenizer for the given model configuration or fallback to a\
    \ tiktoken based tokenizer.\n\nArgs:\n    model_config: LanguageModelConfig |\
    \ None, optional\n        The model configuration. If not provided or model_config.encoding_model\
    \ is manually set,\n        use a tiktoken based tokenizer. Otherwise, use a LitellmTokenizer\
    \ based on the model name.\n        LiteLLM supports token encoding/decoding for\
    \ the range of models it supports.\n    encoding_model: str, optional\n      \
    \  A tiktoken encoding model to use if no model configuration is provided. Only\
    \ used if a\n        model configuration is not provided.\n\nReturns:\n    Tokenizer:\
    \ An instance of a Tokenizer.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 13
  line_end: 41
  dependencies:
  - graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer
  - graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
  - graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
  - graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
  - graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
  - graphrag/query/context_builder/community_context.py::build_community_context
  - graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
  - graphrag/query/context_builder/local_context.py::build_entity_context
  - graphrag/query/context_builder/local_context.py::build_covariates_context
  - graphrag/query/context_builder/local_context.py::build_relationship_context
  - graphrag/query/context_builder/source_context.py::build_text_unit_context
  - graphrag/query/factory.py::get_local_search_engine
  - graphrag/query/factory.py::get_global_search_engine
  - graphrag/query/factory.py::get_drift_search_engine
  - graphrag/query/factory.py::get_basic_search_engine
  - graphrag/query/llm/text_utils.py::chunk_text
  - graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
  - graphrag/query/structured_search/base.py::BaseSearch.__init__
  - graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
  - graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
  - graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
  - graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
  - graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
  - graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
  - tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
  - tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
  - tests/unit/utils/test_encoding.py::test_encode_basic
  - tests/unit/utils/test_encoding.py::test_num_tokens_empty_input
- node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_get_creation_date
  file: tests/integration/storage/test_blob_pipeline_storage.py
  name: test_get_creation_date
  signature: def test_get_creation_date()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that BlobPipelineStorage.get_creation_date returns a correctly
    formatted creation timestamp for a blob.


    Returns:

    A string representing the creation date of the blob, formatted as "%Y-%m-%d %H:%M:%S
    %z".

    Type: str'
  code_example: null
  example_source: null
  line_start: 66
  line_end: 81
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_start
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: workflow_start
  signature: 'def workflow_start(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n    name\
    \ (str): The name of the workflow starting.\n    instance (object): The workflow\
    \ instance object associated with this start event.\n\nReturns:\n    None\n\n\
    Raises:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 36
  line_end: 40
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/layout_graph/typing.py::NodePosition.to_pandas
  file: graphrag/index/operations/layout_graph/typing.py
  name: to_pandas
  signature: def to_pandas(self) -> tuple[str, float, float, str, float]
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Convert this NodePosition to a pandas-friendly 5-tuple.\n\nArgs:\n\
    \    self (NodePosition): The NodePosition instance to convert.\n\nReturns:\n\
    \    tuple[str, float, float, str, float]: The tuple containing label, x, y, cluster,\
    \ and size.\n\n\"\"\""
  code_example: null
  example_source: null
  line_start: 22
  line_end: 24
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_reporting_base_dir
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_reporting_base_dir
  signature: def _validate_reporting_base_dir(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the reporting base directory.\n\nArgs:\n    self (GraphRagConfig):\
    \ The instance of the configuration model containing the reporting configuration\
    \ and root_dir.\n\nReturns:\n    None. When the reporting type is file, this method\
    \ validates that the base_dir is non-empty and then converts it to an absolute\
    \ path using root_dir.\n\nRaises:\n    ValueError: If the reporting type is file\
    \ and the reporting.base_dir is empty or whitespace."
  code_example: null
  example_source: null
  line_start: 229
  line_end: 237
  dependencies: []
  called_by: []
- node_id: graphrag/utils/api.py::MultiVectorStore.search_by_id
  file: graphrag/utils/api.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Search for a document by id across the configured vector stores.\n\n\
    Args:\n    id: The composite identifier used to locate the document. It should\
    \ be formatted as \"<internal_id>-<index_name>\", where\n        <internal_id>\
    \ is the id within the specific vector store and <index_name> is the name of that\
    \ store.\n\nReturns:\n    VectorStoreDocument: The document corresponding to the\
    \ provided id from the matching vector store.\n\nRaises:\n    ValueError: If the\
    \ index name extracted from the id is not found among the configured index stores."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 65
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn.__str__
  file: graphrag/query/context_builder/conversation_history.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return string representation of the conversation turn.\n\nArgs:\n  \
    \  self: The ConversationTurn instance for which to obtain the string representation.\n\
    \nReturns:\n    str: The string representation of the turn in the format \"<role>:\
    \ <content>\".\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 56
  line_end: 58
  dependencies: []
  called_by: []
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.keys
  file: graphrag/storage/memory_pipeline_storage.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the keys in the storage.\n\nArgs:\n    self (MemoryPipelineStorage):\
    \ The instance of the MemoryPipelineStorage.\n\nReturns:\n    list[str]: The keys\
    \ currently stored in the storage."
  code_example: null
  example_source: null
  line_start: 76
  line_end: 78
  dependencies: []
  called_by: []
- node_id: graphrag/utils/api.py::update_context_data
  file: graphrag/utils/api.py
  name: update_context_data
  signature: "def update_context_data(\n    context_data: Any,\n    links: dict[str,\
    \ Any],\n) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update context data with index_name and index_id fields derived from\
    \ the links mapping.\n\nArgs:\n    context_data (dict[str, pandas.DataFrame]):\
    \ The context data to update. Each value should be a DataFrame containing records\
    \ with an 'id' field and other relevant columns. The dict's keys typically include\
    \ 'reports', 'entities', 'relationships', 'claims', and 'sources'.\n    links\
    \ (dict[str, Any]): A dictionary of links to the original dataframes. Expected\
    \ to contain the following mappings:\n        - 'community_reports': dict-like\
    \ mapping int(id) -> {'index_name', 'id'}\n        - 'entities': dict-like mapping\
    \ int(id) -> {'index_name', 'id'}\n        - 'relationships': dict-like mapping\
    \ int(id) -> {'index_name', 'id'}\n        - 'covariates': dict-like mapping int(id)\
    \ -> {'index_name', 'id'}\n        - 'text_units': dict-like mapping int(id) ->\
    \ {'index_name', 'id'}\n\nReturns:\n    dict[str, list[dict]]: The updated context\
    \ data. For each key in the input, the function returns a list of dictionaries\
    \ representing the records augmented with:\n        - index_name and index_id\
    \ based on the provided links\n        - Additional key-specific transformations\
    \ (e.g., 'entity' trimmed to the portion before the first dash for 'entities',\
    \ 'source' and 'target' trimmed for 'relationships', etc.)\n\nRaises:\n    KeyError:\
    \ If required keys are missing in the links dictionary or if expected fields are\
    \ missing in a context_data entry (e.g., missing 'id').\n    TypeError: If context_data\
    \ is not a dict[str, pandas.DataFrame] or if a value does not support to_dict(orient='records').\n\
    \    ValueError: If an entry['id'] cannot be cast to int."
  code_example: null
  example_source: null
  line_start: 175
  line_end: 247
  dependencies: []
  called_by:
  - graphrag/api/query.py::multi_index_global_search
  - graphrag/api/query.py::multi_index_local_search
  - graphrag/api/query.py::multi_index_drift_search
- node_id: graphrag/config/load_config.py::_apply_overrides
  file: graphrag/config/load_config.py
  name: _apply_overrides
  signature: 'def _apply_overrides(data: dict[str, Any], overrides: dict[str, Any])
    -> None'
  decorators: []
  raises:
  - TypeError
  visibility: protected
  docstring: "Apply the overrides to the raw configuration.\n\nArgs:\n    data: dict[str,\
    \ Any]\n        The raw configuration dictionary to be updated in place.\n   \
    \ overrides: dict[str, Any]\n        A flat mapping of dot-separated keys to values\
    \ to override in the configuration.\n\nReturns:\n    None\n\nRaises:\n    TypeError\n\
    \        If attempting to override a non-dict value along the path."
  code_example: null
  example_source: null
  line_start: 115
  line_end: 129
  dependencies: []
  called_by:
  - graphrag/config/load_config.py::load_config
- node_id: graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  file: graphrag/language_model/providers/fnllm/utils.py
  name: run_coroutine_sync
  signature: 'def run_coroutine_sync(coroutine: Coroutine[Any, Any, T]) -> T'
  decorators: []
  raises: []
  visibility: public
  docstring: "Run a coroutine synchronously.\n\nArgs:\n    coroutine: Coroutine[Any,\
    \ Any, T] The coroutine to run.\n\nReturns:\n    T: The result of the coroutine.\n\
    \nRaises:\n    Exception: If the coroutine raises an exception, it will be propagated\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 118
  line_end: 132
  dependencies: []
  called_by:
  - graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
  - graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
  - graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_embedder
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: mock_embedder
  signature: 'def mock_embedder(text: str) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "A simple text embedder used for testing that returns a fixed embedding\
    \ vector. The embedding is independent of the input text and always returns [0.1,\
    \ 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text: Input text to embed.\n\nReturns:\n\
    \    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4, 0.5].\n\nRaises:\n\
    \    None: This function does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 232
  line_end: 233
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/search.py::render_html_table
  file: unified-search-app/app/ui/search.py
  name: render_html_table
  signature: 'def render_html_table(df: pd.DataFrame, search_type: str, key: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Render HTML table into the UI.


    Builds an HTML fragment representing the given DataFrame as a table with a header
    and body. It applies per-cell formatting: strings may be truncated for display,
    and if a string value starts with a JSON-like object (for example, a dictionary
    in string form), the code attempts to extract the "summary" field from that JSON.
    It also generates per-row HTML ids to enable UI interactions; ids are constructed
    from the lowercased and stripped search_type and key with the row''s id when available,
    otherwise using the row index. The function returns the HTML string suitable for
    insertion into the UI (for example via Streamlit''s st.markdown with unsafe_allow_html=True).


    Args:

    - df (pd.DataFrame): DataFrame to render as HTML table for UI display.

    - search_type (str): Type of search; used to generate the per-row HTML id (lowercased
    and stripped).

    - key (str): Key associated with the search type; used in ID generation when an
    id column exists.


    Returns:

    - str: HTML string representing the rendered table.


    Raises:

    - json.JSONDecodeError: If a string value that begins with ''{'' cannot be parsed
    as JSON to extract a summary.

    - AttributeError or TypeError: If a row contains an id value that is not a string
    or otherwise cannot be stripped, or if input types are incompatible.

    - ValueError: If inputs are of an unexpected type or contain invalid data for
    rendering.'
  code_example: null
  example_source: null
  line_start: 192
  line_end: 264
  dependencies: []
  called_by:
  - unified-search-app/app/ui/search.py::display_citations
  - unified-search-app/app/ui/search.py::display_graph_citations
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
  file: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py
  name: with_rate_limiter
  signature: "def with_rate_limiter(\n    *,\n    sync_fn: LitellmRequestFunc,\n \
    \   async_fn: AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\"\
    ,\n    rpm: int | None = None,\n    tpm: int | None = None,\n) -> tuple[LitellmRequestFunc,\
    \ AsyncLitellmRequestFunc]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"\nWrap the synchronous and asynchronous Litellm request functions\
    \ with rate limiting.\n\nArgs\n----\nsync_fn: The synchronous chat/embedding request\
    \ function to wrap.\nasync_fn: The asynchronous chat/embedding request function\
    \ to wrap.\nmodel_config: LanguageModelConfig containing rate_limit_strategy and\
    \ related model parameters.\nrpm: Optional rate limit in requests per minute.\
    \ If None or 0, the RPM limit is disabled.\ntpm: Optional rate limit in tokens\
    \ per minute. If None or 0, the TPM limit is disabled.\n\nReturns\n-------\ntuple[LitellmRequestFunc,\
    \ AsyncLitellmRequestFunc]\n    The wrapped synchronous and asynchronous request\
    \ functions.\n\nRaises\n------\nValueError\n    If the rate limiter strategy in\
    \ model_config is None or not registered with the RateLimiterFactory.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 22
  line_end: 97
  dependencies:
  - graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter_factory.py::RateLimiterFactory
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.find
  file: graphrag/storage/pipeline_storage.py
  name: find
  signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n \
    \       base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
    \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Find files in the storage that match a compiled file_pattern, with optional\
    \ base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern (re.Pattern[str]):\
    \ A compiled regular expression to match file paths.\n    base_dir (str | None):\
    \ The base directory to search within. If None, search starts from the storage\
    \ root.\n    file_filter (dict[str, Any] | None): Optional dictionary of metadata\
    \ field names to values to filter on. Implementations may perform exact-value\
    \ matching against the file's metadata; keys correspond to metadata attributes\
    \ present in the returned dictionaries. If None, no additional filtering is applied.\n\
    \    max_count (int): Maximum number of results to yield. -1 means no limit. When\
    \ a non-negative value is provided, at most that many results are yielded.\n\n\
    Returns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator yielding (path,\
    \ metadata) pairs where path is the matched file path as a string and metadata\
    \ is a dictionary of attributes describing the file. The exact metadata keys are\
    \ implementation-dependent and may vary between storage backends.\n\nNotes:\n\
    \    This is an abstract method. Concrete subclasses must provide an implementation.\n\
    \nRaises:\n    Exceptions raised depend on the concrete subclass implementation\
    \ (e.g., I/O or filesystem errors)."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 24
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/base.py::BaseSearch.stream_search
  file: graphrag/query/structured_search/base.py
  name: stream_search
  signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Stream search for the given query.\nArgs:\n    query (str): The search\
    \ query to execute.\n    conversation_history (ConversationHistory | None): Optional\
    \ conversation history to consider during the search.\nReturns:\n    AsyncGenerator[str,\
    \ None]: An asynchronous generator yielding strings representing streamed search\
    \ results.\nRaises:\n    NotImplementedError: Subclasses must implement this method."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 92
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_source_ids
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: _unpack_source_ids
  signature: 'def _unpack_source_ids(data: Mapping) -> list[str]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Unpack source_id values from a mapping.\n\nArgs:\n    data (Mapping):\
    \ A mapping that may contain the key \"source_id\" whose value is a string of\
    \ IDs separated by comma-space.\n\nReturns:\n    list[str]: The list of source\
    \ IDs. If the key is missing or the value is None, returns an empty list."
  code_example: null
  example_source: null
  line_start: 298
  line_end: 300
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_llm_new_token
  file: graphrag/callbacks/query_callbacks.py
  name: on_llm_new_token
  signature: def on_llm_new_token(self, token) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n   \
    \     The new token generated by the LLM.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 32
  line_end: 33
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/create_community_reports.py::_prep_claims
  file: graphrag/index/workflows/create_community_reports.py
  name: _prep_claims
  signature: 'def _prep_claims(input: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Prepare claims data by filling missing descriptions and constructing\
    \ the CLAIM_DETAILS field.\n\nArgs:\n    input: The input DataFrame containing\
    \ claims data. Missing DESCRIPTION values are filled with \"No Description\" and\
    \ a new CLAIM_DETAILS column is created from SHORT_ID, CLAIM_SUBJECT, TYPE, CLAIM_STATUS,\
    \ and DESCRIPTION.\n\nReturns:\n    pd.DataFrame: The input DataFrame augmented\
    \ with a CLAIM_DETAILS column and with missing DESCRIPTION filled as \"No Description\"\
    ."
  code_example: null
  example_source: null
  line_start: 180
  line_end: 196
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
- node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_get_parameters
  file: graphrag/language_model/providers/litellm/get_cache_key.py
  name: _get_parameters
  signature: "def _get_parameters(\n    model_config: \"LanguageModelConfig\",\n \
    \   **kwargs: Any,\n) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Pluck out the parameters that define a cache key.\n\nUse the same parameters\
    \ as fnllm except request timeout.\n- embeddings: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/embeddings/parameters.py#L12\n\
    - chat: https://github.com/microsoft/essex-toolkit/blob/main/python/fnllm/fnllm/openai/types/chat/parameters.py#L25\n\
    \nArgs:\n    model_config: The configuration of the language model.\n    kwargs:\
    \ Additional model input parameters.\n\nReturns:\n    dict[str, Any]: A dictionary\
    \ of parameters that define the cache key."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 135
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
- node_id: unified-search-app/app/app_logic.py::dataset_name
  file: unified-search-app/app/app_logic.py
  name: dataset_name
  signature: 'def dataset_name(key: str, sv: SessionVariables) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get dataset name.\n\nArgs:\n    key: The dataset key to look up.\n \
    \   sv: SessionVariables containing dataset information; sv.datasets.value is\
    \ an iterable of objects with key and name attributes.\n\nReturns:\n    The name\
    \ of the dataset whose key matches the provided key.\n\nRaises:\n    AttributeError:\
    \ If no dataset with the given key is found, since the implementation accesses\
    \ .name on None."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 65
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/get_cache_key.py::_hash
  file: graphrag/language_model/providers/litellm/get_cache_key.py
  name: _hash
  signature: 'def _hash(input: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Generate a SHA-256 hash for the input string.\n\nArgs:\n    input: str\
    \ - the input string to hash\n\nReturns:\n    str - hexadecimal digest of the\
    \ SHA-256 hash of the input\n\nRaises:\n    AttributeError - if the input object\
    \ does not support the encode() method"
  code_example: null
  example_source: null
  line_start: 138
  line_end: 140
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.__init__
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Initialize a new WorkflowCallbacksManager.


    This manager holds registered WorkflowCallbacks instances in the _callbacks list
    and dispatches relevant lifecycle events to them (pipeline_start, pipeline_end,
    workflow_start, workflow_end, progress). The _callbacks list is initialized to
    an empty list during construction.


    Returns: None (implicit)'
  code_example: null
  example_source: null
  line_start: 16
  line_end: 18
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed_batch
  file: tests/mock_provider.py
  name: aembed_batch
  signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
    \    ) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n    text_list:\
    \ A batch of text inputs to generate embeddings for.\n    **kwargs: Additional\
    \ keyword arguments (e.g., model parameters).\n\nReturns:\n    list[list[float]]:\
    \ A batch of embeddings corresponding to the input texts."
  code_example: null
  example_source: null
  line_start: 119
  line_end: 125
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_summarize_descriptions
  file: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py
  name: run_summarize_descriptions
  signature: "def run_summarize_descriptions(\n    model: ChatModel,\n    id: str\
    \ | tuple[str, str],\n    descriptions: list[str],\n    args: StrategyConfig,\n\
    ) -> SummarizedDescriptionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the entity extraction chain to summarize descriptions for graph\
    \ intelligence.\n\nArgs:\n    model: ChatModel\n        The chat model instance\
    \ used to invoke summarization.\n    id: str | tuple[str, str]\n        Identifier\
    \ for the target item; could be a string or a pair of strings.\n    descriptions:\
    \ list[str]\n        The descriptions to summarize.\n    args: StrategyConfig\n\
    \        Strategy configuration, including max_input_tokens, max_summary_length,\
    \ and optional summarize_prompt.\n\nReturns:\n    SummarizedDescriptionResult\n\
    \        The summarized description along with its identifier.\n\nRaises:\n  \
    \  Exception\n        If the underlying extraction process raises an exception\
    \ during processing."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 65
  dependencies:
  - graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor
  - graphrag/index/operations/summarize_descriptions/typing.py::SummarizedDescriptionResult
  called_by:
  - graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_graph_intelligence
- node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext._map_ids
  file: graphrag/query/structured_search/basic_search/basic_context.py
  name: _map_ids
  signature: def _map_ids(self) -> dict[str, str]
  decorators: []
  raises: []
  visibility: protected
  docstring: "Map id to short_id in the text units.\n\nArgs:\n    self (object): The\
    \ instance containing the text_units attribute used to build the mapping.\n\n\
    Returns:\n    dict[str, str]: A mapping from each text unit's id to its short_id.\n\
    \nRaises:\n    AttributeError: If any text unit is missing 'id' or 'short_id'\
    \ attributes."
  code_example: null
  example_source: null
  line_start: 107
  line_end: 113
  dependencies: []
  called_by: []
- node_id: graphrag/logger/factory.py::LoggerFactory.create_logger
  file: graphrag/logger/factory.py
  name: create_logger
  signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create a logger handler for the requested type using the built-in registry.\n\
    \nThis method looks up the given reporting_type in the internal registry and invokes\
    \ the registered\ncreator with the provided kwargs to create and return a logging.Handler\
    \ instance.\n\nArgs:\n    reporting_type (str): The type identifier of the logger/handler\
    \ to create.\n    kwargs (dict): Keyword arguments forwarded to the registered\
    \ creator to configure the handler.\n\nReturns:\n    logging.Handler: The configured\
    \ handler instance for the requested type.\n\nRaises:\n    ValueError: If the\
    \ reporting_type is not registered in the registry."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.encode
  file: graphrag/tokenizer/tiktoken_tokenizer.py
  name: encode
  signature: 'def encode(self, text: str) -> list[int]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Encode the given text into a list of tokens.\n\nArgs:\n    text (str):\
    \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
    \ the encoded text."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::_get_context_string
  file: graphrag/index/operations/summarize_communities/graph_context/sort_context.py
  name: _get_context_string
  signature: "def _get_context_string(\n        entities: list[dict],\n        edges:\
    \ list[dict],\n        claims: list[dict],\n        sub_community_reports: list[dict]\
    \ | None = None,\n    ) -> str"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Concatenate structured data into a context string.\n\nArgs:\n    entities:\
    \ List of entity dictionaries to include in the context.\n    edges: List of edge/relationship\
    \ dictionaries to include in the context.\n    claims: List of claim dictionaries\
    \ to include in the context.\n    sub_community_reports: Optional list of dictionaries\
    \ for sub-community reports to include at the top.\n\nReturns:\n    str: The concatenated\
    \ context string with optional reports and sections for Entities, Claims, and\
    \ Relationships, formatted as CSV blocks."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 54
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
- node_id: graphrag/language_model/manager.py::ModelManager.list_chat_models
  file: graphrag/language_model/manager.py
  name: list_chat_models
  signature: def list_chat_models(self) -> dict[str, ChatModel]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a copy of all registered ChatModel instances.\n\nReturns:\n \
    \   dict[str, ChatModel]: A dictionary mapping model names to ChatModel instances."
  code_example: null
  example_source: null
  line_start: 147
  line_end: 149
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_end
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: workflow_end
  signature: 'def workflow_end(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
    \        The name of the workflow.\n    instance: object\n        The workflow\
    \ instance object.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 42
  line_end: 46
  dependencies: []
  called_by: []
- node_id: graphrag/factory/factory.py::Factory.__contains__
  file: graphrag/factory/factory.py
  name: __contains__
  signature: 'def __contains__(self, strategy: str) -> bool'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Check if a strategy is registered.\n\nArgs:\n    strategy: str\
    \ The name of the strategy.\n\nReturns:\n    bool: True if the strategy is registered,\
    \ False otherwise.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 29
  line_end: 31
  dependencies: []
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get_creation_date
  file: graphrag/storage/pipeline_storage.py
  name: get_creation_date
  signature: 'def get_creation_date(self, key: str) -> str'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Get the creation date for the given key.\n\nArgs:\n    key (str): The\
    \ key for which to retrieve the creation date.\n\nReturns:\n    str: The creation\
    \ date as a string."
  code_example: null
  example_source: null
  line_start: 83
  line_end: 92
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.get
  file: graphrag/language_model/providers/fnllm/cache.py
  name: get
  signature: 'def get(self, key: str) -> Any | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous retrieval of a value from the underlying cache.\n\nThis\
    \ method is part of the FNLLMCacheProvider and delegates to the underlying cache\
    \ (self._cache).\n\nArgs:\n    key: The key to retrieve from the cache.\n\nReturns:\n\
    \    Any | None: The value associated with the key, or None if not present.\n\n\
    Notes:\n    Exceptions may propagate from the underlying cache."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 25
  dependencies: []
  called_by: []
- node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.generate
  file: graphrag/query/question_gen/base.py
  name: generate
  signature: "def generate(\n        self,\n        question_history: list[str],\n\
    \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
    \    ) -> QuestionResult"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Generate questions.\n\nArgs:\n    question_history: History of previously\
    \ generated questions.\n    context_data: Optional context data used to influence\
    \ generation; None if unavailable.\n    question_count: Number of questions to\
    \ generate.\n    kwargs: Additional keyword arguments for extensibility.\n\nReturns:\n\
    \    QuestionResult: The generated results including the response list, context_data,\
    \ completion_time, llm_calls, and prompt_tokens.\n\nRaises:\n    NotImplementedError:\
    \ If the method is not implemented by a subclass."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 55
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/strategies.py::encode
  file: graphrag/index/operations/chunk_text/strategies.py
  name: encode
  signature: 'def encode(text: str) -> list[int]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Encode the input text into token IDs using the configured encoding model.\n\
    \nArgs:\n    text (str): The input to encode. If not a string, it will be converted\
    \ to a string.\n\nReturns:\n    list[int]: The encoded token IDs produced by the\
    \ encoding model.\n\nRaises:\n    Exception: If encoding fails with the configured\
    \ encoding model."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_factories
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_factories
  signature: def _validate_factories(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Validate the factories used in the configuration.\n\nArgs:\n    self:\
    \ The GraphRagConfig instance.\n\nReturns:\n    None: This method does not return\
    \ a value.\n\nRaises:\n    Exception: If validation fails in the underlying retry\
    \ or rate limiter validations."
  code_example: null
  example_source: null
  line_start: 350
  line_end: 353
  dependencies:
  - graphrag/config/models/graph_rag_config.py::_validate_rate_limiter_services
  - graphrag/config/models/graph_rag_config.py::_validate_retry_services
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed_batch
  file: tests/integration/language_model/test_factory.py
  name: aembed_batch
  signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
    \        ) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously compute embeddings for a batch of input texts.\n\nArgs:\n\
    \    text_list: A batch of text inputs to generate embeddings for.\n\nReturns:\n\
    \    list[list[float]]: A batch of embeddings corresponding to the input texts."
  code_example: null
  example_source: null
  line_start: 76
  line_end: 79
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/storage_config.py::StorageConfig.validate_base_dir
  file: graphrag/config/models/storage_config.py
  name: validate_base_dir
  signature: def validate_base_dir(cls, value, info)
  decorators:
  - '@field_validator("base_dir", mode="before")'
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Normalize base_dir to a filesystem path string for local storage.\n\n\
    This validator does not verify that the path exists or is valid beyond conversion\
    \ to a string. It only performs normalization when the storage type is local (StorageType.file);\
    \ for all other storage types, the input value is returned unchanged.\n\nArgs:\n\
    \    cls (type): The class that defines the validator.\n    value (Any): The input\
    \ base_dir value to normalize.\n    info (object): Validation context containing\
    \ other field values, including the 'type' field.\n\nReturns:\n    str: The normalized\
    \ path string when using local storage; otherwise returns the input value.\n\n\
    Raises:\n    None"
  code_example: null
  example_source: null
  line_start: 30
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dataframes.py::select
  file: graphrag/index/utils/dataframes.py
  name: select
  signature: 'def select(df: pd.DataFrame, *columns: str) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Select columns from a DataFrame.\n\nArgs:\n    df: The DataFrame to\
    \ select columns from.\n    columns: The names of the columns to select from df.\n\
    \nReturns:\n    pd.DataFrame: A DataFrame containing only the specified columns,\
    \ in the order provided.\n\nRaises:\n    KeyError: If any of the provided column\
    \ names do not exist in df."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 53
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
- node_id: graphrag/language_model/manager.py::ModelManager.remove_chat
  file: graphrag/language_model/manager.py
  name: remove_chat
  signature: 'def remove_chat(self, name: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Remove the ChatLLM instance registered under the given name.\n\nArgs:\n\
    \    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 139
  line_end: 141
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_container
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _delete_container
  signature: def _delete_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Delete the container with the current container name if it exists.\n\
    \nArgs:\n    self: Any. The storage instance.\n\nReturns:\n    None: This method\
    \ does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError: If the\
    \ container to delete does not exist."
  code_example: null
  example_source: null
  line_start: 113
  line_end: 118
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__init__
  file: unified-search-app/app/state/session_variable.py
  name: __init__
  signature: 'def __init__(self, default: Any = "", prefix: str = "")'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create a managed session variable with a default value and an optional\
    \ prefix.\n\nThe prefix is used to avoid collisions between variables with the\
    \ same name.\n\nArgs:\n    default: The initial/default value for the session\
    \ variable.\n    prefix: Optional prefix to prepend to the key to differentiate\
    \ this variable from others with the same name.\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 15
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.add_turn
  file: graphrag/query/context_builder/conversation_history.py
  name: add_turn
  signature: 'def add_turn(self, role: ConversationRole, content: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Add a new turn to the conversation history.\n\nArgs:\n    role (ConversationRole):\
    \ The role for the new turn.\n    content (str): The content of the turn.\n\n\
    Returns:\n    None"
  code_example: null
  example_source: null
  line_start: 119
  line_end: 121
  dependencies: []
  called_by: []
- node_id: graphrag/data_model/text_unit.py::TextUnit.from_dict
  file: graphrag/data_model/text_unit.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n        text_key:\
    \ str = \"text\",\n        entities_key: str = \"entity_ids\",\n        relationships_key:\
    \ str = \"relationship_ids\",\n        covariates_key: str = \"covariate_ids\"\
    ,\n        n_tokens_key: str = \"n_tokens\",\n        document_ids_key: str =\
    \ \"document_ids\",\n        attributes_key: str = \"attributes\",\n    ) -> \"\
    TextUnit\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new TextUnit from the dict data.\n\nArgs:\n    cls: The class.\n\
    \    d (dict[str, Any]): The source dictionary containing the values for the TextUnit\
    \ fields.\n    id_key (str): Key in d for the text unit's identifier. Defaults\
    \ to \"id\".\n    short_id_key (str): Key in d for the optional short identifier.\
    \ Defaults to \"human_readable_id\".\n    text_key (str): Key in d for the text\
    \ content. Defaults to \"text\".\n    entities_key (str): Key in d for the related\
    \ entity IDs. Defaults to \"entity_ids\".\n    relationships_key (str): Key in\
    \ d for the related relationship IDs. Defaults to \"relationship_ids\".\n    covariates_key\
    \ (str): Key in d for covariate IDs. Defaults to \"covariate_ids\".\n    n_tokens_key\
    \ (str): Key in d for the number of tokens. Defaults to \"n_tokens\".\n    document_ids_key\
    \ (str): Key in d for the document IDs. Defaults to \"document_ids\".\n    attributes_key\
    \ (str): Key in d for additional attributes. Defaults to \"attributes\".\n\nReturns:\n\
    \    TextUnit: A new TextUnit instance populated with values from d.\n\nRaises:\n\
    \    KeyError: If id_key is not present in d."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 62
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.connect
  file: graphrag/vector_stores/base.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> None'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
    \ arguments for connecting to the vector storage.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 63
  line_end: 64
  dependencies: []
  called_by: []
- node_id: graphrag/storage/factory.py::StorageFactory.create_storage
  file: graphrag/storage/factory.py
  name: create_storage
  signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create a storage object from the provided type.\n\nArgs:\n    storage_type:\
    \ The type of storage to create.\n    kwargs: Additional keyword arguments for\
    \ the storage constructor.\n\nReturns:\n    A PipelineStorage instance.\n\nRaises:\n\
    \    ValueError: If the storage type is not registered."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 66
  dependencies: []
  called_by: []
- node_id: graphrag/cli/main.py::wildcard_match
  file: graphrag/cli/main.py
  name: wildcard_match
  signature: 'def wildcard_match(string: str, pattern: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Determine whether the entire string matches a wildcard pattern.\nThe\
    \ pattern uses ? to match any single character and * to match any sequence of\
    \ characters.\n\nArgs:\n    string: The input string to test against the pattern.\n\
    \    pattern: The wildcard pattern, where ? matches any single character and *\
    \ matches any sequence of characters.\n\nReturns:\n    bool: True if the string\
    \ matches the wildcard pattern, otherwise False.\n\nRaises:\n    TypeError: If\
    \ string or pattern are not of type str."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 41
  dependencies: []
  called_by:
  - graphrag/cli/main.py::completer
- node_id: graphrag/index/utils/is_null.py::is_nan
  file: graphrag/index/utils/is_null.py
  name: is_nan
  signature: def is_nan() -> bool
  decorators: []
  raises: []
  visibility: public
  docstring: "Check if value is NaN.\n\nReturns:\n    bool: True if value is a float\
    \ and NaN, otherwise False."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 17
  dependencies: []
  called_by:
  - graphrag/index/utils/is_null.py::is_null
- node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat
  file: tests/integration/language_model/test_factory.py
  name: chat
  signature: "def chat(\n            self, prompt: str, history: list | None = None,\
    \ **kwargs: Any\n        ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Process a chat prompt and return the corresponding model response.\n\
    \nArgs:\n    self: The instance of the class that contains this chat method.\n\
    \    prompt (str): The prompt to send to the model.\n    history (list | None):\
    \ Optional chat history to provide context for the model.\n    **kwargs: Additional\
    \ keyword arguments passed to the underlying model.\n\nReturns:\n    ModelResponse:\
    \ The model response object. In this implementation, it is a BaseModelResponse\n\
    \        containing a BaseModelOutput with content set to \"content\" and full_response\
    \ set to {\"key\": \"value\"}."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 40
  dependencies:
  - graphrag/language_model/response/base.py::BaseModelOutput
  - graphrag/language_model/response/base.py::BaseModelResponse
  called_by: []
- node_id: tests/integration/logging/test_standard_logging.py::test_standard_logging
  file: tests/integration/logging/test_standard_logging.py
  name: test_standard_logging
  signature: def test_standard_logging()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that standard logging returns a logger whose name matches the requested\
    \ value 'graphrag.test' by asserting logger.name == 'graphrag.test'.\n\nArgs:\n\
    \    none: This test takes no parameters.\n\nReturns:\n    None: This test returns\
    \ no value.\n\nRaises:\n    AssertionError: If the logger name does not equal\
    \ 'graphrag.test'."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 17
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.encode
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: encode
  signature: def encode(self, text)
  decorators: []
  raises: []
  visibility: public
  docstring: "Encode the input text as a list of Unicode code points.\n\nArgs:\n \
    \   text: str\n        The input text to encode as Unicode code points.\n\nReturns:\n\
    \    list[int]\n        A list of integers where each integer is the Unicode code\
    \ point of the\n        corresponding character in text.\n\nRaises:\n    TypeError:\n\
    \        If text is None or not iterable."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 28
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat_stream
  file: graphrag/language_model/providers/fnllm/models.py
  name: chat_stream
  signature: "def chat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs\n    ) -> Generator[str, None]"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "\"\"\"\nStream Chat with the Model asynchronously using the given prompt.\n\
    \nThis is an asynchronous generator that streams chunks of the model's response\
    \ as they become available. Each yielded value is a non-None string from response.output.content.\n\
    \nArgs:\n    prompt (str): The prompt to chat with.\n    history (list[str] |\
    \ None): Optional history to pass to the Model. If provided, the model will consider\
    \ this history when generating streamed output.\n    kwargs: Additional keyword\
    \ arguments to pass to the Model. (type: dict[str, Any])\n\nReturns:\n    AsyncGenerator[str,\
    \ None]: An asynchronous generator yielding the streamed response chunks as non-None\
    \ strings.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 133
  line_end: 148
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_attribute
  file: graphrag/query/input/retrieval/entities.py
  name: get_entity_by_attribute
  signature: "def get_entity_by_attribute(\n    entities: Iterable[Entity], attribute_name:\
    \ str, attribute_value: Any\n) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get entities by attribute.\n\nArgs:\n    entities: Iterable[Entity],\
    \ the collection of entities to search.\n    attribute_name: str, the name of\
    \ the attribute to match.\n    attribute_value: Any, the value to match for the\
    \ given attribute.\n\nReturns:\n    list[Entity], a list of entities whose attributes\
    \ contain attribute_name with the given attribute_value.\n\nRaises:\n    TypeError:\
    \ if entities is not an iterable.\n    AttributeError: if an element in entities\
    \ does not have an attributes attribute."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 54
  dependencies: []
  called_by: []
- node_id: graphrag/config/errors.py::ApiKeyMissingError.__init__
  file: graphrag/config/errors.py
  name: __init__
  signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None = None)
    -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Init method for ApiKeyMissingError (internal API).\n\nArgs:\n    llm_type:\
    \ The LLM type for which the API Key is required.\n    azure_auth_type: Optional\
    \ Azure authentication type; if provided, include in the message.\n\nReturns:\n\
    \    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 9
  line_end: 15
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.__init__
  file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        max_retries: int =\
    \ 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n       \
    \ **kwargs: Any,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize a LiteLLM Exponential Retry Service with retry configuration.\n\
    \nArgs:\n    max_retries: The maximum number of retry attempts (int). Must be\
    \ greater than 0.\n    base_delay: The base delay between retries in seconds (float).\
    \ Must be greater than 1.0.\n    jitter: Whether to apply jitter to the delay\
    \ (bool).\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n    None\n\
    \nRaises:\n    ValueError: max_retries must be greater than 0.\n    ValueError:\
    \ base_delay must be greater than 1.0."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 39
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_customization
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: test_vector_store_customization
  signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
    \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
    \    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Test vector store customization with Azure AI Search.\n\nArgs:\n   \
    \ self: The test case instance.\n    vector_store_custom: Custom Azure AI Search\
    \ vector store used in the test.\n    sample_documents: Documents used to load\
    \ into the vector store.\n    mock_search_client: Mock for the Azure AI Search\
    \ search client.\n    mock_index_client: Mock for the Azure AI Search index client.\n\
    \nReturns:\n    None."
  code_example: null
  example_source: null
  line_start: 176
  line_end: 243
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey.from_string
  file: graphrag/query/context_builder/entity_extraction.py
  name: from_string
  signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
  decorators:
  - '@staticmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Convert string to EntityVectorStoreKey.\n\nArgs:\n    value: str.\
    \ The string key to convert. Expected to be \"id\" or \"title\".\n\nReturns:\n\
    \    EntityVectorStoreKey: The corresponding enum member (EntityVectorStoreKey.ID\
    \ for \"id\", EntityVectorStoreKey.TITLE for \"title\").\n\nRaises:\n    ValueError:\
    \ If value is not a valid EntityVectorStoreKey (i.e., not \"id\" or \"title\"\
    ).\n\"\"\""
  code_example: null
  example_source: null
  line_start: 26
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.create_vector_store
  file: graphrag/vector_stores/factory.py
  name: create_vector_store
  signature: "def create_vector_store(\n        cls,\n        vector_store_type: str,\n\
    \        vector_store_schema_config: VectorStoreSchemaConfig,\n        **kwargs:\
    \ dict,\n    ) -> BaseVectorStore"
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create a vector store object from the provided type via a registry lookup.\n\
    \nThis function looks up the registered vector store implementation by vector_store_type\
    \ and instantiates it by passing vector_store_schema_config and any additional\
    \ keyword arguments to the concrete vector store constructor. The concrete vector\
    \ store may require or accept additional kwargs; these are forwarded via kwargs.\n\
    \nArgs:\n    vector_store_type (str): The type identifier for the vector store\
    \ to create.\n    vector_store_schema_config (VectorStoreSchemaConfig): Configuration\
    \ describing the vector store schema; it is forwarded to the concrete vector store\
    \ implementation.\n    **kwargs: Additional keyword arguments for the concrete\
    \ vector store constructor.\n\nReturns:\n    BaseVectorStore: A vector store instance.\n\
    \nRaises:\n    ValueError: If the vector store type is not registered."
  code_example: null
  example_source: null
  line_start: 52
  line_end: 78
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: read
  signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
    \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Read method for a datasource.\n\nArgs:\n    table: str - The name of\
    \ the table to read from.\n    throw_on_missing: bool - If True, raise an error\
    \ when the table is missing; otherwise, handle gracefully.\n    columns: list[str]\
    \ | None - Specific columns to read from the table, or None to read all columns.\n\
    \nReturns:\n    pd.DataFrame - The data read from the specified table.\n\nRaises:\n\
    \    NotImplementedError - If the method is not implemented."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 40
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.init_local_context_builder
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: init_local_context_builder
  signature: def init_local_context_builder(self) -> LocalSearchMixedContext
  decorators: []
  raises: []
  visibility: public
  docstring: "Initialize and return the local search mixed context built from the\
    \ current DRIFT context attributes.\n\nArgs:\n    self: The DRIFT drift context\
    \ builder instance.\n\nReturns:\n    LocalSearchMixedContext: The initialized\
    \ local context constructed from the builder's state, including:\n      - community_reports\n\
    \      - text_units\n      - entities\n      - relationships\n      - covariates\n\
    \      - entity_text_embeddings\n      - embedding_vectorstore_key\n      - text_embedder\n\
    \      - tokenizer\n\nRaises:\n    None\n\nExamples:\n    context = self.init_local_context_builder()"
  code_example: null
  example_source: null
  line_start: 80
  line_end: 98
  dependencies:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_parquet
  file: graphrag/storage/blob_pipeline_storage.py
  name: _set_df_parquet
  signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Set a parquet dataframe.\n\nStores the provided dataframe as a\
    \ Parquet file at the path derived from the key. If a storage account name is\
    \ configured and no connection string is provided, the Parquet file is written\
    \ using Azure storage options with the storage account name and DefaultAzureCredential;\
    \ otherwise, the Parquet file is written using the provided connection string.\n\
    \nArgs:\n    key: The key under which to store the parquet export of the dataframe.\n\
    \    dataframe: The dataframe to serialize and store.\n\nReturns:\n    None\n\n\
    Raises:\n    Exceptions raised by the dataframe serialization or the storage write\
    \ operations.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 236
  line_end: 250
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_abfs_url
  called_by: []
- node_id: tests/verbs/test_pipeline_state.py::run_workflow_1
  file: tests/verbs/test_pipeline_state.py
  name: run_workflow_1
  signature: "def run_workflow_1(  # noqa: RUF029\n    _config: GraphRagConfig, context:\
    \ PipelineRunContext\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Async function that initializes the pipeline state by setting context.state[\"\
    count\"] to 1 and returns a WorkflowFunctionOutput with result=None.\n\nArgs:\n\
    \    _config: Configuration for Graphrag configuration\n    context: The PipelineRunContext\
    \ for the current run; its state is updated by this function\n\nReturns:\n   \
    \ WorkflowFunctionOutput: The function output; the result is None in this implementation"
  code_example: null
  example_source: null
  line_start: 15
  line_end: 19
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.retry
  file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  name: retry
  signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry a synchronous function using exponential backoff.\n\nRetries the\
    \ provided function on failure up to the configured max_retries with an exponential\
    \ backoff delay. The initial delay is 1.0 second and increases by the base_delay\
    \ factor; if jitter is enabled, a small random amount is added.\n\nArgs:\n   \
    \ func: Callable[..., Any] - The function to invoke. It will be called as func(**kwargs)\
    \ and its return value will be returned on success.\n    kwargs: Any - Keyword\
    \ arguments to pass to func when calling it.\n\nReturns:\n    Any: The value returned\
    \ by func when it succeeds.\n\nRaises:\n    Exception: The last exception raised\
    \ by func when the maximum number of retries is exceeded."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 59
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_model
  file: graphrag/config/models/vector_store_schema_config.py
  name: _validate_model
  signature: def _validate_model(self)
  decorators:
  - '@model_validator(mode="after")'
  raises: []
  visibility: protected
  docstring: "Validate the model after the initial schema validation.\n\nArgs:\n \
    \   self: The instance being validated.\n\nReturns:\n    The instance after validation.\n\
    \nRaises:\n    ValueError: If an unsafe or invalid field name is encountered during\
    \ validation."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 66
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::_validate_schema
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_aembedding
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: _base_aembedding
  signature: 'def _base_aembedding(**kwargs: Any) -> EmbeddingResponse'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Base asynchronous embedding wrapper that forwards to litellm.aembedding\
    \ with merged base arguments.\n\nArgs\n    kwargs: Any\n        Additional keyword\
    \ arguments to pass to the underlying aembedding call. The keys are merged with\
    \ base_args, and the key \"name\" is removed from the resulting arguments if present\
    \ before invocation.\n\nReturns\n    EmbeddingResponse\n        The embedding\
    \ response produced by aembedding, obtained by awaiting the underlying call with\
    \ the merged arguments.\n\nRaises\n    Exception: Exceptions raised by aembedding\
    \ may be propagated."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 95
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
  file: graphrag/query/input/retrieval/relationships.py
  name: sort_relationships_by_rank
  signature: "def sort_relationships_by_rank(\n    relationships: list[Relationship],\n\
    \    ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Sort relationships by a ranking_attribute.\n\nThis function sorts the\
    \ provided list of Relationship objects in descending order\nbased on a ranking\
    \ attribute. If the ranking_attribute exists as a key in a\nRelationship's attributes\
    \ dictionary, its value is used (converted to int)\nfor sorting. If not, and ranking_attribute\
    \ is \"rank\" or \"weight\", the\ncorresponding attributes on the Relationship\
    \ are used (with a default of 0\nor 0.0 when missing). The input list is sorted\
    \ in place and returned. If the\ninput list is empty, it is returned unchanged.\n\
    \nArgs:\n    relationships: List of Relationship objects to be sorted.\n    ranking_attribute:\
    \ Attribute name used for ranking. May be a key in each\n        Relationship's\
    \ attributes, or one of \"rank\" or \"weight\". Defaults to\n        \"rank\"\
    .\n\nReturns:\n    list[Relationship]: The same list object, now sorted in descending\
    \ order by the\n        chosen ranking attribute.\n\nRaises:\n    ValueError:\
    \ If a ranking_attribute value exists in a Relationship.attributes\n        dictionary\
    \ but cannot be converted to int when ranking_attribute is present\n        in\
    \ attribute_names."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 102
  dependencies: []
  called_by:
  - graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
  - graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
- node_id: tests/unit/litellm_services/utils.py::assert_stagger
  file: tests/unit/litellm_services/utils.py
  name: assert_stagger
  signature: 'def assert_stagger(time_values: list[float], stagger: float)'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Assert that consecutive time values are at least the specified\
    \ stagger apart.\n\nArgs:\n    time_values (list[float]): Sequence of time values.\n\
    \    stagger (float): Minimum allowed difference between consecutive values.\n\
    \nReturns:\n    None: This function does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any consecutive pair of time values is closer than stagger.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 34
  line_end: 37
  dependencies: []
  called_by:
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
  - tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
- node_id: tests/integration/storage/test_cosmosdb_storage.py::test_get_creation_date
  file: tests/integration/storage/test_cosmosdb_storage.py
  name: test_get_creation_date
  signature: def test_get_creation_date()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that CosmosDBPipelineStorage.get_creation_date returns a correctly
    formatted creation timestamp.


    This test creates a storage, writes a JSON file, retrieves its creation date using
    get_creation_date, and asserts that the returned string matches the expected format
    "%Y-%m-%d %H:%M:%S %z" when parsed as a datetime.


    Returns:

    None

    Type: None'
  code_example: null
  example_source: null
  line_start: 113
  line_end: 133
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  called_by: []
- node_id: graphrag/prompt_tune/generator/persona.py::generate_persona
  file: graphrag/prompt_tune/generator/persona.py
  name: generate_persona
  signature: "def generate_persona(\n    model: ChatModel, domain: str, task: str\
    \ = DEFAULT_TASK\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Generate an LLM persona to use for GraphRAG prompts.\n\nArgs:\n\
    \    model (ChatModel): The LLM to use for generation\n    domain (str): The domain\
    \ to generate a persona for\n    task (str): The task to generate a persona for.\
    \ Default is DEFAULT_TASK\n\nReturns:\n    str: The generated persona string\n\
    \nRaises:\n    Exception: If the underlying model call fails\n\"\"\""
  code_example: null
  example_source: null
  line_start: 11
  line_end: 27
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: tests/integration/vector_stores/test_cosmosdb.py::test_clear
  file: tests/integration/vector_stores/test_cosmosdb.py
  name: test_clear
  signature: def test_clear()
  decorators: []
  raises: []
  visibility: public
  docstring: '"""Test clearing the vector store.


    Initializes a CosmosDBVectorStore with index_name "testclear", connects to the
    Cosmos DB instance using WELL_KNOWN_COSMOS_CONNECTION_STRING and database_name
    "testclear", loads a VectorStoreDocument, verifies it can be retrieved by its
    id, clears the store, and asserts that the underlying database no longer exists
    via _database_exists().


    Returns:

    None

    """'
  code_example: null
  example_source: null
  line_start: 79
  line_end: 105
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
  called_by: []
- node_id: graphrag/language_model/response/base.py::ModelResponse.output
  file: graphrag/language_model/response/base.py
  name: output
  signature: def output(self) -> ModelOutput
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the output of the response. This is a property-like member of\
    \ the response object (no parentheses needed).\n\nReturns:\n    ModelOutput: The\
    \ output associated with this response. The returned object provides:\n      \
    \  content: str - The textual content of the output.\n        full_response: dict[str,\
    \ Any] | None - The complete JSON response from the LLM provider, or None if not\
    \ available.\n\nNotes:\n    - The ModelOutput is always produced; accessing this\
    \ property does not raise exceptions in normal operation.\n    - If content is\
    \ empty, the content field may be an empty string."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 33
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.find
  file: graphrag/storage/blob_pipeline_storage.py
  name: find
  signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n \
    \       base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
    \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Find blobs in a container using a file pattern, as well as a custom\
    \ filter function.\n\nArgs:\n    base_dir: The name of the base container.\n \
    \   file_pattern: The file pattern to use.\n    file_filter: A dictionary of key-value\
    \ pairs to filter the blobs.\n    max_count: The maximum number of blobs to return.\
    \ If -1, all blobs are returned.\n\nReturns:\n    An iterator of blob names and\
    \ their corresponding regex matches."
  code_example: null
  example_source: null
  line_start: 100
  line_end: 176
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_blobname
  - graphrag/storage/blob_pipeline_storage.py::item_filter
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.search_by_id
  file: graphrag/vector_stores/lancedb.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators: []
  raises: []
  visibility: public
  docstring: "Search for a document by id.\n\nArgs:\n    id: The identifier of the\
    \ document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document corresponding\
    \ to the provided id. If a matching document is found, its id, text, vector, and\
    \ attributes are populated; otherwise a VectorStoreDocument with id set to the\
    \ provided id and text=None, vector=None is returned."
  code_example: null
  example_source: null
  line_start: 162
  line_end: 176
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_communities_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_communities_data
  signature: "def get_communities_data(\n    _datasource: Datasource,\n) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "\"\"\"Return a dataframe with communities data from the indexed-data.\n\
    \nArgs:\n    _datasource: Datasource to read the communities data from the indexed-data.\n\
    \nReturns:\n    A dataframe with communities data loaded from the indexed-data.\n\
    \nRaises:\n    Exception: If the underlying data source read operation fails.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 71
  line_end: 75
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dataframes.py::drop_columns
  file: graphrag/index/utils/dataframes.py
  name: drop_columns
  signature: 'def drop_columns(df: pd.DataFrame, *column: str) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Drop specified columns from a DataFrame.\n\nArgs:\n    df (pd.DataFrame):\
    \ The DataFrame from which to drop columns.\n    column (str): One or more column\
    \ names to drop from the DataFrame.\n\nReturns:\n    pd.DataFrame: The DataFrame\
    \ with the specified columns dropped.\n\nRaises:\n    KeyError: If any of the\
    \ specified column names do not exist in df."
  code_example: null
  example_source: null
  line_start: 13
  line_end: 15
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
- node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions_with_llm
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: _summarize_descriptions_with_llm
  signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
    \ str] | list[str], descriptions: list[str]\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Summarize descriptions using a large language model (LLM).\n\nArgs:\n\
    \    id: str | tuple[str, str] | list[str] - Identifier(s) for the entity or entities.\n\
    \    descriptions: list[str] - Descriptions to be summarized.\n\nReturns:\n  \
    \  str - The summarized descriptions as a string.\n\nRaises:\n    Exception -\
    \ If the underlying LLM call fails or processing encounters an error."
  code_example: null
  example_source: null
  line_start: 118
  line_end: 133
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed
  file: graphrag/language_model/providers/fnllm/models.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Embed the given text using the Model.\n\nArgs:\n    text: The text to\
    \ embed.\n    kwargs: Additional arguments to pass to the Model.\n\nReturns:\n\
    \    The embeddings of the text.\n\nRaises:\n    ValueError: If no embeddings\
    \ are found in the response."
  code_example: null
  example_source: null
  line_start: 398
  line_end: 415
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
  file: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py
  name: is_compound
  signature: 'def is_compound(tokens: list[str]) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return True if any token in the provided list is a hyphenated compound\
    \ token.\n\nArgs:\n    tokens: List[str] - The list of tokens to inspect.\n\n\
    Returns:\n    bool - True if at least one token contains a hyphen, has length\
    \ greater than 1 after stripping whitespace, and splits into more than one part\
    \ when split by hyphen; otherwise False.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 7
  line_end: 12
  dependencies: []
  called_by:
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
  - graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
- node_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder.build_context
  file: graphrag/query/context_builder/builders.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
    \    ) -> tuple[pd.DataFrame, dict[str, int]]"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Build the context used to prime subsequent search actions for the given\
    \ query.\n\nThis asynchronous method constructs a DataFrame of contextual items\
    \ and a metrics dictionary\nthat can be used to warm up or seed downstream DRIFT\
    \ search processes.\n\nArgs:\n    self: The instance of the class.\n    query\
    \ (str): The search query for which to build the context.\n    **kwargs: Additional\
    \ keyword arguments to customize context construction.\n\nReturns:\n    tuple[pd.DataFrame,\
    \ dict[str, int]]: A pair where the first element is a pandas DataFrame containing\n\
    \    the contextual items to be used for downstream search (columns and contents\
    \ vary by\n    implementation but typically include the text and related metadata),\
    \ and the second element is\n    a mapping from metric names (strings) to integers\
    \ representing context-related counters or scores\n    produced during construction.\n\
    \nRaises:\n    Exception: Implementation-specific errors may be raised during\
    \ context construction. Callers should\n               handle broad exceptions\
    \ and consider fallback or retry as appropriate."
  code_example: null
  example_source: null
  line_start: 57
  line_end: 62
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_create_blob_storage
  file: tests/integration/storage/test_factory.py
  name: test_create_blob_storage
  signature: def test_create_blob_storage()
  decorators:
  - '@pytest.mark.skip(reason="Blob storage emulator is not available in this environment")'
  raises: []
  visibility: public
  docstring: "Test creating a blob storage via the StorageFactory.\n\nReturns:\n \
    \   None\n\nRaises:\n    AssertionError: If the created storage is not an instance\
    \ of BlobPipelineStorage."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 35
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_operations
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: test_vector_store_operations
  signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
    \ mock_search_client, mock_index_client\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Test basic vector store operations with Azure AI Search.\n\nArgs:\n\
    \  self: The test case instance.\n  vector_store: AzureAISearchVectorStore used\
    \ in the test.\n  sample_documents: Documents loaded into the vector store for\
    \ testing.\n  mock_search_client: Mock for the Azure AI Search search client.\n\
    \  mock_index_client: Mock for the Azure AI Search index client.\n\nReturns:\n\
    \  None."
  code_example: null
  example_source: null
  line_start: 101
  line_end: 161
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_end
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: pipeline_end
  signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback to signal when the entire pipeline ends.\n\nArgs:\n\
    \    results: A list of PipelineRunResult objects representing the results of\
    \ the pipeline runs.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 25
  line_end: 27
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_community_reports_configs
  file: tests/unit/config/utils.py
  name: assert_community_reports_configs
  signature: "def assert_community_reports_configs(\n    actual: CommunityReportsConfig,\
    \ expected: CommunityReportsConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two CommunityReportsConfig objects have the same values\
    \ for their configuration fields.\n\nArgs:\n    actual: The actual CommunityReportsConfig\
    \ instance.\n    expected: The expected CommunityReportsConfig instance.\n\nReturns:\n\
    \    None\n\nRaises:\n    AssertionError: If graph_prompt, text_prompt, max_length,\
    \ max_input_length, strategy, or model_id differ between actual and expected."
  code_example: null
  example_source: null
  line_start: 281
  line_end: 289
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/language_model/factory.py::ModelFactory.get_embedding_models
  file: graphrag/language_model/factory.py
  name: get_embedding_models
  signature: def get_embedding_models(cls) -> list[str]
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Get the registered EmbeddingModel implementations.\n\nArgs:\n    cls:\
    \ The class that maintains the _embedding_registry mapping of EmbeddingModel implementations.\n\
    \nReturns:\n    list[str]: A list of the registered EmbeddingModel implementation\
    \ names."
  code_example: null
  example_source: null
  line_start: 83
  line_end: 85
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/graphs.py::get_upper_threshold_by_std
  file: graphrag/index/utils/graphs.py
  name: get_upper_threshold_by_std
  signature: 'def get_upper_threshold_by_std(data: list[float] | list[int], std_trim:
    float) -> float'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get upper threshold by standard deviation.\n\nArgs:\n    data: list[float]\
    \ | list[int], a list of numeric values used to compute the threshold.\n    std_trim:\
    \ float, multiplier for the standard deviation to offset the mean.\n\nReturns:\n\
    \    float: The upper threshold computed as mean + std_trim * std of the data."
  code_example: null
  example_source: null
  line_start: 238
  line_end: 242
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::test_create_custom_chat_model
  file: tests/integration/language_model/test_factory.py
  name: test_create_custom_chat_model
  signature: def test_create_custom_chat_model()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test creating and using a custom chat model via the ModelFactory and\
    \ ModelManager.\n\nThis test defines a CustomChatModel with chat and achat methods,\
    \ registers it with the factory,\ncreates an instance via ModelManager, and asserts\
    \ behavior of the methods (achat returning\ncontent-only response and chat returning\
    \ content with a full_response payload).\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 21
  line_end: 60
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  called_by: []
- node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
    \ params: see https://platform.openai.com/docs/api-reference/chat/create\n   \
    \     messages: list = [],  # type: ignore  # noqa: B006\n        stream: bool\
    \ | None = None,\n        stream_options: dict | None = None,  # type: ignore\n\
    \        stop=None,  # type: ignore\n        max_completion_tokens: int | None\
    \ = None,\n        max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
    \ | None = None,\n        prediction: ChatCompletionPredictionContentParam | None\
    \ = None,\n        audio: ChatCompletionAudioParam | None = None,\n        logit_bias:\
    \ dict | None = None,  # type: ignore\n        user: str | None = None,\n    \
    \    # openai v1.0+ new params\n        response_format: dict | type[BaseModel]\
    \ | None = None,  # type: ignore\n        seed: int | None = None,\n        tools:\
    \ list | None = None,  # type: ignore\n        tool_choice: str | dict | None\
    \ = None,  # type: ignore\n        logprobs: bool | None = None,\n        top_logprobs:\
    \ int | None = None,\n        parallel_tool_calls: bool | None = None,\n     \
    \   web_search_options: OpenAIWebSearchOptions | None = None,\n        deployment_id=None,\
    \  # type: ignore\n        extra_headers: dict | None = None,  # type: ignore\n\
    \        # soon to be deprecated params by OpenAI\n        functions: list | None\
    \ = None,  # type: ignore\n        function_call: str | None = None,\n       \
    \ # Optional liteLLM function params\n        thinking: AnthropicThinkingParam\
    \ | None = None,\n        **kwargs: Any,\n    ) -> ModelResponse | CustomStreamWrapper"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronous chat completion function for litellm integration; calls\
    \ the OpenAI-compatible chat completion API and supports streaming responses.\n\
    \nArgs:\n    messages: list\n        Chat messages to include in the request.\n\
    \    stream: bool | None\n        If True, stream partial responses as they arrive.\n\
    \    stream_options: dict | None\n        Options for streaming.\n    stop: Any\n\
    \        Stop sequences for the generation.\n    max_completion_tokens: int |\
    \ None\n        Maximum tokens for the completion.\n    max_tokens: int | None\n\
    \        Maximum tokens to generate.\n    modalities: list[ChatCompletionModality]\
    \ | None\n        Modality configuration.\n    prediction: ChatCompletionPredictionContentParam\
    \ | None\n        Prediction content.\n    audio: ChatCompletionAudioParam | None\n\
    \        Audio parameters.\n    logit_bias: dict | None\n        Logit bias overrides.\n\
    \    user: str | None\n        User identifier.\n    response_format: dict | type[BaseModel]\
    \ | None\n        Response format or model.\n    seed: int | None\n        Random\
    \ seed.\n    tools: list | None\n        Tools to use.\n    tool_choice: str |\
    \ dict | None\n        Tool selection.\n    logprobs: bool | None\n        Include\
    \ log probabilities.\n    top_logprobs: int | None\n        Top logprobs to return.\n\
    \    parallel_tool_calls: bool | None\n        Run tool calls in parallel.\n \
    \   web_search_options: OpenAIWebSearchOptions | None\n        Web search options.\n\
    \    deployment_id: str | None\n        Optional deployment identifier.\n    extra_headers:\
    \ dict | None\n        Extra HTTP headers.\n    functions: list | None\n     \
    \   Deprecated OpenAI functions.\n    function_call: str | None\n        Function\
    \ call.\n    thinking: AnthropicThinkingParam | None\n        LiteLLM thinking\
    \ parameter.\n    kwargs: Any\n        Additional keyword arguments accepted.\n\
    \nReturns:\n    ModelResponse | CustomStreamWrapper\n\nRaises:\n    Exception\
    \ types raised by the underlying API client or streaming wrapper (e.g., OpenAI\
    \ API errors, network errors, or litellm runtime errors)."
  code_example: null
  example_source: null
  line_start: 113
  line_end: 147
  dependencies: []
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.__init__
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: __init__
  signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the MockBaseVectorStore with the provided documents for testing.\n\
    \nArgs:\n    documents: list[VectorStoreDocument] - Documents to initialize the\
    \ mock vector store with.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 22
  line_end: 26
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.clear
  file: graphrag/storage/pipeline_storage.py
  name: clear
  signature: def clear(self) -> None
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Asynchronously clear all entries from the storage.\n\nThis coroutine\
    \ clears all data stored in the storage backend and does not return a value. It\
    \ must be awaited.\n\nArgs:\n    self: The storage instance.\n\nReturns:\n   \
    \ None: The coroutine completes without returning a value.\n\nRaises:\n    Exception:\
    \ If the storage backend fails to clear the storage."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 72
  dependencies: []
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.set
  file: graphrag/storage/pipeline_storage.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Set the value for the given key.\n\nThis is an asynchronous operation\
    \ that stores the provided value under the given key. If the key already exists,\
    \ its value will be overwritten.\n\nArgs:\n    key (str): The key to set the value\
    \ for.\n    value (Any): The value to set.\n    encoding (str | None): Optional\
    \ encoding to apply when serializing the value.\n\nReturns:\n    None: This coroutine\
    \ completes when the value has been stored.\n\nRaises:\n    StorageError: If a\
    \ storage-related error occurs during storage."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 48
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_context
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_context
  signature: 'def on_context(self, context: Any) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Handle when context data is constructed.\n\nThis no-op implementation\
    \ does not modify, store, or otherwise affect the given context.\n\nArgs:\n  \
    \  context (Any): The context data provided to the callback. This implementation\
    \ performs no operations on it.\n\nReturns:\n    None: The function returns no\
    \ value and has no side effects.\n\nRaises:\n    None: This function does not\
    \ raise exceptions by itself.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 15
  line_end: 16
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_cosmosdb_storage.py::test_clear
  file: tests/integration/storage/test_cosmosdb_storage.py
  name: test_clear
  signature: def test_clear()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that CosmosDBPipelineStorage.clear() removes all stored items and\
    \ resets internal client references.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 83
  line_end: 110
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.split_reports
  file: graphrag/query/structured_search/drift_search/primer.py
  name: split_reports
  signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Split the input reports into folds to enable parallel processing.\n\n\
    Args:\n    reports (pd.DataFrame): DataFrame of community reports.\n\nReturns:\n\
    \    list[pd.DataFrame]: List of report folds."
  code_example: null
  example_source: null
  line_start: 187
  line_end: 201
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType.__repr__
  file: graphrag/index/operations/summarize_communities/typing.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of this CreateCommunityReportsStrategyType\
    \ enum member.\n\nArgs:\n    self: CreateCommunityReportsStrategyType, the enum\
    \ member to represent as a string.\n\nReturns:\n    str: The enum member's value\
    \ enclosed in double quotes."
  code_example: null
  example_source: null
  line_start: 61
  line_end: 63
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_db_uri
  file: graphrag/config/models/vector_store_config.py
  name: _validate_db_uri
  signature: def _validate_db_uri(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the database URI. If the vector store type is LanceDB and db_uri\
    \ is missing or empty, set it to the default value from vector_store_defaults.\n\
    \nArgs:\n    self: The VectorStoreConfig instance being validated.\n\nReturns:\n\
    \    None\n\nRaises:\n    ValueError: If vector_store.type is not LanceDB and\
    \ a non-empty db_uri is provided. The error message is: vector_store.db_uri is\
    \ only used when vector_store.type is LanceDB. Please rerun graphrag init and\
    \ select the correct vector store type."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 38
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_start
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: pipeline_start
  signature: 'def pipeline_start(self, names: list[str]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Dispatch the pipeline_start event to all registered callbacks.\n\nAs\
    \ a manager, this forwards the pipeline_start event to every registered WorkflowCallbacks\
    \ instance that implements a pipeline_start method. The names argument is passed\
    \ unchanged to each callback's pipeline_start.\n\nArgs:\n    names: list[str]\
    \ The names of the pipelines that started.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 24
  line_end: 28
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.create_chat_model
  file: graphrag/language_model/factory.py
  name: create_chat_model
  signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create a ChatModel instance from a registered implementation.\n\nArgs:\n\
    \    model_type: The type of ChatModel to create.\n    **kwargs: Additional keyword\
    \ arguments for the ChatModel constructor.\n\nReturns:\n    A ChatModel instance.\n\
    \nRaises:\n    ValueError: If the provided model_type is not registered."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 57
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__init__
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
    \ int,\n        include_named_entities: bool,\n        exclude_entity_tags: list[str],\n\
    \        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n   \
    \     word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
    \        noun_phrase_tags: list[str],\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Noun phrase extractor combining CFG-based noun-chunk extraction\
    \ and NER.\n\nCFG-based extraction was based on TextBlob's fast NP extractor implementation:\n\
    This extractor tends to be faster than the dependency-parser-based extractors\
    \ but grammars may need to be changed for different languages.\n\nArgs:\n    model_name\
    \ (str): SpaCy model name.\n    max_word_length (int): Maximum length (in character)\
    \ of each extracted word.\n    include_named_entities (bool): Whether to include\
    \ named entities in noun phrases\n    exclude_entity_tags (list[str]): list of\
    \ named entity tags to exclude in noun phrases.\n    exclude_pos_tags (list[str]):\
    \ List of POS tags to remove in noun phrases.\n    word_delimiter (str): Delimiter\
    \ for joining words.\n    noun_phrase_grammars (dict[tuple, str]): CFG for matching\
    \ noun phrases.\n\nReturns:\n    None\n\nRaises:\n    None\n\"\"\"\n}"
  code_example: null
  example_source: null
  line_start: 23
  line_end: 69
  dependencies: []
  called_by: []
- node_id: graphrag/config/get_embedding_settings.py::get_embedding_settings
  file: graphrag/config/get_embedding_settings.py
  name: get_embedding_settings
  signature: "def get_embedding_settings(\n    settings: GraphRagConfig,\n    vector_store_params:\
    \ dict | None = None,\n) -> dict"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transform GraphRAG config into settings for workflows.\n\nArgs:\n  \
    \  settings: GraphRagConfig\n        GraphRagConfig containing embed_text and\
    \ vector_store configuration.\n    vector_store_params: dict | None\n        Optional\
    \ dictionary of vector store parameters to override defaults.\n\nReturns:\n  \
    \  dict\n        A dictionary with a single key \"strategy\" containing the embedding\
    \ strategy\n        configured using language model settings and merged vector\
    \ store settings\n        from both the config and any provided vector_store_params.\n\
    \nRaises:\n    Exceptions propagated from GraphRagConfig methods or underlying\
    \ calls may occur."
  code_example: null
  example_source: null
  line_start: 9
  line_end: 38
  dependencies: []
  called_by:
  - graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  - graphrag/index/workflows/update_text_embeddings.py::run_workflow
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.child
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: child
  signature: 'def child(self, name: str | None) -> PipelineStorage'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the current storage instance (no new child is created).\n\nThis\
    \ method accepts an optional name parameter for API compatibility but does not\
    \ create a new child. It returns the current instance (self).\n\nArgs:\n    name:\
    \ str | None, optional name for the child storage. This parameter is accepted\
    \ for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage: The\
    \ current instance (self)."
  code_example: null
  example_source: null
  line_start: 333
  line_end: 335
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/extract_graph.py::_load_strategy
  file: graphrag/index/operations/extract_graph/extract_graph.py
  name: _load_strategy
  signature: 'def _load_strategy(strategy_type: ExtractEntityStrategyType) -> EntityExtractStrategy'
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Load the strategy method implementation for the given strategy type.\n\
    \nArgs:\n    strategy_type (ExtractEntityStrategyType): The type of extraction\
    \ strategy to load.\n\nReturns:\n    EntityExtractStrategy: The loaded strategy\
    \ callable.\n\nRaises:\n    ValueError: If an unknown strategy_type is provided."
  code_example: null
  example_source: null
  line_start: 85
  line_end: 97
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
- node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_dotprefix
  file: tests/integration/storage/test_blob_pipeline_storage.py
  name: test_dotprefix
  signature: def test_dotprefix()
  decorators: []
  raises: []
  visibility: public
  docstring: Test that a dot-prefix path is handled correctly by BlobPipelineStorage
    when setting and listing files. The test creates a storage with path_prefix='.'
    and writes input/christmas.txt, then searches for .txt files and asserts that
    the resulting path is ['input/christmas.txt'].
  code_example: null
  example_source: null
  line_start: 51
  line_end: 63
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.__init__
  file: graphrag/config/environment_reader.py
  name: __init__
  signature: 'def __init__(self, env: Env)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the EnvironmentReader with the provided environment.\n\n\
    This constructor stores the given environment for later reads and initializes\
    \ an internal configuration stack to an empty list.\n\nArgs:\n    env: Environment\
    \ instance used to read configuration values.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 32
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::ModelResponse.parsed_response
  file: graphrag/language_model/response/base.pyi
  name: parsed_response
  signature: def parsed_response(self) -> _T | None
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The instance\
    \ of the implementing class providing the parsed_response property.\n\nReturns:\n\
    \    _T | None: The parsed response, or None if not available.\n\nRaises:\n  \
    \  None"
  code_example: null
  example_source: null
  line_start: 20
  line_end: 20
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel._get_kwargs
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: _get_kwargs
  signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get model arguments supported by litellm.\n\nArgs:\n  kwargs (dict[str,\
    \ Any]): Arbitrary keyword arguments. Only keys in the following set will be included\
    \ in the returned dictionary: \"name\", \"modalities\", \"prediction\", \"audio\"\
    , \"logit_bias\", \"metadata\", \"user\", \"response_format\", \"seed\", \"tools\"\
    , \"tool_choice\", \"logprobs\", \"top_logprobs\", \"parallel_tool_calls\", \"\
    web_search_options\", \"extra_headers\", \"functions\", \"function_call\", \"\
    thinking\".\n\nReturns:\n  dict[str, Any]: A dictionary containing the subset\
    \ of keyword arguments that litellm supports. If a 'json' keyword argument is\
    \ provided, response_format is set to {\"type\": \"json_object\"}. If a 'json_model'\
    \ keyword argument is provided and it is a subclass of pydantic.BaseModel, response_format\
    \ is set to that model."
  code_example: null
  example_source: null
  line_start: 228
  line_end: 264
  dependencies: []
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.keys
  file: graphrag/storage/pipeline_storage.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"List all keys in the storage.\n\nArgs:\n    self (PipelineStorage):\
    \ The instance of the PipelineStorage.\n\nReturns:\n    list[str]: The keys currently\
    \ stored in the storage.\n\nRaises:\n    NotImplementedError: If key listing is\
    \ not supported by the storage backend.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 79
  line_end: 80
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_descriptions
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: _unpack_descriptions
  signature: 'def _unpack_descriptions(data: Mapping) -> list[str]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Unpack descriptions from a mapping by splitting the description string\
    \ into lines.\n\nArgs:\n    data (Mapping): input mapping that may contain a \"\
    description\" key with a string value.\n\nReturns:\n    list[str]: The list of\
    \ description lines. If no description is provided, returns an empty list.\n\n\
    Raises:\n    AttributeError: If the description value exists but does not support\
    \ the split method."
  code_example: null
  example_source: null
  line_start: 293
  line_end: 295
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
- node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::load_strategy
  file: graphrag/index/operations/summarize_communities/summarize_communities.py
  name: load_strategy
  signature: "def load_strategy(\n    strategy: CreateCommunityReportsStrategyType,\n\
    ) -> CommunityReportsStrategy"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load the strategy method for community reports based on the provided\
    \ type.\n\nArgs:\n    strategy (CreateCommunityReportsStrategyType): The strategy\
    \ type used to determine which community reports strategy to load.\n\nReturns:\n\
    \    CommunityReportsStrategy: The callable strategy function corresponding to\
    \ the supplied strategy type.\n\nRaises:\n    ValueError: If an unknown strategy\
    \ type is provided."
  code_example: null
  example_source: null
  line_start: 117
  line_end: 130
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__str__
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Returns the string representation used for cache key generation.\n\n\
    The returned string encodes the extractor's configuration to uniquely identify\n\
    cache entries. It is constructed from the following attributes: model_name,\n\
    max_word_length, include_named_entities, exclude_entity_tags, exclude_pos_tags,\n\
    exclude_nouns, and word_delimiter.\n\nParameters:\n    self: The instance of the\
    \ extractor.\n\nReturns:\n    str: The cache-key string in the form:\n       \
    \ syntactic_<model_name>_<max_word_length>_<include_named_entities>_<exclude_entity_tags>_<exclude_pos_tags>_<exclude_nouns>_<word_delimiter>\n\
    \nThere are no other parameters beyond self."
  code_example: null
  example_source: null
  line_start: 160
  line_end: 162
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_embeddings_schema
  file: graphrag/config/models/vector_store_config.py
  name: _validate_embeddings_schema
  signature: def _validate_embeddings_schema(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the embeddings schema. This method performs two checks:\n1)\
    \ Each entry in embeddings_schema must correspond to a known embedding schema\
    \ name present in all_embeddings. If any name is invalid, it raises a ValueError\
    \ with the message: vector_store.embeddings_schema contains an invalid embedding\
    \ schema name: {name}. Please update your settings.yaml and select the correct\
    \ embedding schema names.\n2) When vector_store.type == CosmosDB, every key in\
    \ embeddings_schema must be 'id'. If any key differs, it raises a ValueError with\
    \ the message: When using CosmosDB, the id_field in embeddings_schema must be\
    \ 'id'. Please update your settings.yaml and set the id_field to 'id'.\n\nThis\
    \ method does not return a value. It raises ValueError on invalid configurations,\
    \ which are handled by the caller during model validation.\nArgs:\n    self: The\
    \ instance containing embeddings_schema and type attributes.\nReturns:\n    None\n\
    Raises:\n    ValueError: vector_store.embeddings_schema contains an invalid embedding\
    \ schema name: {name}. Please update your settings.yaml and select the correct\
    \ embedding schema names.\n    ValueError: When using CosmosDB, the id_field in\
    \ embeddings_schema must be 'id'. Please update your settings.yaml and set the\
    \ id_field to 'id'."
  code_example: null
  example_source: null
  line_start: 92
  line_end: 103
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.__init__
  file: graphrag/vector_stores/azure_ai_search.py
  name: __init__
  signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
    \ **kwargs: Any\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize Azure AI Search vector store by delegating to the base class\
    \ constructor.\n\nArgs:\n    vector_store_schema_config: VectorStoreSchemaConfig\
    \ - The schema configuration for the vector store.\n    **kwargs: Any - Additional\
    \ keyword arguments forwarded to the base class initializer.\n\nReturns:\n   \
    \ None\n\nRaises:\n    Exceptions raised by the base class __init__ are propagated."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 46
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  file: tests/unit/indexing/verbs/helpers/mock_llm.py
  name: create_mock_llm
  signature: 'def create_mock_llm(responses: list[str | BaseModel], name: str = "mock")
    -> ChatModel'
  decorators: []
  raises: []
  visibility: public
  docstring: "Creates a mock LLM that returns the given responses.\n\nArgs:\n    responses\
    \ (list[str | BaseModel]): The responses to be returned by the mock LLM.\n   \
    \ name (str): The name of the mock LLM. Defaults to \"mock\".\n\nReturns:\n  \
    \  ChatModel: A mock ChatModel configured to return the provided responses.\n\n\
    Raises:\n    Exception: If an error occurs while creating or retrieving the mock\
    \ chat model via ModelManager."
  code_example: null
  example_source: null
  line_start: 9
  line_end: 13
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  called_by:
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
- node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  file: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py
  name: run_extract_graph
  signature: "def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
    \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "async def run_extract_graph(\n    model: ChatModel,\n    docs: list[Document],\n\
    \    entity_types: EntityTypes,\n    args: StrategyConfig,\n) -> EntityExtractionResult:\n\
    \    \"\"\"Run the entity extraction chain.\"\"\"\n\n    Args:\n        model:\
    \ ChatModel\n            The chat model instance used to invoke the extraction.\n\
    \        docs: list[Document]\n            The input documents from which to extract\
    \ entities.\n        entity_types: EntityTypes\n            The types of entities\
    \ to extract.\n        args: StrategyConfig\n            Strategy configuration\
    \ for extraction. May include:\n                tuple_delimiter: delimiter for\
    \ grouping tuples (or None)\n                record_delimiter: delimiter for grouping\
    \ records (or None)\n                completion_delimiter: delimiter for completing\
    \ extractions (or None)\n                extraction_prompt: optional prompt used\
    \ by the extractor\n                max_gleanings: maximum number of gleanings;\
    \ defaults to graphrag_config_defaults.extract_graph.max_gleanings\n\n    Returns:\n\
    \        EntityExtractionResult\n            The extracted entities, relationships,\
    \ and the graph representing the extraction.\n\n    Raises:\n        Exception\n\
    \            If an error occurs during extraction (propagated from the underlying\
    \ GraphExtractor or processing steps)."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 102
  dependencies:
  - graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor
  - graphrag/index/operations/extract_graph/typing.py::EntityExtractionResult
  called_by:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_graph_intelligence
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
  - tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get
  file: graphrag/storage/pipeline_storage.py
  name: get
  signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
    \ str | None = None\n    ) -> Any"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Get the value for the given key.\n\n        Args:\n            key (str):\
    \ The key to retrieve the value for.\n            as_bytes (bool | None): If True,\
    \ return the value as bytes. If None, use the backend's default representation.\n\
    \            encoding (str | None): The text encoding to use when decoding bytes\
    \ to str. If None, the backend's default encoding is used.\n\n        Returns:\n\
    \            Any: The value for the given key. The concrete return type depends\
    \ on as_bytes and encoding:\n                - If as_bytes is True: bytes\n  \
    \              - If as_bytes is False or None and encoding is not None: str decoded\
    \ using the provided encoding\n                - If as_bytes is None and encoding\
    \ is None: backend-specific type or None\n\n        Raises:\n            KeyError:\
    \ If the key does not exist in storage.\n            ValueError: If the provided\
    \ encoding is invalid or not supported for the stored value.\n            RuntimeError:\
    \ If a backend-specific error occurs."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 39
  dependencies: []
  called_by: []
- node_id: graphrag/config/enums.py::SearchMethod.__str__
  file: graphrag/config/enums.py
  name: __str__
  signature: def __str__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Return the string representation of the enum value.\n\nArgs:\n\
    \    self: The enum member.\n\nReturns:\n    str: The string representation of\
    \ the enum value.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 138
  line_end: 140
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents_categories
  file: tests/integration/vector_stores/test_lancedb.py
  name: sample_documents_categories
  signature: def sample_documents_categories(self)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create sample documents with different categories for testing.\n\nArgs:\n\
    \    self: Instance of the test class used by pytest to provide fixture context.\n\
    \nReturns:\n    List[VectorStoreDocument]: A list of VectorStoreDocument objects\
    \ with\n        varying category attributes in the attributes dictionary to support\n\
    \        category-based tests (e.g., animals and vehicles)."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 66
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._stream_reduce_response
  file: graphrag/query/structured_search/global_search/search.py
  name: _stream_reduce_response
  signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
    \ list[SearchResult],\n        query: str,\n        max_length: int,\n       \
    \ **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Stream and reduce multiple map responses into a single streamed output\
    \ by ranking key points and querying the LLM.\n\nArgs:\n    map_responses (list[SearchResult]):\
    \ List of SearchResult objects to extract key points from. Each result may contain\
    \ a response that is a list of dictionaries with keys 'answer' and 'score'.\n\
    \    query (str): User query string to pass as the prompt for the LLM during streaming.\n\
    \    max_length (int): Maximum length to request in the reduce system prompt (limits\
    \ the generated content).\n    llm_kwargs (dict[str, Any]): Additional keyword\
    \ arguments forwarded to the language model streaming method (e.g., model_parameters).\
    \ This function forwards these to the underlying streaming API via the async generator.\n\
    \nReturns:\n    AsyncGenerator[str, None]: An asynchronous generator yielding\
    \ chunks of text produced by the streaming LLM.\n\nNotes:\n- Key points are collected\
    \ from all map_responses, filtered to keep only entries with a positive 'score',\
    \ and labeled with the originating analyst index (Analyst 1, Analyst 2, ...).\n\
    - If no positive-scoring key points exist and allow_general_knowledge is False,\
    \ the function yields NO_DATA_ANSWER and terminates. This provides a canned empty\
    \ answer to avoid hallucinations when no relevant data is available.\n- If general\
    \ knowledge is allowed (allow_general_knowledge is True) and no data points exist,\
    \ the NO_DATA_ANSWER path is skipped and the LLM may supplement with generic knowledge.\n\
    - The function enforces a token budget via self.max_data_tokens, constructing\
    \ text_data by concatenating stacked analyst blocks until the token limit would\
    \ be exceeded.\n- The constructed report (text_data) feeds reduce_system_prompt\
    \ via its format with report_data, response_type, and max_length. If allow_general_knowledge\
    \ is enabled, an additional general knowledge inclusion prompt is appended.\n\
    - The final prompt used to query the LLM is built from search_prompt, and the\
    \ function streams chunks from self.model.achat_stream, yielding each chunk while\
    \ notifying registered callbacks through on_llm_new_token.\n- Analytic ordering\
    \ is determined by descending score after filtering, while analysts are preserved\
    \ by their original indices for labeling in the response."
  code_example: null
  example_source: null
  line_start: 415
  line_end: 495
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.delete
  file: tests/integration/storage/test_factory.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Deletes the item associated with the specified key from storage.\n\n\
    Args:\n    key: The key of the item to delete.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 133
  line_end: 134
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.search
  file: graphrag/query/structured_search/basic_search/search.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Build rag search context that fits a single context window and generates\
    \ an answer for the user query.\n\nArgs:\n  query: The user query to process.\n\
    \  conversation_history: Optional conversation history to incorporate into the\
    \ search context.\n  **kwargs: Additional keyword arguments passed to the context\
    \ builder and the model.\n\nReturns:\n  SearchResult: The search result containing\
    \ the generated response, context data, and timing information."
  code_example: null
  example_source: null
  line_start: 52
  line_end: 127
  dependencies:
  - graphrag/query/structured_search/base.py::SearchResult
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_bool
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_text_str_bool
  signature: def test_split_text_str_bool()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that TokenTextSplitter.split_text returns an empty list when the\
    \ input is None.\n\nThis test initializes a TokenTextSplitter with chunk_size=5\
    \ and chunk_overlap=2, calls split_text with None, and asserts that the result\
    \ is [].\n\nReturns:\n    None: this test does not return a value.\n\nRaises:\n\
    \    AssertionError: if the result is not an empty list."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 45
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/model.py::load_entities
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_entities
  signature: "def load_entities(\n    dataset: str,\n    _datasource: Datasource,\n\
    ) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a DataFrame of Entity data loaded from the given dataset and\
    \ datasource.\n\nArgs:\n    dataset: The dataset identifier to load entities from.\n\
    \    _datasource: The Datasource descriptor used to access the data.\n\nReturns:\n\
    \    pd.DataFrame: DataFrame containing the loaded Entity data.\n\nRaises:\n \
    \   Exception: Propagates any exceptions raised by get_entity_data."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 35
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: extract
  signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Extract noun phrases from text using a combination of CFG-based noun\
    \ phrase matching and optional named-entity recognition (NER), with post-filtering\
    \ rules applied to remove unlikely phrases.\n\nThis extractor is configurable\
    \ via instance attributes that influence its behavior and the NLP model loaded:\n\
    \n- include_named_entities (bool): If True, include named entities in the noun\
    \ phrase results; otherwise, NER is disabled by loading the model with the ner\
    \ component excluded.\n- exclude_entity_tags (list[str]): Named entity labels\
    \ to exclude from consideration (e.g., PERSON, ORG).\n- exclude_pos_tags (list[str]):\
    \ POS tags to remove from consideration when forming CFG-based noun phrases.\n\
    - noun_phrase_grammars (dict[tuple[str,str], str]): CFG rules used to merge adjacent\
    \ words into noun phrases based on their POS tags.\n- noun_phrase_tags (set[str]):\
    \ The POS-like labels that qualify a token as a noun phrase after CFG merging.\n\
    - word_delimiter (str): Delimiter used to join tokens when merging matched words.\n\
    - model_name, max_word_length, exclude_nouns, etc.: Other configuration inherited\
    \ from the base extractor that affect tokenization, length filtering, and joining.\n\
    \nReturns:\n    list[str]: A deduplicated list of noun phrases extracted from\
    \ the input text. Duplicates are removed by using a set; the resulting order is\
    \ not guaranteed and may appear in arbitrary order. If you need deterministic\
    \ ordering, post-process the results (e.g., by preserving original offsets or\
    \ sorting).\n\nRaises:\n    RuntimeError, ValueError, or spaCy-related exceptions\
    \ that may be raised by the underlying NLP pipeline if the configured model cannot\
    \ be loaded, if processing fails, or if input text is not valid."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 124
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::_tag_noun_phrases
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::extract_cfg_matches
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.filter_by_id
  file: tests/integration/vector_stores/test_factory.py
  name: filter_by_id
  signature: def filter_by_id(self, include_ids)
  decorators: []
  raises: []
  visibility: public
  docstring: "Filter vector store results by a set of IDs.\n\nArgs:\n    include_ids:\
    \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    Any\
    \ - The filtered results. This implementation returns a dictionary (empty by default\
    \ in the test)."
  code_example: null
  example_source: null
  line_start: 143
  line_end: 144
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat
  file: graphrag/language_model/providers/fnllm/models.py
  name: achat
  signature: "def achat(\n        self, prompt: str, history: list | None = None,\
    \ **kwargs\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt: The\
    \ prompt to chat with.\n    history: The chat history to include in the chat,\
    \ or None for no history.\n    kwargs: Additional arguments to pass to the Model.\n\
    \nReturns:\n    ModelResponse: The response from the Model.\n\nRaises:\n    Exception:\
    \ Exceptions raised by the underlying model call are propagated."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 95
  dependencies:
  - graphrag/language_model/response/base.py::BaseModelOutput
  - graphrag/language_model/response/base.py::BaseModelResponse
  called_by: []
- node_id: unified-search-app/app/ui/sidebar.py::reset_app
  file: unified-search-app/app/ui/sidebar.py
  name: reset_app
  signature: def reset_app()
  decorators: []
  raises: []
  visibility: public
  docstring: "Reset app to its original state.\n\nClears the Streamlit data cache\
    \ and the session state, then reruns the app to restore its initial state.\n\n\
    Returns:\n    None"
  code_example: null
  example_source: null
  line_start: 11
  line_end: 15
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_deployment_name
  file: graphrag/config/models/language_model_config.py
  name: _validate_deployment_name
  signature: def _validate_deployment_name(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Validate the deployment name for Azure-hosted models.\n\nThis internal\
    \ validator checks whether a deployment_name is provided when using Azure OpenAI\
    \ (AOI) configurations. If the deployment_name is missing or consists only of\
    \ whitespace for Azure-hosted models (AzureOpenAIChat, AzureOpenAIEmbedding, or\
    \ when model_provider == \"azure\"), it logs a debug message stating that deployment_name\
    \ is not set and that the service will default to the model name. No exception\
    \ is raised; the behavior is a soft default.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 212
  line_end: 228
  dependencies: []
  called_by: []
- node_id: tests/integration/logging/test_factory.py::test_create_blob_logger
  file: tests/integration/logging/test_factory.py
  name: test_create_blob_logger
  signature: def test_create_blob_logger()
  decorators:
  - '@pytest.mark.skip(reason="Blob storage emulator is not available in this environment")'
  raises: []
  visibility: public
  docstring: "Test for creating a blob logger via LoggerFactory (skipped in this environment).\n\
    \nThis test is marked with pytest.mark.skip(reason=\"Blob storage emulator is\
    \ not available in this environment\"). If executed, it would construct a kwargs\
    \ dictionary containing: type: \"blob\", connection_string: WELL_KNOWN_BLOB_STORAGE_KEY,\
    \ base_dir: \"testbasedir\", container_name: \"testcontainer\". It would then\
    \ call LoggerFactory.create_logger(ReportingType.blob.value, kwargs) and assert\
    \ that the resulting logger is an instance of BlobWorkflowLogger.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 23
  line_end: 31
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_text
  file: graphrag/vector_stores/cosmosdb.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a text-based similarity search.\n\nArgs:\n    text (str): The\
    \ input text to search for similar documents.\n    text_embedder (TextEmbedder):\
    \ The callable used to compute an embedding for the input text.\n    k (int):\
    \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
    \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
    \ VectorStoreSearchResult objects.\n\nRaises:\n    ..."
  code_example: null
  example_source: null
  line_start: 243
  line_end: 252
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::similarity_search_by_vector
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.serialize
  file: graphrag/query/structured_search/drift_search/state.py
  name: serialize
  signature: "def serialize(\n        self, include_context: bool = True\n    ) ->\
    \ dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Serialize the graph to a dictionary representation, optionally including\
    \ contextual information for nodes.\n\nArgs:\n- include_context (bool): If True,\
    \ return a 3-tuple consisting of the graph dictionary, a context_data dictionary,\
    \ and a string representation of the context_data. If False, return only the graph\
    \ dictionary.\n\nReturns:\n- If include_context is False: a dictionary with the\
    \ keys \"nodes\" and \"edges\". \"nodes\" is a list of dictionaries for each node,\
    \ each containing an \"id\" (int), fields from node.serialize(include_follow_ups=False),\
    \ and all attributes from self.graph.nodes[node]. \"edges\" is a list of dictionaries\
    \ with \"source\" (int), \"target\" (int), and \"weight\" (float, defaults to\
    \ 1.0).\n- If include_context is True: a tuple of three elements: (graph_dict,\
    \ context_data, context_text).\n  - graph_dict is the same dictionary as above\
    \ (with keys \"nodes\" and \"edges\").\n  - context_data is a dictionary mapping\
    \ a node query (string) to its context_data (any), built from nodes that have\
    \ a non-empty metadata.context_data and a query.\n  - context_text is the string\
    \ representation of context_data.\n\nNotes and edge cases:\n- The function returns\
    \ different shapes depending on include_context. Callers must handle both possibilities.\n\
    - If no context data exists, context_data will be {}, and context_text will be\
    \ \"{}\".\n- If the graph has no nodes, nodes and edges lists are empty; IDs are\
    \ assigned by enumeration order starting at 0.\n- We rely on node.serialize(include_follow_ups=False)\
    \ for the per-node payload; exact fields depend on the DriftAction implementation.\n\
    - Edge weights default to 1.0 if not present in edge_data."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 117
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.connect
  file: tests/integration/vector_stores/test_factory.py
  name: connect
  signature: def connect(self, **kwargs)
  decorators: []
  raises: []
  visibility: public
  docstring: "Connect to the vector store.\n\nThis base implementation is a placeholder/no-op\
    \ and should be overridden by subclasses to establish an actual connection.\n\n\
    Args:\n    kwargs: Arbitrary keyword arguments.\n\nReturns:\n    None\n\nRaises:\n\
    \    None: This base implementation does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 131
  line_end: 132
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_chunking_configs
  file: tests/unit/config/utils.py
  name: assert_chunking_configs
  signature: 'def assert_chunking_configs(actual: ChunkingConfig, expected: ChunkingConfig)
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two ChunkingConfig objects have equal values for the configured\
    \ fields.\n\nArgs:\n    actual: ChunkingConfig to compare against expected.\n\
    \    expected: ChunkingConfig containing the expected values.\n\nReturns:\n  \
    \  None\n\nRaises:\n    AssertionError: If any of the checked fields do not match:\
    \ size, overlap, group_by_columns, strategy, encoding_model, prepend_metadata,\
    \ chunk_size_includes_metadata."
  code_example: null
  example_source: null
  line_start: 210
  line_end: 217
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: tests/unit/config/utils.py::assert_cache_configs
  file: tests/unit/config/utils.py
  name: assert_cache_configs
  signature: 'def assert_cache_configs(actual: CacheConfig, expected: CacheConfig)
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two CacheConfig objects have identical field values.\n\n\
    Args:\n    actual: The actual CacheConfig to validate.\n    expected: The expected\
    \ CacheConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If any of the fields differ: type, base_dir, connection_string, container_name,\
    \ storage_account_blob_url, cosmosdb_account_url."
  code_example: null
  example_source: null
  line_start: 159
  line_end: 165
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::_prepare_embed_texts
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: _prepare_embed_texts
  signature: "def _prepare_embed_texts(\n    input: list[str], splitter: TokenTextSplitter\n\
    ) -> tuple[list[str], list[int]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Prepare a flat list of text snippets to embed and their per-input sizes\
    \ by splitting each input text.\n\nArgs:\n    input: The list of input strings\
    \ to process.\n    splitter: The TokenTextSplitter used to split each input string\
    \ into chunks.\n\nReturns:\n    tuple[list[str], list[int]]: A tuple (snippets,\
    \ sizes) where:\n        snippets: The concatenated list of non-empty split_texts\
    \ produced from all inputs.\n        sizes: A list containing, for each input\
    \ string, the number of split_texts produced.\n\nRaises:\n    Propagates exceptions\
    \ raised by splitter.split_text."
  code_example: null
  example_source: null
  line_start: 139
  line_end: 155
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig.resolved_strategy
  file: graphrag/config/models/text_embedding_config.py
  name: resolved_strategy
  signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) -> dict'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the resolved text embedding strategy.\n\nArgs:\n    model_config:\
    \ The language model configuration used to resolve the strategy.\n\nReturns:\n\
    \    dict: The resolved text embedding strategy. If a custom strategy is provided\
    \ via self.strategy, that is returned; otherwise, a default strategy dictionary\
    \ is returned with the following keys:\n        type: The strategy type (TextEmbedStrategyType.openai)\n\
    \        llm: The serialized language model configuration (model_config.model_dump())\n\
    \        num_threads: The number of concurrent requests (model_config.concurrent_requests)\n\
    \        batch_size: The configured batch size (self.batch_size)\n        batch_max_tokens:\
    \ The configured max tokens per batch (self.batch_max_tokens)\n\nRaises:\n   \
    \ ImportError: If the import of TextEmbedStrategyType fails."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::ModelOutput.content
  file: graphrag/language_model/response/base.pyi
  name: content
  signature: def content(self) -> str
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Content of the model output as a string.\n\nReturns:\n    str: The content\
    \ of the model output as a string."
  code_example: null
  example_source: null
  line_start: 12
  line_end: 12
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::TestIndexer.__assert_indexer_outputs
  file: tests/smoke/test_fixtures.py
  name: __assert_indexer_outputs
  signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
    \ dict[str, dict[str, Any]]\n    )"
  decorators: []
  raises: []
  visibility: private
  docstring: "Assert that the indexer outputs conform to the provided workflow configuration.\n\
    \nArgs:\n    self: The instance of the containing class.\n    root: Path to the\
    \ root directory containing the indexer outputs (expects an output subdirectory\
    \ with stats.json).\n    workflow_config: Mapping of workflow names to their configuration\
    \ dictionaries. Each config may include:\n        - expected_artifacts: List[str]\
    \ of artifact files to validate (parquet files).\n        - max_runtime: Optional\
    \ number specifying the maximum allowed runtime for the workflow.\n        - row_range:\
    \ List[int] with two elements [min_rows, max_rows] for the number of rows in each\
    \ artifact.\n        - nan_allowed_columns: Optional[List[str]] of column names\
    \ that may contain NaN values.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If the output folder does not exist, if the reported workflows do not match\
    \ the configured ones, if a runtime constraint is violated, or if artifact checks\
    \ fail (row count or NaN values)."
  code_example: null
  example_source: null
  line_start: 150
  line_end: 199
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_container
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _create_container
  signature: def _create_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create a Cosmos DB container for the current container name if it doesn't\
    \ exist.\n\nThis method creates or retrieves the container using the current container\
    \ name as the id and a partition key on the path \"/id\" (Hash). It assigns the\
    \ resulting container proxy to self._container_client. The operation only proceeds\
    \ if a database client exists.\n\nArgs:\n    self: The instance of the class containing\
    \ the Cosmos client references.\n\nReturns:\n    None. Updates the internal _container_client\
    \ attribute with the container proxy.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 102
  line_end: 111
  dependencies: []
  called_by: []
- node_id: graphrag/storage/factory.py::StorageFactory.register
  file: graphrag/storage/factory.py
  name: register
  signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
    \ PipelineStorage]\n    ) -> None"
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a custom storage implementation.\n\nArgs:\n    storage_type\
    \ (str): The type identifier for the storage.\n    creator (Callable[..., PipelineStorage]):\
    \ A class or callable that creates an instance of PipelineStorage.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 34
  line_end: 44
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.find
  file: tests/integration/storage/test_factory.py
  name: find
  signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
    \            base_dir: str | None = None,\n            file_filter: dict[str,\
    \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
    \ dict[str, Any]]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Find files in the storage that match a compiled file_pattern, with optional\
    \ base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern (re.Pattern[str]):\
    \ A compiled regular expression to match file paths.\n    base_dir (str | None):\
    \ The base directory to search within. If None, search starts from the storage\
    \ root.\n    file_filter (dict[str, Any] | None): Optional dictionary of metadata-based\
    \ filters to apply when selecting files.\n    max_count (int): Maximum number\
    \ of results to return. A value of -1 means no limit.\n\nReturns:\n    Iterator[tuple[str,\
    \ dict[str, Any]]]: An iterator yielding tuples of (path, metadata) for each matching\
    \ file."
  code_example: null
  example_source: null
  line_start: 116
  line_end: 123
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.search
  file: graphrag/query/structured_search/drift_search/primer.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
    \ pd.DataFrame,\n    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous search method that processes the query and returns a SearchResult.\n\
    \nArgs:\n    query (str): The search query.\n    top_k_reports (pd.DataFrame):\
    \ DataFrame containing the top-k reports.\n\nReturns:\n    SearchResult: The search\
    \ result containing the response and context data."
  code_example: null
  example_source: null
  line_start: 153
  line_end: 185
  dependencies:
  - graphrag/query/structured_search/base.py::SearchResult
  - graphrag/query/structured_search/drift_search/primer.py::decompose_query
  - graphrag/query/structured_search/drift_search/primer.py::split_reports
  called_by: []
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_start
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_reduce_response_start
  signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
    \ str | dict[str, Any]\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
    \ Context for the reduce response (str | dict[str, Any]).\n\nReturns:\n    None:\
    \ The function does not return a value."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 27
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_search_client
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: mock_search_client
  signature: def mock_search_client(self)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create and yield a mock Azure AI Search client for tests.\n\nArgs:\n\
    \    self: TestAzureAISearchVectorStore instance.\n\nReturns:\n    MagicMock:\
    \ The mocked Azure AI Search client (SearchClient) instance produced by patch."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 30
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_azure_settings
  file: graphrag/config/models/language_model_config.py
  name: _validate_azure_settings
  signature: def _validate_azure_settings(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Validate the Azure settings.\n\nArgs:\n    self: The instance of the\
    \ language model configuration.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
    \ If the API base is missing and is required.\n    AzureApiVersionMissingError:\
    \ If the API version is missing and is required.\n    AzureDeploymentNameMissingError:\
    \ If the deployment name is missing and is required."
  code_example: null
  example_source: null
  line_start: 376
  line_end: 390
  dependencies:
  - graphrag/config/models/language_model_config.py::_validate_api_base
  - graphrag/config/models/language_model_config.py::_validate_api_version
  - graphrag/config/models/language_model_config.py::_validate_deployment_name
  called_by: []
- node_id: tests/unit/config/utils.py::assert_global_search_configs
  file: tests/unit/config/utils.py
  name: assert_global_search_configs
  signature: "def assert_global_search_configs(\n    actual: GlobalSearchConfig, expected:\
    \ GlobalSearchConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that actual and expected GlobalSearchConfig objects have equal\
    \ configuration values.\n\nThe function asserts equality for the following fields:\
    \ map_prompt, reduce_prompt, knowledge_prompt, max_context_tokens, data_max_tokens,\
    \ map_max_length, reduce_max_length, dynamic_search_threshold, dynamic_search_keep_parent,\
    \ dynamic_search_num_repeats, dynamic_search_use_summary, dynamic_search_max_level.\n\
    \nArgs:\n    actual: GlobalSearchConfig - The actual configuration to validate.\n\
    \    expected: GlobalSearchConfig - The expected configuration to compare against.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError - If the configurations do\
    \ not match."
  code_example: null
  example_source: null
  line_start: 329
  line_end: 343
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/logger/progress.py::ProgressTicker.__call__
  file: graphrag/logger/progress.py
  name: __call__
  signature: 'def __call__(self, num_ticks: int = 1) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Emit progress.\n\nArgs:\n    num_ticks (int): Number of ticks to advance\
    \ the progress.\n\nReturns:\n    None: This method updates internal counters and,\
    \ if a callback is set, notifies it with a Progress object."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 60
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/response/base.py::ModelResponse.history
  file: graphrag/language_model/response/base.py
  name: history
  signature: def history(self) -> list
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the history of the response.\n\nReturns:\n    list[Any]: The\
    \ history of the response."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 43
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._process_document
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: _process_document
  signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
    \ int\n    ) -> list[dict]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Process a single document to extract claims from the text using the\
    \ configured extraction prompt and parse the results into dictionaries.\n\nArgs:\n\
    \  prompt_args: A dictionary of additional arguments used to configure the extraction\
    \ prompts and behavior.\n  doc: The document content to process.\n  doc_index:\
    \ The zero-based index of the document within the input collection.\n\nReturns:\n\
    \  A list of dictionaries representing parsed claims as produced by _parse_claim_tuples.\
    \ Each dictionary typically includes keys such as subject_id, object_id, and type.\n\
    \nRaises:\n  Exceptions raised by the underlying language model interactions (e.g.,\
    \ self._model.achat) or by parsing/processing logic can propagate to callers."
  code_example: null
  example_source: null
  line_start: 149
  line_end: 195
  dependencies:
  - graphrag/index/operations/extract_covariates/claim_extractor.py::_parse_claim_tuples
  called_by: []
- node_id: graphrag/data_model/document.py::Document.from_dict
  file: graphrag/data_model/document.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n        title_key:\
    \ str = \"title\",\n        type_key: str = \"type\",\n        text_key: str =\
    \ \"text\",\n        text_units_key: str = \"text_units\",\n        attributes_key:\
    \ str = \"attributes\",\n    ) -> \"Document\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new document from the dict data.\n\nArgs:\n    cls (type):\
    \ The class.\n    d (dict[str, Any]): The source dictionary containing the values\
    \ for the Document fields.\n    id_key (str): Key in d for the document's identifier.\
    \ Defaults to \"id\".\n    short_id_key (str): Key in d for the optional short\
    \ identifier. Defaults to \"human_readable_id\".\n    title_key (str): Key in\
    \ d for the title. Defaults to \"title\".\n    type_key (str): Key in d for the\
    \ document type. Defaults to \"type\".\n    text_key (str): Key in d for the text\
    \ content. Defaults to \"text\".\n    text_units_key (str): Key in d for the list\
    \ of text unit ids. Defaults to \"text_units\".\n    attributes_key (str): Key\
    \ in d for optional attributes dictionary. Defaults to \"attributes\".\n\nReturns:\n\
    \    Document: A Document instance created from the dictionary data.\n\nRaises:\n\
    \    KeyError: If a required key is missing from d."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 49
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat_stream
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: achat_stream
  signature: "def achat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given prompt and history.\n\nArgs:\n   \
    \ prompt: The prompt to generate a response for.\n    history: Optional chat history.\n\
    \    **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
    \    AsyncGenerator[str, None]: The generated response as a stream of strings."
  code_example: null
  example_source: null
  line_start: 316
  line_end: 340
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.delete
  file: graphrag/storage/memory_pipeline_storage.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously delete the given key from the storage.\n\nArgs:\n   \
    \ key (str): The key to delete.\n\nReturns:\n    None\n\nRaises:\n    KeyError:\
    \ If the key does not exist in the storage."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 66
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.list_embedding_models
  file: graphrag/language_model/manager.py
  name: list_embedding_models
  signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a shallow copy of all registered EmbeddingModel instances.\n\n\
    This returns a shallow copy of the internal mapping of embedding models keyed\
    \ by\ntheir registration name. Modifications to the returned dictionary do not\
    \ affect\nthe internal registry.\n\nReturns:\n    dict[str, EmbeddingModel]: A\
    \ shallow copy mapping model names to EmbeddingModel\n    instances."
  code_example: null
  example_source: null
  line_start: 151
  line_end: 153
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_database
  file: graphrag/vector_stores/cosmosdb.py
  name: _delete_database
  signature: def _delete_database(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Delete the database if it exists.\n\nReturns:\n    None: This method\
    \ does not return a value."
  code_example: null
  example_source: null
  line_start: 72
  line_end: 75
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::_database_exists
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.load_documents
  file: graphrag/vector_stores/lancedb.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load documents into LanceDB vector storage.\n\nArgs:\n    documents:\
    \ List of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
    \ If True, overwrite existing table data; otherwise, append to the table.\n\n\
    Returns:\n    None\n\nRaises:\n    May raise exceptions from LanceDB operations\
    \ during table creation or data insertion."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 101
  dependencies: []
  called_by: []
- node_id: graphrag/index/input/util.py::load_files
  file: graphrag/index/input/util.py
  name: load_files
  signature: "def load_files(\n    loader: Any,\n    config: InputConfig,\n    storage:\
    \ PipelineStorage,\n) -> pd.DataFrame"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load files from storage asynchronously and apply a loader function,\
    \ then concatenate the results into a single pandas DataFrame.\n\nThe loader is\
    \ awaited for each file. Failures are logged and the corresponding file is skipped\
    \ rather than raised.\n\nArgs:\n    loader: Any\n        Async loader callable\
    \ that accepts (file, group) and returns a value compatible with pandas.concat.\n\
    \        The loader will be awaited for each file. If it raises, the file is skipped\
    \ with a warning.\n    config: InputConfig\n        Configuration for input files,\
    \ including file_pattern, file_filter, and file_type/storage details used to locate\
    \ files.\n    storage: PipelineStorage\n        Storage backend used to locate\
    \ and read files.\n\nReturns:\n    pd.DataFrame\n        A DataFrame formed by\
    \ concatenating all successfully loaded data.\n\nRaises:\n    ValueError\n   \
    \     If no files matching the pattern are found in the configured storage location.\n\
    \nNotes:\n    - The final concatenation uses pd.concat on the list of successfully\
    \ loaded objects. If none are loaded, pd.concat may raise a ValueError for no\
    \ objects to concatenate. This edge case should be considered by callers.\n  \
    \  - The function logs the number of files found and the number successfully loaded."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 53
  dependencies: []
  called_by:
  - graphrag/index/input/csv.py::load_csv
  - graphrag/index/input/json.py::load_json
  - graphrag/index/input/text.py::load_text
- node_id: graphrag/index/workflows/factory.py::PipelineFactory.register
  file: graphrag/index/workflows/factory.py
  name: register
  signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a custom workflow function.\n\nArgs:\n    cls: The class that\
    \ provides access to the registry (PipelineFactory).\n    name: The name under\
    \ which the workflow will be registered.\n    workflow: The workflow function\
    \ to register for the given name.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 24
  line_end: 26
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.progress
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: progress
  signature: 'def progress(self, progress: Progress) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Forward progress events to registered callbacks that implement a progress\
    \ method.\n\nThis method iterates over the internal _callbacks collection and\
    \ propagates the\nprovided Progress event to each callback by invoking its progress\
    \ method when\npresent.\n\nArgs:\n    progress: Progress object representing the\
    \ current progress event.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 48
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.__init__
  file: graphrag/query/structured_search/local_search/search.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n        system_prompt:\
    \ str | None = None,\n        response_type: str = \"multiple paragraphs\",\n\
    \        callbacks: list[QueryCallbacks] | None = None,\n        model_params:\
    \ dict[str, Any] | None = None,\n        context_builder_params: dict | None =\
    \ None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a LocalSearch instance for local search orchestration.\n\n\
    Args:\n    model: ChatModel - The language model interface used for this local\
    \ search.\n    context_builder: LocalContextBuilder - The builder that constructs\
    \ the context for the local search.\n    tokenizer: Tokenizer | None - Optional\
    \ tokenizer to use.\n    system_prompt: str | None - System prompt for the local\
    \ search. If None, uses LOCAL_SEARCH_SYSTEM_PROMPT.\n    response_type: str -\
    \ The type of response formatting, e.g., \"multiple paragraphs\".\n    callbacks:\
    \ list[QueryCallbacks] | None - Optional list of query callbacks.\n    model_params:\
    \ dict[str, Any] | None - Additional parameters for the model.\n    context_builder_params:\
    \ dict | None - Additional parameters for the context builder.\n\nReturns:\n \
    \   None - The instance is initialized and nothing is returned."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 49
  dependencies: []
  called_by: []
- node_id: graphrag/data_model/relationship.py::Relationship.from_dict
  file: graphrag/data_model/relationship.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n        source_key:\
    \ str = \"source\",\n        target_key: str = \"target\",\n        description_key:\
    \ str = \"description\",\n        rank_key: str = \"rank\",\n        weight_key:\
    \ str = \"weight\",\n        text_unit_ids_key: str = \"text_unit_ids\",\n   \
    \     attributes_key: str = \"attributes\",\n    ) -> \"Relationship\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new Relationship from the dictionary data.\n\nArgs:\n  cls\
    \ (type): The class.\n  d (dict[str, Any]): The source dictionary containing the\
    \ values for the Relationship fields.\n  id_key (str): Key in d for the relationship's\
    \ identifier. Defaults to \"id\".\n  short_id_key (str): Key in d for the optional\
    \ short identifier. Defaults to \"human_readable_id\".\n  source_key (str): Key\
    \ in d for the source entity. Defaults to \"source\".\n  target_key (str): Key\
    \ in d for the target entity. Defaults to \"target\".\n  description_key (str):\
    \ Key in d for the description. Defaults to \"description\".\n  rank_key (str):\
    \ Key in d for the rank. Defaults to \"rank\".\n  weight_key (str): Key in d for\
    \ the weight. Defaults to \"weight\".\n  text_unit_ids_key (str): Key in d for\
    \ text unit IDs. Defaults to \"text_unit_ids\".\n  attributes_key (str): Key in\
    \ d for additional attributes. Defaults to \"attributes\".\n\nReturns:\n  Relationship:\
    \ A Relationship instance constructed from the dictionary data.\n\nRaises:\n \
    \ KeyError: If id_key is not found in d."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 65
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/app_logic.py::run_global_search_question_generation
  file: unified-search-app/app/app_logic.py
  name: run_global_search_question_generation
  signature: "def run_global_search_question_generation(\n    query: str,\n    sv:\
    \ SessionVariables,\n) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run global search question generation process.\n\nArgs:\n  query: The\
    \ search query string used to generate questions from the global search.\n  sv:\
    \ The SessionVariables instance containing configuration and state for the current\
    \ session.\n\nReturns:\n  SearchResult: The result of the global search question\
    \ generation, including the search_type set to Global, the textual response, and\
    \ the context data (a dict of context data if available, otherwise an empty dict)."
  code_example: null
  example_source: null
  line_start: 122
  line_end: 145
  dependencies:
  - graphrag.api::global_search
  called_by:
  - unified-search-app/app/app_logic.py::run_generate_questions
- node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_covariate
  file: graphrag/index/operations/extract_covariates/extract_covariates.py
  name: create_covariate
  signature: 'def create_covariate(item: dict[str, Any]) -> Covariate'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a Covariate instance from the provided item.\n\nArgs:\n    item:\
    \ dict[str, Any]\n        The dictionary containing covariate fields. The function\
    \ reads\n        keys such as subject_id, object_id, type, status, start_date,\
    \ end_date,\n        description, source_text, record_id, and id to construct\
    \ the Covariate.\n\nReturns:\n    Covariate\n        The Covariate object created\
    \ from the item."
  code_example: null
  example_source: null
  line_start: 140
  line_end: 153
  dependencies:
  - graphrag/index/operations/extract_covariates/typing.py::Covariate
  called_by:
  - graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
- node_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError.__init__
  file: graphrag/index/utils/derive_from_rows.py
  name: __init__
  signature: 'def __init__(self, num_errors: int, example: str | None = None)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Initialize a ParallelizationError with details about errors during\
    \ parallel transformation.\n\nArgs:\n    num_errors: The number of errors that\
    \ occurred while running parallel transformation.\n    example: Optional example\
    \ error string to include in the message.\n\nReturns:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 27
  line_end: 31
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_vector
  file: graphrag/vector_stores/lancedb.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
    \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Perform a vector-based similarity search against the LanceDB document\
    \ collection.\n\nQuery the underlying document collection for documents whose\
    \ embeddings are close to the provided query_embedding. If a query filter has\
    \ been configured (via filter_by_id), the search results are restricted to that\
    \ subset using a prefilter.\n\nThe top-k results are returned in order of increasing\
    \ distance. Each result is a VectorStoreSearchResult containing:\n- document:\
    \ VectorStoreDocument with id, text, vector, and attributes (attributes parsed\
    \ from JSON in the attributes field)\n- score: 1 - abs(float(doc[\"_distance\"\
    ])) (a similarity score in [0, 1])\n\nArgs:\n  query_embedding: list[float] |\
    \ np.ndarray - Embedding vector to search with\n  k: int - Number of top results\
    \ to return\n  **kwargs: Any - Additional keyword arguments for compatibility;\
    \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]\
    \ - Top-k results with associated documents and similarity scores\n\nRaises:\n\
    \  ValueError, TypeError, RuntimeError - If the input embedding is invalid or\
    \ the search operation fails due to backend issues.\n\nNotes:\n  - If self.query_filter\
    \ is set, results are filtered by the provided condition before applying the top-k\
    \ limit.\n  - The attributes field is parsed from JSON; ensure the underlying\
    \ column contains valid JSON.\n\n\"\"\""
  code_example: null
  example_source: null
  line_start: 117
  line_end: 151
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/base.py::VectorStoreSearchResult
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract_cfg_matches
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: extract_cfg_matches
  signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return noun phrases that match a given context-free grammar.\n\nArgs:\n\
    \    doc (Doc): The spaCy Doc to process for noun phrase extraction.\n\nReturns:\n\
    \    list[tuple[str, str]]: A list of noun phrases as (text, tag) pairs, where\
    \ text is the merged noun phrase string and tag is the corresponding noun phrase\
    \ tag."
  code_example: null
  example_source: null
  line_start: 126
  line_end: 151
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::_base_embedding
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: _base_embedding
  signature: 'def _base_embedding(**kwargs: Any) -> EmbeddingResponse'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Base synchronous embedding wrapper that forwards to litellm.embedding\
    \ with merged base arguments.\n\nArgs\n    kwargs: Any\n        Additional keyword\
    \ arguments to pass to the underlying embedding function. The keys are merged\
    \ with base_args, and if the resulting dictionary contains the key \"name\", it\
    \ will be removed before invocation.\n\nReturns\n    EmbeddingResponse\n     \
    \   The embedding response produced by the underlying embedding call.\n\nRaises\n\
    \    Exception\n        Propagates exceptions raised by the underlying embedding\
    \ function."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 87
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: chat
  signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs: Any)
    -> "MR"'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given prompt and history.\n\nArgs\n----\n\
    \    prompt: The prompt to generate a response for.\n    history: Optional chat\
    \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
    \nReturns\n-------\n    LitellmModelResponse: The generated model response."
  code_example: null
  example_source: null
  line_start: 342
  line_end: 388
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig.resolved_strategy
  file: graphrag/config/models/community_reports_config.py
  name: resolved_strategy
  signature: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
    \    ) -> dict"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the resolved community report extraction strategy.\n\nArgs:\n\
    \    root_dir: The root directory used to resolve the graph and text prompt file\
    \ paths.\n    model_config: The LanguageModelConfig instance containing the model\
    \ configuration; its\n        model_dump() result is included in the strategy\
    \ as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy is provided,\
    \ it is returned as-is; otherwise\n        a default strategy dictionary is constructed\
    \ with the following keys:\n        type, llm, graph_prompt, text_prompt, max_report_length,\
    \ max_input_length.\n        graph_prompt is the contents of the file at the path\
    \ root_dir / self.graph_prompt when\n        self.graph_prompt is provided, otherwise\
    \ None. text_prompt similarly uses root_dir / self.text_prompt.\n\nRaises:\n \
    \   FileNotFoundError: If a provided graph_prompt or text_prompt path does not\
    \ exist.\n    OSError: If an I/O error occurs while reading prompt files.\n\"\"\
    \""
  code_example: null
  example_source: null
  line_start: 42
  line_end: 65
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::load_local_prompt_config
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: load_local_prompt_config
  signature: def load_local_prompt_config(base_path="") -> dict[str, str]
  decorators: []
  raises: []
  visibility: public
  docstring: "Load local prompt configuration.\n\nArgs:\n    base_path: Path to the\
    \ folder containing prompt files.\n\nReturns:\n    dict[str, str]: Mapping from\
    \ the prompt name (filename without extension) to the file contents as a string.\n\
    \nRaises:\n    FileNotFoundError: If base_path does not exist.\n    OSError: If\
    \ an OS error occurs while listing or reading files."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 30
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__call__
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: __call__
  signature: "def __call__(\n        self, texts: list[str], prompt_variables: dict[str,\
    \ Any] | None = None\n    ) -> GraphExtractionResult"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronously run graph extraction on a list of input texts and return\
    \ the results.\n\nArgs:\n  texts: List[str] - List of input texts to process;\
    \ each element is treated as a separate document.\n  prompt_variables: dict[str,\
    \ Any] | None - Optional mapping of prompt variables to customize the extraction\
    \ prompts and delimiters. If None, defaults are used.\n\nReturns:\n  GraphExtractionResult\
    \ - An object containing the aggregated extraction output and a mapping of document\
    \ indices to their source texts.\n\nRaises:\n  Exception - If an error occurs\
    \ during document processing or result aggregation."
  code_example: null
  example_source: null
  line_start: 90
  line_end: 141
  dependencies:
  - graphrag/index/operations/extract_graph/graph_extractor.py::_process_document
  - graphrag/index/operations/extract_graph/graph_extractor.py::_process_results
  called_by: []
- node_id: graphrag/language_model/response/base.py::ModelResponse.parsed_response
  file: graphrag/language_model/response/base.py
  name: parsed_response
  signature: def parsed_response(self) -> T | None
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The instance\
    \ of the implementing class providing the parsed_response property.\n\nReturns:\n\
    \    T | None: The parsed response, or None if not available.\n\nRaises:\n   \
    \ None..."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 38
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.stream_search
  file: graphrag/query/structured_search/global_search/search.py
  name: stream_search
  signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Stream the global search response.\n\nArgs:\n    query: str\n      \
    \  The search query to process.\n    conversation_history: ConversationHistory\
    \ | None\n        Optional conversation history to provide context for the search.\n\
    \nReturns:\n    AsyncGenerator[str, None]\n        An asynchronous generator yielding\
    \ string fragments that represent streaming portions of the final answer. Fragments\
    \ are produced by the streaming reduction step as results become available.\n\n\
    Raises:\n    Exception\n        Propagates exceptions raised by the context builder,\
    \ mapping, and streaming components (e.g., I/O or LLM errors)."
  code_example: null
  example_source: null
  line_start: 99
  line_end: 133
  dependencies:
  - graphrag/query/structured_search/global_search/search.py::_map_response_single_batch
  - graphrag/query/structured_search/global_search/search.py::_stream_reduce_response
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.clear
  file: graphrag/storage/blob_pipeline_storage.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously clear all entries from the blob storage cache. This implementation\
    \ is a no-op and does not remove any blobs from the container. To perform an actual\
    \ clear, implement iteration over the container's blobs and delete each one.\n\
    \nReturns:\n    None: The coroutine completes without returning a value."
  code_example: null
  example_source: null
  line_start: 270
  line_end: 271
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/utils.py::is_reasoning_model
  file: graphrag/language_model/providers/fnllm/utils.py
  name: is_reasoning_model
  signature: 'def is_reasoning_model(model: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Check if a model name is a known OpenAI reasoning model.\n\nArgs:\n\
    \    model: The name of the model to check.\n\nReturns:\n    bool: True if the\
    \ model is one of the known OpenAI reasoning models (o1, o1-mini, o3-mini); otherwise\
    \ False."
  code_example: null
  example_source: null
  line_start: 135
  line_end: 137
  dependencies: []
  called_by:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
- node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::create_row_from_claim_data
  file: graphrag/index/operations/extract_covariates/extract_covariates.py
  name: create_row_from_claim_data
  signature: 'def create_row_from_claim_data(row, covariate_data: Covariate, covariate_type:
    str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a row from claim data and the input row.\n\nArgs:\n  row: The\
    \ input row to extend with covariate data.\n  covariate_data: Covariate data to\
    \ be merged into the row (converted to a dict via asdict).\n  covariate_type:\
    \ The covariate type to include in the returned row.\n\nReturns:\n  dict: A new\
    \ dictionary containing the original row data, the covariate data fields, and\
    \ the covariate_type field.\n\nRaises:\n  TypeError: If row is not a mapping that\
    \ can be expanded with **, or if covariate_data cannot be converted to a dict\
    \ via asdict."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 81
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
- node_id: tests/mock_provider.py::MockEmbeddingLLM.embed_batch
  file: tests/mock_provider.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n  text_list:\
    \ A batch of input texts to generate embeddings for.\n  **kwargs: Additional keyword\
    \ arguments (e.g., model parameters).\n\nReturns:\n  list[list[float]]: A batch\
    \ of embeddings corresponding to the input texts."
  code_example: null
  example_source: null
  line_start: 105
  line_end: 109
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py::download_if_not_exists
  file: graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py
  name: download_if_not_exists
  signature: def download_if_not_exists(resource_name) -> bool
  decorators: []
  raises: []
  visibility: public
  docstring: "Download nltk resources if they haven't been already.\n\nArgs:\n   \
    \ resource_name: The name of the nltk resource to locate or download.\n\nReturns:\n\
    \    bool: True if the resource was found without downloading; False if the resource\
    \ was not found and had to be downloaded."
  code_example: null
  example_source: null
  line_start: 9
  line_end: 38
  dependencies: []
  called_by:
  - graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
- node_id: graphrag/utils/api.py::MultiVectorStore.__init__
  file: graphrag/utils/api.py
  name: __init__
  signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
    \        index_names: list[str],\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a MultiVectorStore with embedding stores and index names.\n\
    \nArgs:\n    embedding_stores: list[BaseVectorStore]\n        The vector stores\
    \ to combine in this multi-store.\n    index_names: list[str]\n        The corresponding\
    \ index names for each embedding store.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 35
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.__call__
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: __call__
  signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
  decorators: []
  raises:
  - NotImplementedError
  visibility: protected
  docstring: "Call method definition for a datasource to retrieve a DataFrame for\
    \ the given table and optional columns.\n\nArgs:\n  table: name of the table to\
    \ query\n  columns: optional list of column names to include; if None, all columns\
    \ are returned\n\nReturns:\n  pd.DataFrame: the DataFrame resulting from the call\n\
    \nRaises:\n  NotImplementedError"
  code_example: null
  example_source: null
  line_start: 28
  line_end: 30
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed_batch
  file: graphrag/language_model/protocol/base.py
  name: aembed_batch
  signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
    \    ) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously generate embedding vectors for the given list of strings.\n\
    \nArgs:\n    text_list: The list of strings to generate embeddings for.\n    **kwargs:\
    \ Additional keyword arguments (e.g., model parameters).\n\nReturns\n-------\n\
    \    list[list[float]]: A list of embedding vectors for each input item in the\
    \ batch.\n\nRaises:\n    Exception: If an error occurs during embedding generation."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 41
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_create_file_storage
  file: tests/integration/storage/test_factory.py
  name: test_create_file_storage
  signature: def test_create_file_storage()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test creating a file-based storage via the StorageFactory.\n\nReturns:\n\
    \    None\n\nRaises:\n    AssertionError: if the created storage is not an instance\
    \ of FilePipelineStorage."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 56
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat
  file: graphrag/language_model/providers/fnllm/models.py
  name: achat
  signature: "def achat(\n        self, prompt: str, history: list | None = None,\
    \ **kwargs\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Chat with the Model using the given prompt.\n\nThis method supports\
    \ an optional conversation history. If history is None, the\nmodel is called with\
    \ the prompt and any provided kwargs. If history is provided, it\nis sent to the\
    \ model along with the prompt.\n\nArgs:\n    prompt (str): The prompt to chat\
    \ with.\n    history (list | None): The conversation history to include in the\
    \ chat, or None for no history.\n    kwargs (dict[str, Any]): Additional keyword\
    \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
    \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
    \ model call may propagate."
  code_example: null
  example_source: null
  line_start: 268
  line_end: 297
  dependencies:
  - graphrag/language_model/response/base.py::BaseModelOutput
  - graphrag/language_model/response/base.py::BaseModelResponse
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model_provider
  file: graphrag/config/models/language_model_config.py
  name: _validate_model_provider
  signature: def _validate_model_provider(self) -> None
  decorators: []
  raises:
  - KeyError
  visibility: protected
  docstring: "Validate the model provider.\n\nThis validation applies only when the\
    \ model type is Chat or Embedding. If the model type is Chat or Embedding and\
    \ the model_provider is missing or blank, a KeyError is raised indicating that\
    \ a model provider must be specified for that type. For other model types, this\
    \ method performs no validation.\n\nArgs:\n    self: The instance of the language\
    \ model configuration. Note that self is an implicit parameter for Python methods.\n\
    \nReturns:\n    None\n\nRaises:\n    KeyError: If the model provider is missing\
    \ when the model type is Chat or Embedding."
  code_example: null
  example_source: null
  line_start: 115
  line_end: 129
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_model
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_model
  signature: def _validate_model(self)
  decorators:
  - '@model_validator(mode="after")'
  raises: []
  visibility: protected
  docstring: "Validate the model configuration after the initial schema validation.\n\
    \nThis is a post-schema, after-hook validator that returns the same instance after\
    \ performing a series of internal checks to ensure the model configuration is\
    \ consistent and ready for use.\n\nArgs:\n    self: The GraphRagConfig instance\
    \ being validated.\n\nReturns:\n    GraphRagConfig: The same instance after validation.\n\
    \nRaises:\n    ValueError: If a required configuration value is missing or invalid\
    \ during validation.\n\nValidations performed:\n- root_dir is valid\n- models\
    \ configuration is present and valid\n- input_pattern is correct\n- input_base_dir\
    \ is valid\n- reporting_base_dir is valid\n- output_base_dir is valid\n- multi_output_base_dirs\
    \ are valid\n- update_index_output_base_dir is valid\n- vector_store_db_uri is\
    \ valid\n- factories configuration is valid"
  code_example: null
  example_source: null
  line_start: 404
  line_end: 416
  dependencies:
  - graphrag/config/models/graph_rag_config.py::_validate_factories
  - graphrag/config/models/graph_rag_config.py::_validate_input_base_dir
  - graphrag/config/models/graph_rag_config.py::_validate_input_pattern
  - graphrag/config/models/graph_rag_config.py::_validate_models
  - graphrag/config/models/graph_rag_config.py::_validate_multi_output_base_dirs
  - graphrag/config/models/graph_rag_config.py::_validate_output_base_dir
  - graphrag/config/models/graph_rag_config.py::_validate_reporting_base_dir
  - graphrag/config/models/graph_rag_config.py::_validate_root_dir
  - graphrag/config/models/graph_rag_config.py::_validate_update_index_output_base_dir
  - graphrag/config/models/graph_rag_config.py::_validate_vector_store_db_uri
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_model
  file: graphrag/language_model/factory.py
  name: is_supported_model
  signature: 'def is_supported_model(cls, model_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Determine whether the provided model_type is supported by any registered\
    \ model backends (chat or embedding).\n\nArgs:\n    model_type: The type of model\
    \ to check.\n\nReturns:\n    bool: True if the model_type is registered as either\
    \ a chat model or an embedding model; otherwise False."
  code_example: null
  example_source: null
  line_start: 98
  line_end: 102
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: "def __call__(\n        self,\n        *,\n        request_id: str |\
    \ None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n    \
    \    # Optional params\n        dimensions: int | None = None,\n        encoding_format:\
    \ str | None = None,\n        timeout: int = 600,  # default to 10 minutes\n \
    \       # set api_base, api_version, api_key\n        api_base: str | None = None,\n\
    \        api_version: str | None = None,\n        api_key: str | None = None,\n\
    \        api_type: str | None = None,\n        caching: bool = False,\n      \
    \  user: str | None = None,\n        **kwargs: Any,\n    ) -> EmbeddingResponse"
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Embedding function.


    Args:

    - request_id: Optional request identifier

    - input: List input to embed

    - dimensions: Optional embedding dimensions

    - encoding_format: Optional encoding format

    - timeout: Timeout in seconds for the request (default 600)

    - api_base: Optional API base

    - api_version: Optional API version

    - api_key: Optional API key

    - api_type: Optional API type

    - caching: Whether to enable caching

    - user: Optional user identifier

    - kwargs: Additional keyword arguments that will be forwarded to the underlying
    request


    Returns:

    - EmbeddingResponse: The embedding result


    Raises:

    - None'
  code_example: null
  example_source: null
  line_start: 190
  line_end: 209
  dependencies: []
  called_by: []
- node_id: graphrag/cli/query.py::run_streaming_search
  file: graphrag/cli/query.py
  name: run_streaming_search
  signature: def run_streaming_search()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Run a streaming search and collect the full response while printing
    streamed chunks.


    Args:

    - None: The function does not take any parameters.


    Returns:

    - tuple[str, dict[str, Any]]: The full streaming response string and the context
    data captured during streaming.


    Raises:

    - Exceptions raised by the underlying streaming API or asyncio operations may
    propagate to the caller.'
  code_example: null
  example_source: null
  line_start: 438
  line_end: 460
  dependencies:
  - graphrag.api::basic_search_streaming
  - graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  called_by:
  - graphrag/cli/query.py::run_global_search
  - graphrag/cli/query.py::run_local_search
  - graphrag/cli/query.py::run_drift_search
  - graphrag/cli/query.py::run_basic_search
- node_id: graphrag/query/input/loaders/dfs.py::_prepare_records
  file: graphrag/query/input/loaders/dfs.py
  name: _prepare_records
  signature: 'def _prepare_records(df: pd.DataFrame) -> list[dict]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Reset the index of the DataFrame, rename the reset index column to 'Index',\
    \ and convert the result to a list of dictionaries.\n\nArgs:\n    df: The DataFrame\
    \ to process.\n\nReturns:\n    list[dict]: A list of dictionaries representing\
    \ the DataFrame rows. Each dictionary includes an 'Index' key for the original\
    \ row index."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 32
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_entities
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_covariates
  - graphrag/query/input/loaders/dfs.py::read_communities
  - graphrag/query/input/loaders/dfs.py::read_community_reports
  - graphrag/query/input/loaders/dfs.py::read_text_units
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.serialize
  file: graphrag/query/structured_search/drift_search/action.py
  name: serialize
  signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str, Any]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Serialize the action to a dictionary.\n\nSerializes the DriftAction\
    \ into a dictionary representation, including the core fields and, optionally,\
    \ serialized follow-up actions.\n\nArgs:\n    include_follow_ups (bool): Whether\
    \ to include follow-up actions in the serialization. The default is True; when\
    \ True, the result includes a \"follow_ups\" key containing a list of serialized\
    \ follow-up actions, each serialized by its own serialize() method.\n\nReturns:\n\
    \    dict[str, Any]: Serialized action as a dictionary with the following keys:\n\
    \        - \"query\": The query string.\n        - \"answer\": The answer.\n \
    \       - \"score\": The score.\n        - \"metadata\": Metadata dictionary.\n\
    \        - \"follow_ups\": (optional) List of serialized follow-up actions if\
    \ include_follow_ups is True (empty list if none)."
  code_example: null
  example_source: null
  line_start: 113
  line_end: 133
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._process_primer_results
  file: graphrag/query/structured_search/drift_search/search.py
  name: _process_primer_results
  signature: "def _process_primer_results(\n        self, query: str, search_results:\
    \ SearchResult\n    ) -> DriftAction"
  decorators: []
  raises:
  - RuntimeError
  - ValueError
  visibility: protected
  docstring: "Process the results from the primer search to extract intermediate answers\
    \ and follow-up queries.\n\nArgs:\n    query (str): The original search query.\n\
    \    search_results (SearchResult): The results from the primer search.\n\nReturns:\n\
    \    DriftAction: Action generated from the primer response.\n\nRaises:\n    RuntimeError:\
    \ If no intermediate answers or follow-up queries are found in the primer response.\n\
    \    ValueError: If the primer response is not a list of dictionaries."
  code_example: null
  example_source: null
  line_start: 110
  line_end: 156
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.progress
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: progress
  signature: 'def progress(self, progress: Progress) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle when progress occurs.\n\nArgs:\n    progress: Progress object\
    \ representing the current progress event.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 26
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/events/base.py::ModelEventHandler.on_error
  file: graphrag/language_model/events/base.py
  name: on_error
  signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
    \        traceback: str | None = None,\n        arguments: dict[str, Any] | None\
    \ = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle an model error.\n\nArgs:\n    error: BaseException | None: The\
    \ error that occurred, or None if no error is provided.\n    traceback: str |\
    \ None: The traceback string associated with the error, or None if not available.\n\
    \    arguments: dict[str, Any] | None: Additional contextual arguments related\
    \ to the error, or None.\nReturns:\n    None: The function does not return a value."
  code_example: null
  example_source: null
  line_start: 12
  line_end: 19
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/sidebar.py::update_global_search
  file: unified-search-app/app/ui/sidebar.py
  name: update_global_search
  signature: 'def update_global_search(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Update global rag state.\n\nArgs:\n    sv: SessionVariables\n      \
    \  The container of session variables; used to read and update the include_global_search\
    \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The function\
    \ does not return a value.\n\nRaises:\n    KeyError\n        If the expected key\
    \ sv.include_global_search.key is not found in st.session_state."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 45
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_end
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_map_response_end
  signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the end of map operation.\n\nArgs:\n    map_response_outputs:\
    \ list[SearchResult] - The outputs produced by the map operation.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 21
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.get_np_extractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  name: get_np_extractor
  signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Get the noun phrase extractor instance based on the configured type.\n\
    \nArgs:\n    cls: The class (used as a classmethod parameter).\n    config: TextAnalyzerConfig\
    \ containing extractor_type and related options such as model_name, max_word_length,\
    \ include_named_entities, exclude_entity_tags, exclude_pos_tags, exclude_nouns,\
    \ word_delimiter, noun_phrase_grammars, and noun_phrase_tags.\n\nReturns:\n  \
    \  BaseNounPhraseExtractor: An instance of the selected noun phrase extractor\
    \ (Syntactic, CFG, or RegexEnglish)."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 75
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor
  - graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor
  - graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor
  called_by: []
- node_id: graphrag/query/input/retrieval/relationships.py::get_candidate_relationships
  file: graphrag/query/input/retrieval/relationships.py
  name: get_candidate_relationships
  signature: "def get_candidate_relationships(\n    selected_entities: list[Entity],\n\
    \    relationships: list[Relationship],\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get candidate relationships associated with the selected entities.\n\
    \nArgs:\n    selected_entities: The selected entities for which to retrieve candidate\
    \ relationships.\n    relationships: The pool of relationships to filter.\n\n\
    Returns:\n    list[Relationship]: Relationships involving any of the selected\
    \ entities, i.e., where the relationship's source or target matches a selected\
    \ entity's title."
  code_example: null
  example_source: null
  line_start: 57
  line_end: 68
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType.__repr__
  file: graphrag/index/operations/extract_graph/typing.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of this ExtractEntityStrategyType enum member.\n\
    \nArgs:\n    self: ExtractEntityStrategyType, the enum member to represent as\
    \ a string.\n\nReturns:\n    str: The enum member's value enclosed in double quotes."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 57
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::ChatModel.chat
  file: graphrag/language_model/protocol/base.py
  name: chat
  signature: "def chat(\n        self, prompt: str, history: list | None = None, **kwargs:\
    \ Any\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given text.\n\nArgs:\n    prompt (str):\
    \ The text to generate a response for.\n    history (list | None): The conversation\
    \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
    \nReturns:\n    ModelResponse: The response for the given text.\n\nRaises:\n \
    \   Exception: If an error occurs during generation."
  code_example: null
  example_source: null
  line_start: 133
  line_end: 149
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.is_complete
  file: graphrag/query/structured_search/drift_search/action.py
  name: is_complete
  signature: def is_complete(self) -> bool
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Check if the action is complete (i.e., an answer is available).\n\n\
    Returns:\n    bool: True if an answer is available, False otherwise."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 51
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/builders.py::BasicContextBuilder.build_context
  file: graphrag/query/context_builder/builders.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Build the context for the basic search mode.\n\nArgs:\n    query: The\
    \ user query to build context for.\n    conversation_history: Optional conversation\
    \ history to consider while constructing the context.\n    **kwargs: Additional\
    \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
    \    ContextBuilderResult: The result containing the built context for the basic\
    \ search mode."
  code_example: null
  example_source: null
  line_start: 69
  line_end: 75
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/graphs.py::calculate_root_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_root_modularity
  signature: "def calculate_root_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the modularity of the graph's root clusters.\n\nThis function\
    \ applies Hierarchical Leiden to the input graph to generate a hierarchical clustering\
    \ and then uses the first_level_hierarchical_clustering (root level) to define\
    \ the root clusters. It returns the modularity of the graph with respect to these\
    \ root clusters.\n\nArgs:\n    graph (nx.Graph): The input graph.\n    max_cluster_size\
    \ (int): Maximum cluster size for the root-level clustering produced by Hierarchical\
    \ Leiden.\n    random_seed (int): Seed for the randomized algorithm to ensure\
    \ reproducibility.\n\nReturns:\n    float: The modularity score of the graph computed\
    \ against its root-level clusters.\n\nNotes:\n- The computation operates on the\
    \ entire graph and does not compare to any target modularity.\n- Root clusters\
    \ are obtained via first_level_hierarchical_clustering; internal steps involve\
    \ Hierarchical Leiden."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 30
  dependencies: []
  called_by:
  - graphrag/index/utils/graphs.py::calculate_graph_modularity
  - graphrag/index/utils/graphs.py::calculate_lcc_modularity
  - graphrag/index/utils/graphs.py::calculate_weighted_modularity
- node_id: graphrag/index/workflows/create_final_documents.py::create_final_documents
  file: graphrag/index/workflows/create_final_documents.py
  name: create_final_documents
  signature: "def create_final_documents(\n    documents: pd.DataFrame, text_units:\
    \ pd.DataFrame\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transforms input documents and text units into final documents.\n\n\
    Args:\n    documents: pd.DataFrame\n        Input documents data frame. Expected\
    \ to contain at least the columns referenced by DOCUMENTS_FINAL_COLUMNS.\n   \
    \ text_units: pd.DataFrame\n        Input text units data frame. Expected to contain\
    \ an 'document_ids' column indicating related document ids.\n\nReturns:\n    pd.DataFrame\n\
    \        Final documents data frame with columns defined by DOCUMENTS_FINAL_COLUMNS.\
    \ The function ensures a metadata column exists and assigns a human_readable_id\
    \ based on the row index.\n\nRaises:\n    Exception: Propagates exceptions raised\
    \ by pandas operations or data frame manipulations if inputs are invalid."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 77
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_final_documents.py::run_workflow
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::encode
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: encode
  signature: 'def encode(text: str) -> list[int]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Encode the input text into token IDs using the configured encoding model.\n\
    \nArgs:\n    text (str): The input to encode. If not a string, it will be converted\
    \ to a string.\n\nReturns:\n    list[int]: The encoded token IDs produced by the\
    \ encoding model.\n\nRaises:\n    Exception: If encoding fails with the configured\
    \ encoding model...."
  code_example: null
  example_source: null
  line_start: 136
  line_end: 139
  dependencies: []
  called_by:
  - tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
- node_id: graphrag/data_model/community_report.py::CommunityReport.from_dict
  file: graphrag/data_model/community_report.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
    \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n \
    \       summary_key: str = \"summary\",\n        full_content_key: str = \"full_content\"\
    ,\n        rank_key: str = \"rank\",\n        attributes_key: str = \"attributes\"\
    ,\n        size_key: str = \"size\",\n        period_key: str = \"period\",\n\
    \    ) -> \"CommunityReport\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a CommunityReport instance from the dict data.\n\nArgs:\n  cls\
    \ (type): The class.\n  d (dict[str, Any]): The source dictionary containing the\
    \ values for the CommunityReport fields.\n  id_key (str): Key in d for the report's\
    \ identifier. Defaults to \"id\".\n  title_key (str): Key in d for the report\
    \ title. Defaults to \"title\".\n  community_id_key (str): Key in d for the associated\
    \ community's id. Defaults to \"community\".\n  short_id_key (str): Key in d for\
    \ the optional short identifier. Defaults to \"human_readable_id\".\n  summary_key\
    \ (str): Key in d for the summary. Defaults to \"summary\".\n  full_content_key\
    \ (str): Key in d for the full content. Defaults to \"full_content\".\n  rank_key\
    \ (str): Key in d for the rank value. Defaults to \"rank\".\n  attributes_key\
    \ (str): Key in d for optional attributes dictionary. Defaults to \"attributes\"\
    .\n  size_key (str): Key in d for the size value. Defaults to \"size\".\n  period_key\
    \ (str): Key in d for the period. Defaults to \"period\".\n\nReturns:\n  CommunityReport:\
    \ The constructed CommunityReport instance.\n\nRaises:\n  KeyError: If a required\
    \ key is missing from d (e.g., id_key, title_key, community_id_key, summary_key,\
    \ full_content_key, rank_key)."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 67
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/derive_from_rows.py::execute_task
  file: graphrag/index/utils/derive_from_rows.py
  name: execute_task
  signature: 'def execute_task(task: Coroutine) -> ItemType | None'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Execute a coroutine task under a concurrency-limiting semaphore and
    return the awaited result.


    Args:

    - task: Coroutine. The coroutine to be awaited to obtain a thread-like awaitable,
    which is then awaited to produce the final ItemType result.


    Returns:

    - ItemType | None: The result produced by awaiting the retrieved thread from the
    task. May be None if the thread yields no value.


    Raises:

    - Propagates any exception raised by awaiting the input task or the retrieved
    thread (no error handling is performed here).'
  code_example: null
  example_source: null
  line_start: 78
  line_end: 82
  dependencies: []
  called_by:
  - graphrag/index/utils/derive_from_rows.py::gather
- node_id: tests/verbs/test_pipeline_state.py::run_workflow_2
  file: tests/verbs/test_pipeline_state.py
  name: run_workflow_2
  signature: "def run_workflow_2(  # noqa: RUF029\n    _config: GraphRagConfig, context:\
    \ PipelineRunContext\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Async function that increments the pipeline state's count by one.\n\n\
    Args:\n    _config: GraphRagConfig - Configuration for Graphrag configuration\n\
    \    context: PipelineRunContext - The PipelineRunContext for the current run;\
    \ its state is updated by this function\n\nReturns:\n    WorkflowFunctionOutput:\
    \ The function output; the result is None in this implementation.\n\nRaises:\n\
    \    KeyError: If 'count' is not present in context.state when attempting to increment."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 26
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.get_chat_model
  file: graphrag/language_model/manager.py
  name: get_chat_model
  signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Retrieve the ChatLLM instance registered under the given name.\n\nArgs:\n\
    \    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n    ChatModel:\
    \ The ChatLLM instance registered under the given name.\n\nRaises:\n    ValueError:\
    \ If no ChatLLM is registered under the name."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 90
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_requests_per_minute
  file: graphrag/config/models/language_model_config.py
  name: _validate_requests_per_minute
  signature: def _validate_requests_per_minute(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the requests per minute.\n\nThis is a Pydantic model validator\
    \ invoked on the LanguageModelConfig instance. Accepted values are integers >=\
    \ 1, the string 'auto', or null. When the type is 'Chat' or 'Embedding' and rate_limit_strategy\
    \ is not None, 'auto' is not allowed.\n\nRaises\n------\nValueError\n    If the\
    \ requests_per_minute is less than 1.\n    If requests_per_minute is 'auto' when\
    \ using type 'Chat' or 'Embedding' with a non-null rate_limit_strategy."
  code_example: null
  example_source: null
  line_start: 281
  line_end: 300
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_indexer
  file: tests/smoke/test_fixtures.py
  name: __run_indexer
  signature: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
    \ str,\n    )"
  decorators: []
  raises: []
  visibility: private
  docstring: "Run the indexer command for the given root and input file type and ensure\
    \ it completes successfully by invoking uv run poe index, including --verbose\
    \ when a debug flag is set.\n\nArgs:\n    root: Path\n        Path to the root\
    \ directory used for indexing.\n    input_file_type: str\n        The input file\
    \ type. This parameter is accepted for interface compatibility but is not used\
    \ in the function body.\n\nReturns:\n    None\n\nRaises:\n    AssertionError\n\
    \        If the indexer finishes with a non-zero return code."
  code_example: null
  example_source: null
  line_start: 127
  line_end: 148
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/create_graph.py::create_graph
  file: graphrag/index/operations/create_graph.py
  name: create_graph
  signature: "def create_graph(\n    edges: pd.DataFrame,\n    edge_attr: list[str\
    \ | int] | None = None,\n    nodes: pd.DataFrame | None = None,\n    node_id:\
    \ str = \"title\",\n) -> nx.Graph"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a NetworkX graph from edges and optional nodes dataframes.\n\n\
    Args:\n    edges (pd.DataFrame): DataFrame containing edge information for the\
    \ graph.\n    edge_attr (list[str | int] | None): List of edge attribute column\
    \ names (or None) to include as edge attributes.\n    nodes (pd.DataFrame | None):\
    \ Optional DataFrame containing node attributes to add to the graph. If provided,\
    \ nodes are added with attributes from this DataFrame.\n    node_id (str): Column\
    \ name to use as the node identifier when adding nodes from the nodes DataFrame.\n\
    \nReturns:\n    graph (nx.Graph): The constructed NetworkX Graph.\n\nRaises:\n\
    \    KeyError: If the specified node_id column does not exist in the nodes DataFrame\
    \ when indexing.\n    ValueError: If the edges DataFrame is missing required columns\
    \ or edge_attr is invalid for from_pandas_edgelist.\n    TypeError: If input data\
    \ types are incompatible with the underlying operations."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 23
  dependencies: []
  called_by:
  - graphrag/index/operations/finalize_entities.py::finalize_entities
  - graphrag/index/operations/finalize_relationships.py::finalize_relationships
  - graphrag/index/workflows/create_communities.py::create_communities
  - graphrag/index/workflows/finalize_graph.py::run_workflow
  - graphrag/index/workflows/prune_graph.py::prune_graph
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents
  file: tests/integration/vector_stores/test_lancedb.py
  name: sample_documents
  signature: def sample_documents(self)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create sample documents for testing.\n\nArgs:\n    self: Instance of\
    \ the test class used by pytest to provide fixture context.\n\nReturns:\n    List[VectorStoreDocument]:\
    \ A list of three VectorStoreDocument objects representing\n        the sample\
    \ documents with ids \"1\", \"2\", and \"3\"; texts \"This is document 1\",\n\
    \        \"This is document 2\", \"This is document 3\"; vectors as [0.1, 0.2,\
    \ 0.3, 0.4, 0.5],\n        [0.2, 0.3, 0.4, 0.5, 0.6], and [0.3, 0.4, 0.5, 0.6,\
    \ 0.7]; and attributes including\n        \"title\" as \"Doc 1\"/\"Doc 2\"/\"\
    Doc 3\" and \"category\" as \"test\" for each.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 21
  line_end: 42
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: unified-search-app/app/home_page.py::main
  file: unified-search-app/app/home_page.py
  name: main
  signature: def main()
  decorators: []
  raises: []
  visibility: public
  docstring: "Render the main Streamlit UI for the application as an asynchronous\
    \ coroutine that renders the UI as a side effect.\n\nThis coroutine renders the\
    \ primary GraphRAG UI by constructing the layout and widgets and displaying them\
    \ via Streamlit calls. It takes no parameters and returns None.\n\nArgs:\n   \
    \ None: This function takes no parameters.\n\nReturns:\n    None: This coroutine\
    \ does not return a value."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 256
  dependencies: []
  called_by: []
- node_id: graphrag/logger/factory.py::LoggerFactory.is_supported_type
  file: graphrag/logger/factory.py
  name: is_supported_type
  signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Check if the given logger type is supported.\n\nArgs:\n    cls: The\
    \ class reference (classmethod parameter).\n    reporting_type (str): The type\
    \ identifier for the logger.\n\nReturns:\n    bool: True if the reporting type\
    \ is registered in the registry, False otherwise."
  code_example: null
  example_source: null
  line_start: 76
  line_end: 78
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.from_primer_response
  file: graphrag/query/structured_search/drift_search/action.py
  name: from_primer_response
  signature: "def from_primer_response(\n        cls, query: str, response: str |\
    \ dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create a DriftAction from a DRIFTPrimer response.\n\nArgs:\n    query\
    \ (str): The query string.\n    response (str | dict[str, Any]): Primer response\
    \ data. The runtime accepts:\n        - a dictionary with keys:\n            -\
    \ follow_up_queries (list[dict[str, Any]]): actions to follow up with\n      \
    \      - intermediate_answer: the answer to present\n            - score (optional,\
    \ numeric): a score for the action\n        - a JSON string that decodes to such\
    \ a dictionary.\n\nReturns:\n    DriftAction: A new DriftAction instance populated\
    \ from the response. The instance's\n    query is set to the provided query; follow_ups,\
    \ answer, and score are populated\n    from the corresponding keys in the response.\n\
    \nRaises:\n    ValueError: If the response is not a dictionary or a JSON string\
    \ that decodes to a dictionary.\n    ValueError: If a JSON string cannot be parsed\
    \ or decodes to a non-dictionary value."
  code_example: null
  example_source: null
  line_start: 166
  line_end: 208
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_create_unknown_storage
  file: tests/integration/storage/test_factory.py
  name: test_create_unknown_storage
  signature: def test_create_unknown_storage()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that creating an unknown storage type raises a ValueError.\n\n\
    Returns:\n    None\n\nRaises:\n    ValueError: Unknown storage type: unknown"
  code_example: null
  example_source: null
  line_start: 99
  line_end: 101
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/search.py::convert_numbered_list_to_array
  file: unified-search-app/app/ui/search.py
  name: convert_numbered_list_to_array
  signature: def convert_numbered_list_to_array(numbered_list_str)
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a numbered-list string into an array of extracted items.\n\n\
    Args:\n    numbered_list_str: str-like\n        A string-like object containing\
    \ a numbered list. Each line that matches a numeric dot pattern (one or more digits\
    \ followed by a dot and optional whitespace) will have the text after the marker\
    \ extracted as an item. Non-matching lines are ignored. The order of extracted\
    \ items matches their appearance in the input.\n\nReturns:\n    list[str]\n  \
    \      A list of extracted items in the input order.\n\nRaises:\n    AttributeError\n\
    \        If numbered_list_str does not support strip or split (i.e., is not a\
    \ string-like object)."
  code_example: null
  example_source: null
  line_start: 158
  line_end: 169
  dependencies: []
  called_by:
  - unified-search-app/app/ui/search.py::format_suggested_questions
- node_id: unified-search-app/app/knowledge_loader/model.py::load_entity_relationships
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_entity_relationships
  signature: "def load_entity_relationships(\n    dataset: str,\n    _datasource:\
    \ Datasource,\n) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a DataFrame containing the entity-relationship data loaded from\
    \ the given dataset and datasource.\n\nArgs:\n  dataset: str \u2014 The dataset\
    \ identifier to load the entity-relationship data from.\n  _datasource: Datasource\
    \ \u2014 The Datasource descriptor used to access the data.\n\nReturns:\n  pd.DataFrame\
    \ \u2014 DataFrame containing the relationship data as produced by get_relationship_data.\
    \ The specific columns depend on the underlying data_prep implementation.\n\n\
    Raises:\n  Exception \u2014 Propagates any exceptions raised by get_relationship_data."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 44
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.has
  file: graphrag/storage/memory_pipeline_storage.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Return True if the given key exists in the storage.\n\nArgs:\n\
    \    key (str): The key to check for.\n\nReturns:\n    bool: True if the key exists\
    \ in the storage, False otherwise.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 48
  line_end: 58
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockChatLLM.achat_stream
  file: tests/mock_provider.py
  name: achat_stream
  signature: "def achat_stream(\n        self,\n        prompt: str,\n        history:\
    \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously stream the configured responses from the mock provider.\n\
    \nThis generator yields each configured response in order. It does not use the\
    \ input\nprompt or history for generation.\n\nArgs:\n    prompt (str): The input\
    \ prompt to process. This implementation ignores it.\n    history (list | None):\
    \ Optional conversation history. This implementation ignores it.\n    **kwargs:\
    \ Additional keyword arguments forwarded to the underlying handler.\n\nReturns:\n\
    \    AsyncGenerator[str, None]: An asynchronous generator yielding response strings.\
    \ If a configured response is a BaseModel, it is converted to JSON using model_dump_json();\
    \ otherwise the response is yielded as-is.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 45
  line_end: 62
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_start
  file: graphrag/callbacks/workflow_callbacks.py
  name: workflow_start
  signature: 'def workflow_start(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n    name\
    \ (str): The name of the workflow starting.\n    instance (object): The workflow\
    \ instance object associated with this start event.\n\nReturns:\n    None\n\n\
    Raises:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 27
  line_end: 29
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/covariates.py::to_covariate_dataframe
  file: graphrag/query/input/retrieval/covariates.py
  name: to_covariate_dataframe
  signature: 'def to_covariate_dataframe(covariates: list[Covariate]) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a list of covariates to a pandas DataFrame.\n\nArgs:\n    covariates:\
    \ list[Covariate] - Covariate objects to convert. Each covariate is expected to\
    \ have short_id, subject_id, and attributes (a dict). The resulting DataFrame\
    \ will have columns: id, entity, and one column per attribute key found in the\
    \ first covariate's attributes (excluding id and entity). Values are strings or\
    \ empty strings when missing.\n\nReturns:\n    pd.DataFrame - DataFrame representation\
    \ of the covariates. If covariates is empty, returns an empty DataFrame.\n\nRaises:\n\
    \    AttributeError, TypeError: If covariates do not conform to the expected Covariate\
    \ interface or if attribute access fails."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 53
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: graphrag/query/context_builder/source_context.py::count_relationships
  file: graphrag/query/context_builder/source_context.py
  name: count_relationships
  signature: "def count_relationships(\n    entity_relationships: list[Relationship],\
    \ text_unit: TextUnit\n) -> int"
  decorators: []
  raises: []
  visibility: public
  docstring: "Count the number of relationships of the selected entity that are associated\
    \ with the text unit.\n\nArgs:\n    entity_relationships: list[Relationship]\n\
    \        The relationships for the selected entity.\n    text_unit: TextUnit\n\
    \        The text unit for which to count related relationships.\n\nReturns:\n\
    \    int\n        The number of relationships in entity_relationships that are\
    \ associated with the given text_unit. If the text_unit has no relationship_ids,\
    \ this is the count of relationships whose text_unit_ids contain the text_unit's\
    \ id; otherwise it is the count of relationships whose id appears in text_unit.relationship_ids."
  code_example: null
  example_source: null
  line_start: 82
  line_end: 100
  dependencies: []
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_output_base_dir
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_output_base_dir
  signature: def _validate_output_base_dir(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the output base directory.\n\nArgs:\n    self: The instance\
    \ of the configuration model containing the output configuration and root_dir.\n\
    \nReturns:\n    None. This method updates output.base_dir to an absolute path\
    \ derived from root_dir joined with the provided base_dir when the output type\
    \ is file.\n\nRaises:\n    ValueError: If the output storage type is file and\
    \ the output base_dir is empty."
  code_example: null
  example_source: null
  line_start: 174
  line_end: 182
  dependencies: []
  called_by: []
- node_id: graphrag/index/validate_config.py::validate_config_names
  file: graphrag/index/validate_config.py
  name: validate_config_names
  signature: 'def validate_config_names(parameters: GraphRagConfig) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Validate config file for model deployment name typos, by running a quick\
    \ test message for each.\n\nArgs:\n  parameters: GraphRagConfig containing models\
    \ to validate.\n\nReturns:\n  None\n\nRaises:\n  SystemExit: If validation fails\
    \ for any model; the process exits with status 1."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 53
  dependencies:
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/language_model/manager.py::ModelManager
  called_by:
  - graphrag/cli/index.py::_run_index
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.child
  file: graphrag/storage/pipeline_storage.py
  name: child
  signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Create or return a child storage instance.\n\nThis method creates and\
    \ returns a dedicated child storage instance. The optional name parameter is accepted\
    \ for API compatibility but may be ignored by the implementing class. The behavior\
    \ with the name (whether it selects a specific child or is ignored) is determined\
    \ by the concrete implementation.\n\nArgs:\n    name (str | None): Optional name\
    \ for the child storage. This parameter is accepted for API compatibility but\
    \ may be ignored by the implementation.\n\nReturns:\n    PipelineStorage: The\
    \ child storage instance corresponding to the provided name."
  code_example: null
  example_source: null
  line_start: 75
  line_end: 76
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_large_input
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_text_large_input
  signature: def test_split_text_large_input(mock_split)
  decorators:
  - '@mock.patch("graphrag.index.text_splitting.text_splitting.split_single_text_on_tokens")'
  raises: []
  visibility: public
  docstring: "Tests that TokenTextSplitter.split_text handles a large input by delegating\
    \ to split_single_text_on_tokens and returning the expected number of chunks.\n\
    \nArgs:\n    mock_split: The patched mock for split_single_text_on_tokens used\
    \ to simulate splitting behavior.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If the resulting number of chunks is not 2000 or if the patched function was\
    \ not called exactly once. No exceptions are expected under normal execution."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 63
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: tests/unit/config/utils.py::assert_text_analyzer_configs
  file: tests/unit/config/utils.py
  name: assert_text_analyzer_configs
  signature: "def assert_text_analyzer_configs(\n    actual: TextAnalyzerConfig, expected:\
    \ TextAnalyzerConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two TextAnalyzerConfig objects are equal for all relevant\
    \ fields.\n\nArgs:\n    actual: TextAnalyzerConfig - The actual configuration\
    \ to validate.\n    expected: TextAnalyzerConfig - The expected configuration\
    \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError - If\
    \ any of the compared fields do not match."
  code_example: null
  example_source: null
  line_start: 237
  line_end: 249
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_extract_graph_nlp_configs
- node_id: graphrag/utils/api.py::reformat_context_data
  file: graphrag/utils/api.py
  name: reformat_context_data
  signature: 'def reformat_context_data(context_data: dict) -> dict'
  decorators: []
  raises: []
  visibility: public
  docstring: "Reformats context_data for all query responses.\n\nReformats a dictionary\
    \ of dataframes into a dictionary of lists. One list entry for each\nrecord. Records\
    \ are grouped by original dictionary keys.\n\nNote: depending on which query algorithm\
    \ is used, the context_data may not contain the same information (keys).\nIn this\
    \ case, the default behavior will be to set these keys as empty lists to preserve\
    \ a standard output format.\n\nArgs:\n    context_data: dict\n        A mapping\
    \ from key to either a pandas DataFrame-like object with to_dict(orient='records')\n\
    \        or to a dict, or to None. DataFrames are converted to a list of dictionaries\
    \ representing\n        records. If a value is already a dict, it is used as-is.\
    \ If a key yields no records, the\n        key will be left with its default empty\
    \ list.\n\nReturns:\n    dict\n        A dictionary containing the reformatted\
    \ data. It starts with the keys\n        \"reports\", \"entities\", \"relationships\"\
    , \"claims\", and \"sources\" initialized to empty\n        lists; for input keys\
    \ with data, the corresponding value is replaced with the list of\n        records\
    \ (or the dict if the input value was a dict).\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 144
  line_end: 172
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/create_community_reports.py::_prep_edges
  file: graphrag/index/workflows/create_community_reports.py
  name: _prep_edges
  signature: 'def _prep_edges(input: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Prepare edges data by filling missing descriptions and constructing\
    \ the EDGE_DETAILS field.\n\nArgs:\n    input: pd.DataFrame. The input DataFrame\
    \ containing edge data. Missing DESCRIPTION values are filled with \"No Description\"\
    \ and a new EDGE_DETAILS column is created from SHORT_ID, EDGE_SOURCE, EDGE_TARGET,\
    \ DESCRIPTION, and EDGE_DEGREE.\n\nReturns:\n    pd.DataFrame: The input DataFrame\
    \ augmented with an EDGE_DETAILS column. The function mutates the input in place.\n\
    \nRaises:\n    KeyError: If input is missing any of the required columns: SHORT_ID,\
    \ EDGE_SOURCE, EDGE_TARGET, DESCRIPTION, or EDGE_DEGREE."
  code_example: null
  example_source: null
  line_start: 161
  line_end: 177
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
- node_id: graphrag/query/input/loaders/utils.py::to_optional_int
  file: graphrag/query/input/loaders/utils.py
  name: to_optional_int
  signature: 'def to_optional_int(data: Mapping[str, Any], column_name: str | None)
    -> int | None'
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to an optional int.\n\nIf the specified\
    \ column is missing from data or is None, returns None. If the value is a float,\
    \ it will be converted to an int by truncation. If the resulting value is not\
    \ an int, a TypeError is raised.\n\nArgs:\n    data (Mapping[str, Any]): Input\
    \ data mapping containing potential value\n    column_name (str | None): Key to\
    \ retrieve from data; if None or not present, returns None\n\nReturns:\n    int\
    \ | None: The value converted to int, or None if missing or None\n\nRaises:\n\
    \    TypeError: If the value cannot be interpreted as an int after conversion"
  code_example: null
  example_source: null
  line_start: 102
  line_end: 114
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_entities
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_text_units
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::_wrapped_with_logging_async
  file: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py
  name: _wrapped_with_logging_async
  signature: "def _wrapped_with_logging_async(\n        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wraps the asynchronous request function with logging.\n\nArgs:\n   \
    \ kwargs: Keyword arguments passed to the underlying asynchronous request function.\n\
    \nReturns:\n    Any: The value returned by the underlying async_fn when called\
    \ with the provided kwargs.\n\nRaises:\n    Exception: Re-raised after logging\
    \ the exception encountered during the call."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 54
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat_stream
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: chat_stream
  signature: "def chat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs: Any\n    ) -> Generator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given prompt and history.\n\nArgs:\n   \
    \ prompt (str): The prompt to generate a response for.\n    history (list[dict[str,\
    \ str]] | None): Optional chat history represented as a list of messages. Each\
    \ message is a dict with keys such as \"role\" and \"content\".\n    kwargs (Any):\
    \ Additional keyword arguments (e.g., model parameters).\n\nReturns:\n    Generator[str,\
    \ None]: The generated response as a stream of strings.\n\nRaises:\n    Exception:\
    \ Exceptions raised by the underlying streaming mechanism or model client may\
    \ propagate to the caller."
  code_example: null
  example_source: null
  line_start: 390
  line_end: 414
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/query/indexer_adapters.py::_filter_under_community_level
  file: graphrag/query/indexer_adapters.py
  name: _filter_under_community_level
  signature: "def _filter_under_community_level(\n    df: pd.DataFrame, community_level:\
    \ int\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Filter a DataFrame by community level.\n\nArgs:\n    df: pandas DataFrame\
    \ that must contain a column named level used for filtering.\n    community_level:\
    \ int threshold; keep rows where level <= community_level.\n\nReturns:\n    pandas.DataFrame\
    \ with rows where level <= community_level.\n\nRaises:\n    AttributeError: if\
    \ the input DataFrame does not have a level column."
  code_example: null
  example_source: null
  line_start: 238
  line_end: 244
  dependencies: []
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_reports
  - graphrag/query/indexer_adapters.py::read_indexer_entities
- node_id: graphrag/query/input/retrieval/entities.py::is_valid_uuid
  file: graphrag/query/input/retrieval/entities.py
  name: is_valid_uuid
  signature: 'def is_valid_uuid(value: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Determine if a string is a valid UUID.\n\nArgs:\n    value: str.\
    \ The string to validate as a UUID.\n\nReturns:\n    bool: True if the string\
    \ is a valid UUID, False otherwise.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 95
  line_end: 102
  dependencies: []
  called_by:
  - graphrag/query/input/retrieval/entities.py::get_entity_by_id
  - graphrag/query/input/retrieval/entities.py::get_entity_by_key
- node_id: graphrag/logger/standard_logging.py::init_loggers
  file: graphrag/logger/standard_logging.py
  name: init_loggers
  signature: "def init_loggers(\n    config: GraphRagConfig,\n    verbose: bool =\
    \ False,\n    filename: str = DEFAULT_LOG_FILENAME,\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Initialize logging for graphrag based on configuration.\n\nConfigures\
    \ the top-level 'graphrag' logger with a handler derived from the provided GraphRagConfig.\
    \ It sets the log level to DEBUG when verbose is True, otherwise INFO. Before\
    \ attaching the new handler, all existing handlers on the logger are removed;\
    \ any FileHandler instances are closed to avoid resource leaks and duplicate logs.\n\
    \nArgs:\n    config (GraphRagConfig): The GraphRagConfig instance providing logging\
    \ settings (including reporting and root_dir).\n    verbose (bool): If True, enable\
    \ DEBUG logging; otherwise INFO.\n    filename (str): The log filename on disk.\
    \ If not provided, defaults to DEFAULT_LOG_FILENAME.\n\nReturns:\n    None\n\n\
    Raises:\n    Propagates exceptions from internal components (for example, LoggerFactory.create_logger)\
    \ if encountered."
  code_example: null
  example_source: null
  line_start: 50
  line_end: 83
  dependencies: []
  called_by:
  - graphrag/api/index.py::build_index
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
  - graphrag/api/query.py::global_search
  - graphrag/api/query.py::global_search_streaming
  - graphrag/api/query.py::multi_index_global_search
  - graphrag/api/query.py::local_search
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::multi_index_local_search
  - graphrag/api/query.py::drift_search
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/api/query.py::multi_index_drift_search
  - graphrag/api/query.py::basic_search
  - graphrag/api/query.py::basic_search_streaming
  - graphrag/api/query.py::multi_index_basic_search
  - graphrag/cli/index.py::_run_index
  - graphrag/cli/prompt_tune.py::prompt_tune
  - tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_token_text_splitter
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_token_text_splitter
  signature: def test_token_text_splitter(mock_tokenizer, mock_split_text)
  decorators:
  - '@mock.patch("graphrag.index.text_splitting.text_splitting.split_single_text_on_tokens")'
  - '@mock.patch("graphrag.index.text_splitting.text_splitting.TokenChunkerOptions")'
  raises: []
  visibility: public
  docstring: "Test that TokenTextSplitter.split_text delegates to split_single_text_on_tokens\
    \ with the given text and tokenizer.\n\nParameters:\n    mock_tokenizer (MagicMock):\
    \ Patch object that mocks the tokenizer factory; its return_value is the mocked\
    \ tokenizer used as the tokenizer argument to split_text.\n    mock_split_text\
    \ (MagicMock): Patch object for split_single_text_on_tokens; its return value\
    \ is the expected list of chunks.\n\nReturns:\n    None: This test does not return\
    \ a value.\n\nRaises:\n    AssertionError: If the expected call to split_single_text_on_tokens\
    \ is not made with the correct arguments."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 80
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig.resolved_strategy
  file: graphrag/config/models/extract_claims_config.py
  name: resolved_strategy
  signature: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
    \    ) -> dict"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the resolved claim extraction strategy.\n\nArgs:\n    root_dir:\
    \ The root directory used to resolve the graph and text prompt file paths.\n \
    \   model_config: The LanguageModelConfig instance containing the model configuration;\
    \ its model_dump() result is included in the strategy as llm.\n\nReturns:\n  \
    \  dict: The resolved strategy. If self.strategy is provided, it is returned as-is;\
    \ otherwise, a dict with the following keys:\n        llm: The result of model_config.model_dump().\n\
    \        extraction_prompt: The contents of the prompt file located at Path(root_dir)\
    \ / self.prompt read as UTF-8, or None if no prompt is configured.\n        claim_description:\
    \ The description from self.description.\n        max_gleanings: The max_gleanings\
    \ value from self.max_gleanings.\n\nRaises:\n    FileNotFoundError: If a prompt\
    \ is configured and the prompt file cannot be read."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 55
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.aretry
  file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  name: aretry
  signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
    \        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry an asynchronous function until it succeeds or the maximum number\
    \ of retries is reached.\n\nArgs:\n    func: The asynchronous function to retry.\n\
    \    kwargs: Additional keyword arguments to pass to the function.\n\nReturns:\n\
    \    Any: The result of the awaited function.\n\nRaises:\n    Exception: If the\
    \ wrapped function keeps raising and the maximum number of retries is exceeded."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 66
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.filter_by_id
  file: graphrag/vector_stores/cosmosdb.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Builds a query filter to filter documents by a list of ids.\n\nArgs:\n\
    \  include_ids: list[str] | list[int]\n    The IDs to include in the filter. If\
    \ None or an empty list is provided, the filter is cleared (set to None) and None\
    \ is returned.\n\nReturns:\n  Any\n    The constructed query filter string to\
    \ filter documents by the provided IDs, or None if no IDs are provided."
  code_example: null
  example_source: null
  line_start: 254
  line_end: 266
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.keys
  file: tests/integration/storage/test_factory.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises: []
  visibility: public
  docstring: 'Return a list of keys stored in the storage.


    Args:

    self: The storage instance.


    Returns:

    list[str]: A list of keys as strings.


    Raises:

    This implementation does not raise any exceptions.'
  code_example: null
  example_source: null
  line_start: 145
  line_end: 146
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat_stream
  file: graphrag/language_model/providers/fnllm/models.py
  name: achat_stream
  signature: "def achat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
    \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
    \ Additional arguments to pass to the Model.\n\nReturns:\n    An asynchronous\
    \ generator that yields non-None strings representing the response.\n\nRaises:\n\
    \    Propagates exceptions raised by the underlying model call or streaming response."
  code_example: null
  example_source: null
  line_start: 299
  line_end: 320
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_create_cosmosdb_vector_store
  file: tests/integration/vector_stores/test_factory.py
  name: test_create_cosmosdb_vector_store
  signature: def test_create_cosmosdb_vector_store()
  decorators:
  - '@pytest.mark.skip(reason="CosmosDB requires credentials and setup")'
  raises: []
  visibility: public
  docstring: "Test creating a CosmosDB vector store via the VectorStoreFactory.\n\n\
    Returns:\n    None"
  code_example: null
  example_source: null
  line_start: 51
  line_end: 65
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder.build_context
  file: graphrag/query/context_builder/builders.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Build the context for the global search mode.\n\nArgs:\n  query: The\
    \ user query to build context for.\n  conversation_history: Optional conversation\
    \ history to consider while constructing the context.\n  **kwargs: Additional\
    \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
    \  ContextBuilderResult: The result containing the built context."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/strategies.py::decode
  file: graphrag/index/operations/chunk_text/strategies.py
  name: decode
  signature: 'def decode(tokens: list[int]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Decode a list of tokens back into a string.\n\nArgs:\n    tokens\
    \ (list[int]): A list of tokens to decode.\n\nReturns:\n    str: The decoded string\
    \ from the list of tokens.\n\nRaises:\n    Exception: If decoding fails due to\
    \ an underlying error in the encoding.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 29
  line_end: 30
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::cleanup
  file: tests/smoke/test_fixtures.py
  name: cleanup
  signature: 'def cleanup(skip: bool = False)'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Decorator to cleanup the output and cache folders after each test.


    Args:

    - skip: bool, optional. If True, skip cleanup of output and cache folders. Default
    is False.


    Returns:

    - A decorator that wraps a test function and performs cleanup after the test.


    Raises:

    - AssertionError: If the wrapped function raises an AssertionError, it is re-raised.'
  code_example: null
  example_source: null
  line_start: 71
  line_end: 89
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/types.py::DocSelectionType.__str__
  file: graphrag/prompt_tune/types.py
  name: __str__
  signature: def __str__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return the string representation of the enum value.\n\nArgs:\n    self:\
    \ The enum member.\n\nReturns:\n    str: The string representation of the enum\
    \ value."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 19
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_empty_collection
  file: tests/integration/vector_stores/test_lancedb.py
  name: test_empty_collection
  signature: def test_empty_collection(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test creating an empty LanceDB collection, deleting a loaded document,\
    \ and then adding a new document.\n\nArgs:\n    self: The test case instance.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails."
  code_example: null
  example_source: null
  line_start: 127
  line_end: 171
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get
  file: graphrag/storage/blob_pipeline_storage.py
  name: get
  signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
    \ str | None = None\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get a value from the cache.\n\nArgs:\n    key: The key to retrieve from\
    \ the cache.\n    as_bytes: If True, return the raw bytes stored for the key;\
    \ if False, decode the data to a string using encoding.\n    encoding: Encoding\
    \ to use when decoding the value if as_bytes is False.\n\nReturns:\n    Any: The\
    \ value associated with the key. If as_bytes is False, the value is decoded to\
    \ a string using the provided encoding or the object's default encoding. If an\
    \ error occurs, returns None.\n\nRaises:\n    None: This method does not raise\
    \ any exceptions; it logs a warning and returns None on error."
  code_example: null
  example_source: null
  line_start: 178
  line_end: 196
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_keyname
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.has
  file: graphrag/language_model/providers/fnllm/cache.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously check if the cache has a value for the given key.\n\n\
    Args:\n    key: The cache key to check.\n\nReturns:\n    bool: True if a value\
    \ exists for the key in the cache, otherwise False."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 21
  dependencies: []
  called_by: []
- node_id: graphrag/query/indexer_adapters.py::read_indexer_report_embeddings
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_report_embeddings
  signature: "def read_indexer_report_embeddings(\n    community_reports: list[CommunityReport],\n\
    \    embeddings_store: BaseVectorStore,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Community Reports from the raw indexing outputs.\n\nArgs:\n\
    \  community_reports: list[CommunityReport] - The community reports to enrich\
    \ with embeddings.\n  embeddings_store: BaseVectorStore - The vector store used\
    \ to fetch embeddings by report id.\n\nReturns:\n  None - This function mutates\
    \ the input CommunityReport objects by setting their full_content_embedding."
  code_example: null
  example_source: null
  line_start: 130
  line_end: 136
  dependencies: []
  called_by:
  - graphrag/api/query.py::drift_search_streaming
- node_id: graphrag/query/input/retrieval/relationships.py::to_relationship_dataframe
  file: graphrag/query/input/retrieval/relationships.py
  name: to_relationship_dataframe
  signature: "def to_relationship_dataframe(\n    relationships: list[Relationship],\
    \ include_relationship_weight: bool = True\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a list of relationships to a pandas dataframe.\n\nArgs:\n  relationships:\
    \ list[Relationship] - List of Relationship objects to convert to a pandas DataFrame.\n\
    \  include_relationship_weight: bool - Whether to include the weight column in\
    \ the output.\n\nReturns:\n  pd.DataFrame - A DataFrame representing the relationships.\
    \ If the relationships list is empty, an empty DataFrame is returned. The DataFrame\
    \ contains columns: id, source, target, description, and optionally weight. It\
    \ also includes any additional attribute columns derived from the first relationship's\
    \ attributes (excluding any columns already present in the header). The rows reflect\
    \ each relationship's short_id, source, target, description, weight, and attribute\
    \ values as strings where available."
  code_example: null
  example_source: null
  line_start: 105
  line_end: 139
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
  file: graphrag/index/utils/graphs.py
  name: calculate_pmi_edge_weights
  signature: "def calculate_pmi_edge_weights(\n    nodes_df: pd.DataFrame,\n    edges_df:\
    \ pd.DataFrame,\n    node_name_col: str = \"title\",\n    node_freq_col: str =\
    \ \"frequency\",\n    edge_weight_col: str = \"weight\",\n    edge_source_col:\
    \ str = \"source\",\n    edge_target_col: str = \"target\",\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate pointwise mutual information (PMI) edge weights for a graph.\n\
    \nArgs:\n    nodes_df (pd.DataFrame): DataFrame containing node information with\
    \ at least the columns\n        specified by node_name_col and node_freq_col.\n\
    \    edges_df (pd.DataFrame): DataFrame containing edge information with at least\
    \ the columns\n        specified by edge_weight_col, edge_source_col, and edge_target_col.\n\
    \    node_name_col (str): Column in nodes_df that identifies the node name.\n\
    \    node_freq_col (str): Column in nodes_df that contains the frequency/count\
    \ for each node.\n    edge_weight_col (str): Column in edges_df that contains\
    \ the raw edge weights.\n    edge_source_col (str): Column in edges_df that identifies\
    \ the source node.\n    edge_target_col (str): Column in edges_df that identifies\
    \ the target node.\n\nReturns:\n    pd.DataFrame: A DataFrame with PMI-weighted\
    \ edge weights computed as:\n        pmi(x,y) = p(x,y) * log2(p(x,y) / (p(x) *\
    \ p(y)))\n        where p(x,y) = edge_weight(x,y) / total_edge_weights and\n \
    \       p(x) = freq(x) / total_freq_occurrences. The result is the input edges_df\n\
    \        with the edge weights updated to the PMI value, and intermediate temporary\n\
    \        columns removed (prop_weight, source_prop, target_prop).\n\nRaises:\n\
    \    KeyError: If any of the required columns are missing from the input DataFrames."
  code_example: null
  example_source: null
  line_start: 155
  line_end: 201
  dependencies: []
  called_by:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
  - graphrag/index/utils/graphs.py::calculate_rrf_edge_weights
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.__init__
  file: graphrag/vector_stores/cosmosdb.py
  name: __init__
  signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
    \ **kwargs: Any\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize CosmosDB vector store by delegating to the base class constructor.\n\
    \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema configuration\
    \ for the vector store.\n  **kwargs: Any - Additional keyword arguments forwarded\
    \ to the base class initializer.\n\nReturns:\n  None\n\nRaises:\n  Exceptions\
    \ raised by the base class __init__ are propagated."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/index/run/utils.py::create_callback_chain
  file: graphrag/index/run/utils.py
  name: create_callback_chain
  signature: "def create_callback_chain(\n    callbacks: list[WorkflowCallbacks] |\
    \ None,\n) -> WorkflowCallbacks"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a callback manager that encompasses multiple callbacks.\n\nArgs:\n\
    \    callbacks: list[WorkflowCallbacks] | None. The callbacks to register on the\
    \ manager. If None, an empty list is used.\n\nReturns:\n    WorkflowCallbacks:\
    \ A manager that aggregates the provided callbacks."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 48
  dependencies:
  - graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager
  called_by:
  - graphrag/api/index.py::build_index
- node_id: graphrag/logger/factory.py::LoggerFactory.register
  file: graphrag/logger/factory.py
  name: register
  signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
    \ logging.Handler]\n    ) -> None"
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a custom logger implementation.\n\nThis is a classmethod on\
    \ LoggerFactory. It updates the internal registry (cls._registry) by storing a\
    \ mapping from the provided reporting_type to the given creator callable. The\
    \ registry is consulted by create_logger to instantiate loggers for the requested\
    \ type.\n\nArgs:\n    reporting_type: The type identifier for the logger.\n  \
    \  creator: A class or callable that initializes and returns a logging.Handler\
    \ instance.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 36
  line_end: 45
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._create_container
  file: graphrag/storage/blob_pipeline_storage.py
  name: _create_container
  signature: def _create_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create the blob container if it does not exist.\n\nArgs:\n    self:\
    \ The instance of the class containing the BlobServiceClient and container configuration.\n\
    \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
    \ Blob Storage client operations may be raised."
  code_example: null
  example_source: null
  line_start: 76
  line_end: 85
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_container_exists
  called_by: []
- node_id: graphrag/index/workflows/extract_graph.py::_validate_data
  file: graphrag/index/workflows/extract_graph.py
  name: _validate_data
  signature: 'def _validate_data(df: pd.DataFrame) -> bool'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Validate that the dataframe has data.\n\nArgs:\n    df (pd.DataFrame):\
    \ DataFrame to validate.\n\nReturns:\n    bool: True if the DataFrame contains\
    \ at least one row, False otherwise."
  code_example: null
  example_source: null
  line_start: 162
  line_end: 164
  dependencies: []
  called_by:
  - graphrag/index/workflows/extract_graph.py::extract_graph
- node_id: graphrag/utils/cli.py::dir_exist
  file: graphrag/utils/cli.py
  name: dir_exist
  signature: def dir_exist(path)
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check for directory existence.\n\nArgs:\n    path (str): Path\
    \ to the directory.\n\nReturns:\n    str: The input path if the directory exists.\n\
    \nRaises:\n    argparse.ArgumentTypeError: If the directory does not exist.\n\"\
    \"\""
  code_example: null
  example_source: null
  line_start: 19
  line_end: 24
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.cosine_similarity
  file: graphrag/vector_stores/cosmosdb.py
  name: cosine_similarity
  signature: def cosine_similarity(a, b)
  decorators: []
  raises: []
  visibility: public
  docstring: "Cosine similarity between two vectors.\n\nArgs:\n  a: First vector.\n\
    \  b: Second vector.\n\nReturns:\n  float: The cosine similarity between a and\
    \ b. If either vector has zero magnitude, returns 0.0."
  code_example: null
  example_source: null
  line_start: 214
  line_end: 217
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.delete
  file: graphrag/storage/blob_pipeline_storage.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Delete a key from the cache.\n\nArgs:\n    key (str): The key to delete.\n\
    \nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 261
  line_end: 268
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_keyname
  called_by: []
- node_id: graphrag/prompt_tune/loader/input.py::_sample_chunks_from_embeddings
  file: graphrag/prompt_tune/loader/input.py
  name: _sample_chunks_from_embeddings
  signature: "def _sample_chunks_from_embeddings(\n    text_chunks: pd.DataFrame,\n\
    \    embeddings: np.ndarray[float, np.dtype[np.float_]],\n    k: int = K,\n) ->\
    \ pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Sample k text chunks whose embeddings are closest to the center of the\
    \ embedding set.\n\nArgs:\n  text_chunks: DataFrame containing text chunks to\
    \ sample from.\n  embeddings: Array of embedding vectors corresponding to the\
    \ text chunks.\n  k: Number of chunks to sample (default K).\n\nReturns:\n  DataFrame\
    \ containing the sampled text chunks.\n  The rows correspond to the k chunks with\
    \ embeddings closest to the mean embedding.\n\nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 28
  line_end: 38
  dependencies: []
  called_by:
  - graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.register
  file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  name: register
  signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a noun phrase extractor in NounPhraseExtractorFactory by adding\
    \ it to the class-level registry np_extractor_types. This is a classmethod that\
    \ updates the mapping from extractor type identifiers to extractor classes, enabling\
    \ get_np_extractor to instantiate the correct extractor based on configuration.\n\
    \nArgs:\n    cls: The class on which this classmethod is invoked.\n    np_extractor_type:\
    \ str The identifier for the noun phrase extractor type.\n    np_extractor: type\
    \ The extractor class/type to register for the given type.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 33
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/update_text_units.py::_update_and_merge_text_units
  file: graphrag/index/workflows/update_text_units.py
  name: _update_and_merge_text_units
  signature: "def _update_and_merge_text_units(\n    old_text_units: pd.DataFrame,\n\
    \    delta_text_units: pd.DataFrame,\n    entity_id_mapping: dict,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update and merge text units.\n\nArgs:\n  old_text_units: pd.DataFrame\n\
    \      The old text units.\n  delta_text_units: pd.DataFrame\n      The delta\
    \ text units.\n  entity_id_mapping: dict\n      The entity id mapping.\n\nReturns:\n\
    \  pd.DataFrame\n  The updated text units.\n\nRaises:\n  KeyError: If required\
    \ columns are missing from the input dataframes (e.g., 'entity_ids' or 'human_readable_id')."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 92
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_text_units.py::_update_text_units
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed_batch
  file: graphrag/language_model/providers/fnllm/models.py
  name: aembed_batch
  signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list:\
    \ The texts to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
    \nReturns:\n    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n\
    \    ValueError: If no embeddings are found in the response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 379
  line_end: 396
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_language_model_configs
  file: tests/unit/config/utils.py
  name: assert_language_model_configs
  signature: "def assert_language_model_configs(\n    actual: LanguageModelConfig,\
    \ expected: LanguageModelConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that actual and expected LanguageModelConfig objects have equivalent\
    \ field values, including optional responses when present.\n\nArgs:\n    actual:\
    \ LanguageModelConfig instance containing the actual configuration to validate.\n\
    \    expected: LanguageModelConfig instance containing the expected configuration\
    \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If\
    \ any corresponding field differs between actual and expected."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 106
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._parse_claim_tuples
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: _parse_claim_tuples
  signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
    \ dict\n    ) -> list[dict[str, Any]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Parse claim tuples.\n\nArgs:\n    claims: The raw claims text to parse\
    \ into structured claim dictionaries. The text is\n        expected to contain\
    \ multiple claims separated by the configured record delimiter,\n        with\
    \ fields within each claim separated by the configured tuple delimiter. The\n\
    \        trailing completion delimiter is ignored during parsing.\n    prompt_variables:\
    \ A mapping used to determine the delimiters for records, tuples, and\n      \
    \  completions. The method looks up keys corresponding to internal delimiter keys\
    \ and\n        falls back to DEFAULT_RECORD_DELIMITER, DEFAULT_TUPLE_DELIMITER,\
    \ and\n        DEFAULT_COMPLETION_DELIMITER when not present.\n\nReturns:\n  \
    \  list[dict[str, Any]]: A list of dictionaries where each dictionary represents\
    \ a parsed claim\n        with the following keys: subject_id, object_id, type,\
    \ status, start_date, end_date,\n        description, source_text. Values are\
    \ strings or None depending on whether a field was\n        present.\n\nRaises:\n\
    \    None: This method does not raise any documented exceptions during normal\
    \ operation."
  code_example: null
  example_source: null
  line_start: 197
  line_end: 236
  dependencies:
  - graphrag/index/operations/extract_covariates/claim_extractor.py::pull_field
  called_by: []
- node_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback.on_llm_new_token
  file: graphrag/callbacks/llm_callbacks.py
  name: on_llm_new_token
  signature: 'def on_llm_new_token(self, token: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n   \
    \     The new token generated by the LLM.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 12
  line_end: 14
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/is_null.py::is_none
  file: graphrag/index/utils/is_null.py
  name: is_none
  signature: def is_none() -> bool
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check if the input value is None or NaN.\n\nArgs:\n    value (Any):\
    \ The value to check.\n\nReturns:\n    bool: True if value is None or NaN (NaN\
    \ is recognized only for floating-point values); otherwise False.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 13
  line_end: 14
  dependencies: []
  called_by:
  - graphrag/index/utils/is_null.py::is_null
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.filter_by_id
  file: graphrag/vector_stores/base.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids\
    \ (Optional[list[str] | list[int]]): The IDs to include in the filter. If None\
    \ or an empty list is provided, the filter is cleared (set to None) and None is\
    \ returned. Concrete implementations may define different handling for empty input.\n\
    \nReturns:\n  Any: The constructed query filter to filter documents by the provided\
    \ IDs, or None if no IDs are provided."
  code_example: null
  example_source: null
  line_start: 85
  line_end: 86
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/string.py::clean_str
  file: graphrag/index/utils/string.py
  name: clean_str
  signature: 'def clean_str(input: Any) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Clean an input string by removing HTML escapes, control characters,\
    \ and other unwanted characters.\n\nArgs:\n    input: Any\n        The value to\
    \ sanitize. If the value is not a string, it is returned unchanged.\n\nReturns:\n\
    \    str\n        The sanitized string if the input is a string; otherwise, the\
    \ original value is returned unchanged."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 19
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_document
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: _process_document
  signature: "def _process_document(\n        self, text: str, prompt_variables: dict[str,\
    \ str]\n    ) -> str"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Process a single document to extract entities using the configured extraction\
    \ prompts, and accumulate the results. If gleanings are enabled (max_gleanings\
    \ > 0), this may perform multiple continuation prompts to extract additional entities\
    \ until the limit is reached or the model indicates there are no more entities.\n\
    \nArgs:\n  text: str - The document content to process.\n  prompt_variables: dict[str,\
    \ str] - A dictionary of prompt variables used to configure the extraction prompts\
    \ and behavior.\n\nReturns:\n  str - The concatenated extraction results for the\
    \ document.\n\nRaises:\n  Exception - If an error occurs calling the model or\
    \ during continuation prompts."
  code_example: null
  example_source: null
  line_start: 143
  line_end: 177
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_database
  file: graphrag/vector_stores/cosmosdb.py
  name: _create_database
  signature: def _create_database(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create the database if it doesn't exist.\n\nThis method uses the Cosmos\
    \ client to ensure the database exists and then obtains a database client for\
    \ subsequent operations.\n\nReturns:\n    None: This method does not return a\
    \ value.\n\nRaises:\n    CosmosHttpResponseError: If the Cosmos DB service returns\
    \ an HTTP error."
  code_example: null
  example_source: null
  line_start: 65
  line_end: 70
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.sample_documents
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: sample_documents
  signature: def sample_documents(self)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create sample documents for testing.\n\nParameters:\n    self: Instance\
    \ of the test class used by pytest to provide fixture context.\n\nReturns:\n \
    \   List[VectorStoreDocument]: A list of VectorStoreDocument objects with ids\
    \ \"doc1\" and \"doc2\",\n        texts \"This is document 1\" and \"This is document\
    \ 2\",\n        vectors [0.1, 0.2, 0.3, 0.4, 0.5] and [0.2, 0.3, 0.4, 0.5, 0.6],\n\
    \        and attributes {\"title\": \"Doc 1\", \"category\": \"test\"} and {\"\
    title\": \"Doc 2\", \"category\": \"test\"}."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 99
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.search_by_id
  file: graphrag/vector_stores/azure_ai_search.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators: []
  raises: []
  visibility: public
  docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
    \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
    \ corresponding to the provided id with its id, text, vector, and attributes populated\
    \ from the stored document.\n\nRaises:\n    json.JSONDecodeError: If the attributes\
    \ field cannot be decoded as JSON.\n    Exception: If the underlying database\
    \ connection fails to retrieve the document."
  code_example: null
  example_source: null
  line_start: 206
  line_end: 214
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/utils.py::_create_cache
  file: graphrag/language_model/providers/fnllm/utils.py
  name: _create_cache
  signature: 'def _create_cache(cache: PipelineCache | None, name: str) -> FNLLMCacheProvider
    | None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create an FNLLM cache provider from a pipeline cache.\n\nArgs:\n   \
    \ cache: PipelineCache | None - The pipeline cache to wrap. If None, returns None.\n\
    \    name: str - The name to assign to the child cache provider.\n\nReturns:\n\
    \    FNLLMCacheProvider | None - The created cache provider, or None if cache\
    \ is None."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 37
  dependencies:
  - graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider
  called_by:
  - graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__init__
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
    \ int,\n        include_named_entities: bool,\n        exclude_entity_tags: list[str],\n\
    \        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n   \
    \     word_delimiter: str,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the SyntacticNounPhraseExtractor.\n\nThis initializer configures\
    \ the base class and prepares the SpaCy NLP pipeline used for noun phrase extraction\
    \ via dependency parsing and optional NER.\n\nIt delegates to the base class BaseNounPhraseExtractor\
    \ to set common configuration, including excluding nouns. The call to the base\
    \ initializer passes model_name, max_word_length, exclude_nouns, and word_delimiter.\n\
    \nArgs:\n    model_name: SpaCy model name.\n    max_word_length: Maximum length\
    \ in character of each extracted word.\n    include_named_entities: Whether to\
    \ include named entities in noun phrases. When True, named entities are loaded\
    \ and considered; when False, NER is disabled in the pipeline.\n    exclude_entity_tags:\
    \ List of named entity tags to exclude in noun phrases.\n    exclude_pos_tags:\
    \ List of POS tags to remove in noun phrases.\n    exclude_nouns: List of nouns\
    \ to exclude (passed to the base initializer).\n    word_delimiter: Delimiter\
    \ for joining words.\n\nReturns:\n    None\n\nNotes:\n    This initializer sets\
    \ up the self.nlp pipeline and stores configuration attributes for later use.\n\
    \nDynamic model loading behavior:\n    If include_named_entities is False: load_spacy_model\
    \ with exclude including lemmatizer and ner to disable NER.\n    If include_named_entities\
    \ is True: load_spacy_model with exclude including lemmatizer to keep NER enabled."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 61
  dependencies: []
  called_by: []
- node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._get_log_type
  file: graphrag/logger/blob_workflow_logger.py
  name: _get_log_type
  signature: 'def _get_log_type(self, level: int) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get log type string for a given numeric log level.\n\nArgs:\n    level:\
    \ int - The numeric log level (e.g., logging.INFO, logging.WARNING, logging.ERROR).\n\
    \nReturns:\n    str - The log type: \"error\" if level >= logging.ERROR, \"warning\"\
    \ if level >= logging.WARNING, otherwise \"log\"."
  code_example: null
  example_source: null
  line_start: 93
  line_end: 99
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/session_variable.py::SessionVariable.value
  file: unified-search-app/app/state/session_variable.py
  name: value
  signature: 'def value(self, value: Any) -> None'
  decorators:
  - '@value.setter'
  raises: []
  visibility: public
  docstring: "Set the session variable value.\n\nArgs:\n    value: The new value to\
    \ assign to the session variable. Can be of any type.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 47
  line_end: 49
  dependencies: []
  called_by: []
- node_id: graphrag/config/enums.py::ChunkStrategyType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of this ChunkStrategyType enum member.\n\
    \nArgs:\n    self: ChunkStrategyType, the enum member to represent as a string.\n\
    \nReturns:\n    str: The enum member's value enclosed in double quotes."
  code_example: null
  example_source: null
  line_start: 125
  line_end: 127
  dependencies: []
  called_by: []
- node_id: graphrag/data_model/entity.py::Entity.from_dict
  file: graphrag/data_model/entity.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n        title_key:\
    \ str = \"title\",\n        type_key: str = \"type\",\n        description_key:\
    \ str = \"description\",\n        description_embedding_key: str = \"description_embedding\"\
    ,\n        name_embedding_key: str = \"name_embedding\",\n        community_key:\
    \ str = \"community\",\n        text_unit_ids_key: str = \"text_unit_ids\",\n\
    \        rank_key: str = \"degree\",\n        attributes_key: str = \"attributes\"\
    ,\n    ) -> \"Entity\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new entity from the dict data.\n\nArgs:\n  cls (type): The\
    \ class.\n  d (dict[str, Any]): The source dictionary containing the values for\
    \ the Entity fields.\n  id_key (str): Key in d for the entity's identifier. Defaults\
    \ to \"id\".\n  short_id_key (str): Key in d for the optional short identifier.\
    \ Defaults to \"human_readable_id\".\n  title_key (str): Key in d for the title.\
    \ Defaults to \"title\".\n  type_key (str): Key in d for the type. Defaults to\
    \ \"type\".\n  description_key (str): Key in d for the description. Defaults to\
    \ \"description\".\n  description_embedding_key (str): Key in d for the description\
    \ embedding. Defaults to \"description_embedding\".\n  name_embedding_key (str):\
    \ Key in d for the name embedding. Defaults to \"name_embedding\".\n  community_key\
    \ (str): Key in d for the community IDs. Defaults to \"community\".\n  text_unit_ids_key\
    \ (str): Key in d for the text unit IDs. Defaults to \"text_unit_ids\".\n  rank_key\
    \ (str): Key in d for the rank. Defaults to \"degree\".\n  attributes_key (str):\
    \ Key in d for the attributes. Defaults to \"attributes\".\n\nReturns:\n  Entity:\
    \ The newly created Entity instance.\n\nRaises:\n  KeyError: If the dictionary\
    \ does not contain the keys specified by id_key and title_key."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 69
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.__init__
  file: graphrag/language_model/providers/fnllm/events.py
  name: __init__
  signature: 'def __init__(self, on_error: ErrorHandlerFn)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Initialize FNLLMEvents with an error handler to be called on errors.\n\
    \nArgs:\n    on_error: ErrorHandlerFn to be invoked on errors.\n\nReturns:\n \
    \   None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 16
  line_end: 17
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.decode
  file: graphrag/tokenizer/litellm_tokenizer.py
  name: decode
  signature: 'def decode(self, tokens: list[int]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
    \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from the\
    \ list of tokens.\n\nRaises:\n    Exception: If decoding fails due to an underlying\
    \ error in the decoding process."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Async embed.\n\nArgs:\n    text: The text to generate an embedding for.\n\
    \    kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
    \    list[float]: The embedding."
  code_example: null
  example_source: null
  line_start: 223
  line_end: 242
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._container_exists
  file: graphrag/storage/blob_pipeline_storage.py
  name: _container_exists
  signature: def _container_exists(self) -> bool
  decorators: []
  raises: []
  visibility: protected
  docstring: "Check if the configured blob container exists.\n\nArgs:\n    self: The\
    \ instance of the class that holds _container_name and _blob_service_client.\n\
    \nReturns:\n    bool: True if the container exists, otherwise False.\n\nRaises:\n\
    \    Exception: If the underlying blob service client call to list_containers\
    \ raises an exception."
  code_example: null
  example_source: null
  line_start: 92
  line_end: 98
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.aretry
  file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  name: aretry
  signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
    \        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry an asynchronous function with incremental delay until it succeeds\
    \ or the maximum number of retries is reached.\n\nArgs:\n  func: The asynchronous\
    \ function to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword\
    \ arguments to pass to the function. (Any)\n\nReturns:\n  Any: The result of the\
    \ awaited function.\n\nRaises:\n  Exception: If the wrapped function keeps raising\
    \ and the maximum number of retries is exceeded."
  code_example: null
  example_source: null
  line_start: 59
  line_end: 81
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__call__
  file: graphrag/query/structured_search/drift_search/primer.py
  name: __call__
  signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Call method to process the query by expanding it and embedding the result.\n\
    \nThis asynchronous method delegates to expand_query to produce an expanded\n\
    query text and a token-count dictionary, then computes the embedding for the\n\
    expanded text using the text embedding model.\n\nArgs:\n    query (str): The original\
    \ search query.\n\nReturns:\n    tuple[list[float], dict[str, int]]: A tuple containing\n\
    \        - the embedding vector (as a list of floats) for the expanded query,\
    \ and\n        - a token-count dictionary produced by expand_query (e.g., containing\n\
    \          keys such as \"llm_calls\", \"prompt_tokens\", and \"output_tokens\"\
    ).\n\nRaises:\n    Propagates exceptions raised by expand_query or by the embedding\
    \ model."
  code_example: null
  example_source: null
  line_start: 85
  line_end: 98
  dependencies:
  - graphrag/query/structured_search/drift_search/primer.py::expand_query
  called_by: []
- node_id: tests/notebook/test_notebooks.py::_notebook_run
  file: tests/notebook/test_notebooks.py
  name: _notebook_run
  signature: 'def _notebook_run(filepath: Path)'
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Execute a notebook via nbconvert and collect error outputs.


    Args:

    - filepath: Path to the notebook file to execute.


    Returns:

    - list: A list of error outputs collected from the executed notebook cells.


    Raises:

    - subprocess.CalledProcessError: If the nbconvert command fails to execute.'
  code_example: null
  example_source: null
  line_start: 19
  line_end: 43
  dependencies: []
  called_by:
  - tests/notebook/test_notebooks.py::test_notebook
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.set
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Insert the contents of a file into a Cosmos DB container for the given\
    \ filename key.\n\nFor better optimization, the file is destructured such that\
    \ each row is a unique Cosmos DB item.\n\nArgs:\n  key (str): The filename key\
    \ under which to insert the item in the Cosmos DB container.\n  value (Any): The\
    \ content to insert. If value is bytes, it is treated as a parquet file and each\
    \ row becomes a separate item (with an id derived from the key prefix and the\
    \ row index or the existing row id). If the input row lacks an id, a unique id\
    \ is constructed as \"<prefix>:<index>\" and the prefix is tracked for downstream\
    \ handling. If value is not bytes, it is treated as a cache output or stats.json\
    \ and stored as a single item with id=key and body parsed from JSON.\n  encoding\
    \ (str | None): Optional encoding to use. This parameter is unused by this method\
    \ and is ignored.\n\nReturns:\n  None\n\nRaises:\n  ValueError: If the database\
    \ or container are not initialized. This exception is raised internally but is\
    \ caught and logged within the method and is not propagated to callers."
  code_example: null
  example_source: null
  line_start: 241
  line_end: 279
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::load_strategy
  file: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  name: load_strategy
  signature: 'def load_strategy(strategy_type: SummarizeStrategyType) -> SummarizationStrategy'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load the summarization strategy callable for the given strategy_type.\n\
    \nArgs:\n    strategy_type (SummarizeStrategyType): The strategy type used to\
    \ determine which summarization strategy to load.\n\nReturns:\n    SummarizationStrategy:\
    \ The loaded strategy callable corresponding to the provided strategy_type.\n\n\
    Raises:\n    ValueError: If an unknown strategy_type is provided."
  code_example: null
  example_source: null
  line_start: 111
  line_end: 122
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
- node_id: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py::prep_text_units
  file: graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py
  name: prep_text_units
  signature: "def prep_text_units(\n    text_unit_df: pd.DataFrame,\n    node_df:\
    \ pd.DataFrame,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate text unit degree and concatenate text unit details.\n\nArgs:\n\
    \    text_unit_df (pd.DataFrame): DataFrame containing text unit information to\
    \ be enriched with degree information.\n    node_df (pd.DataFrame): DataFrame\
    \ of nodes. Each row should include TEXT_UNIT_IDS, TITLE, COMMUNITY_ID, and NODE_DEGREE\
    \ used to compute text unit degrees.\n\nReturns:\n    pd.DataFrame: DataFrame\
    \ with columns [COMMUNITY_ID, TEXT_UNIT_ID, ALL_DETAILS]."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 45
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_empty_embedding
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: test_empty_embedding
  signature: def test_empty_embedding(self, vector_store, mock_search_client)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test similarity search by text with empty embedding.\n\nArgs:\n    self:\
    \ The test method's instance.\n    vector_store: The AzureAISearchVectorStore\
    \ instance under test.\n    mock_search_client: Mock search client used to verify\
    \ interactions.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 163
  line_end: 174
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.filter_by_id
  file: graphrag/vector_stores/lancedb.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
    \ list[str] | list[int]\n    The IDs to include in the filter. If the list is\
    \ empty, the filter is cleared\n    (set to None) and None is returned.\n\nReturns:\n\
    \  Any\n    The constructed query filter string to filter documents by the provided\
    \ IDs, or None\n    if no IDs are provided."
  code_example: null
  example_source: null
  line_start: 103
  line_end: 115
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/graphs.py::calculate_leaf_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_leaf_modularity
  signature: "def calculate_leaf_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n) -> float"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the modularity score of the graph using the leaf-level partition\
    \ produced by hierarchical Leiden. The function applies hierarchical Leiden to\
    \ the input graph, derives the leaf-level clustering via final_level_hierarchical_clustering,\
    \ and returns the modularity of the graph with respect to that partition.\n\n\
    Args:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int): Maximum\
    \ size of clusters considered during hierarchical Leiden.\n    random_seed (int):\
    \ Seed for random number generation to ensure reproducibility.\n\nReturns:\n \
    \   float: The modularity score of the graph computed using the leaf-cluster partition.\n\
    \nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 33
  line_end: 43
  dependencies: []
  called_by:
  - graphrag/index/utils/graphs.py::calculate_graph_modularity
  - graphrag/index/utils/graphs.py::calculate_lcc_modularity
  - graphrag/index/utils/graphs.py::calculate_weighted_modularity
- node_id: graphrag/index/typing/pipeline.py::Pipeline.remove
  file: graphrag/index/typing/pipeline.py
  name: remove
  signature: 'def remove(self, name: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Remove all workflows from the pipeline that have the given name.\n\n\
    This method mutates the Pipeline's internal state by removing every workflow\n\
    whose first element (the name) equals the provided value. All matching workflows\n\
    are removed; not just the first match.\n\nTime complexity: O(n), where n is the\
    \ number of workflows in the pipeline.\n\nIf no workflows match, the pipeline\
    \ remains unchanged and no exception is raised.\n\nArgs:\n    name: The name of\
    \ the workflows to remove.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 25
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
  file: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py
  name: has_valid_token_length
  signature: 'def has_valid_token_length(tokens: list[str], max_length: int) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check if all tokens have valid length.\n\nArgs:\n    tokens: List\
    \ of tokens to validate lengths for.\n    max_length: Maximum allowed length for\
    \ any token.\n\nReturns:\n    bool: True if all tokens have length <= max_length,\
    \ otherwise False.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 15
  line_end: 17
  dependencies: []
  called_by:
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
  - graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
- node_id: unified-search-app/app/ui/sidebar.py::lookup_label
  file: unified-search-app/app/ui/sidebar.py
  name: lookup_label
  signature: 'def lookup_label(key: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the display label for the given dataset key.\n\nArgs:\n    key:\
    \ The dataset key to lookup the label for.\n\nReturns:\n    The label corresponding\
    \ to the dataset key, as determined by dataset_name(key, sv).\n\nRaises:\n   \
    \ Exception: Exceptions raised by dataset_name may be propagated."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 56
  dependencies: []
  called_by: []
- node_id: graphrag/config/errors.py::LanguageModelConfigMissingError.__init__
  file: graphrag/config/errors.py
  name: __init__
  signature: 'def __init__(self, key: str = "") -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize LanguageModelConfigMissingError with provided key.\n\nArgs:\n\
    \    key: The key of the missing model configuration. Used to customize the error\
    \ message in settings.yaml.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 39
  line_end: 42
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.remove
  file: graphrag/language_model/providers/fnllm/cache.py
  name: remove
  signature: 'def remove(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Remove a value from the cache.\n\nArgs:\n    key (str): The key to remove\
    \ from the cache.\n\nReturns:\n    None: This method does not return a value.\n\
    \nRaises:\n    Exceptions may propagate from the underlying cache."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_container
  file: graphrag/vector_stores/cosmosdb.py
  name: _create_container
  signature: def _create_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create the Cosmos DB container for the current container name if it\
    \ doesn't exist.\n\nConfigures a partition key based on the id field, a vector\
    \ embedding policy for the vector field and size, and an indexing policy. It first\
    \ attempts to apply a diskANN vector index; if that raises CosmosHttpResponseError,\
    \ it retries without vector indexes to ensure compatibility.\n\nArgs:\n    self:\
    \ Any. The instance containing the Cosmos DB client and vector store configuration.\n\
    \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
    \ Cosmos client operations may be raised."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 139
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.__init__
  file: graphrag/language_model/providers/fnllm/cache.py
  name: __init__
  signature: 'def __init__(self, cache: PipelineCache)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize FNLLMCacheProvider with a PipelineCache.\n\nArgs:\n  cache:\
    \ The underlying PipelineCache instance used by this provider.\n\nReturns:\n \
    \ None"
  code_example: null
  example_source: null
  line_start: 16
  line_end: 17
  dependencies: []
  called_by: []
- node_id: graphrag/config/load_config.py::_parse
  file: graphrag/config/load_config.py
  name: _parse
  signature: 'def _parse(file_extension: str, contents: str) -> dict[str, Any]'
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Parse configuration contents based on the file extension.\n\nParses\
    \ YAML (.yaml/.yml) using yaml.safe_load and JSON (.json) using json.loads.\n\n\
    Args:\n    file_extension: The file extension determining the parsing method (e.g.,\
    \ \".yaml\", \".yml\", \".json\").\n    contents: The raw string content of the\
    \ configuration file to parse.\n\nReturns:\n    Any: The parsed configuration.\
    \ Typically a dict[str, Any], but the result can be None (for empty content) or\
    \ other JSON/YAML types depending on the input.\n\nRaises:\n    ValueError: If\
    \ the file extension is not supported.\n    yaml.YAMLError: If YAML parsing fails.\n\
    \    json.JSONDecodeError: If JSON parsing fails."
  code_example: null
  example_source: null
  line_start: 132
  line_end: 143
  dependencies: []
  called_by:
  - graphrag/config/load_config.py::load_config
- node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::do_summarize_descriptions
  file: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  name: do_summarize_descriptions
  signature: "def do_summarize_descriptions(\n        id: str | tuple[str, str],\n\
    \        descriptions: list[str],\n        ticker: ProgressTicker,\n        semaphore:\
    \ asyncio.Semaphore,\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run a summarization strategy on the provided descriptions for a given\
    \ id or pair of ids.\n\nArgs:\n  id: Identifier for the descriptions to summarize;\
    \ can be a string or a tuple of two strings.\n  descriptions: Descriptions to\
    \ summarize.\n  ticker: ProgressTicker used to report progress.\n  semaphore:\
    \ Semaphore controlling concurrency for the operation.\n\nReturns:\n  The results\
    \ produced by strategy_exec for the given id and descriptions."
  code_example: null
  example_source: null
  line_start: 95
  line_end: 104
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_database
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _create_database
  signature: def _create_database(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create the database if it doesn't exist.\n\nReturns:\n    None: This\
    \ method does not return a value.\n\nRaises:\n    CosmosHttpResponseError: If\
    \ the Cosmos DB service returns an HTTP error."
  code_example: null
  example_source: null
  line_start: 88
  line_end: 92
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed
  file: tests/mock_provider.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The input\
    \ text to generate the embedding for.\n    kwargs: Additional keyword arguments\
    \ passed to the embedding model.\n\nReturns:\n    list[float]: A list of floating-point\
    \ numbers representing the embedding.\n\nRaises:\n    This function does not raise\
    \ any exceptions."
  code_example: null
  example_source: null
  line_start: 115
  line_end: 117
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.keys
  file: graphrag/storage/file_pipeline_storage.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the keys in the storage.\n\nThe keys represent file names located\
    \ directly in the root directory (self._root_dir). Only files are included; directories\
    \ are ignored. This operation is non-recursive and may return an empty list if\
    \ no files are present. The keys are provided as file names (not full paths).\n\
    \nArgs:\n  self (FilePipelineStorage): The instance of the FilePipelineStorage.\n\
    \nReturns:\n  list[str]: The file names (not full paths) of files directly under\
    \ root_dir; directories are excluded. The list may be empty if no files exist."
  code_example: null
  example_source: null
  line_start: 155
  line_end: 157
  dependencies: []
  called_by: []
- node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.agenerate
  file: graphrag/query/question_gen/local_gen.py
  name: agenerate
  signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
    \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
    \    ) -> QuestionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a question based on the question history and context data.\n\
    \nIf context data is not provided, it will be generated by the local context builder\
    \ using the current question and conversation history, along with any additional\
    \ keyword arguments and the configured context_builder_params.\n\nArgs:\n    question_history:\
    \ list[str] - History of previously asked questions.\n    context_data: str |\
    \ None - Optional context data used to influence generation; None to generate\
    \ automatically.\n    question_count: int - Number of questions to generate.\n\
    \    **kwargs: Additional keyword arguments passed to the local context builder\
    \ when constructing context data.\n\nReturns:\n    QuestionResult: The generated\
    \ results including:\n        response: list[str] - The generated response split\
    \ by newline.\n        context_data: dict[str, Any] - The context data associated\
    \ with this question. If context_data was provided, contains {\"context_data\"\
    : <value>}; otherwise contains keys from the context builder result (e.g., \"\
    question_context\" and context_records).\n        completion_time: float - Time\
    \ elapsed for generation in seconds.\n        llm_calls: int - Number of LLM API\
    \ calls performed (typically 1).\n        prompt_tokens: int - Number of tokens\
    \ in the system prompt.\n\nRaises:\n    None: Exceptions are caught within the\
    \ function; on error, a QuestionResult with an empty response is returned along\
    \ with the current context data."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 130
  dependencies:
  - graphrag/query/question_gen/base.py::QuestionResult
  called_by: []
- node_id: graphrag/index/operations/layout_graph/zero.py::get_zero_positions
  file: graphrag/index/operations/layout_graph/zero.py
  name: get_zero_positions
  signature: "def get_zero_positions(\n    node_labels: list[str],\n    node_categories:\
    \ list[int] | None = None,\n    node_sizes: list[int] | None = None,\n    three_d:\
    \ bool | None = False,\n) -> list[NodePosition]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create zero-coordinate positions for nodes\n\nThis function returns\
    \ a list of NodePosition objects for every label in node_labels. No embedding\
    \ or projection is performed; coordinates are initialized to zeros.\n\nArgs:\n\
    \  node_labels (list[str]): Labels for each node.\n  node_categories (list[int]\
    \ | None): Optional list of category/cluster identifiers. If None, defaults to\
    \ 1 for all nodes.\n  node_sizes (list[int] | None): Optional list of node sizes.\
    \ If None, defaults to 1 for all nodes.\n  three_d (bool | None): If False or\
    \ None, return 2D positions (x and y). If True, return 3D positions (x, y, z).\n\
    \nReturns:\n  list[NodePosition]: A list of NodePosition objects, one per input\
    \ label. Each position includes:\n    label: str(node label)\n    x, y (and z\
    \ if three_d is True): coordinates initialized to 0\n    cluster: str(int(node_category))\n\
    \    size: int(node_size)\n\nNotes:\n  - If node_categories or node_sizes are\
    \ provided and their lengths are shorter than the number of labels, an IndexError\
    \ may be raised when indexing into these lists.\n  - The cluster is derived from\
    \ node_categories (default 1) and converted to a string."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 96
  dependencies:
  - graphrag/index/operations/layout_graph/typing.py::NodePosition
  called_by:
  - graphrag/index/operations/layout_graph/zero.py::run
- node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.on_error
  file: graphrag/language_model/providers/fnllm/events.py
  name: on_error
  signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
    \        traceback: str | None = None,\n        arguments: dict[str, Any] | None\
    \ = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle an fnllm error.\n\nArgs:\n    error (BaseException | None): The\
    \ error to handle, or None if no error is available.\n    traceback (str | None):\
    \ The traceback string, or None if not provided.\n    arguments (dict[str, Any]\
    \ | None): Additional arguments related to the error, or None.\n\nReturns:\n \
    \   None\n\nRaises:\n    Exception: If the configured error handler raises an\
    \ exception."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 26
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.retry
  file: graphrag/language_model/providers/litellm/services/retry/retry.py
  name: retry
  signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Retry a synchronous function.\n\nAbstract method that subclasses must\
    \ implement to retry the provided function using their configured retry strategy.\n\
    \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
    \ as func(**kwargs).\n    kwargs: Any - Additional keyword arguments to pass to\
    \ the function being retried or to control the retry behavior.\n\nReturns:\n \
    \   Any - The result of the retried function invocation.\n\nRaises:\n    NotImplementedError\
    \ - Subclasses must implement this method."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 23
  dependencies: []
  called_by: []
- node_id: tests/unit/litellm_services/test_retries.py::test_retries_async
  file: tests/unit/litellm_services/test_retries.py
  name: test_retries_async
  signature: "def test_retries_async(\n    strategy: str, max_retries: int, max_retry_wait:\
    \ int, expected_time: float\n) -> None"
  decorators:
  - "@pytest.mark.parametrize(\n    (\"strategy\", \"max_retries\", \"max_retry_wait\"\
    , \"expected_time\"),\n    [\n        (\n            \"native\",\n           \
    \ 3,  # 3 retries\n            0,  # native retry does not adhere to max_retry_wait\n\
    \            0,  # immediate retry, expect 0 seconds elapsed time\n        ),\n\
    \        (\n            \"exponential_backoff\",\n            3,  # 3 retries\n\
    \            0,  # exponential retry does not adhere to max_retry_wait\n     \
    \       14,  # (2^1 + jitter) + (2^2 + jitter) + (2^3 + jitter) = 2 + 4 + 8 +\
    \ 3*jitter = 14 seconds min total runtime\n        ),\n        (\n           \
    \ \"random_wait\",\n            3,  # 3 retries\n            2,  # random wait\
    \ [0, 2] seconds\n            0,  # unpredictable, don't know what the total runtime\
    \ will be\n        ),\n        (\n            \"incremental_wait\",\n        \
    \    3,  # 3 retries\n            3,  # wait for a max of 3 seconds on a single\
    \ retry.\n            6,  # Wait 3/3 * 1 on first retry, 3/3 * 2 on second, 3/3\
    \ * 3 on third, 1 + 2 + 3 = 6 seconds total runtime.\n        ),\n    ],\n)"
  raises:
  - ValueError
  visibility: public
  docstring: "Test various retry strategies asynchronously.\n\nArgs:\n    strategy:\
    \ The retry strategy to use.\n    max_retries: The maximum number of retry attempts.\n\
    \    max_retry_wait: The maximum wait time between retries.\n    expected_time:\
    \ The minimum elapsed time expected for the retries to complete.\nReturns:\n \
    \   None\nRaises:\n    ValueError"
  code_example: null
  example_source: null
  line_start: 113
  line_end: 148
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::_wrapped_with_rate_limiter
  file: graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py
  name: _wrapped_with_rate_limiter
  signature: 'def _wrapped_with_rate_limiter(**kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wrapped synchronous request function with rate limiting.\n\nArgs:\n\
    \    kwargs: Any\n        Arbitrary keyword arguments forwarded to the wrapped\
    \ synchronous function.\n        The wrapper computes the rate-limiting token\
    \ count from max_tokens plus\n        a token count derived from the provided\
    \ 'messages' or 'input' in kwargs.\n\nReturns:\n    Any\n        The result of\
    \ the wrapped synchronous request function.\n\nRaises:\n    Exception\n      \
    \  Propagates exceptions raised by the rate limiter or by the wrapped function."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 77
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_start
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: pipeline_start
  signature: 'def pipeline_start(self, names: list[str]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Execute this callback to signal when the entire pipeline starts.\n\
    \nArgs:\n    names: list[str] The names of the workflows that started.\n\nReturns:\n\
    \    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 21
  line_end: 23
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_embedding_model
  file: graphrag/language_model/manager.py
  name: get_or_create_embedding_model
  signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
    \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retrieve the EmbeddingsLLM instance registered under the given name.\n\
    \nIf the EmbeddingsLLM does not exist, it is created and registered.\n\nArgs:\n\
    \    name: Unique identifier for the EmbeddingsLLM instance.\n    model_type:\
    \ Key for the EmbeddingsLLM implementation in LLMFactory.\n    **embedding_kwargs:\
    \ Additional parameters for instantiation.\n\nReturns:\n    EmbeddingModel: The\
    \ EmbeddingModel instance associated with the given name."
  code_example: null
  example_source: null
  line_start: 122
  line_end: 137
  dependencies:
  - graphrag/language_model/manager.py::register_embedding
  called_by: []
- node_id: tests/integration/vector_stores/test_cosmosdb.py::mock_embedder
  file: tests/integration/vector_stores/test_cosmosdb.py
  name: mock_embedder
  signature: 'def mock_embedder(text: str) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a fixed embedding vector for testing.\n\nArgs:\n    text: Input\
    \ text to embed.\n\nReturns:\n    list[float]: The fixed embedding vector [0.1,\
    \ 0.2, 0.3, 0.4, 0.5].\n\nRaises:\n    None: This function does not raise any\
    \ exceptions."
  code_example: null
  example_source: null
  line_start: 153
  line_end: 154
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_retry_services
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_retry_services
  signature: def _validate_retry_services(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "\"\"\"Validate the retry services configuration.\n\nThis method iterates\
    \ over all language model configurations contained in self.models\nand validates\
    \ each model's retry_strategy. If a model's retry_strategy is \"none\",\n the\
    \ strategy is skipped. For any other strategy, the strategy must be registered\
    \ with\nRetryFactory; otherwise a ValueError is raised. The error message lists\
    \ the available\nretry strategies. When a registered strategy is found, a retry\
    \ object is instantiated by\nRetryFactory.create using the model's max_retries\
    \ and max_retry_wait to validate the\nconfiguration.\n\nArgs:\n  self: GraphRagConfig.\
    \ The instance containing the models dictionary to validate.\n\nReturns:\n  None\n\
    \nRaises:\n  ValueError: If a model's retry_strategy is not registered with RetryFactory.\
    \ The exception\n    message includes the model id and the list of available strategies.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 98
  line_end: 112
  dependencies:
  - graphrag/language_model/providers/litellm/services/retry/retry_factory.py::RetryFactory
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.connect
  file: graphrag/vector_stores/cosmosdb.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> Any'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Connect to CosmosDB vector storage.\n\nArgs:\n    connection_string:\
    \ The Cosmos DB connection string. If provided, a CosmosClient is created from\
    \ it.\n    url: The Cosmos DB account URL. Used when connection_string is not\
    \ provided.\n    database_name: The name of the database to use. This must be\
    \ provided.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If neither connection_string\
    \ nor url is provided.\n    ValueError: If database_name is not provided.\n  \
    \  ValueError: If index_name is empty or not provided."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 63
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::_create_container
  - graphrag/vector_stores/cosmosdb.py::_create_database
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_rank_report_context
  file: graphrag/query/context_builder/community_context.py
  name: _rank_report_context
  signature: "def _rank_report_context(\n    report_df: pd.DataFrame,\n    weight_column:\
    \ str | None = \"occurrence weight\",\n    rank_column: str | None = \"rank\"\
    ,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Sorts the report context by the provided weight and rank columns in\
    \ descending order, in-place.\n\nArgs:\n  report_df (pd.DataFrame): The DataFrame\
    \ containing the report context to sort. The function mutates this DataFrame in\
    \ place by casting the configured columns to float and sorting by them in descending\
    \ order. If neither weight_column nor rank_column is provided, the DataFrame is\
    \ returned unchanged.\n\n  weight_column (str | None): Name of the column to use\
    \ for weighting. If not None, the column is cast to float and used for sorting;\
    \ defaults to \"occurrence weight\". If None, this column is ignored.\n\n  rank_column\
    \ (str | None): Name of the column to use for ranking. If not None, the column\
    \ is cast to float and used for sorting; defaults to \"rank\". If None, this column\
    \ is ignored.\n\nReturns:\n  pd.DataFrame: The input DataFrame, sorted by the\
    \ specified columns in descending order. This is the same object that was passed\
    \ in.\n\nRaises:\n  KeyError: If a provided column name does not exist in report_df.\n\
    \  ValueError: If a provided column cannot be cast to float.\n  TypeError: If\
    \ an invalid argument type is provided."
  code_example: null
  example_source: null
  line_start: 228
  line_end: 243
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
- node_id: graphrag/utils/storage.py::load_table_from_storage
  file: graphrag/utils/storage.py
  name: load_table_from_storage
  signature: 'def load_table_from_storage(name: str, storage: PipelineStorage) ->
    pd.DataFrame'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load a parquet from the storage instance.\n\nArgs:\n  name: str\n  \
    \    Base name for the parquet file to load. The expected file name is {name}.parquet.\n\
    \  storage: PipelineStorage\n      The storage backend to read the parquet file\
    \ from.\n\nReturns:\n  pd.DataFrame\n      DataFrame loaded from the parquet file\
    \ stored at {name}.parquet.\n\nRaises:\n  ValueError\n      Could not find {name}.parquet\
    \ in storage!\n  Exception\n      Exceptions raised by the storage backend or\
    \ parquet reader during the load operation may propagate."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 27
  dependencies: []
  called_by:
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/index/run/run_pipeline.py::_copy_previous_output
  - graphrag/index/update/incremental_index.py::get_delta_docs
  - graphrag/index/update/incremental_index.py::concat_dataframes
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/index/workflows/create_communities.py::run_workflow
  - graphrag/index/workflows/create_community_reports.py::run_workflow
  - graphrag/index/workflows/create_community_reports_text.py::run_workflow
  - graphrag/index/workflows/create_final_documents.py::run_workflow
  - graphrag/index/workflows/create_final_text_units.py::run_workflow
  - graphrag/index/workflows/extract_covariates.py::run_workflow
  - graphrag/index/workflows/extract_graph.py::run_workflow
  - graphrag/index/workflows/extract_graph_nlp.py::run_workflow
  - graphrag/index/workflows/finalize_graph.py::run_workflow
  - graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  - graphrag/index/workflows/prune_graph.py::run_workflow
  - graphrag/index/workflows/update_communities.py::_update_communities
  - graphrag/index/workflows/update_community_reports.py::_update_community_reports
  - graphrag/index/workflows/update_covariates.py::_update_covariates
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
  - graphrag/index/workflows/update_text_units.py::_update_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_communities.py::test_create_communities
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
  - tests/verbs/test_extract_covariates.py::test_extract_covariates
  - tests/verbs/test_extract_graph.py::test_extract_graph
  - tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
  - tests/verbs/test_finalize_graph.py::test_finalize_graph
  - tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
  - tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
  - tests/verbs/test_prune_graph.py::test_prune_graph
  - tests/verbs/util.py::update_document_metadata
- node_id: graphrag/factory/factory.py::Factory.__new__
  file: graphrag/factory/factory.py
  name: __new__
  signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return the per-subclass singleton instance for the class that invokes\
    \ __new__.\n\nIf an instance for this class has not been created yet, create one\
    \ using the base object's __new__ and store it on the class. Subsequent calls\
    \ return the same instance.\n\nArgs\n----\n    cls: The class for which to obtain\
    \ the singleton instance. This is the class that invoked __new__, and may be a\
    \ subclass of Factory.\n    *args: Additional positional arguments forwarded to\
    \ the class's constructor.\n    **kwargs: Additional keyword arguments forwarded\
    \ to the class's constructor.\n\nReturns\n----\n    The singleton instance of\
    \ the calling class (cls). Each subclass maintains its own _instance, so the return\
    \ value is an instance of the subclass rather than the base Factory."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 22
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/app_logic.py::run_global_search
  file: unified-search-app/app/app_logic.py
  name: run_global_search
  signature: 'def run_global_search(query: str, sv: SessionVariables) -> SearchResult'
  decorators: []
  raises: []
  visibility: public
  docstring: "Run global search.\n\nArgs:\n    query: str\n        The search query\
    \ string used to perform the global search.\n    sv: SessionVariables\n      \
    \  The SessionVariables instance containing configuration and state for the current\
    \ session.\n\nReturns:\n    SearchResult\n        The result of the global search,\
    \ including the search_type (Global), the textual response, and the context data.\n\
    \nRaises:\n    Exception\n        If an error occurs during the global search\
    \ process."
  code_example: null
  example_source: null
  line_start: 202
  line_end: 251
  dependencies:
  - graphrag.api::global_search
  called_by:
  - unified-search-app/app/app_logic.py::run_all_searches
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._search_step
  file: graphrag/query/structured_search/drift_search/search.py
  name: _search_step
  signature: "def _search_step(\n        self, global_query: str, search_engine: LocalSearch,\
    \ actions: list[DriftAction]\n    ) -> list[DriftAction]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Perform an asynchronous search step (internal API) by executing each\
    \ DriftAction concurrently and collecting the results.\n\nArgs:\n    global_query\
    \ (str): The global query for the search.\n    search_engine (LocalSearch): The\
    \ local search engine instance.\n    actions (list[DriftAction]): A list of actions\
    \ to perform.\n\nReturns:\n    list[DriftAction]: The results from executing the\
    \ search actions asynchronously.\n\nRaises:\n    Exception: May propagate exceptions\
    \ raised by the underlying action searches."
  code_example: null
  example_source: null
  line_start: 158
  line_end: 177
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/update_covariates.py::_merge_covariates
  file: graphrag/index/workflows/update_covariates.py
  name: _merge_covariates
  signature: "def _merge_covariates(\n    old_covariates: pd.DataFrame, delta_covariates:\
    \ pd.DataFrame\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Merge the covariates.\n\nThis function merges the existing old covariates\
    \ with the delta covariates. The delta covariates are mutated in-place to assign\
    \ new human_readable_id values that are consecutive and start from max(old_covariates.human_readable_id)\
    \ + 1. The function then concatenates the old covariates and the mutated delta\
    \ covariates using ignore_index=True and returns the resulting DataFrame.\n\n\
    Args:\n    old_covariates (pd.DataFrame): The old covariates.\n    delta_covariates\
    \ (pd.DataFrame): The delta covariates to be merged into the old covariates. This\
    \ DataFrame is mutated in-place to assign new IDs.\n\nReturns:\n    pd.DataFrame:\
    \ The merged covariates, with old_covariates preceding delta_covariates and a\
    \ reset index (ignore_index=True)."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 82
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_covariates.py::_update_covariates
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::_create_progress_status
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _create_progress_status
  signature: "def _create_progress_status(\n    num_loaded: int, num_filtered: int,\
    \ num_total: int\n) -> Progress"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create a Progress object representing the current load progress.\n\n\
    Args:\n    num_loaded: int. The number of files that have been loaded.\n    num_filtered:\
    \ int. The number of files that were filtered out.\n    num_total: int. The total\
    \ number of items.\n\nReturns:\n    Progress: Progress object with total_items=num_total,\
    \ completed_items=num_loaded + num_filtered, and description set to \"<num_loaded>\
    \ files loaded (<num_filtered> filtered)\"."
  code_example: null
  example_source: null
  line_start: 356
  line_end: 363
  dependencies:
  - graphrag/logger/progress.py::Progress
  called_by:
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
- node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.get_vector_store_types
  file: graphrag/vector_stores/factory.py
  name: get_vector_store_types
  signature: def get_vector_store_types(cls) -> list[str]
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Get the registered vector store implementations.\n\nArgs:\n    cls:\
    \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
    \ The list of registered vector store type keys (i.e., the keys of cls._registry)."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 83
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.build_context
  file: graphrag/query/structured_search/basic_search/basic_context.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
    \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
    \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col: str\
    \ = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Build the context for the basic search mode.\n\nArgs:\n  query\
    \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
    \ | None): Optional conversation history to consider while constructing the context.\n\
    \  k (int): Number of top related texts to retrieve.\n  max_context_tokens (int):\
    \ Maximum total number of tokens allowed for the context.\n  context_name (str):\
    \ Name assigned to the context section in the results (default \"Sources\").\n\
    \  column_delimiter (str): Delimiter used to separate columns in the generated\
    \ context text.\n  text_id_col (str): Name of the column containing text identifiers.\n\
    \  text_col (str): Name of the column containing the text content.\n  **kwargs:\
    \ Additional keyword arguments that may influence how the context is built.\n\n\
    Returns:\n  ContextBuilderResult: The result containing the built context for\
    \ the basic search mode.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 43
  line_end: 105
  dependencies:
  - graphrag/query/context_builder/builders.py::ContextBuilderResult
  called_by: []
- node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_all
  file: graphrag/index/workflows/factory.py
  name: register_all
  signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a dict of custom workflow functions.\n\nArgs:\n    cls: The\
    \ class that provides access to the registry (PipelineFactory).\n    workflows:\
    \ A dictionary mapping workflow names to workflow functions.\n\nReturns:\n   \
    \ None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 32
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.child
  file: tests/integration/storage/test_factory.py
  name: child
  signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Return the current storage instance (no new child is created).\n\
    \nThis method accepts an optional name parameter for API compatibility but does\
    \ not create a new child. It returns the current instance (self).\n\nArgs:\n \
    \   name (str | None): Optional name for the child storage. This parameter is\
    \ accepted for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage:\
    \ The current instance (self).\n\"\"\""
  code_example: null
  example_source: null
  line_start: 142
  line_end: 143
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed
  file: graphrag/language_model/protocol/base.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text:\
    \ The text to generate an embedding for.\n    **kwargs: Additional keyword arguments\
    \ (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding vector.\n\
    \nRaises:\n    Exception: If an error occurs during embedding generation."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 83
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.__init__
  file: graphrag/query/context_builder/dynamic_community_selection.py
  name: __init__
  signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
    \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
    \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
    \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
    \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
    \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the DynamicCommunitySelection with the provided data and\
    \ prepare internal state for dynamic community selection.\n\nArgs:\n    community_reports\
    \ (list[CommunityReport]): Reports for communities to consider, mapped by community_id.\n\
    \    communities (list[Community]): Community objects used to build the hierarchy\
    \ and starting points.\n    model (ChatModel): Language model instance used to\
    \ rate relevance of communities.\n    tokenizer (Tokenizer): Tokenizer used for\
    \ text processing.\n    rate_query (str): Query string used for rate prompting.\
    \ Defaults to RATE_QUERY.\n    use_summary (bool): If True, use the summary content\
    \ when rating; otherwise use full_content.\n    threshold (int): Minimum rating\
    \ threshold for a report to be considered relevant.\n    keep_parent (bool): Whether\
    \ to preserve parent relationships during selection.\n    num_repeats (int): Number\
    \ of times to repeat the relevance rating process.\n    max_level (int): Maximum\
    \ depth of levels to explore.\n    concurrent_coroutines (int): Maximum number\
    \ of concurrent asynchronous tasks.\n    model_params (dict[str, Any] | None):\
    \ Optional additional parameters to pass to the model; if None, an empty dict\
    \ is used.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__str__
  file: graphrag/config/models/graph_rag_config.py
  name: __str__
  signature: def __str__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"\nGet a string representation of this GraphRagConfig as a JSON\
    \ string.\n\nThis representation is produced by Pydantic's model_dump_json with\
    \ an indentation of 4 spaces, yielding a pretty-printed JSON string.\n\nArgs:\n\
    \    self: GraphRagConfig. The instance to serialize. Note: self is implicit in\
    \ Python method calls; this description is informational.\n\nReturns:\n    str:\
    \ JSON string representation of the GraphRagConfig with indent=4.\n\nRaises:\n\
    \    ValueError, TypeError: If the underlying serialization fails due to non-serializable\
    \ fields or data.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 55
  line_end: 57
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_end
  file: graphrag/callbacks/noop_query_callbacks.py
  name: on_reduce_response_end
  signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "No-op callback for the end of the reduce operation in NoopQueryCallbacks.\n\
    \nThis method intentionally performs no action and exists solely to conform to\
    \ the NoopQueryCallbacks interface as a placeholder without side effects.\n\n\
    Args:\n    reduce_response_output: str\n        The output produced by the end\
    \ of the reduce operation.\n\nReturns:\n    None: The function does not return\
    \ a value."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 30
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_chat_model
  file: graphrag/language_model/factory.py
  name: is_supported_chat_model
  signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Check if the given chat model type is supported by registered chat model\
    \ implementations.\n\nArgs:\n    cls: type The class reference (classmethod parameter).\n\
    \    model_type: str The type identifier for the chat model to check.\n\nReturns:\n\
    \    bool: True if model_type is registered as a chat model, otherwise False."
  code_example: null
  example_source: null
  line_start: 88
  line_end: 90
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_end
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: pipeline_end
  signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback to signal when the entire pipeline ends.\n\nArgs:\n\
    \    results: A list of PipelineRunResult objects representing the results of\
    \ the pipeline runs.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 17
  line_end: 18
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockEmbeddingLLM.embed
  file: tests/mock_provider.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The input\
    \ text to generate the embedding for.\n    kwargs: Additional keyword arguments\
    \ passed to the embedding model.\n\nReturns:\n    list[float]: A list of floating-point\
    \ numbers representing the embedding.\n\nRaises:\n    This function does not raise\
    \ any exceptions...."
  code_example: null
  example_source: null
  line_start: 111
  line_end: 113
  dependencies: []
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.config_context
  file: graphrag/config/environment_reader.py
  name: config_context
  signature: def config_context()
  decorators:
  - '@contextmanager'
  raises: []
  visibility: public
  docstring: "Create a context manager to push a value into the config_stack for the\
    \ duration of the context.\n\nReturns:\n    A context manager that appends the\
    \ value (or {} if the value is falsy) to the internal _config_stack upon entry,\
    \ and pops it on exit."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 69
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.build_context
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: build_context
  signature: "def build_context(\n        self, query: str, **kwargs\n    ) -> tuple[pd.DataFrame,\
    \ dict[str, int]]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Build DRIFT search context.\n\nArgs\n  query: str\n      Search query\
    \ string.\n\nReturns\n  tuple[pd.DataFrame, dict[str, int]]: Top-k most similar\
    \ documents, and a dictionary containing the number of LLM calls, and prompts\
    \ and output tokens.\n\nRaises\n  ValueError: If no community reports are available,\
    \ or embeddings are incompatible."
  code_example: null
  example_source: null
  line_start: 166
  line_end: 227
  dependencies:
  - graphrag/query/structured_search/drift_search/drift_context.py::check_query_doc_encodings
  - graphrag/query/structured_search/drift_search/drift_context.py::convert_reports_to_df
  - graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor
  called_by: []
- node_id: graphrag/utils/api.py::MultiVectorStore.load_documents
  file: graphrag/utils/api.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Load documents into the vector store.\n\nArgs:\n    documents: List\
    \ of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
    \ Whether to overwrite existing data in the vector store if True; otherwise, preserve\
    \ existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: load_documents\
    \ method not implemented"
  code_example: null
  example_source: null
  line_start: 37
  line_end: 42
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_vector_store_config
  file: graphrag/config/models/graph_rag_config.py
  name: get_vector_store_config
  signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: '"""Get a vector store configuration by ID.


    Args:

    vector_store_id: The ID of the vector store to get. Should match an ID in the
    vector_store list.


    Returns:

    VectorStoreConfig: The vector store configuration if found.


    Raises:

    ValueError: If the vector store ID is not found in the configuration.

    """'
  code_example: null
  example_source: null
  line_start: 379
  line_end: 401
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph_with_edges_flipped
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: _create_strongly_connected_graph_with_edges_flipped
  signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create and return a test graph used for stability tests of the largest\
    \ connected component. This variant builds a five-node graph where the first edge\
    \ introduces the new node 5 (edge 5-4). The remaining edges form a path: 4-3,\
    \ 3-2, 2-1, with edge attributes degree=4, degree=3, degree=2, and degree=1 respectively.\
    \ If digraph is False, an undirected Graph is created; if digraph is True, a directed\
    \ DiGraph is created and the edges are oriented accordingly. The function name\
    \ indicates the first edge's orientation is flipped relative to the non-flipped\
    \ version (which uses 4-5). The graph contains five nodes labeled 1 through 5;\
    \ nodes 1-4 have node_name attributes 1-4, and node 5 is introduced by the first\
    \ edge.\n\nArgs:\n    self: The instance of the test class. Type: unittest.TestCase\n\
    \    digraph (bool): If True, create a directed graph (nx.DiGraph); otherwise\
    \ an undirected graph (nx.Graph).\n\nReturns:\n    graph (networkx.Graph or networkx.DiGraph):\
    \ The constructed graph.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 61
  line_end: 71
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::prepare_azurite_data
  file: tests/smoke/test_fixtures.py
  name: prepare_azurite_data
  signature: 'def prepare_azurite_data(input_path: str, azure: dict) -> Callable[[],
    None]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare Azurite test data for the fixtures.\n\nThis coroutine uses the\
    \ azure configuration to create or reset a blob storage\ncontainer, uploads test\
    \ data from the local input directory (txt and csv files),\nand returns a callable\
    \ that will delete the container when invoked.\n\nArgs:\n  input_path: Path on\
    \ disk containing test input data. The function looks for an\n    input subdirectory\
    \ with .txt and .csv files to upload.\n  azure: Dictionary with Azure/Azurite\
    \ configuration. Expected keys include:\n    input_container: name of the blob\
    \ container to use\n    input_base_dir: optional base directory inside the container\
    \ for the uploaded files\n\nReturns:\n  A callable with no arguments that deletes\
    \ the blob container when called.\n\nRaises:\n  Exceptions raised by BlobPipelineStorage\
    \ operations or file I/O"
  code_example: null
  example_source: null
  line_start: 92
  line_end: 119
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  called_by:
  - tests/smoke/test_fixtures.py::TestIndexer.test_fixture
- node_id: graphrag/utils/api.py::create_storage_from_config
  file: graphrag/utils/api.py
  name: create_storage_from_config
  signature: 'def create_storage_from_config(output: StorageConfig) -> PipelineStorage'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a storage object from the config.\n\nArgs:\n    output: StorageConfig\
    \ The storage configuration to create a storage object from.\n\nReturns:\n   \
    \ PipelineStorage: The created storage object.\n\nRaises:\n    ValueError: If\
    \ the storage type is not registered."
  code_example: null
  example_source: null
  line_start: 264
  line_end: 270
  dependencies:
  - graphrag/storage/factory.py::StorageFactory
  called_by:
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/index/run/run_pipeline.py::run_pipeline
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
- node_id: tests/unit/config/utils.py::assert_reporting_configs
  file: tests/unit/config/utils.py
  name: assert_reporting_configs
  signature: "def assert_reporting_configs(\n    actual: ReportingConfig, expected:\
    \ ReportingConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: 'Assert that two ReportingConfig objects have identical field values.


    Parameters:

    - actual: ReportingConfig - The actual ReportingConfig instance.

    - expected: ReportingConfig - The expected ReportingConfig instance to compare
    against.


    Returns:

    - None


    Raises:

    - AssertionError: If any of the fields differ between actual and expected.'
  code_example: null
  example_source: null
  line_start: 129
  line_end: 136
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.decode
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: decode
  signature: def decode(self, token_ids)
  decorators: []
  raises: []
  visibility: public
  docstring: "Decode token ids to string by converting each integer to a character\
    \ and concatenating.\n\nArgs:\n    token_ids: An iterable of integers representing\
    \ token IDs to decode into a string.\n\nReturns:\n    str: The decoded string.\n\
    \nRaises:\n    TypeError: If token_ids contains non-integer elements or elements\
    \ cannot be processed by chr.\n    ValueError: If token_id is outside the valid\
    \ range for chr (0 <= id <= 0x10FFFF)."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 31
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/questions_list.py::create_questions_list_ui
  file: unified-search-app/app/ui/questions_list.py
  name: create_questions_list_ui
  signature: 'def create_questions_list_ui(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create and render a Streamlit UI component to display a list of generated\
    \ questions and update the selected question when a row is selected.\n\nArgs:\n\
    \    sv (SessionVariables): The session state object containing generated_questions\
    \ and selected_question attributes used to render the UI and handle selection\
    \ updates.\n\nReturns:\n    None: This function does not return a value."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 23
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.expand_query
  file: graphrag/query/structured_search/drift_search/primer.py
  name: expand_query
  signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Expand the query using a random community report template.\n\nArgs:\n\
    \    query (str): The original search query.\n\nReturns:\n    tuple[str, dict[str,\
    \ int]]: Expanded query text and a dictionary with token usage details:\n    \
    \    llm_calls: number of language model calls made (usually 1)\n        prompt_tokens:\
    \ number of tokens in the prompt\n        output_tokens: number of tokens in the\
    \ model output"
  code_example: null
  example_source: null
  line_start: 52
  line_end: 83
  dependencies: []
  called_by: []
- node_id: tests/mock_provider.py::MockChatLLM.chat
  file: tests/mock_provider.py
  name: chat
  signature: "def chat(\n        self,\n        prompt: str,\n        history: list\
    \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the next response in the configured sequence.\n\nThis mock chat\
    \ provider cycles through configured responses and returns them one at a time.\
    \ If no responses are configured, an empty response with content \"\" is returned.\n\
    \nArgs:\n    prompt (str): The input prompt to process. The mock uses no prompt\
    \ data to generate the response.\n    history (list | None): Optional history\
    \ for context. Not used by this mock implementation.\n    **kwargs: Additional\
    \ keyword arguments forwarded to the underlying chat handler. These are ignored\
    \ by this mock implementation but accepted for compatibility.\n\nReturns:\n  \
    \  ModelResponse: The next response in the configured list as a BaseModelResponse.\
    \ The response content is accessible via response.output.content, and if the stored\
    \ response was a BaseModel it will be serialized to JSON for the content and exposed\
    \ via parsed_response.\n\nNotes:\n    - The next response is selected using a\
    \ modulo operation on the internal index and then the index is incremented.\n\
    \    - If a response is a Pydantic BaseModel, its JSON representation is used\
    \ as the content (via model_dump_json()). The original BaseModel is exposed in\
    \ parsed_response.\n    - If no responses are configured, the content is an empty\
    \ string \"\" and parsed_response is None."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 85
  dependencies:
  - graphrag/language_model/response/base.py::BaseModelOutput
  - graphrag/language_model/response/base.py::BaseModelResponse
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_get_vector_store_types
  file: tests/integration/vector_stores/test_factory.py
  name: test_get_vector_store_types
  signature: def test_get_vector_store_types()
  decorators: []
  raises: []
  visibility: public
  docstring: "Verify that VectorStoreFactory.get_vector_store_types returns a collection\
    \ containing the values of built-in vector store types LanceDB, AzureAISearch,\
    \ and CosmosDB.\n\nArgs:\n    None\n\nReturns:\n    List[str] - a collection of\
    \ built-in vector store type values from VectorStoreFactory.get_vector_store_types().\
    \ Note: The test function itself does not return a value; it asserts that the\
    \ expected values are present in the collection.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 97
  line_end: 102
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.load_documents
  file: graphrag/vector_stores/cosmosdb.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load documents into the CosmosDB vector store.\n\nIf overwrite is True,\
    \ the existing container will be deleted and recreated before loading. Documents\
    \ with non-null vectors are stored. Each document is stored as an item with fields\
    \ corresponding to the configured id_field, vector_field, text_field, and attributes_field\
    \ (JSON-serialized). Upload uses upsert semantics; existing items with the same\
    \ id will be updated.\n\nArgs:\n    documents (list[VectorStoreDocument]): List\
    \ of VectorStoreDocument objects to load into CosmosDB. Only documents with a\
    \ non-null vector are stored. Each stored item includes fields corresponding to\
    \ the configured id_field, vector_field, text_field, and attributes_field.\n \
    \   overwrite (bool): If True, delete and recreate the container before loading\
    \ documents; otherwise preserve existing data.\n\nReturns:\n    None\n\nRaises:\n\
    \    ValueError: If the container client is not initialized."
  code_example: null
  example_source: null
  line_start: 153
  line_end: 179
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::_create_container
  - graphrag/vector_stores/cosmosdb.py::_delete_container
  called_by: []
- node_id: graphrag/api/index.py::_get_method
  file: graphrag/api/index.py
  name: _get_method
  signature: 'def _get_method(method: IndexingMethod | str, is_update_run: bool) ->
    str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return the method name to use for the indexing pipeline, optionally\
    \ indicating an update run.\n\nArgs:\n    method: IndexingMethod | str\n     \
    \   The indexing method. If an IndexingMethod is provided, its value is used;\
    \ otherwise the string value is used directly.\n    is_update_run: bool\n    \
    \    True if this is an update run, in which case the method name will be suffixed\
    \ with \"-update\".\n\nReturns:\n    str: The computed method name, possibly suffixed\
    \ with \"-update\" when is_update_run is True."
  code_example: null
  example_source: null
  line_start: 99
  line_end: 101
  dependencies: []
  called_by:
  - graphrag/api/index.py::build_index
- node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.split_text
  file: graphrag/index/text_splitting/text_splitting.py
  name: split_text
  signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Split text into chunks according to the concrete implementation.\n\n\
    Args:\n    text: str | list[str]\n        The input text to split. Can be a single\
    \ string or a list of strings.\n\nReturns:\n    Iterable[str]\n        An iterable\
    \ of text chunks produced by the split operation."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 72
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.create_embedding_model
  file: graphrag/language_model/factory.py
  name: create_embedding_model
  signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any) -> EmbeddingModel'
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Create an EmbeddingModel instance.\n\nArgs:\n    model_type: str The\
    \ type of EmbeddingModel to create.\n    **kwargs: Any Additional keyword arguments\
    \ for the EmbeddingModel constructor.\n\nReturns:\n    EmbeddingModel: The EmbeddingModel\
    \ instance.\n\nRaises:\n    ValueError: If the provided model_type is not registered."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 75
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_vector_store_db_uri
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_vector_store_db_uri
  signature: def _validate_vector_store_db_uri(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the vector store configuration for LanceDB vector stores and\
    \ normalize their db_uri paths to absolute paths.\n\nArgs:\n    self: The Graph\
    \ Rag configuration instance being validated.\n\nReturns:\n    None\n\nRaises:\n\
    \    ValueError: If a LanceDB vector store has a missing or empty db_uri. The\
    \ error message is: Vector store URI is required for LanceDB. Please rerun graphrag\
    \ init and set the vector store configuration."
  code_example: null
  example_source: null
  line_start: 341
  line_end: 348
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_register_and_create_custom_vector_store
  file: tests/integration/vector_stores/test_factory.py
  name: test_register_and_create_custom_vector_store
  signature: def test_register_and_create_custom_vector_store()
  decorators: []
  raises: []
  visibility: public
  docstring: '"""Test registering and creating a custom vector store type."""'
  code_example: null
  example_source: null
  line_start: 68
  line_end: 94
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/config/environment_reader.py::read_key
  file: graphrag/config/environment_reader.py
  name: read_key
  signature: 'def read_key(value: KeyValue) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Read a key value and normalize it to lowercase string.\n\nArgs:\n  \
    \  value: KeyValue\n        The key to normalize. It can be a string or an Enum.\
    \ If value is a string, the result is the string converted to lowercase. If value\
    \ is an Enum, the Enum.value attribute is used and lowercased. The Enum.value\
    \ is assumed to be a string; otherwise a runtime error may occur.\n\nReturns:\n\
    \    str\n        The lowercase representation of the key.\n\nRaises:\n    AttributeError\n\
    \        If value is not a string and does not have a value attribute, or if value.value\
    \ is not a string or cannot call lower()."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 23
  dependencies: []
  called_by:
  - graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
  - graphrag/config/environment_reader.py::EnvironmentReader.str
  - graphrag/config/environment_reader.py::EnvironmentReader.int
  - graphrag/config/environment_reader.py::EnvironmentReader.bool
  - graphrag/config/environment_reader.py::EnvironmentReader.float
  - graphrag/config/environment_reader.py::EnvironmentReader.list
- node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__call__
  file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  name: __call__
  signature: 'def __call__(self, input_text: str)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Generate a community report for the given input text using the configured\
    \ model and return both structured and text outputs.\n\nArgs:\n  input_text: str\
    \ - The input text to generate the report from.\n\nReturns:\n  CommunityReportsResult\
    \ - The result containing:\n    structured_output: CommunityReportResponse | None\
    \ - The parsed structured report from the model.\n    output: str - The human-readable\
    \ text representation of the report.\n\nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 72
  line_end: 96
  dependencies:
  - graphrag/index/operations/summarize_communities/community_reports_extractor.py::_get_text_output
  called_by: []
- node_id: tests/integration/logging/test_factory.py::test_create_unknown_logger
  file: tests/integration/logging/test_factory.py
  name: test_create_unknown_logger
  signature: def test_create_unknown_logger()
  decorators: []
  raises: []
  visibility: public
  docstring: "Tests that creating an unknown logger type raises a ValueError.\n\n\
    Args:\n    None: The test function does not take any parameters.\n\nReturns:\n\
    \    None: The test does not return a value.\n\nRaises:\n    ValueError: Unknown\
    \ reporting type: unknown"
  code_example: null
  example_source: null
  line_start: 63
  line_end: 65
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_cluster_graph_configs
  file: tests/unit/config/utils.py
  name: assert_cluster_graph_configs
  signature: "def assert_cluster_graph_configs(\n    actual: ClusterGraphConfig, expected:\
    \ ClusterGraphConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that actual and expected ClusterGraphConfig objects are equal\
    \ for cluster graph settings.\n\nArgs:\n    actual: ClusterGraphConfig\n     \
    \   The actual cluster graph configuration to validate.\n    expected: ClusterGraphConfig\n\
    \        The expected cluster graph configuration to validate.\n\nReturns:\n \
    \   None\n        The function does not return a value. It will raise AssertionError\
    \ if the compared fields differ.\n\nRaises:\n    AssertionError\n        If actual\
    \ and expected values for max_cluster_size, use_lcc, or seed differ."
  code_example: null
  example_source: null
  line_start: 303
  line_end: 308
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.__init__
  file: graphrag/index/text_splitting/text_splitting.py
  name: __init__
  signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
    \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
    \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n        length_function:\
    \ LengthFn = len,\n        keep_separator: bool = False,\n        add_start_index:\
    \ bool = False,\n        strip_whitespace: bool = True,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Init method for TextSplitter.\n\nInitialize the text splitter with the\
    \ given configuration.\n\nArgs:\n    chunk_size (int): Maximum length of a chunk\
    \ as measured by length_function. This follows the OpenAI embedding model's max\
    \ input buffer length guidance.\n    chunk_overlap (int): Overlap between consecutive\
    \ chunks, measured using length_function.\n    length_function (LengthFn): Function\
    \ to compute the length of text; defaults to len.\n    keep_separator (bool):\
    \ If True, keep separators when splitting text.\n    add_start_index (bool): If\
    \ True, add the starting index to chunks.\n    strip_whitespace (bool): If True,\
    \ strip leading and trailing whitespace from text.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 51
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: _summarize_descriptions
  signature: "def _summarize_descriptions(\n        self, id: str | tuple[str, str],\
    \ descriptions: list[str]\n    ) -> str"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronously summarize descriptions into a single description.\n\n\
    Args:\n  id: str | tuple[str, str] - Identifier(s) for the entity or entities.\n\
    \  descriptions: list[str] - Descriptions to be summarized.\n\nReturns:\n  str\
    \ - The summarized descriptions as a string.\n\nRaises:\n  Exception - If the\
    \ underlying LLM call fails or processing encounters an error."
  code_example: null
  example_source: null
  line_start: 73
  line_end: 116
  dependencies:
  - graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::_summarize_descriptions_with_llm
  called_by: []
- node_id: tests/unit/config/utils.py::assert_output_configs
  file: tests/unit/config/utils.py
  name: assert_output_configs
  signature: 'def assert_output_configs(actual: StorageConfig, expected: StorageConfig)
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two StorageConfig objects have identical field values.\n\
    \nArgs:\n    actual: The actual StorageConfig to validate.\n    expected: The\
    \ expected StorageConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n\
    \    AssertionError: If any of the fields differ: type, base_dir, connection_string,\
    \ container_name, storage_account_blob_url, cosmosdb_account_url."
  code_example: null
  example_source: null
  line_start: 139
  line_end: 145
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_completion
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: _base_completion
  signature: 'def _base_completion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Merge base arguments with provided keyword arguments and invoke the\
    \ litellm completion.\n\nArgs:\n  kwargs: Any\n      Additional keyword arguments\
    \ to merge with base_args and pass to completion after removing the \"name\" key\
    \ if present.\n\nReturns:\n  ModelResponse | CustomStreamWrapper\n      The result\
    \ from the underlying completion call.\n\nRaises:\n  Exception\n      Exceptions\
    \ raised by the underlying completion call."
  code_example: null
  example_source: null
  line_start: 95
  line_end: 101
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::_load_fixtures
  file: tests/smoke/test_fixtures.py
  name: _load_fixtures
  signature: def _load_fixtures()
  decorators: []
  raises: []
  visibility: protected
  docstring: "Load all fixtures from the tests/fixtures directory and return their\
    \ configurations (internal helper).\n\nIf GH_PAGES is set, only the min-csv fixture\
    \ is loaded; otherwise all subdirectories under tests/fixtures are considered.\n\
    \nReturns:\n  list of tuple (str, dict): a list where each item is a pair consisting\
    \ of the subfolder name and the parsed JSON configuration loaded from config.json\
    \ for that fixture. The first entry is omitted in order to disable the azure blob\
    \ connection test.\n\nRaises:\n  FileNotFoundError: If a subfolder is missing\
    \ config.json.\n  json.JSONDecodeError: If config.json cannot be parsed as JSON."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 48
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/snapshot_graphml.py::snapshot_graphml
  file: graphrag/index/operations/snapshot_graphml.py
  name: snapshot_graphml
  signature: "def snapshot_graphml(\n    input: str | nx.Graph,\n    name: str,\n\
    \    storage: PipelineStorage,\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Take a snapshot of a graph in GraphML format and persist it to storage.\n\
    \nThis function accepts either a GraphML content string or a NetworkX Graph. If\
    \ input is a string, it is treated as GraphML content (not a file path). If input\
    \ is a Graph, it is converted to GraphML using NetworkX's GraphML generator. The\
    \ resulting GraphML content is written asynchronously to the provided storage\
    \ backend under the key name + \".graphml\".\n\nArgs:\n    input: str | nx.Graph\n\
    \        GraphML content as a string, or a NetworkX graph object to be converted\
    \ to GraphML.\n    name: str\n        Base name for the stored GraphML entry;\
    \ the actual storage key will be name + \".graphml\".\n    storage: PipelineStorage\n\
    \        Storage backend used to persist the GraphML representation asynchronously.\n\
    \nReturns:\n    None\n        The function completes the storage operation asynchronously\
    \ and does not return a value.\n\nRaises:\n    Exceptions raised by the storage\
    \ backend or by GraphML generation may be propagated to the caller."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 18
  dependencies: []
  called_by:
  - graphrag/index/workflows/finalize_graph.py::run_workflow
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_end
  file: graphrag/callbacks/query_callbacks.py
  name: on_map_response_end
  signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "End of map operation callback. This default implementation is a no-op\
    \ and does not mutate state or produce side effects. Subclasses may override this\
    \ method to handle the map outputs as needed.\n\nArgs:\n    map_response_outputs\
    \ (list[SearchResult]): The outputs produced by the map operation.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 21
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/index/update/relationships.py::_update_and_merge_relationships
  file: graphrag/index/update/relationships.py
  name: _update_and_merge_relationships
  signature: "def _update_and_merge_relationships(\n    old_relationships: pd.DataFrame,\
    \ delta_relationships: pd.DataFrame\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update and merge relationships.\n\nArgs:\n    old_relationships: pd.DataFrame\
    \ The old relationships.\n    delta_relationships: pd.DataFrame The delta relationships.\n\
    \nReturns:\n    pd.DataFrame The updated relationships, containing the final columns\
    \ as defined by RELATIONSHIPS_FINAL_COLUMNS.\n\nRaises:\n    KeyError: If required\
    \ columns are missing from the input DataFrames.\n    TypeError: If the inputs\
    \ are not pandas DataFrames."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 85
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rate_limiter_validation
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_rate_limiter_validation
  signature: def test_rate_limiter_validation()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test rate limiter creation covering both valid and invalid parameter
    scenarios.


    This test exercises creating a rate limiter with valid parameters as well as handling
    various invalid inputs, including an unknown strategy, missing TPM/RPM, negative
    RPM or TPM, and an invalid period_in_seconds. The test function takes no explicit
    parameters and returns None implicitly.'
  code_example: null
  example_source: null
  line_start: 49
  line_end: 90
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::create_noun_phrase_extractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  name: create_noun_phrase_extractor
  signature: "def create_noun_phrase_extractor(\n    analyzer_config: TextAnalyzerConfig,\n\
    ) -> BaseNounPhraseExtractor"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a noun phrase extractor from a configuration.\n\nArgs:\n    analyzer_config\
    \ (TextAnalyzerConfig): Configuration for text analysis used to configure the\
    \ noun phrase extractor.\n\nReturns:\n    BaseNounPhraseExtractor: An instance\
    \ of a noun phrase extractor created according to the given configuration.\n\n\
    Raises:\n    Exception: If the underlying factory fails to create the extractor\
    \ (propagates from NounPhraseExtractorFactory.get_np_extractor)."
  code_example: null
  example_source: null
  line_start: 78
  line_end: 82
  dependencies: []
  called_by:
  - graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_start
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: workflow_start
  signature: 'def workflow_start(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
    \ The name of the workflow starting.\n    instance (object): The workflow instance\
    \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 31
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_max_retries
  file: graphrag/config/models/language_model_config.py
  name: _validate_max_retries
  signature: def _validate_max_retries(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "\"\"\"Validate the maximum retries.\n\nArgs:\n    self: The instance\
    \ of the language model configuration.\n\nReturns:\n    None: This method does\
    \ not return a value.\n\nRaises:\n    ValueError: If the maximum retries is less\
    \ than 1.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 316
  line_end: 326
  dependencies: []
  called_by: []
- node_id: graphrag/query/llm/text_utils.py::try_parse_json_object
  file: graphrag/query/llm/text_utils.py
  name: try_parse_json_object
  signature: 'def try_parse_json_object(input: str, verbose: bool = True) -> tuple[str,
    dict]'
  decorators: []
  raises: []
  visibility: public
  docstring: 'JSON cleaning and formatting utilities for strings that may contain
    JSON or JSON-like content produced by an LLM.


    Args:

    - input: str. The raw string to parse and clean.

    - verbose: bool. If True, log warnings or exceptions encountered during parsing.


    Returns:

    - tuple[str, dict]. The input string (potentially cleaned) and the parsed JSON
    object as a dict. If parsing ultimately fails, the dict will be empty.'
  code_example: null
  example_source: null
  line_start: 45
  line_end: 103
  dependencies: []
  called_by:
  - graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
  - graphrag/query/structured_search/drift_search/action.py::DriftAction.search
  - graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__init__
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n  \
    \      extraction_prompt: str | None = None,\n        input_text_key: str | None\
    \ = None,\n        input_entity_spec_key: str | None = None,\n        input_claim_description_key:\
    \ str | None = None,\n        input_resolved_entities_key: str | None = None,\n\
    \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
    \ str | None = None,\n        completion_delimiter_key: str | None = None,\n \
    \       max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn |\
    \ None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize ClaimExtractor.\n\nArgs:\n  model_invoker: ChatModel\n  \
    \    The model invoker used to run prompts.\n  extraction_prompt: str | None\n\
    \      Custom prompt for extraction. If None, defaults to EXTRACT_CLAIMS_PROMPT.\n\
    \  input_text_key: str | None\n      Key in inputs for the input text. Defaults\
    \ to \"input_text\".\n  input_entity_spec_key: str | None\n      Key for the entity\
    \ specifications. Defaults to \"entity_specs\".\n  input_claim_description_key:\
    \ str | None\n      Key for the claim description. Defaults to \"claim_description\"\
    .\n  input_resolved_entities_key: str | None\n      Key for resolved entities.\
    \ Defaults to \"resolved_entities\".\n  tuple_delimiter_key: str | None\n    \
    \  Key for the tuple delimiter. Defaults to \"tuple_delimiter\".\n  record_delimiter_key:\
    \ str | None\n      Key for the record delimiter. Defaults to \"record_delimiter\"\
    .\n  completion_delimiter_key: str | None\n      Key for the completion delimiter.\
    \ Defaults to \"completion_delimiter\".\n  max_gleanings: int | None\n      Maximum\
    \ number of gleanings to perform. If None, uses graphrag_config_defaults.extract_claims.max_gleanings.\n\
    \  on_error: ErrorHandlerFn | None\n      Error handler function. If None, a no-op\
    \ handler is used.\nReturns:\n  None"
  code_example: null
  example_source: null
  line_start: 50
  line_end: 85
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/generator/entity_types.py::generate_entity_types
  file: graphrag/prompt_tune/generator/entity_types.py
  name: generate_entity_types
  signature: "def generate_entity_types(\n    model: ChatModel,\n    domain: str,\n\
    \    persona: str,\n    docs: str | list[str],\n    task: str = DEFAULT_TASK,\n\
    \    json_mode: bool = False,\n) -> str | list[str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate entity type categories from a given set of documents.\n\nArgs:\n\
    \    model: ChatModel. The chat model to use for generation.\n    domain: str.\
    \ The domain context to tailor prompts.\n    persona: str. The system persona\
    \ content used as the initial system prompt.\n    docs: str | list[str]. A single\
    \ string or a list of strings containing the documents to extract entity types\
    \ from.\n    task: str. Task specification to format with domain; defaults to\
    \ DEFAULT_TASK.\n    json_mode: bool. If True, parse the response as JSON using\
    \ EntityTypesResponse and return a list of entity types; otherwise return the\
    \ raw text output.\n\nReturns:\n    str | list[str]. When json_mode is True, returns\
    \ a list of entity types extracted from the documents (or empty list on failure).\
    \ When json_mode is False, returns the raw string output from the model.\n\nRaises:\n\
    \    Exception. If the underlying model call fails or returns an unexpected structure."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 59
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks_by_key
  file: unified-search-app/app/ui/search.py
  name: format_response_hyperlinks_by_key
  signature: "def format_response_hyperlinks_by_key(\n    str_response: str, key:\
    \ str, anchor: str, search_type: str = \"\"\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Format response to show hyperlinks inside the response UI by key.\n\n\
    Args:\n    str_response: The response string to process.\n    key: The key label\
    \ in the response to locate citation patterns (e.g., \"Entities\").\n    anchor:\
    \ The anchor component used to construct hyperlink targets.\n    search_type:\
    \ The search type value; used to build the hyperlink href. Optional.\n\nReturns:\n\
    \    str: The response string with the matched citation numbers converted to hyperlinks."
  code_example: null
  example_source: null
  line_start: 121
  line_end: 148
  dependencies: []
  called_by:
  - unified-search-app/app/ui/search.py::format_response_hyperlinks
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.set
  file: graphrag/storage/memory_pipeline_storage.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously set the value for the given key in memory storage.\n\n\
    This method updates the in-memory storage dictionary by assigning the provided\
    \ value to the specified key. The encoding parameter is accepted for compatibility\
    \ but is not used in this implementation.\n\nArgs:\n    key (str): The key to\
    \ set the value for.\n    value (Any): The value to set.\n    encoding (str |\
    \ None): Optional encoding to apply when serializing the value (unused).\n\nReturns:\n\
    \    None: The coroutine completes when the value has been set.\n\nRaises:\n \
    \   None: This coroutine does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 46
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/sidebar.py::update_drift_search
  file: unified-search-app/app/ui/sidebar.py
  name: update_drift_search
  signature: 'def update_drift_search(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Update drift rag state.\n\nArgs:\n    sv: SessionVariables\n       \
    \ The container of session variables; used to read and update the include_drift_search\
    \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The function\
    \ does not return a value.\n\nRaises:\n    KeyError\n        If the expected key\
    \ sv.include_drift_search.key is not found in st.session_state."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._container_exists
  file: graphrag/vector_stores/cosmosdb.py
  name: _container_exists
  signature: def _container_exists(self) -> bool
  decorators: []
  raises: []
  visibility: protected
  docstring: "Check if the configured Cosmos DB container exists in the database.\n\
    \nArgs:\n    self: The instance that holds _container_name and _database_client\
    \ used to query Cosmos DB.\n\nReturns:\n    bool: True if a container with id\
    \ equal to self._container_name exists, otherwise False. The check is performed\
    \ against the 'id' field of containers returned by list_containers.\n\nRaises:\n\
    \    CosmosHttpResponseError: If an HTTP error occurs while listing containers.\
    \ This method does not catch it."
  code_example: null
  example_source: null
  line_start: 146
  line_end: 151
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_customization
  file: tests/integration/vector_stores/test_lancedb.py
  name: test_vector_store_customization
  signature: def test_vector_store_customization(self, sample_documents)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test vector store customization with LanceDB.\n\nArgs:\n    self: The\
    \ test case instance.\n    sample_documents: list[VectorStoreDocument] - Documents\
    \ used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\nRaises:\n\
    \    AssertionError: If any assertion in the test fails."
  code_example: null
  example_source: null
  line_start: 202
  line_end: 264
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.__init__
  file: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        rpm: int | None =\
    \ None,\n        tpm: int | None = None,\n        default_stagger: float = 0.0,\n\
    \        period_in_seconds: int = 60,\n        **kwargs: Any,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize the static rate limiter with optional RPM/TPM limits and\
    \ configuration.\n\nArgs:\n    rpm: int | None\n        RPM limit; positive integer\
    \ or None to disable.\n    tpm: int | None\n        TPM limit; positive integer\
    \ or None to disable.\n    default_stagger: float\n        Default stagger between\
    \ requests; must be >= 0.\n    period_in_seconds: int\n        Length of the period\
    \ in seconds; must be a positive integer.\n    kwargs: Any\n        Additional\
    \ keyword arguments (ignored).\n\nReturns:\n    None\n        This initializer\
    \ does not return a value.\n\nRaises:\n    ValueError\n        If both rpm and\
    \ tpm are None (disabled), or if rpm/tpm are non-positive, or if default_stagger\
    \ is negative, or if period_in_seconds is not positive."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: "def __call__(\n        self,\n        *,\n        messages: list = [],\
    \  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n       \
    \ stream_options: dict | None = None,  # type: ignore\n        stop=None,  # type:\
    \ ignore\n        max_completion_tokens: int | None = None,\n        max_tokens:\
    \ int | None = None,\n        modalities: list[ChatCompletionModality] | None\
    \ = None,\n        prediction: ChatCompletionPredictionContentParam | None = None,\n\
    \        audio: ChatCompletionAudioParam | None = None,\n        logit_bias: dict\
    \ | None = None,  # type: ignore\n        user: str | None = None,\n        #\
    \ openai v1.0+ new params\n        response_format: dict | type[BaseModel] | None\
    \ = None,  # type: ignore\n        seed: int | None = None,\n        tools: list\
    \ | None = None,  # type: ignore\n        tool_choice: str | dict | None = None,\
    \  # type: ignore\n        logprobs: bool | None = None,\n        top_logprobs:\
    \ int | None = None,\n        parallel_tool_calls: bool | None = None,\n     \
    \   web_search_options: OpenAIWebSearchOptions | None = None,\n        deployment_id=None,\
    \  # type: ignore\n        extra_headers: dict | None = None,  # type: ignore\n\
    \        # soon to be deprecated params by OpenAI\n        functions: list | None\
    \ = None,  # type: ignore\n        function_call: str | None = None,\n       \
    \ # Optional liteLLM function params\n        thinking: AnthropicThinkingParam\
    \ | None = None,\n        **kwargs: Any,\n    ) -> ModelResponse | CustomStreamWrapper"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Compute a chat completion using a language model and return the model's\
    \ response (or a streaming wrapper).\n\nArgs:\n    messages: list = []\n     \
    \   List of ChatCompletion messages to include in the request.\n    stream: bool\
    \ | None\n        If True, stream partial responses as they arrive.\n    stream_options:\
    \ dict | None\n        Options for streaming.\n    stop: Any\n        Stop sequence\
    \ or token.\n    max_completion_tokens: int | None\n        Maximum number of\
    \ tokens for the completion.\n    max_tokens: int | None\n        Maximum number\
    \ of tokens to generate.\n    modalities: list[ChatCompletionModality] | None\n\
    \        Modalities for the chat completion.\n    prediction: ChatCompletionPredictionContentParam\
    \ | None\n        Prediction content parameter for the chat completion.\n    audio:\
    \ ChatCompletionAudioParam | None\n        Audio parameters for the chat completion.\n\
    \    logit_bias: dict | None\n        Biases to apply to token logits.\n    user:\
    \ str | None\n        User identifier.\n    response_format: dict | type[BaseModel]\
    \ | None\n        Response format specification.\n    seed: int | None\n     \
    \   Random seed for deterministic sampling.\n    tools: list | None\n        Tools\
    \ to use during the chat completion.\n    tool_choice: str | dict | None\n   \
    \     Tool selection to use for the request.\n    logprobs: bool | None\n    \
    \    Include log probabilities in the response.\n    top_logprobs: int | None\n\
    \        Number of top log probabilities to return.\n    parallel_tool_calls:\
    \ bool | None\n        Enable parallel tool calls during processing.\n    web_search_options:\
    \ OpenAIWebSearchOptions | None\n        Web search options used during retrieval.\n\
    \    deployment_id: Any\n        Deployment identifier.\n    extra_headers: dict\
    \ | None\n        Extra HTTP headers to include in the request.\n    functions:\
    \ list | None\n        Deprecated OpenAI functions parameter.\n    function_call:\
    \ str | None\n        Function call specification.\n    thinking: AnthropicThinkingParam\
    \ | None\n        Optional liteLLM thinking parameter.\n    kwargs: Any\n    \
    \    Additional keyword arguments.\n\nReturns:\n    ModelResponse | CustomStreamWrapper\n\
    \        The model response object or a streaming wrapper for streaming responses.\n\
    \nRaises:\n    NotImplementedError\n        If invoked on the base protocol without\
    \ an implementing class.\n\nExample:\n    # Synchronous usage\n    response =\
    \ model(messages=[{'role': 'user', 'content': 'Hello'}], max_tokens=50)\n\n  \
    \  # Streaming usage\n    stream = model(messages=[{'role': 'user', 'content':\
    \ 'Explain this concept'}], stream=True)\n    for chunk in stream:\n        pass\
    \  # handle streaming chunks"
  code_example: null
  example_source: null
  line_start: 68
  line_end: 101
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.write
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: write
  signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
    \ | None = None\n    ) -> None"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Write data to a table.\n\nArgs:\n    table (str): The name of the target\
    \ table.\n    df (pd.DataFrame): The data to write to the table.\n    mode (WriteMode\
    \ | None): The write mode to use; None indicates default behavior.\n\nReturns:\n\
    \    None\n\nRaises:\n    NotImplementedError"
  code_example: null
  example_source: null
  line_start: 47
  line_end: 51
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/derive_from_rows.py::execute
  file: graphrag/index/utils/derive_from_rows.py
  name: execute
  signature: 'def execute(row: tuple[Any, pd.Series]) -> ItemType | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Apply the provided transform to the row data and await if necessary.\n\
    \nArgs:\n    row: tuple[Any, pd.Series] - A row, where row[1] is the pd.Series\
    \ passed to the transform.\n\nReturns:\n    ItemType | None - The transformed\
    \ value cast to ItemType, or None if an error occurred during transformation.\n\
    \nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 148
  line_end: 159
  dependencies: []
  called_by:
  - graphrag/index/utils/derive_from_rows.py::execute_row_protected
- node_id: graphrag/language_model/response/base.py::ModelOutput.content
  file: graphrag/language_model/response/base.py
  name: content
  signature: def content(self) -> str
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the textual content of the output.\n\nReturns:\n    str: The\
    \ textual content of the output."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 19
  dependencies: []
  called_by: []
- node_id: graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
  file: graphrag/index/text_splitting/text_splitting.py
  name: split_single_text_on_tokens
  signature: 'def split_single_text_on_tokens(text: str, tokenizer: TokenChunkerOptions)
    -> list[str]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Split a single text into chunks using the provided tokenizer.\n\nArgs:\n\
    \    text: str The input text to split into chunks.\n    tokenizer: TokenChunkerOptions\
    \ The tokenizer configuration used to encode the text into tokens and decode chunks.\
    \ It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\nReturns:\n\
    \    list[str] The list of chunked text strings produced.\n\nRaises:\n    Exception:\
    \ If the underlying tokenizer raises an error during encoding or decoding operations."
  code_example: null
  example_source: null
  line_start: 119
  line_end: 137
  dependencies: []
  called_by:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
  - tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens
  - tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.__init__
  file: graphrag/storage/memory_pipeline_storage.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize in-memory storage backend.\n\nThis constructor initializes\
    \ the internal storage by creating an empty\ndictionary bound to self._storage\
    \ and by calling the base class initializer.\n\nArgs:\n    self (MemoryPipelineStorage):\
    \ The instance being initialized. No additional\n        parameters.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 19
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.find_incomplete_actions
  file: graphrag/query/structured_search/drift_search/state.py
  name: find_incomplete_actions
  signature: def find_incomplete_actions(self) -> list[DriftAction]
  decorators: []
  raises: []
  visibility: public
  docstring: "Find all unanswered actions in the graph.\n\n        Args:\n       \
    \     self: The instance containing the graph where actions reside.\n\n      \
    \  Returns:\n            list[DriftAction]: A list of DriftAction objects that\
    \ are not complete."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 57
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.num_tokens
  file: graphrag/tokenizer/tokenizer.py
  name: num_tokens
  signature: 'def num_tokens(self, text: str) -> int'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the number of tokens in the given text.\n\nParameters\n    text\
    \ (str): The input text to analyze.\n\nReturns\n    int: The number of tokens\
    \ in the input text.\n\nRaises\n    NotImplementedError: If the encode method\
    \ is not implemented by a subclass."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 53
  dependencies:
  - graphrag/tokenizer/tokenizer.py::encode
  called_by: []
- node_id: graphrag/index/utils/uuid.py::gen_uuid
  file: graphrag/index/utils/uuid.py
  name: gen_uuid
  signature: 'def gen_uuid(rd: Random | None = None)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a random UUID v4 and return its hex representation.\n\nArgs:\n\
    \    rd: Random | None. Optional random number generator to use. If None, randomness\
    \ is sourced from the default RNG.\n\nReturns:\n    str: Hexadecimal string representation\
    \ of the generated UUID v4."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 14
  dependencies: []
  called_by: []
- node_id: tests/unit/litellm_services/test_retries.py::test_retries
  file: tests/unit/litellm_services/test_retries.py
  name: test_retries
  signature: "def test_retries(\n    strategy: str, max_retries: int, max_retry_wait:\
    \ int, expected_time: float\n) -> None"
  decorators:
  - "@pytest.mark.parametrize(\n    (\"strategy\", \"max_retries\", \"max_retry_wait\"\
    , \"expected_time\"),\n    [\n        (\n            \"native\",\n           \
    \ 3,  # 3 retries\n            0,  # native retry does not adhere to max_retry_wait\n\
    \            0,  # immediate retry, expect 0 seconds elapsed time\n        ),\n\
    \        (\n            \"exponential_backoff\",\n            3,  # 3 retries\n\
    \            0,  # exponential retry does not adhere to max_retry_wait\n     \
    \       14,  # (2^1 + jitter) + (2^2 + jitter) + (2^3 + jitter) = 2 + 4 + 8 +\
    \ 3*jitter = 14 seconds min total runtime\n        ),\n        (\n           \
    \ \"random_wait\",\n            3,  # 3 retries\n            2,  # random wait\
    \ [0, 2] seconds\n            0,  # unpredictable, don't know what the total runtime\
    \ will be\n        ),\n        (\n            \"incremental_wait\",\n        \
    \    3,  # 3 retries\n            3,  # wait for a max of 3 seconds on a single\
    \ retry.\n            6,  # Wait 3/3 * 1 on first retry, 3/3 * 2 on second, 3/3\
    \ * 3 on third, 1 + 2 + 3 = 6 seconds total runtime.\n        ),\n    ],\n)"
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Test various retry strategies by exercising a mock function that\
    \ raises an exception to verify retry behavior and timing.\n\nArgs\n    strategy:\
    \ The retry strategy to use.\n    max_retries: The maximum number of retry attempts.\n\
    \    max_retry_wait: The maximum wait time between retries.\n    expected_time:\
    \ The minimum elapsed time expected for the retries to complete.\n\nReturns\n\
    \    None\n\nRaises\n    ValueError: Raised by the mock function to simulate a\
    \ failed operation.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 46
  line_end: 81
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
  file: graphrag/index/operations/summarize_communities/explode_communities.py
  name: explode_communities
  signature: "def explode_communities(\n    communities: pd.DataFrame, entities: pd.DataFrame\n\
    ) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Explode a list of communities into nodes for filtering.\n\nArgs:\n \
    \   communities: pd.DataFrame\n        DataFrame containing an entity_ids column\
    \ with the IDs of entities in each community, along with community and level metadata\
    \ used after exploding.\n    entities: pd.DataFrame\n        DataFrame containing\
    \ an id column used to join with the exploded entity_ids, and a community identifier\
    \ column (the one named by COMMUNITY_ID) for filtering.\n\nReturns:\n    pd.DataFrame\n\
    \        A DataFrame of entities enriched with community information after the\
    \ explode and merge, filtered to exclude rows where the community identifier equals\
    \ -1.\n\nRaises:\n    KeyError\n        If required columns are missing from the\
    \ input DataFrames (for example entity_ids in communities, id in entities, or\
    \ the COMMUNITY_ID column)."
  code_example: null
  example_source: null
  line_start: 13
  line_end: 23
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
- node_id: graphrag/data_model/covariate.py::Covariate.from_dict
  file: graphrag/data_model/covariate.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        subject_id_key: str = \"subject_id\",\n        covariate_type_key:\
    \ str = \"covariate_type\",\n        short_id_key: str = \"human_readable_id\"\
    ,\n        text_unit_ids_key: str = \"text_unit_ids\",\n        attributes_key:\
    \ str = \"attributes\",\n    ) -> \"Covariate\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new Covariate from the dict data.\n\nArgs:\n    cls (type):\
    \ The Covariate class; this is a classmethod.\n    d (dict[str, Any]): The dictionary\
    \ containing covariate fields. The function reads keys including id_key, subject_id_key,\
    \ covariate_type_key, short_id_key, text_unit_ids_key, and attributes_key to construct\
    \ the Covariate.\n    id_key (str): The key in d that corresponds to the covariate's\
    \ id.\n    subject_id_key (str): The key in d that corresponds to the subject's\
    \ id.\n    covariate_type_key (str): The key in d for the covariate type.\n  \
    \  short_id_key (str): The key in d that corresponds to the covariate's short\
    \ id (human readable).\n    text_unit_ids_key (str): The key in d that contains\
    \ text unit ids (optional).\n    attributes_key (str): The key in d that contains\
    \ additional attributes (optional).\n\nReturns:\n    Covariate: The Covariate\
    \ instance created from the dictionary.\n\nRaises:\n    KeyError: If d does not\
    \ contain the required keys identified by id_key or subject_id_key."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 54
  dependencies: []
  called_by: []
- node_id: graphrag/cli/main.py::path_autocomplete
  file: graphrag/cli/main.py
  name: path_autocomplete
  signature: "def path_autocomplete(\n    file_okay: bool = True,\n    dir_okay: bool\
    \ = True,\n    readable: bool = True,\n    writable: bool = False,\n    match_wildcard:\
    \ str | None = None,\n) -> Callable[[str], list[str]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Autocomplete file and directory paths.\n\nArgs:\n    file_okay: bool\n\
    \        If True, include files in the completions; otherwise, exclude files.\n\
    \    dir_okay: bool\n        If True, include directories in the completions;\
    \ otherwise, exclude directories.\n    readable: bool\n        If True, include\
    \ only items that are readable (os.R_OK).\n    writable: bool\n        If True,\
    \ include only items that are writable (os.W_OK).\n    match_wildcard: str | None\n\
    \        Optional wildcard pattern to filter items; supports '?' and '*' characters.\n\
    \nReturns:\n    Callable[[str], list[str]]\n        A function that takes the\
    \ current incomplete string and returns a list of matching item names.\n\nRaises:\n\
    \    OSError\n        If an I/O error occurs during directory listing or permission\
    \ checks."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 76
  dependencies: []
  called_by: []
- node_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter.split_text
  file: graphrag/index/text_splitting/text_splitting.py
  name: split_text
  signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Split text into chunks.\n\nArgs:\n    text: str | list[str]\n      \
    \  The input text to split. A single string or a list of strings.\n\nReturns:\n\
    \    Iterable[str]\n        An iterable of text chunks. If a string is provided,\
    \ returns a single-element list containing the string; if a list of strings is\
    \ provided, returns that list as-is.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 78
  line_end: 80
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.delete
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Delete all cosmosdb items belonging to the given filename key.\n\nThis\
    \ coroutine does nothing if the database or container client is not initialized.\
    \ For keys containing \".parquet\", it deletes all items whose id starts with\
    \ the prefix of the key; otherwise it deletes the single item with id equal to\
    \ the key and the corresponding partition key.\n\nIf a CosmosResourceNotFoundError\
    \ is raised, it is ignored. Any other exception is logged.\n\nArgs:\n    key:\
    \ The filename key identifying the items to delete.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 298
  line_end: 318
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
  called_by: []
- node_id: graphrag/query/input/retrieval/text_units.py::to_text_unit_dataframe
  file: graphrag/query/input/retrieval/text_units.py
  name: to_text_unit_dataframe
  signature: 'def to_text_unit_dataframe(text_units: list[TextUnit]) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a list of text units to a pandas DataFrame.\n\nArgs:\n    text_units\
    \ (list[TextUnit]): The text units to convert into a DataFrame. If the list is\
    \ empty, an empty DataFrame is returned.\n\nReturns:\n    pd.DataFrame: A DataFrame\
    \ where each row corresponds to a text unit. The columns are:\n        - \"id\"\
    : the text unit's short_id\n        - \"text\": the text of the text unit\n  \
    \      - any additional attribute columns derived from the first text unit's attributes\
    \ keys (excluding \"id\" and \"text\"). Attribute values are converted to strings;\
    \ missing attributes result in empty strings.\n\nRaises:\n    None: This function\
    \ does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 53
  dependencies: []
  called_by:
  - graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
- node_id: graphrag/config/enums.py::InputFileType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Get a string representation of the enum member.\n\nArgs:\n   \
    \ self: The enum member instance.\n\nReturns:\n    str: The string representation\
    \ of the enum member, with its value enclosed in double quotes.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 40
  line_end: 42
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/home_page.py::on_click_reset
  file: unified-search-app/app/home_page.py
  name: on_click_reset
  signature: 'def on_click_reset(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Reset the relevant session variables on reset action.\n\nArgs:\n   \
    \ sv (SessionVariables): The session variables container; resets sv.generated_questions.value\
    \ to [], sv.selected_question.value to '', and sv.show_text_input.value to True.\n\
    \nReturns:\n    None: This function does not return a value."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 36
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed_batch
  file: graphrag/language_model/protocol/base.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nGenerate embedding vectors for the given list of strings.\n\n\
    Args:\n    text_list: The list of strings to generate embeddings for.\n    **kwargs:\
    \ Additional keyword arguments (e.g., model parameters).\n\nReturns:\n    list[list[float]]:\
    \ A list of embedding vectors for each input item in the batch.\n\nRaises:\n \
    \   Exception: If an error occurs during embedding generation.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 57
  line_end: 69
  dependencies: []
  called_by: []
- node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.delete
  file: graphrag/storage/pipeline_storage.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"Delete the given key from the storage.\n\nArgs:\n    key (str):\
    \ The key to delete.\n\nReturns:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 63
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.deserialize
  file: graphrag/query/structured_search/drift_search/action.py
  name: deserialize
  signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
  decorators:
  - '@classmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Deserialize the action from a dictionary.\n\nArgs:\n    data (dict[str,\
    \ Any]): Serialized action data.\n\nReturns:\n    DriftAction: A deserialized\
    \ instance of DriftAction.\n\nRaises:\n    ValueError: If the 'query' key is missing\
    \ in serialized data."
  code_example: null
  example_source: null
  line_start: 136
  line_end: 163
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_create_azure_ai_search_vector_store
  file: tests/integration/vector_stores/test_factory.py
  name: test_create_azure_ai_search_vector_store
  signature: def test_create_azure_ai_search_vector_store()
  decorators:
  - '@pytest.mark.skip(reason="Azure AI Search requires credentials and setup")'
  raises: []
  visibility: public
  docstring: "Test creating an Azure AI Search vector store using the VectorStoreFactory.\n\
    \nArgs:\n    None: The test function has no input parameters.\n\nReturns:\n  \
    \  None: The test does not return a value.\n\nRaises:\n    AssertionError: If\
    \ the created vector_store is not an instance of AzureAISearchVectorStore."
  code_example: null
  example_source: null
  line_start: 35
  line_end: 47
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
  file: graphrag/index/operations/finalize_community_reports.py
  name: finalize_community_reports
  signature: "def finalize_community_reports(\n    reports: pd.DataFrame,\n    communities:\
    \ pd.DataFrame,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Merge input reports with communities to create final community reports.\n\
    \nArgs:\n    reports: The input reports data to be enriched with community metadata.\n\
    \    communities: The communities dataset containing metadata used for enrichment\
    \ (including fields used for the merge: 'community', 'parent', 'children', 'size',\
    \ 'period').\n\nReturns:\n    The finalized community reports DataFrame containing\
    \ only the columns defined by COMMUNITY_REPORTS_FINAL_COLUMNS, augmented with\
    \ a human_readable_id and an id per row.\n\nRaises:\n    KeyError: If required\
    \ columns are missing from the inputs or if the final column set referenced by\
    \ COMMUNITY_REPORTS_FINAL_COLUMNS is not present in the merged result."
  code_example: null
  example_source: null
  line_start: 13
  line_end: 33
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.__str__
  file: graphrag/query/context_builder/conversation_history.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return string representation of the enum value.\n\nArgs:\n    self:\
    \ The enum member.\n\nReturns:\n    str: The string representation of the enum\
    \ value."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 41
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.child
  file: graphrag/storage/file_pipeline_storage.py
  name: child
  signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a storage instance for a child or the current instance if no\
    \ name is provided.\n\nArgs:\n    name: str | None, optional name for the child\
    \ storage. If None, the current instance is returned.\n\nReturns:\n    PipelineStorage:\
    \ The current instance (self) when name is None; otherwise a new FilePipelineStorage\
    \ representing the child storage located at the path formed by joining the current\
    \ root_dir with the provided name."
  code_example: null
  example_source: null
  line_start: 148
  line_end: 153
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_register_and_create_custom_storage
  file: tests/integration/storage/test_factory.py
  name: test_register_and_create_custom_storage
  signature: def test_register_and_create_custom_storage()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test registering and creating a custom storage type.


    This test registers a custom storage type named "custom" using StorageFactory.register
    with a factory function, creates storage via StorageFactory.create_storage, and
    verifies that the factory was invoked and that the returned storage is the expected
    instance. It also asserts that the created instance has initialized set to True,
    and that "custom" appears in StorageFactory.get_storage_types() and is reported
    as a supported type by StorageFactory.is_supported_type.


    Returns: None'
  code_example: null
  example_source: null
  line_start: 65
  line_end: 87
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.extract
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: extract
  signature: 'def extract(self, text: str) -> list[str]'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Extract noun phrases from text.\n\nArgs:\n    text (str): Text.\n\n\
    Returns:\n    list[str]: List of noun phrases."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 40
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/stable_lcc.py::_sort_source_target
  file: graphrag/index/utils/stable_lcc.py
  name: _sort_source_target
  signature: def _sort_source_target(edge)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Sorts a graph edge so that the source and target are in a stable, canonical\
    \ order.\n\nArgs:\n    edge: A 3-tuple (source, target, edge_data) representing\
    \ an edge from a graph.\n\nReturns:\n    A 3-tuple (source, target, edge_data)\
    \ with source and target ordered such that source <= target.\n\nRaises:\n    ValueError:\
    \ If edge does not contain exactly three elements.\n    TypeError: If edge elements\
    \ do not support comparison."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 51
  dependencies: []
  called_by:
  - graphrag/index/utils/stable_lcc.py::_stabilize_graph
- node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.aretry
  file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  name: aretry
  signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
    \        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry an asynchronous function.\n\nArgs:\n  func: The asynchronous function\
    \ to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword arguments\
    \ to pass to the function. (Any)\n\nReturns:\n  Any: The result of the awaited\
    \ function.\n\nRaises:\n  Exception: If the wrapped function keeps raising and\
    \ the maximum number of retries is exceeded."
  code_example: null
  example_source: null
  line_start: 61
  line_end: 83
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/utils.py::get_levels
  file: graphrag/index/operations/summarize_communities/utils.py
  name: get_levels
  signature: "def get_levels(\n    df: pd.DataFrame, level_column: str = schemas.COMMUNITY_LEVEL\n\
    ) -> list[int]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the levels of the communities.\n\nArgs:\n    df (pd.DataFrame):\
    \ The data frame containing community data.\n    level_column (str): The name\
    \ of the column that contains the level values. Defaults to schemas.COMMUNITY_LEVEL.\n\
    \nReturns:\n    list[int]: A list of integer levels in descending order, with\
    \ -1 and NaN values ignored.\n\nRaises:\n    KeyError: If level_column is not\
    \ a column in df."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 17
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
- node_id: graphrag/index/workflows/create_final_text_units.py::_join
  file: graphrag/index/workflows/create_final_text_units.py
  name: _join
  signature: def _join(left, right)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Join two DataFrames on the id column using a left merge.\n\nArgs:\n\
    \    left: pd.DataFrame\n        Left DataFrame to join on id.\n    right: pd.DataFrame\n\
    \        Right DataFrame to join on id.\n\nReturns:\n    pd.DataFrame\n      \
    \  The result of merging left and right on 'id' with a left join, applying suffixes\
    \ '_1' and '_2' to overlapping columns.\n\nRaises:\n    Exception: Propagates\
    \ exceptions raised by pandas DataFrame.merge during the join operation."
  code_example: null
  example_source: null
  line_start: 121
  line_end: 127
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_final_text_units.py::create_final_text_units
- node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
  file: unified-search-app/app/knowledge_loader/data_sources/loader.py
  name: _get_base_path
  signature: "def _get_base_path(\n    dataset: str | None, root: str | None, extra_path:\
    \ str | None = None\n) -> str"
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Construct and return the base path for the given dataset and extra\
    \ path.\n\nArgs:\n    dataset (str | None): The dataset folder name, or None to\
    \ omit.\n    root (str | None): The root path segment, or None to omit.\n    extra_path\
    \ (str | None): Additional path segments separated by '/' (if provided).\n\nReturns:\n\
    \    str: The constructed base path as a string.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 31
  line_end: 40
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::create_datasource
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::load_dataset_listing
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::load_prompts
- node_id: graphrag/language_model/manager.py::ModelManager.get_instance
  file: graphrag/language_model/manager.py
  name: get_instance
  signature: def get_instance(cls) -> ModelManager
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Return the singleton instance of ModelManager.\n\nThis is a classmethod\
    \ that returns the existing ModelManager singleton by delegating to the class's\
    \ __new__ method. No additional parameters are required beyond cls.\n\nArgs:\n\
    \    cls: The ModelManager class used to access the singleton instance.\n\nReturns:\n\
    \    ModelManager: The singleton ModelManager instance."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 43
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/generator/entity_relationship.py::generate_entity_relationship_examples
  file: graphrag/prompt_tune/generator/entity_relationship.py
  name: generate_entity_relationship_examples
  signature: "def generate_entity_relationship_examples(\n    model: ChatModel,\n\
    \    persona: str,\n    entity_types: str | list[str] | None,\n    docs: str |\
    \ list[str],\n    language: str,\n    json_mode: bool = False,\n) -> list[str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a list of entity/relationships examples for use in generating\
    \ an entity configuration.\n\nArgs:\n    model: ChatModel to use for generating\
    \ responses.\n    persona: Persona content used to seed the system history for\
    \ the chat.\n    entity_types: Optional entity types to guide generation. Can\
    \ be a string or a list of strings; if None, untyped prompts are generated.\n\
    \    docs: Documentation text to base the examples on. Can be a string or a list\
    \ of strings.\n    language: Target language for the prompts.\n    json_mode:\
    \ Whether to format the output as JSON (True) or as a tuple_delimiter format (False).\n\
    \nReturns:\n    list[str]: The generated examples as strings. If json_mode is\
    \ True, each string is a JSON-formatted example; otherwise the examples are in\
    \ tuple_delimiter format. Up to MAX_EXAMPLES items (5).\n\nRaises:\n    Exceptions\
    \ raised by the underlying model interactions (e.g., model.achat) may propagate\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 65
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.filter_by_id
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Filter vector store documents by a set of IDs.\n\nArgs:\n    include_ids:\
    \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    list[VectorStoreDocument]\
    \ - The documents from self.documents whose id is in include_ids."
  code_example: null
  example_source: null
  line_start: 57
  line_end: 58
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_filter_search
  file: tests/integration/vector_stores/test_lancedb.py
  name: test_filter_search
  signature: def test_filter_search(self, sample_documents_categories)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test filtered search with LanceDB to verify that filtering by document\
    \ IDs correctly constrains the results of a vector similarity search to the filtered\
    \ documents. Key steps: load the sample_documents_categories into the vector store,\
    \ apply filter_by_id(['1','2']), run similarity_search_by_vector with a sample\
    \ vector and k=3, and assert that the results respect the filter (at most two\
    \ results, no document with id '3', and all result ids are within {'1','2'}).\n\
    \nArgs:\n  self: The test case instance.\n  sample_documents_categories: list[VectorStoreDocument]\
    \ - Documents loaded into the LanceDB vector store for this test.\n\nReturns:\n\
    \  None.\n\nRaises:\n  AssertionError: If any assertion in the test fails."
  code_example: null
  example_source: null
  line_start: 173
  line_end: 200
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response_streaming
  file: graphrag/query/structured_search/drift_search/search.py
  name: _reduce_response_streaming
  signature: "def _reduce_response_streaming(\n        self,\n        responses: str\
    \ | dict[str, Any],\n        query: str,\n        model_params: dict[str, Any],\n\
    \    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Stream a reduced response by constructing a reduced prompt from the\
    \ provided responses and streaming tokens from the model.\n\nParameters\n----------\n\
    responses : str | dict[str, Any]\n    The responses to reduce. If a string, treated\
    \ as a single response; otherwise, if a dict,\n    extract the \"answer\" value\
    \ from each node in responses.get(\"nodes\", []) that has an \"answer\".\nquery\
    \ : str\n    The original query.\nmodel_params : dict[str, Any]\n    Parameters\
    \ for the underlying model used during streaming.\n\nReturns\n-------\nAsyncGenerator[str,\
    \ None]\n    An asynchronous generator yielding streamed tokens (strings) from\
    \ the model. Each yielded token\n    corresponds to a portion of the reduced response.\
    \ Tokens are also emitted to registered callbacks\n    via on_llm_new_token as\
    \ they arrive.\n\nRaises\n------\nException\n    Propagates exceptions raised\
    \ by the underlying model streaming or by registered callbacks."
  code_example: null
  example_source: null
  line_start: 402
  line_end: 448
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_drift_search_configs
  file: tests/unit/config/utils.py
  name: assert_drift_search_configs
  signature: "def assert_drift_search_configs(\n    actual: DRIFTSearchConfig, expected:\
    \ DRIFTSearchConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two DRIFTSearchConfig objects have equal drift-search configuration\
    \ values.\n\nArgs:\n    actual: The actual DRIFTSearchConfig to validate.\n  \
    \  expected: The expected DRIFTSearchConfig to compare against.\n\nReturns:\n\
    \    None\n\nRaises:\n    AssertionError: If any corresponding fields differ between\
    \ actual and expected."
  code_example: null
  example_source: null
  line_start: 346
  line_end: 376
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/config/enums.py::CacheType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
    \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's value\
    \ wrapped in double quotes.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 25
  line_end: 27
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_update_output_configs
  file: tests/unit/config/utils.py
  name: assert_update_output_configs
  signature: "def assert_update_output_configs(\n    actual: StorageConfig, expected:\
    \ StorageConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two StorageConfig objects have identical field values for\
    \ update output configurations.\n\nArgs:\n    actual: The actual StorageConfig\
    \ to validate.\n    expected: The expected StorageConfig to compare against.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If any of the fields differ:\
    \ type, base_dir, connection_string, container_name, storage_account_blob_url,\
    \ cosmosdb_account_url...."
  code_example: null
  example_source: null
  line_start: 148
  line_end: 156
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.set
  file: graphrag/language_model/providers/fnllm/cache.py
  name: set
  signature: "def set(\n        self, key: str, value: Any, metadata: dict[str, Any]\
    \ | None = None\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Write a value into the cache.\n\nArgs:\n    key: str \u2014 The key\
    \ under which to store the value.\n    value: Any \u2014 The value to store in\
    \ the cache.\n    metadata: dict[str, Any] | None \u2014 Optional metadata associated\
    \ with the value.\n\nReturns:\n    None \u2014 The method does not return a value.\n\
    \nRaises:\n    Exceptions raised by the underlying cache operation are propagated\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 31
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_cosmosdb_storage.py::test_child
  file: tests/integration/storage/test_cosmosdb_storage.py
  name: test_child
  signature: def test_child()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a child CosmosDBPipelineStorage can be created from a parent\
    \ storage.\n\nThe test initializes a CosmosDBPipelineStorage, uses the child()\
    \ method to obtain a child storage, and asserts that the returned object is a\
    \ CosmosDBPipelineStorage.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 70
  line_end: 80
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/utils.py::on_error
  file: graphrag/language_model/providers/fnllm/utils.py
  name: on_error
  signature: "def on_error(\n        error: BaseException | None = None,\n       \
    \ stack: str | None = None,\n        details: dict | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Log an error that occurred while invoking the LLM.\n\nArgs:\n    error\
    \ (BaseException | None): The exception to log; passed to exc_info in logger.error.\n\
    \    stack (str | None): Optional stack trace or contextual information; included\
    \ in the log's extra under \"stack\".\n    details (dict | None): Optional additional\
    \ details; included in the log's extra under \"details\".\n\nReturns:\n    None:\
    \ This function does not return a value.\n\nRaises:\n    None: This function does\
    \ not raise exceptions."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.progress
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: progress
  signature: 'def progress(self, progress: Progress) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Writes a live progress bar to stdout that updates in place as\
    \ progress events occur.\n\nThis callback renders a simple, in-place progress\
    \ bar on a single stdout line and\noverwrites the previous line using a carriage\
    \ return. It flushes the output to\nensure timely updates.\n\nProgress calculation:\n\
    - completed_items: number of items completed (defaults to 0 if None or falsy)\n\
    - total_items: total items to process (defaults to 1 if None or falsy to avoid\
    \ division by zero)\n- percent: integer percentage of completion, computed as\
    \ round((completed / total) * 100)\n\nOutput behavior:\n- Prints a line showing\
    \ the current progress as \"completed / total\" followed by a dot-filled\n  bar\
    \ whose width is proportional to percent. The bar is created by left-justifying\
    \ the\n  start string within a field of width percent using '.' as the fill character.\n\
    - The line uses end=\"\\r\" to return the cursor to the start of the line for\
    \ in-place updates.\n\nArgs:\n    progress: Progress object containing completed_items\
    \ and total_items attributes.\n\nReturns:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 40
  line_end: 46
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_is_supported_type
  file: tests/integration/vector_stores/test_factory.py
  name: test_is_supported_type
  signature: def test_is_supported_type()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that VectorStoreFactory.is_supported_type returns True for built-in\
    \ vector store type values LanceDB, AzureAISearch, CosmosDB and returns False\
    \ for an unknown type string.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 113
  line_end: 120
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_int
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_text_str_int
  signature: def test_split_text_str_int()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that TokenTextSplitter.split_text raises TypeError when the input\
    \ is an integer (non-string). \n\nReturns:\n    None: this test does not return\
    \ a value.\n\nRaises:\n    TypeError: if the input to split_text is not a string\
    \ (e.g., an integer)."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 51
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.set
  file: graphrag/storage/blob_pipeline_storage.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Set a value in the cache.\n\nArgs:\n  key: str \u2014 The key under\
    \ which to store the value.\n  value: Any \u2014 The value to store in the cache.\n\
    \  encoding: str | None \u2014 Optional encoding to use when encoding non-bytes\
    \ to bytes. If None, uses the default encoding.\n\nReturns:\n  None \u2014 The\
    \ method does not return a value.\n\nRaises:\n  Exception \u2014 Exceptions raised\
    \ by the underlying cache operation are caught and logged; they are not propagated\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 198
  line_end: 212
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_keyname
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::test_register_class_directly_works
  file: tests/integration/vector_stores/test_factory.py
  name: test_register_class_directly_works
  signature: def test_register_class_directly_works()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that registering a class directly works (VectorStoreFactory allows
    this).


    Args:

    - None: This test has no parameters.


    Returns:

    - None: This test does not return a value.


    Raises:

    - None: This test does not raise any exceptions.'
  code_example: null
  example_source: null
  line_start: 123
  line_end: 164
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__call__
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: __call__
  signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n \
    \       descriptions: list[str],\n    ) -> SummarizationResult"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronously process the given descriptions for the specified id and\
    \ return the summarization result.\n\nArgs:\n  id: str | tuple[str, str] - The\
    \ identifier for the summarization target. It can be a string or a tuple of two\
    \ strings.\n  descriptions: list[str] - The list of description strings to summarize.\
    \ If empty, the resulting description will be an empty string; if a single element,\
    \ that element is returned; otherwise a summarization is performed.\n\nReturns:\n\
    \  SummarizationResult - An object containing the id and the resulting description\
    \ (description is a string, possibly empty)."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 71
  dependencies:
  - graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::_summarize_descriptions
  called_by: []
- node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.stream_search
  file: graphrag/query/structured_search/basic_search/search.py
  name: stream_search
  signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Build basic search context that fits a single context window and stream\
    \ the answer for the user query.\n\nArgs:\n  query (str): The user query to process.\n\
    \  conversation_history (ConversationHistory | None): Optional conversation history\
    \ to incorporate into the search context.\n\nReturns:\n  AsyncGenerator[str, None]:\
    \ An asynchronous generator yielding strings representing chunks of the generated\
    \ answer as they are produced."
  code_example: null
  example_source: null
  line_start: 129
  line_end: 160
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed
  file: tests/integration/language_model/test_factory.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously generate an embedding for the input text.\n\nArgs:\n\
    \  text: The input text to generate the embedding for.\n  kwargs: Additional keyword\
    \ arguments passed to the embedding model.\n\nReturns:\n  list[float]: A list\
    \ of floating-point numbers representing the embedding.\n\nRaises:\n  This function\
    \ does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 70
  line_end: 71
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dataframes.py::join
  file: graphrag/index/utils/dataframes.py
  name: join
  signature: "def join(\n    left: pd.DataFrame, right: pd.DataFrame, key: str, strategy:\
    \ MergeHow = \"left\"\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a table join.\n\nArgs:\n    left: The left DataFrame.\n    right:\
    \ The right DataFrame.\n    key: The column name to join on.\n    strategy: The\
    \ merge strategy to use (how parameter for pandas merge). Defaults to left.\n\n\
    Returns:\n    pd.DataFrame: The joined DataFrame resulting from left.merge(right,\
    \ on=key, how=strategy)."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 43
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
- node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed
  file: graphrag/language_model/protocol/base.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text\
    \ (str): The text to generate an embedding for.\n    **kwargs: Additional keyword\
    \ arguments (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding\
    \ vector."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 55
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.retry
  file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  name: retry
  signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry a synchronous function until it succeeds or max_retries is reached.\n\
    \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
    \ as func(**kwargs) and its result will be returned on success.\n    kwargs: Any\
    \ - Keyword arguments to pass to func.\n\nReturns:\n    Any - The value returned\
    \ by func on a successful invocation.\n\nRaises:\n    Exception - The last exception\
    \ raised by func after exhausting max_retries."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 45
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: _get_container
  signature: 'def _get_container(account_name: str, container_name: str) -> ContainerClient'
  decorators:
  - '@st.cache_data(ttl=60 * 60 * 24)'
  raises: []
  visibility: protected
  docstring: "\"\"\"Return a ContainerClient for the specified Azure Blob Storage\
    \ container.\n\nArgs:\n    account_name: The Azure storage account name.\n   \
    \ container_name: The name of the blob container.\n\nReturns:\n    ContainerClient:\
    \ The container client for the specified container.\n\nRaises:\n    Exception:\
    \ If authentication, network, or other Azure Blob Storage errors occur.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 29
  line_end: 35
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_prompt_config
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
- node_id: graphrag/index/operations/embed_text/strategies/mock.py::_embed_text
  file: graphrag/index/operations/embed_text/strategies/mock.py
  name: _embed_text
  signature: 'def _embed_text(_cache: PipelineCache, _text: str, tick: ProgressTicker)
    -> list[float]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Embed a single piece of text.\n\nArgs:\n    _cache: PipelineCache: Cache\
    \ used for embedding operations.\n    _text: str: Text to embed.\n    tick: ProgressTicker:\
    \ Progress ticker to report progress.\n\nReturns:\n    list[float]: Embedding\
    \ vector as a list of three floats.\n\nRaises:\n    This function does not raise\
    \ any exceptions."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 35
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/strategies/mock.py::run
- node_id: graphrag/language_model/manager.py::ModelManager.register_embedding
  file: graphrag/language_model/manager.py
  name: register_embedding
  signature: "def register_embedding(\n        self, name: str, model_type: str, **embedding_kwargs:\
    \ Any\n    ) -> EmbeddingModel"
  decorators: []
  raises: []
  visibility: public
  docstring: "Register an EmbeddingsLLM instance under a unique name.\n\nRegisters\
    \ a new EmbeddingModel in self.embedding_models using the specified model_type\
    \ and the provided keyword arguments.\n\nArgs:\n    name (str): Unique identifier\
    \ for the EmbeddingsLLM instance.\n    model_type (str): Key for the EmbeddingsLLM\
    \ implementation in the factory (ModelFactory).\n    **embedding_kwargs: Additional\
    \ keyword arguments for instantiation, passed to the model factory.\n\nReturns:\n\
    \    EmbeddingModel: The EmbeddingModel instance registered under the given name.\n\
    \nRaises:\n    ValueError: If the provided model_type is invalid or if the underlying\
    \ factory encounters an error constructing the model.\n\nNotes:\n    The function\
    \ assigns the given name into embedding_kwargs before creation, and stores the\
    \ resulting model in self.embedding_models under the provided name."
  code_example: null
  example_source: null
  line_start: 62
  line_end: 77
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._keyname
  file: graphrag/storage/blob_pipeline_storage.py
  name: _keyname
  signature: 'def _keyname(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get the key name.\n\nArgs:\n    key: The key to be joined with the path\
    \ prefix to form the full key path.\n\nReturns:\n    The full key name as a string."
  code_example: null
  example_source: null
  line_start: 291
  line_end: 293
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_init_batch
  file: graphrag/query/context_builder/community_context.py
  name: _init_batch
  signature: def _init_batch() -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize batch state for the current context.\nThis updates nonlocal\
    \ batch_text, batch_tokens, and batch_records by:\n- building the batch_text header\
    \ as \"-----{context_name}-----\" followed by a newline and the header row joined\
    \ by column_delimiter\n- computing batch_tokens from the batch_text using tokenizer.num_tokens\n\
    - resetting batch_records to an empty list\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 124
  line_end: 130
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: graphrag/factory/factory.py::Factory.register
  file: graphrag/factory/factory.py
  name: register
  signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
    T]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Register a new service factory for a strategy.\n\nStores a factory (callable)\
    \ under the given strategy name. The factory is not invoked at registration time;\
    \ it will be called later by create(**kwargs) to produce an instance of T.\n\n\
    If a factory is already registered under the same strategy name, it will be overwritten\
    \ with the new factory.\n\nArgs:\n    strategy (str): The name of the strategy.\n\
    \    service_initializer (Callable[..., T]): A callable that, when invoked via\
    \ create(**kwargs), returns an instance of T.\n\nReturns:\n    None: This method\
    \ does not return a value."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 46
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_create_cosmosdb_storage
  file: tests/integration/storage/test_factory.py
  name: test_create_cosmosdb_storage
  signature: def test_create_cosmosdb_storage()
  decorators:
  - "@pytest.mark.skipif(\n    not sys.platform.startswith(\"win\"),\n    reason=\"\
    cosmosdb emulator is only available on windows runners at this time\",\n)"
  raises: []
  visibility: public
  docstring: "Test creating a CosmosDB storage via the StorageFactory.\n\nThis test\
    \ is skipped on non-Windows platforms because the Cosmos DB emulator is only available\
    \ on Windows runners.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If\
    \ the created storage is not an instance of CosmosDBPipelineStorage."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 50
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_action
  file: graphrag/query/structured_search/drift_search/state.py
  name: add_action
  signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any] |
    None = None)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Add an action node to the graph with optional metadata.\n\nArgs:\n \
    \   action: DriftAction to add as a node in the graph.\n    metadata: Optional\
    \ dict[str, Any] of node attributes to attach to the action. If None, no attributes\
    \ are added.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 24
  line_end: 26
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_input_configs
  file: tests/unit/config/utils.py
  name: assert_input_configs
  signature: 'def assert_input_configs(actual: InputConfig, expected: InputConfig)
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two InputConfig objects have identical field values.\n\n\
    Args:\n    actual: The actual InputConfig to validate.\n    expected: The expected\
    \ InputConfig to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If any of the fields differ: storage.type, file_type, storage.base_dir, storage.connection_string,\
    \ storage.storage_account_blob_url, storage.container_name, encoding, file_pattern,\
    \ file_filter, text_column, title_column, metadata."
  code_example: null
  example_source: null
  line_start: 168
  line_end: 183
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/index/operations/embed_text/embed_text.py::load_strategy
  file: graphrag/index/operations/embed_text/embed_text.py
  name: load_strategy
  signature: 'def load_strategy(strategy: TextEmbedStrategyType) -> TextEmbeddingStrategy'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load the embedding strategy callable for the given strategy type.\n\n\
    Args:\n    strategy: TextEmbedStrategyType - The strategy type used to determine\
    \ which embedding strategy to load.\n\nReturns:\n    TextEmbeddingStrategy: The\
    \ loaded strategy callable corresponding to the provided strategy.\n\nRaises:\n\
    \    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 229
  line_end: 246
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
  - graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
- node_id: tests/integration/storage/test_factory.py::CustomStorage.has
  file: tests/integration/storage/test_factory.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously checks whether the given key exists in storage.\n\nThis\
    \ coroutine should be awaited to obtain the result.\n\nArgs:\n    key: The key\
    \ to check for existence.\n\nReturns:\n    bool: True if the key exists, False\
    \ otherwise. Note: This is a placeholder implementation that always returns False.\
    \ Subclasses should override this method to provide a real existence check.\n\n\
    Raises:\n    Exception: If a storage backend encounters an error during the check."
  code_example: null
  example_source: null
  line_start: 136
  line_end: 137
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_entities
  file: graphrag/index/operations/extract_graph/extract_graph.py
  name: _merge_entities
  signature: def _merge_entities(entity_dfs) -> pd.DataFrame
  decorators: []
  raises: []
  visibility: protected
  docstring: "Merge and aggregate multiple entity DataFrames into a single aggregated\
    \ entities DataFrame by title and type.\n\nArgs:\n  entity_dfs: List[pandas.DataFrame]\
    \ List of DataFrames containing entity information. Each DataFrame is expected\
    \ to include the columns: \"title\", \"type\", \"description\", and \"source_id\"\
    .\n\nReturns:\n  pandas.DataFrame A DataFrame with one row per (title, type) pair\
    \ containing:\n    - title: entity title\n    - type: entity type\n    - description:\
    \ list of descriptions for this key\n    - text_unit_ids: list of source_id values\
    \ for this key\n    - frequency: number of source_id entries for this key"
  code_example: null
  example_source: null
  line_start: 100
  line_end: 110
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
- node_id: graphrag/storage/factory.py::StorageFactory.get_storage_types
  file: graphrag/storage/factory.py
  name: get_storage_types
  signature: def get_storage_types(cls) -> list[str]
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Get the registered storage implementations.\n\nArgs:\n    cls: The class\
    \ on which this classmethod is invoked.\n\nReturns:\n    list[str]: The list of\
    \ registered storage type keys (i.e., the keys of cls._registry)."
  code_example: null
  example_source: null
  line_start: 69
  line_end: 71
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.item_filter
  file: graphrag/storage/file_pipeline_storage.py
  name: item_filter
  signature: 'def item_filter(item: dict[str, Any]) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Determine whether the given item matches the current file_filter or\
    \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
    \ The keys used by file_filter are read from this dict. If a key referenced by\
    \ file_filter is missing from item, a KeyError may be raised. Values should be\
    \ strings (or objects compatible with re.search).\n\nReturns:\n    bool: True\
    \ if no file_filter is defined; otherwise, True only if all re.search(value, item[key])\
    \ checks succeed for every key, value pair in file_filter.\n\nRaises:\n    KeyError:\
    \ If item lacks a key referenced by file_filter.\n    TypeError: If item[key]\
    \ is not a string (and thus not compatible with re.search).\n    re.error: If\
    \ a regex pattern from file_filter is invalid.\n\nNotes:\n    This function relies\
    \ on file_filter from outer scope. To improve robustness and testability, consider\
    \ passing file_filter as an explicit parameter to the function."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 54
  dependencies: []
  called_by: []
- node_id: graphrag/config/enums.py::ReportingType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of the enumeration member.\n\nArgs:\n  \
    \  self (Enum): The enumeration member.\n\nReturns:\n    str: The member's value\
    \ wrapped in double quotes."
  code_example: null
  example_source: null
  line_start: 78
  line_end: 80
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.__init__
  file: graphrag/vector_stores/lancedb.py
  name: __init__
  signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
    \ **kwargs: Any\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize LanceDB vector store by delegating to the base class constructor.\n\
    \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema configuration\
    \ for the vector store.\n  **kwargs: Any - Additional keyword arguments forwarded\
    \ to the base class initializer.\n\nReturns:\n  None\n\nRaises:\n  Exceptions\
    \ raised by the base class __init__ are propagated...."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 29
  dependencies: []
  called_by: []
- node_id: graphrag/index/typing/pipeline.py::Pipeline.__init__
  file: graphrag/index/typing/pipeline.py
  name: __init__
  signature: 'def __init__(self, workflows: list[Workflow])'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initializes the Pipeline with the provided workflows.\n\nArgs:\n   \
    \ workflows: list[Workflow] The workflows to include in the pipeline.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 14
  line_end: 15
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.acquire
  file: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  name: acquire
  signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
  decorators:
  - '@contextmanager'
  raises: []
  visibility: public
  docstring: "Acquire Rate Limiter.\n\nArgs:\n    token_count: The estimated number\
    \ of tokens for the current request.\n\nReturns:\n    None: This context manager\
    \ yields None and does not return any value."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 133
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_vector_store_configs
  file: tests/unit/config/utils.py
  name: assert_vector_store_configs
  signature: "def assert_vector_store_configs(\n    actual: dict[str, VectorStoreConfig],\n\
    \    expected: dict[str, VectorStoreConfig],\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two dictionaries of VectorStoreConfig objects are equal.\n\
    \nArgs:\n    actual: dict[str, VectorStoreConfig]\n        Actual mapping of vector\
    \ store names to VectorStoreConfig objects to validate.\n    expected: dict[str,\
    \ VectorStoreConfig]\n        Expected mapping of vector store names to VectorStoreConfig\
    \ objects.\n\nReturns:\n    None\n        This function does not return a value;\
    \ it raises AssertionError on mismatches.\n\nRaises:\n    AssertionError\n   \
    \     If actual and expected do not match in type, length, keys, or any VectorStoreConfig\
    \ attributes\n        (type, db_uri, url, api_key, audience, container_name, overwrite,\
    \ database_name)."
  code_example: null
  example_source: null
  line_start: 109
  line_end: 126
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.find
  file: graphrag/storage/file_pipeline_storage.py
  name: find
  signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n \
    \       base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
    \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Find files in the storage that match a compiled file pattern and optional\
    \ metadata-based filtering.\n\nArgs:\n    file_pattern (re.Pattern[str]): A compiled\
    \ regular expression to match file paths.\n    base_dir (str | None): Base directory\
    \ to search within. If None, search starts from the storage root.\n    file_filter\
    \ (dict[str, Any] | None): Optional dictionary mapping named group keys to regular\
    \ expressions; only items whose corresponding fields match are yielded.\n    max_count\
    \ (int): The maximum number of results to yield. If -1, yield all matches.\n\n\
    Returns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator yielding tuples\
    \ of (filename, group) where filename is the path relative to the storage root\
    \ (without a leading path separator) and group is the dictionary of named groups\
    \ extracted by file_pattern.\n\nRaises:\n    KeyError: If file_filter references\
    \ a key not present in the named groups produced by file_pattern."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 85
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::item_filter
  called_by: []
- node_id: graphrag/index/operations/compute_degree.py::compute_degree
  file: graphrag/index/operations/compute_degree.py
  name: compute_degree
  signature: 'def compute_degree(graph: nx.Graph) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a new DataFrame with the degree of each node in the graph.\n\n\
    Args:\n    graph (nx.Graph): NetworkX graph from which to compute node degrees.\n\
    \nReturns:\n    pd.DataFrame: DataFrame with one row per node, containing the\
    \ columns:\n        title: the node identifier\n        degree: the degree of\
    \ the node as an integer."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 15
  dependencies: []
  called_by:
  - graphrag/index/operations/finalize_entities.py::finalize_entities
  - graphrag/index/operations/finalize_relationships.py::finalize_relationships
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_base_embeddings
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: _create_base_embeddings
  signature: "def _create_base_embeddings(\n    model_config: \"LanguageModelConfig\"\
    ,\n) -> tuple[FixedModelEmbedding, AFixedModelEmbedding]"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Wrap the base litellm embedding function with the model configuration.\n\
    \nArgs\n----\n    model_config: LanguageModelConfig\n        The configuration\
    \ for the language model.\n\nReturns\n-------\n    tuple[FixedModelEmbedding,\
    \ AFixedModelEmbedding]\n        A tuple containing the synchronous and asynchronous\
    \ embedding callables\n        produced by wrapping the base embedding with the\
    \ provided model\n        configuration.\n\nRaises\n------\n    ValueError\n \
    \       Azure Managed Identity authentication is only supported for Azure models\
    \ when\n        model_provider is not \"azure\"; in that case authentication setup\
    \ is rejected with\n        the corresponding error message.\n\nNotes\n-----\n\
    \    Azure authentication branch:\n    - Triggered when model_config.auth_type\
    \ == AuthType.AzureManagedIdentity.\n    - If model_config.model_provider == \"\
    azure\":\n      - azure_scope is set from the audience value (audience is moved\
    \ to azure_scope).\n      - azure_ad_token_provider is added, constructed via\
    \ get_bearer_token_provider(\n        DefaultAzureCredential(), model_config.audience\
    \ or COGNITIVE_SERVICES_AUDIENCE).\n    - If model_config.model_provider != \"\
    azure\": a ValueError is raised as described above."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 97
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
- node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_url
  file: graphrag/config/models/vector_store_config.py
  name: _validate_url
  signature: def _validate_url(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the database URL.\n\nArgs:\n    self: The VectorStoreConfig\
    \ instance being validated.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
    \ If vector_store.type == azure_ai_search and vector_store.url is missing or empty.\
    \ The error message is: vector_store.url is required when vector_store.type ==\
    \ azure_ai_search. Please rerun graphrag init and select the correct vector store\
    \ type.\n    ValueError: If vector_store.type == cosmos_db and vector_store.url\
    \ is missing or empty. The error message is: vector_store.url is required when\
    \ vector_store.type == cosmos_db. Please rerun graphrag init and select the correct\
    \ vector store type.\n    ValueError: If vector_store.type == LanceDB and vector_store.url\
    \ is provided (non-empty). The error message is: vector_store.url is only used\
    \ when vector_store.type == azure_ai_search or vector_store.type == cosmos_db.\
    \ Please rerun graphrag init and select the correct vector store type."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 63
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__init__
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n  \
    \      tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
    \ str | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
    \ str | None = None,\n        completion_delimiter_key: str | None = None,\n \
    \       prompt: str | None = None,\n        join_descriptions=True,\n        max_gleanings:\
    \ int | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initializes a GraphExtractor with the given configuration.\n\nCreates\
    \ and configures a GraphExtractor instance using the provided model_invoker and\
    \ optional configuration values. This constructor assigns the model to use for\
    \ prompt execution, defines default keys for various prompt variables (e.g., tuple_delimiter,\
    \ record_delimiter, input_text, entity_types, and completion_delimiter), selects\
    \ the extraction prompt, and establishes max_gleanings and an optional on_error\
    \ handler. It does not return a value.\n\nArgs:\n  model_invoker (ChatModel):\n\
    \      The model invoker used to run prompts.\n  tuple_delimiter_key (str | None):\n\
    \      Key in prompt_variables for the tuple delimiter. Defaults to \"tuple_delimiter\"\
    .\n  record_delimiter_key (str | None):\n      Key in prompt_variables for the\
    \ record delimiter. Defaults to \"record_delimiter\".\n  input_text_key (str |\
    \ None):\n      Key in inputs for the input text. Defaults to \"input_text\".\n\
    \  entity_types_key (str | None):\n      Key for the entity types in prompt variables.\
    \ Defaults to \"entity_types\".\n  completion_delimiter_key (str | None):\n  \
    \    Key for the completion delimiter in prompt variables. Defaults to \"completion_delimiter\"\
    .\n  prompt (str | None):\n      Custom extraction prompt to use. If None, defaults\
    \ to GRAPH_EXTRACTION_PROMPT.\n  join_descriptions (bool):\n      Whether to join\
    \ descriptions in the extraction.\n  max_gleanings (int | None):\n      Maximum\
    \ number of gleanings. If None, defaults to graphrag_config_defaults.extract_graph.max_gleanings.\n\
    \  on_error (ErrorHandlerFn | None):\n      Optional error handler function. If\
    \ None, a no-op handler is used."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 88
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::BaseModelOutput.__init__
  file: graphrag/language_model/response/base.pyi
  name: __init__
  signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
    \ dict[str, Any] | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "BaseModelOutput initialization.\n\nInitializes a BaseModelOutput with\
    \ the given content and optional full_response.\n\nArgs:\n    content: The output\
    \ content as a string.\n    full_response: Optional dict[str, Any] representing\
    \ the full response; defaults to None.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 28
  line_end: 32
  dependencies: []
  called_by: []
- node_id: graphrag/config/errors.py::ConflictingSettingsError.__init__
  file: graphrag/config/errors.py
  name: __init__
  signature: 'def __init__(self, msg: str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize error with the provided message.\n\nArgs:\n    msg: The error\
    \ message to pass to the base ValueError constructor.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 48
  line_end: 50
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.__init__
  file: graphrag/storage/blob_pipeline_storage.py
  name: __init__
  signature: 'def __init__(self, **kwargs: Any) -> None'
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize a new BlobPipelineStorage instance.\n\nThis constructor selects\
    \ the initialization flow as follows:\n- If a connection_string is provided, create\
    \ the BlobServiceClient from the connection string.\n- Otherwise, if storage_account_blob_url\
    \ is provided, create the BlobServiceClient using the account URL and DefaultAzureCredential.\n\
    - If neither is provided, raise a ValueError.\n\ncontainer_name is required. Providing\
    \ container_name as None raises ValueError. If the container_name key is missing\
    \ from kwargs, a KeyError is raised by Python.\n\nbase_dir is optional and sets\
    \ the path prefix within the container (defaults to an empty string). encoding\
    \ defaults to 'utf-8'.\n\nArgs:\n  container_name: The container name to use for\
    \ blob storage. This key is required. If the key is missing from kwargs, a KeyError\
    \ is raised. If the value is None, a ValueError is raised.\n  connection_string:\
    \ The Azure Blob Storage connection string. If provided, used to initialize the\
    \ client.\n  storage_account_blob_url: The URL of the storage account blob endpoint.\
    \ Used with DefaultAzureCredential when connection_string is not provided.\n \
    \ base_dir: The base directory (path prefix) within the container. Optional. Defaults\
    \ to ''.\n  encoding: Encoding to use. Defaults to 'utf-8'.\n\nReturns:\n  None\n\
    \nRaises:\n  KeyError: If container_name is not provided in kwargs.\n  ValueError:\
    \ If container_name is None.\n  ValueError: If neither connection_string nor storage_account_blob_url\
    \ is provided.\n  ValueError: If storage_account_blob_url is None when connection_string\
    \ is not provided."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 74
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_create_container
  called_by: []
- node_id: graphrag/language_model/protocol/base.py::ChatModel.chat_stream
  file: graphrag/language_model/protocol/base.py
  name: chat_stream
  signature: "def chat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs: Any\n    ) -> Generator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a response for the given text using a streaming interface.\n\
    \nArgs:\n    prompt: str \u2014 The text to generate a response for.\n    history:\
    \ list | None \u2014 The conversation history.\n    **kwargs: Any \u2014 Additional\
    \ keyword arguments (e.g., model parameters).\n\nReturns:\n    Generator[str,\
    \ None] \u2014 The generator that yields strings representing the response."
  code_example: null
  example_source: null
  line_start: 151
  line_end: 166
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.__new__
  file: graphrag/language_model/manager.py
  name: __new__
  signature: def __new__(cls) -> Self
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create a new singleton instance of ModelManager if it does not exist.\n\
    \nArgs:\n    cls: Type[ModelManager] The ModelManager class used to access the\
    \ singleton instance.\n\nReturns:\n    Self: The singleton ModelManager instance.\n\
    \nRaises:\n    None: This method does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 31
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.filter_by_entity_keys
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: filter_by_entity_keys
  signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
  decorators: []
  raises: []
  visibility: public
  docstring: "Filter entity text embeddings by entity keys.\n\nArgs:\n    entity_keys:\
    \ List of entity keys to filter by. May be a list of integers or a list of strings.\n\
    \nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 87
  line_end: 89
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_child
  file: tests/integration/storage/test_blob_pipeline_storage.py
  name: test_child
  signature: def test_child()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a child BlobPipelineStorage can be created from a parent storage\
    \ and used to perform basic file operations.\n\nReturns:\n    None\n    The function\
    \ does not return a value."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 115
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::ModelResponse.history
  file: graphrag/language_model/response/base.pyi
  name: history
  signature: def history(self) -> list[Any]
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "History of the model response.\n\nReturns\n    list[Any]: The history\
    \ as a list of items."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_pattern
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_input_pattern
  signature: def _validate_input_pattern(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "Mutates the input file_pattern when it is empty by applying a default\
    \ pattern based on the input type.\n\nIf the pattern is empty:\n- for text input:\
    \ sets the pattern to \".*\\\\.txt$\"\n- for other input types: sets the pattern\
    \ to \".*\\\\.{extension}$\" where extension is the value of input.file_type.value\n\
    \nArgs:\n    self: The GraphRagConfig instance.\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 144
  line_end: 150
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::decode
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: decode
  signature: 'def decode(tokens: list[int]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
    \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from the\
    \ list of tokens.\n\nRaises:\n    Exception: If decoding fails due to an underlying\
    \ error in the encoding."
  code_example: null
  example_source: null
  line_start: 141
  line_end: 142
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.clear
  file: tests/integration/storage/test_factory.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously clear all entries from the storage backend. This no-op\
    \ implementation does not modify any stored data.\n\nThis coroutine completes\
    \ without returning a value.\n\nParameters:\n    None\n\nReturns:\n    None\n\n\
    Raises:\n    None: This no-op implementation does not raise exceptions."
  code_example: null
  example_source: null
  line_start: 139
  line_end: 140
  dependencies: []
  called_by: []
- node_id: tests/integration/logging/test_factory.py::test_get_logger_types
  file: tests/integration/logging/test_factory.py
  name: test_get_logger_types
  signature: def test_get_logger_types()
  decorators: []
  raises: []
  visibility: public
  docstring: "Verify that built-in logger types are registered and returned by LoggerFactory.get_logger_types.\n\
    \nThis test retrieves the list of registered logger types from LoggerFactory and\
    \ asserts that\nReportingType.file.value and ReportingType.blob.value are present\
    \ in the result.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the\
    \ expected logger types are not present in the result."
  code_example: null
  example_source: null
  line_start: 56
  line_end: 60
  dependencies: []
  called_by: []
- node_id: graphrag/logger/factory.py::create_file_logger
  file: graphrag/logger/factory.py
  name: create_file_logger
  signature: def create_file_logger(**kwargs) -> logging.Handler
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a file-based logger handler.\n\nArgs:\n    root_dir: The root\
    \ directory under which logs are stored.\n    base_dir: The base directory under\
    \ root_dir where logs are written.\n    filename: The log filename to use for\
    \ the log file.\n\nReturns:\n    logging.Handler: A configured handler writing\
    \ to the specified log file.\n\nRaises:\n    KeyError: If required keys (root_dir,\
    \ base_dir, filename) are missing in kwargs.\n    OSError: If the log directory\
    \ cannot be created or the log file cannot be opened."
  code_example: null
  example_source: null
  line_start: 82
  line_end: 96
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_end
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: workflow_end
  signature: 'def workflow_end(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
    \        The name of the workflow.\n    instance: object\n        The workflow\
    \ instance object.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 23
  line_end: 24
  dependencies: []
  called_by: []
- node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.generate
  file: graphrag/query/question_gen/local_gen.py
  name: generate
  signature: "def generate(\n        self,\n        question_history: list[str],\n\
    \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
    \    ) -> QuestionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a question based on the question history and context data.\n\
    \nIf context data is not provided, it will be generated by the local context builder.\n\
    \nArgs:\n    question_history (list[str]): History of previously asked questions.\n\
    \    context_data (str | None): Optional context data; if None, context data will\
    \ be generated by the local context builder.\n    question_count (int): Number\
    \ of questions to generate.\n    kwargs: Additional keyword arguments for extensibility\
    \ (passed to the context builder and model configuration).\n\nReturns:\n    QuestionResult:\
    \ The generated results including the response as a list of strings, context_data\
    \ containing the question_context and context_records, completion_time, llm_calls,\
    \ and prompt_tokens.\n\nRaises:\n    Exception: Exceptions are caught internally\
    \ and do not propagate to callers; if an unexpected error occurs during generation,\
    \ it will be logged and a default QuestionResult will be returned."
  code_example: null
  example_source: null
  line_start: 132
  line_end: 213
  dependencies:
  - graphrag/query/question_gen/base.py::QuestionResult
  called_by: []
- node_id: graphrag/index/workflows/factory.py::PipelineFactory.create_pipeline
  file: graphrag/index/workflows/factory.py
  name: create_pipeline
  signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
    \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline"
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a pipeline for executing a sequence of workflows.\n\nArgs:\n\
    \    cls: The class reference (provided automatically for classmethod)\n    config:\
    \ GraphRagConfig\n    method: The indexing method or key to select a predefined\
    \ pipeline. Defaults to IndexingMethod.Standard.\n\nReturns:\n    Pipeline: The\
    \ constructed Pipeline object.\n\nRaises:\n    KeyError: If any workflow name\
    \ in the selected workflows is not registered in the class-level workflows registry."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 48
  dependencies:
  - graphrag/index/typing/pipeline.py::Pipeline
  called_by: []
- node_id: graphrag/language_model/response/base.py::ModelOutput.full_response
  file: graphrag/language_model/response/base.py
  name: full_response
  signature: def full_response(self) -> dict[str, Any] | None
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "\"\"\"Return the complete JSON response returned by the model.\n\nArgs:\n\
    \    self: The instance from which the full_response is accessed.\n\nReturns:\n\
    \    dict[str, Any] | None: The complete JSON response returned by the model.\n\
    \n\"\"\""
  code_example: null
  example_source: null
  line_start: 22
  line_end: 24
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/session_variables.py::SessionVariables.__init__
  file: unified-search-app/app/state/session_variables.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Initialize all SessionVariables for the unified search app.


    SessionVariables.__init__ creates and initializes every session attribute used
    to track the

    state of a unified search session. Each attribute is assigned a default value
    to ensure a

    consistent, predictable initial state.


    Attributes initialized (with defaults):

    - dataset: QueryVariable("dataset", "")

    - datasets: SessionVariable([])

    - dataset_config: SessionVariable()

    - datasource: SessionVariable()

    - graphrag_config: SessionVariable()

    - question: QueryVariable("question", "")

    - suggested_questions: SessionVariable(default_suggested_questions)

    - entities: SessionVariable([])

    - relationships: SessionVariable([])

    - covariates: SessionVariable({})

    - communities: SessionVariable([])

    - community_reports: SessionVariable([])

    - text_units: SessionVariable([])

    - question_in_progress: SessionVariable("")

    - include_global_search: QueryVariable("include_global_search", True)

    - include_local_search: QueryVariable("include_local_search", True)

    - include_drift_search: QueryVariable("include_drift_search", False)

    - include_basic_rag: QueryVariable("include_basic_rag", False)

    - selected_report: SessionVariable()

    - graph_community_level: SessionVariable(0)

    - selected_question: SessionVariable("")

    - generated_questions: SessionVariable([])

    - show_text_input: SessionVariable(True)


    Returns:

    None'
  code_example: null
  example_source: null
  line_start: 16
  line_end: 42
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: read
  signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
    \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  decorators: []
  raises:
  - FileNotFoundError
  visibility: public
  docstring: "Read file from local source.\n\nArgs:\n    table: The table name to\
    \ read (without the .parquet extension).\n    throw_on_missing: If True, raise\
    \ FileNotFoundError when the table file does not exist.\n    columns: Optional\
    \ list of column names to read from the parquet file. If None, all columns are\
    \ read.\n\nReturns:\n    A pandas DataFrame containing the data from the parquet\
    \ file. If the table file does not exist\n    and throw_on_missing is False, returns\
    \ an empty DataFrame; if columns is provided, the DataFrame\n    will contain\
    \ those columns.\n\nRaises:\n    FileNotFoundError: If the table file does not\
    \ exist and throw_on_missing is True."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 62
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.has_table
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: has_table
  signature: 'def has_table(self, table: str) -> bool'
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "\"\"\"Check if table exists.\n\nArgs:\n    table: The name of the table\
    \ to check for existence.\n\nReturns:\n    bool: True if the table exists, otherwise\
    \ False.\n\nRaises:\n    NotImplementedError: If the method is not implemented.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 53
  line_end: 55
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.load_documents
  file: graphrag/vector_stores/base.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Load documents into the vector-store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
    \ - List of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
    \ bool - If True, overwrite existing data in the vector store; otherwise, preserve\
    \ existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: load_documents\
    \ method not implemented."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 70
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__repr__
  file: unified-search-app/app/state/session_variable.py
  name: __repr__
  signature: def __repr__(self) -> Any
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return a string representation of the managed session variable value.\n\
    \nArgs:\n  self: The instance of the class containing the key used to index session_state.\n\
    \nReturns:\n  str: The string representation of the value stored in st.session_state\
    \ for this key.\n\nRaises:\n  KeyError: If the key is not present in st.session_state."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 53
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__str__
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: __str__
  signature: def __str__(self) -> str
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: protected
  docstring: "Return string representation of the extractor.\n\nThis is an abstract\
    \ method and must be implemented by concrete subclasses. The base\nclass cannot\
    \ be instantiated due to ABCMeta and abstractmethod usage; the actual\nstring\
    \ representation is therefore defined by subclass implementations. At runtime,\n\
    the behavior depends on the subclass implementation rather than raising an error\
    \ in\nthe base class.\n\nArgs:\n    self (BaseNounPhraseExtractor): The instance\
    \ of the extractor.\n\nReturns:\n    str: The string representation used for cache\
    \ key generation, encoding the extractor's\n    configuration (e.g., model_name,\
    \ max_word_length, exclude_nouns, word_delimiter)."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 44
  dependencies: []
  called_by: []
- node_id: graphrag/utils/api.py::MultiVectorStore.filter_by_id
  file: graphrag/utils/api.py
  name: filter_by_id
  signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
    \ list[str] | list[int]\n    The IDs to include in the filter.\n\nReturns:\n \
    \ Any\n    The constructed query filter to filter documents by the provided IDs.\n\
    \nRaises:\n  NotImplementedError\n    If the method is not implemented."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_embedding_model
  file: graphrag/language_model/factory.py
  name: is_supported_embedding_model
  signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Check if the given embedding model type is supported.\n\nArgs:\n   \
    \ cls: type The class reference (classmethod parameter).\n    model_type: str\
    \ The type identifier for the embedding model to check.\n\nReturns:\n    bool:\
    \ True if model_type is registered in the embedding registry, otherwise False."
  code_example: null
  example_source: null
  line_start: 93
  line_end: 95
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_end
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: workflow_end
  signature: 'def workflow_end(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
    \        The name of the workflow.\n    instance: object\n        The workflow\
    \ instance object.\n\nReturns:\n    None..."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 38
  dependencies: []
  called_by: []
- node_id: tests/unit/indexing/test_init_content.py::uncomment_line
  file: tests/unit/indexing/test_init_content.py
  name: uncomment_line
  signature: 'def uncomment_line(line: str) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Uncomments a line by removing a leading "# " prefix, preserving indentation.


    Args:

    - line: str - input line that may start with whitespace followed by "# " to be
    removed.


    Returns:

    - str - the line with the first occurrence of a leading "# " removed, preserving
    the original indentation.


    Raises:

    - None'
  code_example: null
  example_source: null
  line_start: 24
  line_end: 26
  dependencies: []
  called_by:
  - tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
- node_id: graphrag/index/run/utils.py::create_run_context
  file: graphrag/index/run/utils.py
  name: create_run_context
  signature: "def create_run_context(\n    input_storage: PipelineStorage | None =\
    \ None,\n    output_storage: PipelineStorage | None = None,\n    previous_storage:\
    \ PipelineStorage | None = None,\n    cache: PipelineCache | None = None,\n  \
    \  callbacks: WorkflowCallbacks | None = None,\n    stats: PipelineRunStats |\
    \ None = None,\n    state: PipelineState | None = None,\n) -> PipelineRunContext"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Create the run context for the pipeline.\n\nArgs:\n    input_storage:\
    \ PipelineStorage | None\n        The input storage to use for the run.\n    output_storage:\
    \ PipelineStorage | None\n        The output storage to use for the run.\n   \
    \ previous_storage: PipelineStorage | None\n        The previous storage to use\
    \ for the run.\n    cache: PipelineCache | None\n        The cache to use for\
    \ the run.\n    callbacks: WorkflowCallbacks | None\n        The workflow callbacks\
    \ to use during the run.\n    stats: PipelineRunStats | None\n        The statistics\
    \ collector for the run.\n    state: PipelineState | None\n        Initial state\
    \ for the run.\n\nReturns:\n    PipelineRunContext: The configured run context.\n\
    \n\"\"\""
  code_example: null
  example_source: null
  line_start: 20
  line_end: 38
  dependencies:
  - graphrag.cache.memory_pipeline_cache::InMemoryCache
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/index/typing/context.py::PipelineRunContext
  - graphrag/index/typing/stats.py::PipelineRunStats
  - graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage
  called_by:
  - graphrag/index/run/run_pipeline.py::run_pipeline
  - tests/verbs/test_pipeline_state.py::test_pipeline_state
  - tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
  - tests/verbs/util.py::create_test_context
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_text_unit_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_text_unit_data
  signature: 'def get_text_unit_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a dataframe containing text units (i.e. chunks of text from raw\
    \ documents) from the indexed data.\n\nArgs:\n    dataset: str The dataset identifier.\n\
    \    _datasource: Datasource The data source to read text units from.\n\nReturns:\n\
    \    pd.DataFrame: A dataframe containing the text unit records from the indexed\
    \ data.\n\nRaises:\n    Exception: If reading from the datasource or processing\
    \ fails."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 56
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::test_create_memory_storage
  file: tests/integration/storage/test_factory.py
  name: test_create_memory_storage
  signature: def test_create_memory_storage()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test creating a memory storage via StorageFactory using StorageType.memory.value\
    \ with an empty kwargs dict, and verify the result is a MemoryPipelineStorage\
    \ instance.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ if the created storage is not an instance of MemoryPipelineStorage."
  code_example: null
  example_source: null
  line_start: 59
  line_end: 62
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._reduce_response
  file: graphrag/query/structured_search/global_search/search.py
  name: _reduce_response
  signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
    \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Combine all intermediate responses from multiple batches into a final\
    \ answer to the user query.\n\nArgs:\n    self: The instance of the containing\
    \ class.\n    map_responses: list[SearchResult]\n        The intermediate responses\
    \ collected from each batch.\n    query: str\n        The original user query.\n\
    \    llm_kwargs: dict\n        Additional keyword arguments to pass to the LLM\
    \ model during reduction.\n\nReturns:\n    SearchResult\n        The reduced final\
    \ response containing the generated answer, context data,\n        and performance\
    \ metrics such as completion time and token usage.\n\nRaises:\n    None\n    \
    \    This method handles exceptions internally and does not raise to callers."
  code_example: null
  example_source: null
  line_start: 296
  line_end: 413
  dependencies:
  - graphrag/query/structured_search/base.py::SearchResult
  called_by: []
- node_id: tests/smoke/test_fixtures.py::pytest_generate_tests
  file: tests/smoke/test_fixtures.py
  name: pytest_generate_tests
  signature: def pytest_generate_tests(metafunc)
  decorators: []
  raises: []
  visibility: public
  docstring: 'Generate parameterized tests for all test functions in this module.


    Args:

    metafunc: The pytest metafunc object used to inspect, filter, and parametrize
    tests.


    Returns:

    None


    Raises:

    KeyError: If the expected per-function configuration is missing from metafunc.cls.params
    for the given function name.

    AttributeError: If expected attributes are not present on metafunc (e.g., metafunc.function
    or metafunc.cls).'
  code_example: null
  example_source: null
  line_start: 51
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.section
  file: graphrag/config/environment_reader.py
  name: section
  signature: def section(self) -> dict
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Get the current section.\n\nReturns:\n    dict: The current section\
    \ dictionary from the configuration stack, or an empty\n    dict if there is no\
    \ active section."
  code_example: null
  example_source: null
  line_start: 74
  line_end: 76
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_extract_graph_configs
  file: tests/unit/config/utils.py
  name: assert_extract_graph_configs
  signature: "def assert_extract_graph_configs(\n    actual: ExtractGraphConfig, expected:\
    \ ExtractGraphConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that the actual and expected ExtractGraphConfig instances have\
    \ equal values for their core fields.\n\nArgs:\n    actual: The actual ExtractGraphConfig\
    \ instance produced by the code under test.\n    expected: The expected ExtractGraphConfig\
    \ instance to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If any of the fields prompt, entity_types, max_gleanings, strategy, or model_id\
    \ differ between actual and expected."
  code_example: null
  example_source: null
  line_start: 227
  line_end: 234
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_tokens_per_minute
  file: graphrag/config/models/language_model_config.py
  name: _validate_tokens_per_minute
  signature: def _validate_tokens_per_minute(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the tokens per minute.\n\nThis in-place validator ensures tokens_per_minute\
    \ is one of: an integer >= 1, the string \"auto\", or None. It also enforces that\
    \ tokens_per_minute cannot be set to 'auto' when using a Chat or Embedding model\
    \ type and a rate_limit_strategy is provided.\n\nReturns:\n    None: This method\
    \ validates in place on the LanguageModelConfig instance and returns None.\n\n\
    Raises:\n    ValueError: If tokens_per_minute is an integer and less than 1.\n\
    \    ValueError: If tokens_per_minute is 'auto' when using type 'Chat' or 'Embedding'\
    \ and rate_limit_strategy is not None."
  code_example: null
  example_source: null
  line_start: 255
  line_end: 274
  dependencies: []
  called_by: []
- node_id: graphrag/index/update/communities.py::_update_and_merge_communities
  file: graphrag/index/update/communities.py
  name: _update_and_merge_communities
  signature: "def _update_and_merge_communities(\n    old_communities: pd.DataFrame,\n\
    \    delta_communities: pd.DataFrame,\n) -> tuple[pd.DataFrame, dict]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update and merge old and delta communities.\n\nThis function mutates\
    \ the provided DataFrames to ensure required structure, remaps delta\ncommunity\
    \ IDs to avoid collisions with old data, and merges them into a single DataFrame\n\
    aligned to COMMUNITIES_FINAL_COLUMNS. It also returns the mapping from original\
    \ delta\ncommunity IDs to the new IDs assigned during the merge.\n\nArgs:\n  \
    \  old_communities (pd.DataFrame): The existing/old communities. If 'size' or\
    \ 'period' columns\n        are missing, they will be added with missing values.\
    \ Must contain an integer or numeric\n        'community' column.\n    delta_communities\
    \ (pd.DataFrame): The delta/new communities to merge into the old data.\n    \
    \    If 'size' or 'period' columns are missing, they will be added with missing\
    \ values. Must contain\n        a numeric 'community' column and a 'parent' column\
    \ that will be remapped using the computed\n        ID mapping.\n\nReturns:\n\
    \    tuple[pd.DataFrame, dict]:\n        - The updated communities DataFrame,\
    \ aligned to COMMUNITIES_FINAL_COLUMNS, with a new 'title'\n          and 'human_readable_id'\
    \ based on the remapped 'community' IDs.\n        - A dictionary mapping from\
    \ original delta_communities IDs to the new IDs assigned during the merge.\n\n\
    Raises:\n    KeyError: If required columns (for example, 'community' in either\
    \ input DataFrame, or 'parent' in delta_communities)\n        are missing.\n\n\
    Notes:\n    - The function mutates old_communities and delta_communities in place\
    \ by adding missing columns and\n      remapping IDs. Downstream code should be\
    \ aware of input mutations.\n    - The internal mapping also includes a sentinel\
    \ mapping {-1: -1} to preserve a special value."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 86
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_communities.py::_update_communities
- node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_index_client
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: mock_index_client
  signature: def mock_index_client(self)
  decorators:
  - '@pytest.fixture'
  raises: []
  visibility: public
  docstring: "Create a mock Azure AI Search index client.\n\nArgs:\n    self (TestAzureAISearchVectorStore):\
    \ The test class instance.\n\nReturns:\n    MagicMock: The mocked Azure AI Search\
    \ index client instance produced by patching SearchIndexClient.\n\nRaises:\n \
    \   None"
  code_example: null
  example_source: null
  line_start: 33
  line_end: 38
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dicts.py::dict_has_keys_with_types
  file: graphrag/index/utils/dicts.py
  name: dict_has_keys_with_types
  signature: "def dict_has_keys_with_types(\n    data: dict, expected_fields: list[tuple[str,\
    \ type]], inplace: bool = False\n) -> bool"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check that a dictionary contains the specified keys and that their\
    \ values can be cast to the provided types.\n\nArgs:\n    data: The dictionary\
    \ to inspect and (optionally) mutate.\n    expected_fields: A list of (key, type)\
    \ pairs describing the required keys and the types their values must be cast to.\n\
    \    inplace: If True, casted values are written back into the dictionary for\
    \ the corresponding keys.\n\nReturns:\n    bool: True if all specified keys exist\
    \ in the dictionary and their values can be cast to the given types; otherwise\
    \ False.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 7
  line_end: 22
  dependencies: []
  called_by: []
- node_id: tests/unit/litellm_services/utils.py::bin_time_intervals
  file: tests/unit/litellm_services/utils.py
  name: bin_time_intervals
  signature: "def bin_time_intervals(\n    time_values: list[float], time_interval:\
    \ int\n) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Bin time values into consecutive time-based intervals.\n\nArgs:\n  \
    \  time_values: list[float] - Input time values to bin.\n    time_interval: int\
    \ - Size of each time interval.\n\nReturns:\n    list[list[float]] - A list of\
    \ bins, where each inner list contains the values that fall into the corresponding\
    \ time interval. The i-th bin contains values in the interval [i * time_interval,\
    \ (i + 1) * time_interval).\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 7
  line_end: 23
  dependencies: []
  called_by:
  - tests/unit/litellm_services/test_rate_limiter.py::test_binning
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_tpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
  - tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_all_follow_ups
  file: graphrag/query/structured_search/drift_search/state.py
  name: add_all_follow_ups
  signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
    \        follow_ups: list[DriftAction] | list[str],\n        weight: float = 1.0,\n\
    \    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Add all follow-up actions and link them to the given action.\n\nArgs:\n\
    \    action: The parent DriftAction to link follow-up actions to.\n    follow_ups:\
    \ A list of follow-up actions to add. Each item can be a DriftAction or a string\
    \ query.\n    weight: The weight to apply to each relationship when linking to\
    \ the parent.\n\nReturns:\n    None. The follow-up actions are added to the graph\
    \ and connected to the given action.\n\nRaises:\n    None. The method logs warnings\
    \ for invalid input types but does not raise exceptions."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 53
  dependencies:
  - graphrag/query/structured_search/drift_search/action.py::DriftAction
  - graphrag/query/structured_search/drift_search/state.py::add_action
  - graphrag/query/structured_search/drift_search/state.py::relate_actions
  called_by: []
- node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_model
  file: graphrag/config/models/vector_store_config.py
  name: _validate_model
  signature: def _validate_model(self)
  decorators:
  - '@model_validator(mode="after")'
  raises: []
  visibility: protected
  docstring: "Validate the model after the initial schema validation.\n\nArgs:\n \
    \   self: The instance being validated.\n\nReturns:\n    The same instance after\
    \ validation.\n\nRaises:\n    ValueError: If an invalid database URI, URL, or\
    \ embeddings schema is encountered during validation."
  code_example: null
  example_source: null
  line_start: 106
  line_end: 111
  dependencies:
  - graphrag/config/models/vector_store_config.py::_validate_db_uri
  - graphrag/config/models/vector_store_config.py::_validate_embeddings_schema
  - graphrag/config/models/vector_store_config.py::_validate_url
  called_by: []
- node_id: graphrag/utils/cli.py::file_exist
  file: graphrag/utils/cli.py
  name: file_exist
  signature: def file_exist(path)
  decorators: []
  raises: []
  visibility: public
  docstring: "Check that the given path points to an existing file.\n\nArgs:\n   \
    \ path (str): Path to the file to validate. May be a string or Path object.\n\n\
    Returns:\n    str: The input path if the file exists.\n\nRaises:\n    argparse.ArgumentTypeError:\
    \ If the file does not exist.\n\nNotes:\n    This check uses Path.is_file() to\
    \ verify that the path refers to a regular file."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 16
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat_stream
  file: tests/integration/language_model/test_factory.py
  name: chat_stream
  signature: "def chat_stream(\n            self, prompt: str, history: list | None\
    \ = None, **kwargs: Any\n        ) -> Generator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Stream a chat response for the given prompt.\n\nArgs:\n  prompt: str\
    \ \u2014 The text to generate a response for.\n  history: list | None \u2014 The\
    \ conversation history.\n  **kwargs: Any \u2014 Additional keyword arguments (e.g.,\
    \ model parameters).\n\nReturns:\n  Generator[str, None] \u2014 The generator\
    \ that yields strings representing the response."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 49
  dependencies: []
  called_by: []
- node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.agenerate
  file: graphrag/query/question_gen/base.py
  name: agenerate
  signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
    \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
    \    ) -> QuestionResult"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"Generate questions asynchronously.\"\"\"\n\nArgs:\n    question_history:\
    \ History of previously asked questions.\n    context_data: Optional context data\
    \ used to influence generation; None if unavailable.\n    question_count: Number\
    \ of questions to generate.\n    kwargs: Additional keyword arguments for extensibility.\n\
    \nReturns:\n    QuestionResult: The generated result containing the questions\
    \ and related data.\n\""
  code_example: null
  example_source: null
  line_start: 58
  line_end: 65
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.register_chat
  file: graphrag/language_model/manager.py
  name: register_chat
  signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
    \ Any\n    ) -> ChatModel"
  decorators: []
  raises: []
  visibility: public
  docstring: "Register a ChatModel instance under a unique name.\n\nThis method injects\
    \ the provided name into the chat_kwargs before instantiation,\ncreates the ChatModel\
    \ using ModelFactory.create_chat_model, and registers it in\nself.chat_models\
    \ under the given name.\n\nArgs:\n    name (str): Unique identifier for the ChatLLM/ChatModel\
    \ instance.\n    model_type (str): Key for the ChatLLM implementation in LLMFactory.\n\
    \    chat_kwargs (dict[str, Any]): Additional keyword arguments for instantiation.\n\
    \        The dictionary will have the key 'name' added prior to the factory call.\n\
    \nReturns:\n    ChatModel: The ChatModel instance registered under the given name.\n\
    \nRaises:\n    Exception types raised by the underlying factory call (ModelFactory.create_chat_model)\n\
    \    or input/validation errors may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 60
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.search_by_id
  file: graphrag/vector_stores/cosmosdb.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Search for a document by id in the Cosmos DB vector store.\n\nArgs:\n\
    \    id: The identifier of the document to retrieve.\n\nReturns:\n    VectorStoreDocument:\
    \ The document corresponding to the provided id with its id, vector, text, and\
    \ attributes populated from the stored item.\n\nRaises:\n    ValueError: If the\
    \ container client is not initialized."
  code_example: null
  example_source: null
  line_start: 268
  line_end: 280
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: graphrag/utils/api.py::MultiVectorStore.connect
  file: graphrag/utils/api.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> Any'
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
    \ arguments for connecting to the vector storage.\n\nReturns:\n    Any: The result\
    \ of the connection operation.\n\nRaises:\n    NotImplementedError: If the method\
    \ is not implemented."
  code_example: null
  example_source: null
  line_start: 44
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_start
  file: graphrag/callbacks/workflow_callbacks.py
  name: pipeline_start
  signature: 'def pipeline_start(self, names: list[str]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback to signal when the entire pipeline starts.\n\n\
    Args:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 19
  line_end: 21
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.child
  file: graphrag/language_model/providers/fnllm/cache.py
  name: child
  signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a child cache.\n\nArgs:\n    key: The key used to create the\
    \ child cache.\n\nReturns:\n    FNLLMCacheProvider: A new FNLLMCacheProvider wrapping\
    \ the child cache created for the given key."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 44
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig.resolved_strategy
  file: graphrag/config/models/extract_graph_config.py
  name: resolved_strategy
  signature: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
    \    ) -> dict"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the resolved entity extraction strategy.\n\nArgs:\n    root_dir\
    \ (str): The root directory used to resolve the graph and text prompt file paths.\n\
    \    model_config (LanguageModelConfig): The LanguageModelConfig instance containing\
    \ the model configuration; its model_dump() result is included in the strategy\
    \ as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy is provided,\
    \ it is returned as-is; otherwise, a default strategy dictionary is returned with\
    \ the following keys:\n        type: The strategy type (ExtractEntityStrategyType.graph_intelligence).\n\
    \        llm: model_config.model_dump().\n        extraction_prompt: The contents\
    \ of the prompt file located at Path(root_dir) / self.prompt, read as UTF-8, if\
    \ self.prompt is set; otherwise None.\n        max_gleanings: self.max_gleanings."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 55
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries_async
  file: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py
  name: _wrapped_with_retries_async
  signature: "def _wrapped_with_retries_async(\n        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wrap the asynchronous request function with retries using the configured\
    \ retry service.\n\nArgs:\n    kwargs: Keyword arguments passed to the underlying\
    \ asynchronous request function.\n\nReturns:\n    Any: The value returned by the\
    \ underlying asynchronous request function when called with the provided kwargs.\n\
    \nRaises:\n    Exception: Propagated from the underlying asynchronous function\
    \ or the retry service."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 52
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_name
  file: graphrag/query/input/retrieval/entities.py
  name: get_entity_by_name
  signature: 'def get_entity_by_name(entities: Iterable[Entity], entity_name: str)
    -> list[Entity]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get entities by name.\n\nArgs:\n    entities: Iterable[Entity], the\
    \ collection of entities to search.\n    entity_name: str, the name to match against\
    \ the entity.title attribute.\n\nReturns:\n    list[Entity], a list of entities\
    \ whose title matches the given name.\n\nRaises:\n    AttributeError: if any entity\
    \ in entities does not have a 'title' attribute."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 42
  dependencies: []
  called_by:
  - graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_base
  file: graphrag/config/models/language_model_config.py
  name: _validate_api_base
  signature: def _validate_api_base(self) -> None
  decorators: []
  raises:
  - AzureApiBaseMissingError
  visibility: protected
  docstring: "Validate the API base.\n\nRequired when using AOI.\n\nArgs:\n    self:\
    \ The LanguageModelConfig instance.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
    \ If the API base is missing and is required."
  code_example: null
  example_source: null
  line_start: 168
  line_end: 183
  dependencies:
  - graphrag/config/errors.py::AzureApiBaseMissingError
  called_by: []
- node_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig.resolved_strategy
  file: graphrag/config/models/summarize_descriptions_config.py
  name: resolved_strategy
  signature: "def resolved_strategy(\n        self, root_dir: str, model_config: LanguageModelConfig\n\
    \    ) -> dict"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the resolved description summarization strategy.\n\nArgs:\n    root_dir:\
    \ The root directory used to resolve the graph and text prompt file paths.\n \
    \   model_config: The LanguageModelConfig instance containing the model configuration;\
    \ its model_dump() result is included in the strategy as llm.\n\nReturns:\n  \
    \  dict: The resolved strategy. If a custom strategy is provided via self.strategy,\
    \ that is returned; otherwise, a default strategy dictionary is returned with\
    \ the following keys:\n        type: The strategy type (graph_intelligence)\n\
    \        llm: The serialized language model configuration from model_config.model_dump()\n\
    \        summarize_prompt: The contents of the prompt file located at Path(root_dir)\
    \ / self.prompt when self.prompt is set; otherwise None\n        max_summary_length:\
    \ The maximum length for the summary from self.max_length\n        max_input_tokens:\
    \ The maximum input tokens from self.max_input_tokens\n\nRaises:\n    IOError\
    \ or OSError: If reading the summarize_prompt file fails (e.g., prompt file is\
    \ missing or unreadable) when self.prompt is provided."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 56
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/query_variable.py::QueryVariable.__init__
  file: unified-search-app/app/state/query_variable.py
  name: __init__
  signature: 'def __init__(self, key: str, default: Any | None)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a QueryVariable to manage a single URL query parameter and\
    \ its corresponding session_state entry.\n\nThis constructor reads the value for\
    \ the given key from the URL query parameters when available; if the key is not\
    \ present, it uses the provided default. When reading from the query string, the\
    \ value is normalized to lowercase to support case-insensitive URLs. If the resulting\
    \ value equals \"true\" or \"false\" (after normalization), it is converted to\
    \ the corresponding Python boolean True or False. If the key is not already present\
    \ in Streamlit's session_state, the derived value is written to session_state\
    \ under that key. If the key already exists in session_state, its existing value\
    \ is preserved and not overwritten during initialization.\n\nArgs:\n    key (str):\
    \ The key of the query parameter to manage.\n    default (Any | None): The default\
    \ value to use if the key is not present in the query parameters.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 20
  line_end: 29
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
  file: graphrag/index/operations/chunk_text/chunk_text.py
  name: _get_num_total
  signature: 'def _get_num_total(output: pd.DataFrame, column: str) -> int'
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Compute the total number of elements in a DataFrame column, counting
    strings as a single element and non-string entries by their length.


    Args:

    output: pandas.DataFrame The DataFrame containing the target column.

    column: str The name of the column to process.


    Returns:

    int The total number of elements in the specified column; strings contribute 1
    each, non-string entries contribute their length.'
  code_example: null
  example_source: null
  line_start: 133
  line_end: 140
  dependencies: []
  called_by:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_default
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_array
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Embed a single text input.\n\nArgs:\n    text: The text to generate\
    \ an embedding for.\n    **kwargs: Additional keyword arguments passed to the\
    \ embedding model. These are forwarded to the underlying embedding request and\
    \ may influence the resulting embedding.\n\nReturns:\n    list[float]: The embedding\
    \ for the input text as a list of floating-point numbers. If no embedding is produced\
    \ or the response contains no data, returns [].\n\nNotes:\n    This function does\
    \ not raise an exception on its own. If the underlying embedding call fails, the\
    \ exception will propagate to the caller."
  code_example: null
  example_source: null
  line_start: 261
  line_end: 280
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_get_kwargs
  called_by: []
- node_id: tests/unit/litellm_services/test_retries.py::mock_func
  file: tests/unit/litellm_services/test_retries.py
  name: mock_func
  signature: def mock_func()
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Mock function used for testing retries.\n\nReturns:\n    None: This\
    \ function does not return normally because it always raises ValueError.\n\nRaises:\n\
    \    ValueError: Mock error for testing retries"
  code_example: null
  example_source: null
  line_start: 133
  line_end: 137
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.__init__
  file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        max_retries: int =\
    \ 5,\n        **kwargs: Any,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Initialize NativeRetry with retry configuration.\n\nArgs:\n  max_retries:\
    \ The maximum number of retry attempts (int). Must be greater than 0.\n  kwargs:\
    \ Additional keyword arguments (Any).\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
    \ max_retries must be greater than 0."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 28
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_multi_output_base_dirs
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_multi_output_base_dirs
  signature: def _validate_multi_output_base_dirs(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the outputs dict base directories.\n\nArgs:\n    self: The\
    \ instance of the configuration model containing the outputs dictionary and root_dir.\n\
    \nReturns:\n    None. This method updates file outputs' base_dir to an absolute\
    \ path by resolving it relative to root_dir.\n\nRaises:\n    ValueError: If any\
    \ file-type output has an empty base_dir."
  code_example: null
  example_source: null
  line_start: 189
  line_end: 199
  dependencies: []
  called_by: []
- node_id: tests/verbs/util.py::load_test_table
  file: tests/verbs/util.py
  name: load_test_table
  signature: 'def load_test_table(output: str) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Load a test table from parquet data using the provided workflow output\
    \ name.\n\nArgs:\n    output: The workflow output name, typically the workflow\
    \ name, used to locate the parquet file at tests/verbs/data/{output}.parquet.\n\
    \nReturns:\n    pd.DataFrame: The DataFrame read from the specified parquet file."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 55
  dependencies: []
  called_by:
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_communities.py::test_create_communities
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
  - tests/verbs/test_extract_covariates.py::test_extract_covariates
  - tests/verbs/test_finalize_graph.py::_prep_tables
  - tests/verbs/util.py::create_test_context
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_text
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Performs a length-based similarity search over stored documents (embeddings\
    \ are ignored).\n\nArgs:\n    text (str): The query text to compare against documents\
    \ by their character length.\n    text_embedder (TextEmbedder): Provided for API\
    \ compatibility but not used by this implementation; embeddings are not computed.\n\
    \    k (int): The number of top results to return. Defaults to 10.\n    **kwargs\
    \ (Any): Additional keyword arguments passed to the underlying search implementation\
    \ (not used).\n\nReturns:\n    list[VectorStoreSearchResult]: A list of VectorStoreSearchResult\
    \ objects sorted by increasing score; the score is the absolute difference between\
    \ the length of the input text and the document's text length. If document.text\
    \ is None, it is treated as an empty string. Only the top k results are returned.\n\
    \nRaises:\n    Not specified. This method does not document any specific exceptions."
  code_example: null
  example_source: null
  line_start: 44
  line_end: 55
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreSearchResult
  called_by: []
- node_id: graphrag/index/operations/extract_graph/extract_graph.py::_merge_relationships
  file: graphrag/index/operations/extract_graph/extract_graph.py
  name: _merge_relationships
  signature: def _merge_relationships(relationship_dfs) -> pd.DataFrame
  decorators: []
  raises: []
  visibility: protected
  docstring: "Merge multiple relationship DataFrames into a single aggregated DataFrame\
    \ by source and target.\n\nArgs:\n    relationship_dfs: Iterable of pandas.DataFrame.\
    \ Each DataFrame should contain at least the columns\n        'source', 'target',\
    \ 'description', 'source_id', and 'weight'.\n\nReturns:\n    pd.DataFrame: A DataFrame\
    \ grouped by 'source' and 'target' with the following aggregated columns:\n  \
    \      - description: list of descriptions for each (source, target) pair\n  \
    \      - text_unit_ids: list of source_id values within the group\n        - weight:\
    \ sum of weights within the group\n    The result has 'source' and 'target' as\
    \ columns with the index reset.\n\nRaises:\n    TypeError: If relationship_dfs\
    \ is not a suitable iterable of DataFrames.\n    KeyError: If required columns\
    \ are missing from any input DataFrame."
  code_example: null
  example_source: null
  line_start: 113
  line_end: 123
  dependencies: []
  called_by:
  - graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_vector
  file: tests/integration/vector_stores/test_factory.py
  name: similarity_search_by_vector
  signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a similarity search by vector and return the top-k results.\n\
    \nArgs:\n  self: The instance of the class.\n  query_embedding: list[float] The\
    \ embedding vector to search with.\n  k: int The number of top results to return.\n\
    \  **kwargs: Any Additional keyword arguments that may influence the search.\n\
    \nReturns:\n  list: The top-k search results. This placeholder implementation\
    \ returns an empty list."
  code_example: null
  example_source: null
  line_start: 137
  line_end: 138
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_start
  file: graphrag/callbacks/query_callbacks.py
  name: on_map_response_start
  signature: 'def on_map_response_start(self, map_response_contexts: list[str]) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the start of map response operation.\n\nArgs:\n    map_response_contexts:\
    \ A list of strings representing contexts for the map response operation to begin\
    \ processing.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 18
  line_end: 19
  dependencies: []
  called_by: []
- node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.__init__
  file: graphrag/logger/blob_workflow_logger.py
  name: __init__
  signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
    \        container_name: str | None,\n        blob_name: str = \"\",\n       \
    \ base_dir: str | None = None,\n        storage_account_blob_url: str | None =\
    \ None,\n        level: int = logging.NOTSET,\n    )"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Create a new instance of the BlobWorkflowLogger class.\n\nArgs:\n  connection_string:\
    \ Connection string for the blob storage, or None\n  container_name: Name of the\
    \ blob container\n  blob_name: Name of the blob to create; if empty, a timestamped\
    \ default will be used\n  base_dir: Base directory to prepend to the blob name,\
    \ or None\n  storage_account_blob_url: URL of the storage account blob service,\
    \ or None\n  level: Logging level\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
    \ No container name provided for blob storage.\n  ValueError: No storage account\
    \ blob url provided for blob storage.\n  ValueError: Either connection_string\
    \ or storage_account_blob_url must be provided."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 70
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_umap_configs
  file: tests/unit/config/utils.py
  name: assert_umap_configs
  signature: 'def assert_umap_configs(actual: UmapConfig, expected: UmapConfig) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that the enabled attribute of two UmapConfig objects matches.\
    \ Only the enabled attribute is checked; other fields are not compared.\n\nNote:\
    \ If full equivalence is intended, align the implementation or docstring accordingly.\n\
    \nParameters:\n    actual (UmapConfig): The actual UmapConfig to validate.\n \
    \   expected (UmapConfig): The expected UmapConfig to compare against.\n\nReturns:\n\
    \    None\n\nRaises:\n    AssertionError: if actual.enabled != expected.enabled"
  code_example: null
  example_source: null
  line_start: 311
  line_end: 312
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/query/context_builder/community_context.py::_get_header
  file: graphrag/query/context_builder/community_context.py
  name: _get_header
  signature: 'def _get_header(attributes: list[str]) -> list[str]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Builds the header row for the community context data table based on\
    \ the given attributes and surrounding configuration.\n\nArgs:\n    attributes:\
    \ List[str] - Attributes to include in the header (after removing the default\
    \ id and title duplicates).\n\nReturns:\n    List[str] - The constructed header\
    \ list, starting with \"id\" and \"title\", followed by filtered attributes, then\
    \ either \"summary\" or \"content\" depending on use_community_summary, and optionally\
    \ the rank name if include_community_rank is True. The exact behavior may depend\
    \ on surrounding configuration variables such as include_community_weight, community_weight_name,\
    \ and community_rank_name."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 63
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_container
  file: graphrag/vector_stores/cosmosdb.py
  name: _delete_container
  signature: def _delete_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Delete the vector store container from the database if it exists.\n\
    \nArgs:\n    self: The instance of the class containing the vector store and database\
    \ client.\n\nReturns:\n    None\n\nRaises:\n    CosmosHttpResponseError: If the\
    \ underlying Cosmos DB client call to delete the container fails.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 141
  line_end: 144
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::_container_exists
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.has
  file: graphrag/storage/blob_pipeline_storage.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously check if a key exists in the cache.\n\nArgs:\n    key\
    \ (str): The cache key to check.\n\nReturns:\n    bool: True if a value exists\
    \ for the key in the cache, otherwise False.\n\nRaises:\n    Exception: If an\
    \ error occurs while performing the underlying blob storage operations."
  code_example: null
  example_source: null
  line_start: 252
  line_end: 259
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_keyname
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.retry
  file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  name: retry
  signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry a synchronous function until it succeeds or the maximum number\
    \ of retries is reached.\n\nArgs:\n    func: Callable[..., Any] - The function\
    \ to invoke. It will be called as func(**kwargs) and its result will be returned\
    \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
    \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
    \    Exception - The last exception raised by func if the maximum number of retries\
    \ is exceeded."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 56
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::to_optional_dict
  file: graphrag/query/input/loaders/utils.py
  name: to_optional_dict
  signature: "def to_optional_dict(\n    data: Mapping[str, Any],\n    column_name:\
    \ str | None,\n    key_type: type | None = None,\n    value_type: type | None\
    \ = None,\n) -> dict | None"
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "\"\"\"Convert and validate a value to an optional dict.\n\nArgs:\n \
    \   data: Mapping[str, Any]\n        Input data container.\n    column_name: str\
    \ | None\n        The key in data to retrieve. If None or not present, returns\
    \ None.\n    key_type: type | None\n        If provided, require all dict keys\
    \ to be instances of this type.\n    value_type: type | None\n        If provided,\
    \ require all dict values to be instances of this type.\n\nReturns:\n    dict\
    \ | None\n        The extracted dict if present and not None; otherwise None.\n\
    \nRaises:\n    TypeError\n        If the value associated with column_name is\
    \ not a dict, or if any dict key does not\n        match key_type, or if any dict\
    \ value does not match value_type.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 162
  line_end: 187
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_communities
  - graphrag/query/input/loaders/dfs.py::read_text_units
- node_id: unified-search-app/app/state/session_variable.py::SessionVariable.key
  file: unified-search-app/app/state/session_variable.py
  name: key
  signature: def key(self) -> str
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Key property that returns the session_state key for this variable.\n\
    \nReturns:\n    str: The key used to access the session_state dictionary for this\
    \ variable."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 39
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/app_logic.py::run_drift_search
  file: unified-search-app/app/app_logic.py
  name: run_drift_search
  signature: "def run_drift_search(\n    query: str,\n    sv: SessionVariables,\n\
    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Run drift search.\n\nExecute a drift-based search using the Drift\
    \ search engine, calling the Drift API with the\nconfiguration and session data\
    \ provided, and return a SearchResult containing the response and\nassociated\
    \ context data for display.\n\nArgs:\n    query: The search query string to send\
    \ to the Drift search API.\n    sv: SessionVariables containing configuration\
    \ and data used to perform the drift search\n        (graphrag_config, entities,\
    \ communities, community_reports, text_units, relationships,\n        and dataset_config\
    \ with community_level).\n\nReturns:\n    SearchResult: The drift search result\
    \ containing the response and its context data.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 254
  line_end: 304
  dependencies:
  - graphrag.api::drift_search
  called_by:
  - unified-search-app/app/app_logic.py::run_all_searches
- node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed_batch
  file: tests/integration/language_model/test_factory.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute embeddings for a batch of input texts.\n\nArgs:\n    text_list:\
    \ list[str] - A batch of text inputs to generate embeddings for.\n    **kwargs:\
    \ Any - Additional keyword arguments (e.g., model parameters).\n\nReturns:\n \
    \   list[list[float]] - A batch of embeddings corresponding to the input texts."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 82
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_context
  file: graphrag/callbacks/query_callbacks.py
  name: on_context
  signature: 'def on_context(self, context: Any) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle when context data is constructed.\n\nArgs:\n    context (Any):\
    \ The context data provided to the callback. This implementation performs no operations\
    \ on it.\n\nReturns:\n    None: The function returns no value.\n\nRaises:\n  \
    \  None: This function does not raise exceptions by itself."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 16
  dependencies: []
  called_by: []
- node_id: graphrag/config/enums.py::StorageType.__repr__
  file: graphrag/config/enums.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
    \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's value\
    \ wrapped in double quotes.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 57
  line_end: 59
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/vector_store_schema_config.py::is_valid_field_name
  file: graphrag/config/models/vector_store_schema_config.py
  name: is_valid_field_name
  signature: 'def is_valid_field_name(field: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Check if a field name is valid for CosmosDB.\n\nArgs:\n    field: The\
    \ field name to validate.\n\nReturns:\n    bool: True if the field name is valid\
    \ for CosmosDB, False otherwise."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 17
  dependencies: []
  called_by:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_type
  file: graphrag/config/models/language_model_config.py
  name: _validate_type
  signature: def _validate_type(self) -> None
  decorators: []
  raises:
  - KeyError
  visibility: protected
  docstring: "Validate the model type.\n\nArgs:\n    self: The instance being validated.\n\
    \nReturns:\n    None\n        The function does not return a value.\n\nRaises:\n\
    \    KeyError: If the model name is not recognized."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 108
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/relationships.py::get_entities_from_relationships
  file: graphrag/query/input/retrieval/relationships.py
  name: get_entities_from_relationships
  signature: "def get_entities_from_relationships(\n    relationships: list[Relationship],\
    \ entities: list[Entity]\n) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get all entities that are associated with the selected relationships.\n\
    \nArgs:\n    relationships (list[Relationship]): The relationships to inspect.\n\
    \    entities (list[Entity]): The pool of entities to filter from.\n\nReturns:\n\
    \    list[Entity]: The subset of entities whose title matches the source or target\
    \ of any relationship in relationships."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 78
  dependencies: []
  called_by:
  - graphrag/query/context_builder/local_context.py::get_candidate_context
- node_id: tests/integration/vector_stores/test_factory.py::test_create_lancedb_vector_store
  file: tests/integration/vector_stores/test_factory.py
  name: test_create_lancedb_vector_store
  signature: def test_create_lancedb_vector_store()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test creating a LanceDB vector store via the VectorStoreFactory.\n\n\
    Args:\n    None: This test takes no input parameters.\n\nReturns:\n    None: The\
    \ test does not return a value.\n\nRaises:\n    AssertionError: If the created\
    \ vector_store is not an instance of LanceDBVectorStore, or if the vector_store's\
    \ index_name does not match the expected value."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 31
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.register
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: register
  signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Register a new WorkflowCallbacks instance.\n\nAppends the provided WorkflowCallbacks\
    \ instance to the internal registry (self._callbacks). This allows multiple callbacks\
    \ to be registered and receive lifecycle event callbacks. There is no deduplication;\
    \ registering the same instance more than once will result in duplicates in the\
    \ registry.\n\nArgs:\n    callbacks (WorkflowCallbacks): The WorkflowCallbacks\
    \ instance to register.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 20
  line_end: 22
  dependencies: []
  called_by: []
- node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.__init__
  file: graphrag/tokenizer/litellm_tokenizer.py
  name: __init__
  signature: 'def __init__(self, model_name: str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Initialize the LiteLLM Tokenizer.\n\nArgs:\n    model_name (str):\
    \ The name of the LiteLLM model to use for tokenization.\n\nReturns:\n    None:\
    \ This initializer does not return a value.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 14
  line_end: 21
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_auth_type
  file: graphrag/config/models/language_model_config.py
  name: _validate_auth_type
  signature: def _validate_auth_type(self) -> None
  decorators: []
  raises:
  - ConflictingSettingsError
  visibility: protected
  docstring: "Validate the authentication type.\n\nauth_type must be api_key when\
    \ using OpenAI and\ncan be either api_key or azure_managed_identity when using\
    \ AOI.\n\nArgs:\n    self: The instance being validated.\n\nReturns:\n    None\n\
    \        The function does not return a value.\n\nRaises:\n    ConflictingSettingsError:\
    \ If the Azure authentication type conflicts with the model being used."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 85
  dependencies:
  - graphrag/config/errors.py::ConflictingSettingsError
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::_create_text_batches
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: _create_text_batches
  signature: "def _create_text_batches(\n    texts: list[str],\n    max_batch_size:\
    \ int,\n    max_batch_tokens: int,\n    splitter: TokenTextSplitter,\n) -> list[list[str]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create batches of texts to embed.\n\nThis function groups input texts\
    \ into batches that respect the given batch constraints. A batch is closed when\
    \ adding the next text would exceed the maximum number of texts or the maximum\
    \ token count for the batch.\n\nArgs:\n    texts: List of input texts to be batched.\n\
    \    max_batch_size: Maximum number of texts per batch.\n    max_batch_tokens:\
    \ Maximum total tokens per batch, as measured by the splitter.\n    splitter:\
    \ TokenTextSplitter used to count tokens per text.\n\nReturns:\n    A list of\
    \ batches, where each batch is a list of strings."
  code_example: null
  example_source: null
  line_start: 107
  line_end: 136
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: unified-search-app/app/knowledge_loader/model.py::load_communities
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_communities
  signature: "def load_communities(\n    _datasource: Datasource,\n) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a dataframe with communities data from the indexed-data.\n\n\
    Args:\n    _datasource: Datasource to read the communities data from the indexed-data.\n\
    \nReturns:\n    A dataframe with communities data loaded from the indexed-data.\n\
    \nRaises:\n    Exception: If the underlying data source read operation fails."
  code_example: null
  example_source: null
  line_start: 62
  line_end: 66
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.decompose_query
  file: graphrag/query/structured_search/drift_search/primer.py
  name: decompose_query
  signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
    \    ) -> tuple[dict, dict[str, int]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Decompose the query into subqueries using global guidance from the provided\
    \ community reports.\n\nArgs:\n    query (str): The original search query.\n \
    \   reports (pd.DataFrame): DataFrame containing community reports. Must include\
    \ a 'full_content' column used to build the concatenated context for the primer\
    \ prompt.\n\nReturns:\n    tuple[dict, dict[str, int]]: A tuple containing:\n\
    \        parsed_response (dict): The parsed JSON response produced by the language\
    \ model.\n        token_ct (dict[str, int]): Token usage counters with keys:\n\
    \            llm_calls (int): number of language model calls performed\n     \
    \       prompt_tokens (int): number of tokens in the constructed prompt\n    \
    \        output_tokens (int): number of tokens in the model output\n\nRaises:\n\
    \    KeyError: if the reports DataFrame does not contain the required 'full_content'\
    \ column.\n    json.JSONDecodeError: if the model response content is not valid\
    \ JSON.\n    Exception: any exceptions raised by the chat model interaction (propagated\
    \ from the API call).\n\nNotes:\n    The function builds community_reports by\
    \ concatenating the 'full_content' field of all reports with double newlines,\
    \ and passes that into DRIFT_PRIMER_PROMPT."
  code_example: null
  example_source: null
  line_start: 122
  line_end: 151
  dependencies: []
  called_by: []
- node_id: graphrag/query/input/retrieval/community_reports.py::to_community_report_dataframe
  file: graphrag/query/input/retrieval/community_reports.py
  name: to_community_report_dataframe
  signature: "def to_community_report_dataframe(\n    reports: list[CommunityReport],\n\
    \    include_community_rank: bool = False,\n    use_community_summary: bool =\
    \ False,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert a list of CommunityReport objects to a pandas DataFrame.\n\n\
    Args:\n  reports: list[CommunityReport] - List of CommunityReport objects to convert\
    \ to a DataFrame.\n  include_community_rank: bool - Whether to include a rank\
    \ column in the output.\n  use_community_summary: bool - Whether to include a\
    \ summary column (summary) instead of content.\n\nReturns:\n  pd.DataFrame - A\
    \ DataFrame representing the provided community reports. If the input list is\
    \ empty, an empty DataFrame is returned.\n\nRaises:\n  None - This function does\
    \ not raise exceptions."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 75
  dependencies: []
  called_by:
  - graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
- node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.__init__
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: __init__
  signature: 'def __init__(self, base_path: str)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize LocalDatasource with the provided base path.\n\nArgs:\n \
    \   base_path: The base directory path for local data sources. Type: str.\n\n\
    Returns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 38
  line_end: 40
  dependencies: []
  called_by: []
- node_id: graphrag/index/typing/pipeline.py::Pipeline.run
  file: graphrag/index/typing/pipeline.py
  name: run
  signature: def run(self) -> Generator[Workflow]
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Yield a generator of (name, workflow) pairs from the pipeline.\n\
    \nThe items yielded come from self.workflows and are tuples of (name, Workflow),\n\
    i.e., each yield is a pair containing the workflow's name (str) and the\ncorresponding\
    \ Workflow object.\n\nArgs:\n    self: The Pipeline instance.\n\nReturns:\n  \
    \  Generator[tuple[str, Workflow]]: A generator that yields (name, workflow) pairs\
    \ in the pipeline.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 17
  line_end: 19
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_language_model_config
  file: graphrag/config/models/graph_rag_config.py
  name: get_language_model_config
  signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Get a model configuration by ID.\n\nArgs:\n    model_id (str):\
    \ The ID of the model to get. Should match an ID in the models list.\n\nReturns:\n\
    \    LanguageModelConfig: The model configuration if found.\n\nRaises:\n    ValueError:\
    \ If the model ID is not found in the configuration.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 355
  line_end: 377
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/dataframes.py::transform_series
  file: graphrag/index/utils/dataframes.py
  name: transform_series
  signature: 'def transform_series(series: pd.Series, fn: Callable[[Any], Any]) ->
    pd.Series'
  decorators: []
  raises: []
  visibility: public
  docstring: "Apply a transformation function to a Pandas Series.\n\nArgs:\n    series:\
    \ The input Pandas Series to transform.\n    fn: A callable that takes a single\
    \ value and returns a transformed value.\n\nReturns:\n    pd.Series: A new Series\
    \ with the transformed values.\n\nRaises:\n    Exception: Any exception raised\
    \ by fn will be propagated to the caller."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 36
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor._tag_noun_phrases
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: _tag_noun_phrases
  signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
    \ list[str] | None = None\n    ) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Extract attributes of a noun phrase for filtering.\n\nArgs:\n    noun_phrase:\
    \ The noun phrase to analyze.\n    all_proper_nouns: Optional list of proper nouns\
    \ (uppercase) to consider when detecting proper nouns within the phrase.\n\nReturns:\n\
    \    dict[str, Any]: A dictionary containing the following keys:\n        cleaned_tokens:\
    \ List[str] of tokens after removing excluded nouns.\n        cleaned_text: str\
    \ of cleaned tokens joined by the configured delimiter, with newline removed and\
    \ uppercased.\n        has_proper_nouns: True if any cleaned token matches an\
    \ entry in all_proper_nouns.\n        has_compound_words: True if any cleaned\
    \ token is a hyphenated compound word.\n        has_valid_tokens: True if all\
    \ cleaned tokens are valid per regex and length constraints."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 119
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat_stream
  file: tests/integration/language_model/test_factory.py
  name: achat_stream
  signature: "def achat_stream(\n            self, prompt: str, history: list | None\
    \ = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Stream Chat with the Model using the given prompt.\n\nArgs:\n\
    \  prompt: The prompt to chat with.\n  history: The conversation history.\n  kwargs:\
    \ Additional arguments to pass to the Model.\n\nReturns:\n  An asynchronous generator\
    \ that yields non-None strings representing the response.\n\nRaises:\n  Propagates\
    \ exceptions raised by the underlying model call or streaming response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 42
  line_end: 45
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__repr__
  file: graphrag/config/models/graph_rag_config.py
  name: __repr__
  signature: def __repr__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of this GraphRagConfig instance.\n\nArgs:\n\
    \  self: GraphRagConfig, the GraphRagConfig instance to represent as a string.\n\
    \nReturns:\n  str: The string representation of this GraphRagConfig instance."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 53
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_cosmosdb_storage.py::test_find
  file: tests/integration/storage/test_cosmosdb_storage.py
  name: test_find
  signature: def test_find()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the integration behavior of CosmosDBPipelineStorage.find() in a\
    \ test container. This test creates a storage instance, verifies an empty result\
    \ set, adds JSON files, verifies the listing order, reads content, checks existence,\
    \ deletes a file, and clears storage at the end.\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 24
  line_end: 67
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  called_by: []
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model
  file: graphrag/config/models/language_model_config.py
  name: _validate_model
  signature: def _validate_model(self)
  decorators:
  - '@model_validator(mode="after")'
  raises: []
  visibility: protected
  docstring: "Validate the LanguageModelConfig after the initial Pydantic schema validation\
    \ as a model_validator(mode='after') post-hook; this reflects the actual validation\
    \ sequence (after initial schema validation) and may raise additional errors.\n\
    \nArgs:\n    self (LanguageModelConfig): The LanguageModelConfig instance being\
    \ validated.\n\nReturns:\n    LanguageModelConfig: The same instance after validation.\n\
    \nRaises:\n    ApiKeyMissingError: If the API key is missing when required.\n\
    \    AzureApiBaseMissingError: If the Azure API base is missing when required.\n\
    \    AzureApiVersionMissingError: If the Azure API version is missing when required.\n\
    \    AzureDeploymentNameMissingError: If the Azure deployment name is missing\
    \ when required.\n    ConflictingSettingsError: If there are conflicting settings\
    \ detected during validation."
  code_example: null
  example_source: null
  line_start: 393
  line_end: 403
  dependencies:
  - graphrag/config/models/language_model_config.py::_validate_api_key
  - graphrag/config/models/language_model_config.py::_validate_auth_type
  - graphrag/config/models/language_model_config.py::_validate_azure_settings
  - graphrag/config/models/language_model_config.py::_validate_encoding_model
  - graphrag/config/models/language_model_config.py::_validate_max_retries
  - graphrag/config/models/language_model_config.py::_validate_model_provider
  - graphrag/config/models/language_model_config.py::_validate_requests_per_minute
  - graphrag/config/models/language_model_config.py::_validate_tokens_per_minute
  - graphrag/config/models/language_model_config.py::_validate_type
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: achat
  signature: "def achat(\n        self, prompt: str, history: list | None = None,\
    \ **kwargs: Any\n    ) -> \"MR\""
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously generate a response for the given prompt and history.\n\
    \nArgs:\n    prompt: The prompt to generate a response for.\n    history: Optional\
    \ chat history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
    \nReturns:\n    MR: The generated model response."
  code_example: null
  example_source: null
  line_start: 266
  line_end: 314
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_get_kwargs
  called_by: []
- node_id: graphrag/index/utils/hashing.py::gen_sha512_hash
  file: graphrag/index/utils/hashing.py
  name: gen_sha512_hash
  signature: 'def gen_sha512_hash(item: dict[str, Any], hashcode: Iterable[str])'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a SHA512 hash from the concatenation of string representations\
    \ of selected fields of an item.\n\nArgs:\n  item: input dictionary containing\
    \ values to hash.\n  hashcode: keys whose corresponding values are used for the\
    \ hash, in order.\n\nReturns:\n  str: Hexadecimal SHA512 digest string.\n\nRaises:\n\
    \  KeyError: if a key from hashcode is not present in item."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 14
  dependencies: []
  called_by:
  - graphrag/index/input/text.py::load_file
  - graphrag/index/input/util.py::process_data_columns
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::extract
  - graphrag/index/workflows/create_base_text_units.py::create_base_text_units
- node_id: unified-search-app/app/ui/sidebar.py::update_local_search
  file: unified-search-app/app/ui/sidebar.py
  name: update_local_search
  signature: 'def update_local_search(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Update local rag state.\n\nArgs:\n    sv: SessionVariables\n       \
    \ The container of session variables; used to read and update the include_local_search\
    \ flag from the Streamlit session state.\n\nReturns:\n    None\n        The function\
    \ does not return a value.\n\nRaises:\n    KeyError\n        If the expected key\
    \ sv.include_local_search.key is not found in st.session_state...."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 40
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.deserialize
  file: graphrag/query/structured_search/drift_search/state.py
  name: deserialize
  signature: 'def deserialize(self, data: dict[str, Any])'
  decorators: []
  raises: []
  visibility: public
  docstring: "Deserialize the dictionary back to a graph.\n\nArgs:\n    data: Serialized\
    \ representation of the graph to deserialize. Contains \"nodes\" and \"edges\"\
    .\n\nReturns:\n    None: This method updates the graph in place and does not return\
    \ a value.\n\nRaises:\n    KeyError: If any node entry in data's \"nodes\" list\
    \ is missing the required \"id\" field."
  code_example: null
  example_source: null
  line_start: 119
  line_end: 137
  dependencies:
  - graphrag/query/structured_search/drift_search/state.py::add_action
  - graphrag/query/structured_search/drift_search/state.py::relate_actions
  called_by: []
- node_id: graphrag/prompt_tune/generator/language.py::detect_language
  file: graphrag/prompt_tune/generator/language.py
  name: detect_language
  signature: 'def detect_language(model: ChatModel, docs: str | list[str]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Detect input language to use for GraphRAG prompts.\n\nParameters\n \
    \   model (ChatModel): The language model to use for language detection\n    docs\
    \ (str | list[str]): The docs to detect language from\n\nReturns\n    str: The\
    \ detected language.\n\nRaises\n    Exception: If the underlying model API raises\
    \ an error during language detection."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 27
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.__init__
  file: graphrag/query/question_gen/local_gen.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n        system_prompt:\
    \ str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback] | None\
    \ = None,\n        model_params: dict[str, Any] | None = None,\n        context_builder_params:\
    \ dict[str, Any] | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a LocalQuestionGen instance.\n\nArgs:\n    model: ChatModel\
    \ - The language model interface to use.\n    context_builder: LocalContextBuilder\
    \ - The builder that constructs the context for local question generation.\n \
    \   tokenizer: Tokenizer | None - Optional tokenizer to use.\n    system_prompt:\
    \ str - System prompt for question generation. Defaults to QUESTION_SYSTEM_PROMPT.\n\
    \    callbacks: list[BaseLLMCallback] | None - Optional callbacks for LLM events.\
    \ If None, an empty list is used.\n    model_params: dict[str, Any] | None - Optional\
    \ parameters to pass to the model.\n    context_builder_params: dict[str, Any]\
    \ | None - Optional parameters to pass to the context builder.\n\nReturns:\n \
    \   None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor._get_text_output
  file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  name: _get_text_output
  signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Get the text output for a CommunityReportResponse.\n\nArgs:\n\
    \    report: CommunityReportResponse\n        The report object containing a title,\
    \ a summary, and a list of findings. Each finding provides a summary and an explanation.\n\
    \nReturns:\n    str\n        A markdown-formatted string. It starts with a top-level\
    \ header using the report title, includes the report summary, and appends a section\
    \ for each finding using its summary as a subheader and its explanation as content.\n\
    \nRaises:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 98
  line_end: 102
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/entity_extraction.py::find_nearest_neighbors_by_entity_rank
  file: graphrag/query/context_builder/entity_extraction.py
  name: find_nearest_neighbors_by_entity_rank
  signature: "def find_nearest_neighbors_by_entity_rank(\n    entity_name: str,\n\
    \    all_entities: list[Entity],\n    all_relationships: list[Relationship],\n\
    \    exclude_entity_names: list[str] | None = None,\n    k: int | None = 10,\n\
    ) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Retrieve entities that have direct connections with the target entity,\
    \ sorted by their rank.\n\nArgs:\n    entity_name: The name of the target entity.\n\
    \    all_entities: The list of all Entity objects to search.\n    all_relationships:\
    \ The list of Relationship objects to consider for connections.\n    exclude_entity_names:\
    \ Optional list of entity titles to exclude from results.\n    k: Optional number\
    \ of neighbors to return; if provided, at most k items are returned; if None,\
    \ all related entities are returned.\n\nReturns:\n    list[Entity]: A list of\
    \ Entity objects representing neighboring entities connected to the target entity,\
    \ sorted by rank in descending order. If k is provided, at most k entities are\
    \ returned; otherwise all related entities are returned."
  code_example: null
  example_source: null
  line_start: 95
  line_end: 121
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/stable_lcc.py::_get_edge_key
  file: graphrag/index/utils/stable_lcc.py
  name: _get_edge_key
  signature: 'def _get_edge_key(source: Any, target: Any) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return a string key for the edge in the format 'source -> target'.\n\
    \nArgs:\n    source (Any): The source node of the edge.\n    target (Any): The\
    \ target node of the edge.\n\nReturns:\n    str: The edge key as a string in the\
    \ format 'source -> target'."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 56
  dependencies: []
  called_by:
  - graphrag/index/utils/stable_lcc.py::_stabilize_graph
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.get
  file: graphrag/storage/memory_pipeline_storage.py
  name: get
  signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
    \ str | None = None\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the value for the given key from in-memory storage.\n\nArgs:\n\
    \    key (str): The key to retrieve the value for.\n    as_bytes (bool | None):\
    \ Unused. This parameter is accepted for API compatibility but is ignored by this\
    \ backend.\n    encoding (str | None): Unused. This parameter is accepted for\
    \ API compatibility but is ignored by this backend.\n\nReturns:\n    Any: The\
    \ value associated with the key if present; None if the key is not in storage.\"\
    \"\""
  code_example: null
  example_source: null
  line_start: 24
  line_end: 37
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.set
  file: tests/integration/storage/test_factory.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Set the value for the given key.\n\nAsynchronously set the value for\
    \ the given key.\n\nArgs:\n    key (str): The key to set the value for.\n    value\
    \ (Any): The value to set.\n    encoding (str | None): Optional encoding to apply\
    \ when serializing the value.\n\nReturns:\n    None: This coroutine completes\
    \ when the value has been set."
  code_example: null
  example_source: null
  line_start: 130
  line_end: 131
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.clear
  file: graphrag/vector_stores/cosmosdb.py
  name: clear
  signature: def clear(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Clear the vector store by deleting its container and database.\n\nDeletes\
    \ the underlying container and database used to store vectors by delegating to\
    \ _delete_container() and _delete_database().\n\nRaises:\n    CosmosHttpResponseError:\
    \ If the underlying Cosmos DB client call to delete the container or database\
    \ fails."
  code_example: null
  example_source: null
  line_start: 282
  line_end: 285
  dependencies:
  - graphrag/vector_stores/cosmosdb.py::_delete_container
  - graphrag/vector_stores/cosmosdb.py::_delete_database
  called_by: []
- node_id: unified-search-app/app/home_page.py::on_change
  file: unified-search-app/app/home_page.py
  name: on_change
  signature: 'def on_change(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Updates the current question in the session variables from the Streamlit\
    \ session state.\n\nParameters:\n    sv (SessionVariables): The session variables\
    \ container; updates sv.question.value from the input.\n\nReturns:\n    None:\
    \ This function does not return a value.\n\nRaises:\n    KeyError: If the key\
    \ 'question_input' is not present in st.session_state."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 39
  dependencies: []
  called_by: []
- node_id: graphrag/index/typing/pipeline.py::Pipeline.names
  file: graphrag/index/typing/pipeline.py
  name: names
  signature: def names(self) -> list[str]
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the names of the workflows in the pipeline.\n\nArgs:\n    self:\
    \ The Pipeline instance.\n\nReturns:\n    list[str]: The names of the workflows\
    \ in the pipeline, extracted from the first element\n        of each workflow\
    \ in self.workflows."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 23
  dependencies: []
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.load_documents
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Load documents into the vector store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
    \ - List of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
    \ bool - If True, overwrite existing data in the vector store; otherwise, preserve\
    \ existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: load_documents\
    \ method not implemented."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/builders.py::LocalContextBuilder.build_context
  file: graphrag/query/context_builder/builders.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"Build the context for the local search mode.\n\nArgs:\n  query\
    \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
    \ | None): Optional conversation history to consider while constructing the context.\n\
    \  **kwargs: Additional keyword arguments that may influence how the context is\
    \ built.\n\nReturns:\n  ContextBuilderResult: The result containing the built\
    \ context for the local search mode.\n\nRaises:\n  NotImplementedError: If invoked\
    \ on the abstract base class.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 44
  line_end: 50
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/strategies.py::run_sentences
  file: graphrag/index/operations/chunk_text/strategies.py
  name: run_sentences
  signature: "def run_sentences(\n    input: list[str], _config: ChunkingConfig, tick:\
    \ ProgressTicker\n) -> Iterable[TextChunk]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Chunks text into multiple parts by sentence.\n\nArgs:\n  input: list[str]\
    \ - list of input documents to chunk into sentences.\n  _config: ChunkingConfig\
    \ - chunking configuration (unused by this strategy).\n  tick: ProgressTicker\
    \ - progress reporter; invoked to indicate progress after processing each input\
    \ document.\n\nReturns:\n  Iterable[TextChunk] - yields TextChunk objects for\
    \ each sentence, with text_chunk set to the sentence and source_doc_indices containing\
    \ the index of the source document.\n\nRaises:\n  Exceptions raised by nltk.sent_tokenize\
    \ during sentence tokenization."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 69
  dependencies:
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by:
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
- node_id: tests/unit/config/utils.py::assert_snapshots_configs
  file: tests/unit/config/utils.py
  name: assert_snapshots_configs
  signature: "def assert_snapshots_configs(\n    actual: SnapshotsConfig, expected:\
    \ SnapshotsConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two SnapshotsConfig objects have equal embeddings and graphml\
    \ configurations.\n\nArgs:\n    actual: The actual SnapshotsConfig instance.\n\
    \    expected: The expected SnapshotsConfig instance.\n\nReturns:\n    None\n\n\
    Raises:\n    AssertionError: If actual.embeddings != expected.embeddings or actual.graphml\
    \ != expected.graphml."
  code_example: null
  example_source: null
  line_start: 220
  line_end: 224
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.retry
  file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  name: retry
  signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Retry a synchronous function.\n\nRetries the provided function until\
    \ it succeeds or the maximum number of retries is reached, applying an incremental\
    \ delay between retries.\n\nArgs:\n    func: Callable[..., Any] - The function\
    \ to invoke. It will be called as func(**kwargs) and its result will be returned\
    \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
    \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
    \    Exception - The last exception raised by func when the maximum number of\
    \ retries is exceeded."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 57
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.is_supported_type
  file: graphrag/vector_stores/factory.py
  name: is_supported_type
  signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Check if the given vector store type is supported.\n\nArgs:\n    cls:\
    \ type The class reference (classmethod parameter).\n    vector_store_type: str\
    \ The type identifier for the vector store.\n\nReturns:\n    bool: True if vector_store_type\
    \ is registered in the registry, False otherwise."
  code_example: null
  example_source: null
  line_start: 86
  line_end: 88
  dependencies: []
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_vector
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the top-k documents using a deterministic mock search that ignores\
    \ the query embedding. This implementation does not perform real vector similarity;\
    \ instead it returns the first k documents with a fixed score of 1. If k exceeds\
    \ the number of available documents, all documents up to that limit are returned.\n\
    \nArgs:\n    self: The instance of the class containing the documents.\n    query_embedding:\
    \ list[float] The embedding vector to search with. This value is ignored in the\
    \ current implementation.\n    k: int The number of top results to return.\n \
    \   **kwargs: Any additional keyword arguments that may influence compatibility\
    \ but are not used.\n\nReturns:\n    list[VectorStoreSearchResult]: The top-k\
    \ search results. Each result contains the associated document and a fixed score\
    \ of 1."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 42
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreSearchResult
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/model.py::load_covariates
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_covariates
  signature: 'def load_covariates(dataset: str, _datasource: Datasource) -> pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Load covariate data as a DataFrame for the given dataset and datasource.\n\
    \nArgs:\n  dataset: str - The dataset identifier to load covariates for.\n  _datasource:\
    \ Datasource - The data source to query for covariates.\n\nReturns:\n  pd.DataFrame\
    \ - A DataFrame containing covariate data loaded for the specified dataset.\n\n\
    Raises:\n  Propagates any exceptions raised by get_covariate_data."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 50
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: unified-search-app/app/knowledge_loader/model.py::load_community_reports
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_community_reports
  signature: "def load_community_reports(\n    _datasource: Datasource,\n) -> pd.DataFrame"
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Load community report data from the indexed data source.\n\nThis function\
    \ delegates to get_community_report_data to obtain a DataFrame of\ncommunity reports\
    \ using the provided Datasource and returns the result.\n\nArgs:\n    _datasource\
    \ (Datasource): Datasource to read the community report data from the indexed\
    \ data.\n\nReturns:\n    pd.DataFrame: DataFrame containing community report data\
    \ loaded from the indexed data.\n\nRaises:\n    Exception: If the underlying data\
    \ source read operation fails."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 58
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: unified-search-app/app/app_logic.py::run_local_search
  file: unified-search-app/app/app_logic.py
  name: run_local_search
  signature: "def run_local_search(\n    query: str,\n    sv: SessionVariables,\n\
    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run local search.\n\nArgs:\n    query: str\n        The search query\
    \ string used to perform the local search.\n    sv: SessionVariables\n       \
    \ The SessionVariables instance containing configuration and state for the current\
    \ session.\n\nReturns:\n    SearchResult\n        The result of the local search,\
    \ including the search_type (Local), the textual response, and the context data.\n\
    \nRaises:\n    Exception\n        If an error occurs during local search execution\
    \ (e.g., API call or UI state access)."
  code_example: null
  example_source: null
  line_start: 148
  line_end: 199
  dependencies:
  - graphrag.api::local_search
  called_by:
  - unified-search-app/app/app_logic.py::run_all_searches
- node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_version
  file: graphrag/config/models/language_model_config.py
  name: _validate_api_version
  signature: def _validate_api_version(self) -> None
  decorators: []
  raises:
  - AzureApiVersionMissingError
  visibility: protected
  docstring: "Validate the API version.\n\n        Required when using AOI.\n\n  \
    \      Args:\n            self: The LanguageModelConfig instance.\n\n        Returns:\n\
    \            None\n\n        Raises:\n            AzureApiVersionMissingError:\
    \ If the API version is missing and is required."
  code_example: null
  example_source: null
  line_start: 190
  line_end: 205
  dependencies:
  - graphrag/config/errors.py::AzureApiVersionMissingError
  called_by: []
- node_id: graphrag/data_model/community.py::Community.from_dict
  file: graphrag/data_model/community.py
  name: from_dict
  signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n        id_key:\
    \ str = \"id\",\n        title_key: str = \"title\",\n        short_id_key: str\
    \ = \"human_readable_id\",\n        level_key: str = \"level\",\n        entities_key:\
    \ str = \"entity_ids\",\n        relationships_key: str = \"relationship_ids\"\
    ,\n        text_units_key: str = \"text_unit_ids\",\n        covariates_key: str\
    \ = \"covariate_ids\",\n        parent_key: str = \"parent\",\n        children_key:\
    \ str = \"children\",\n        attributes_key: str = \"attributes\",\n       \
    \ size_key: str = \"size\",\n        period_key: str = \"period\",\n    ) -> \"\
    Community\""
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Create a new Community from the dict data.\n\nArgs:\n  cls (type): The\
    \ class.\n  d (dict[str, Any]): The source dictionary containing the values for\
    \ the Community fields.\n  id_key (str): Key in d for the community's identifier.\
    \ Defaults to \"id\".\n  title_key (str): Key in d for the community title. Defaults\
    \ to \"title\".\n  short_id_key (str): Key in d for the optional short identifier.\
    \ Defaults to \"human_readable_id\".\n  level_key (str): Key in d for the community\
    \ level. Defaults to \"level\".\n  entities_key (str): Key in d for the related\
    \ entity IDs. Defaults to \"entity_ids\".\n  relationships_key (str): Key in d\
    \ for the related relationship IDs. Defaults to \"relationship_ids\".\n  text_units_key\
    \ (str): Key in d for the related text unit IDs. Defaults to \"text_unit_ids\"\
    .\n  covariates_key (str): Key in d for covariate IDs. Defaults to \"covariate_ids\"\
    .\n  parent_key (str): Key in d for the parent community's ID. Defaults to \"\
    parent\".\n  children_key (str): Key in d for the child community IDs. Defaults\
    \ to \"children\".\n  attributes_key (str): Key in d for additional attributes.\
    \ Defaults to \"attributes\".\n  size_key (str): Key in d for the size of the\
    \ community. Defaults to \"size\".\n  period_key (str): Key in d for the period.\
    \ Defaults to \"period\".\n\nReturns:\n  Community: The newly created Community\
    \ instance.\n\nRaises:\n  KeyError: If required keys are missing from the input\
    \ dictionary."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 79
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.__init__
  file: tests/integration/vector_stores/test_factory.py
  name: __init__
  signature: def __init__(self, **kwargs)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Internal API: Initialize the CustomVectorStore by forwarding keyword\
    \ arguments to the base class initializer.\n\nArgs:\n  kwargs: dict of keyword\
    \ arguments forwarded to BaseVectorStore.__init__\n\nReturns:\n  None\n\nRaises:\n\
    \  Propagates exceptions raised by BaseVectorStore.__init__."
  code_example: null
  example_source: null
  line_start: 128
  line_end: 129
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.get_chat_models
  file: graphrag/language_model/factory.py
  name: get_chat_models
  signature: def get_chat_models(cls) -> list[str]
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Get the registered ChatModel implementations.\n\nArgs:\n    cls: The\
    \ class that maintains the _chat_registry mapping of ChatModel implementations.\n\
    \nReturns:\n    list[str]: A list of the registered ChatModel implementation names."
  code_example: null
  example_source: null
  line_start: 78
  line_end: 80
  dependencies: []
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.use
  file: graphrag/config/environment_reader.py
  name: use
  signature: 'def use(self, value: Any | None)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a context manager to push the value into the config_stack.\n\n\
    Args:\n    value: Any | None. The value to push onto the internal config stack;\
    \ if None, an empty dict is pushed.\n\nReturns:\n    contextmanager. A context\
    \ manager that pushes the provided value (or an empty dict) onto the internal\
    \ config stack for the duration of the context and pops it on exit."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 71
  dependencies:
  - graphrag/config/environment_reader.py::config_context
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_end
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: pipeline_end
  signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when the entire pipeline ends.\n\nArgs:\n    results:\
    \ A list of PipelineRunResult objects representing the results of the pipeline\
    \ runs.\n\nReturns:\n    None..."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/extract_covariates.py::extract_covariates
  file: graphrag/index/workflows/extract_covariates.py
  name: extract_covariates
  signature: "def extract_covariates(\n    text_units: pd.DataFrame,\n    callbacks:\
    \ WorkflowCallbacks,\n    cache: PipelineCache,\n    covariate_type: str,\n  \
    \  extraction_strategy: dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n\
    \    entity_types: list[str] | None = None,\n    num_threads: int = 4,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: 'All the steps to extract and format covariates.


    Args:

    - text_units (pd.DataFrame): Input text units to process. Must contain at least
    the columns "id" and "text". This function mutates text_units in place by adding
    a temporary text_unit_id column equal to id, and then removes it before returning.

    - callbacks (WorkflowCallbacks): Callbacks used during the extraction workflow.

    - cache (PipelineCache): Cache for the extraction process.

    - covariate_type (str): Covariate type to extract (for example, "claim").

    - extraction_strategy (dict[str, Any] | None): Configuration for the extraction
    strategy or None.

    - async_mode (AsyncType): Asynchronous execution mode to use.

    - entity_types (list[str] | None): Entity types to consider; None to include all.

    - num_threads (int): Number of threads for the extraction step.


    Returns:

    pd.DataFrame: A covariates dataframe containing the final columns defined by COVARIATES_FINAL_COLUMNS.
    Each row represents an extracted covariate and is augmented with a unique id and
    a human_readable_id corresponding to the dataframe index.


    Side effects:

    - The input text_units DataFrame is mutated in place by adding a temporary text_unit_id
    column equal to the original id, which is dropped before returning.


    Raises:

    - KeyError if required columns (e.g., "id" or "text") are missing from text_units.

    - ValueError, TypeError, or other exceptions raised by the underlying extractor
    if inputs are invalid.


    Example:

    Suppose text_units is a DataFrame with columns ["id", "text"] and two rows. After
    calling extract_covariates with appropriate callbacks, cache, covariate_type,
    and strategy, the function returns a covariates DataFrame containing the final
    covariate columns as defined by COVARIATES_FINAL_COLUMNS, with additional id (uuid4)
    and human_readable_id (index) columns. The original text_units is restored after
    processing aside from the transient in-place mutation during execution.'
  code_example: null
  example_source: null
  line_start: 64
  line_end: 93
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: 'def __call__(self, /, **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronous request function.\n\nRepresents an asynchronous call to\
    \ either a chat completion or embedding function. The implementation forwards\
    \ all provided keyword arguments to the underlying request function, enabling\
    \ flexible use with different backends.\n\nArgs:\n    kwargs: Arbitrary keyword\
    \ arguments forwarded to the underlying request function. Specific accepted keys\
    \ depend on the concrete implementation (e.g., chat completion or embedding).\n\
    \nReturns:\n    Any: The result produced by the underlying request function. The\
    \ exact type depends on the concrete function being invoked (e.g., a chat completion\
    \ response or an embedding).\n\nRaises:\n    Exception: Exceptions raised by the\
    \ underlying request function are propagated to the caller (e.g., API or network\
    \ errors)."
  code_example: null
  example_source: null
  line_start: 233
  line_end: 235
  dependencies: []
  called_by: []
- node_id: graphrag/logger/progress.py::progress_ticker
  file: graphrag/logger/progress.py
  name: progress_ticker
  signature: "def progress_ticker(\n    callback: ProgressHandler | None, num_total:\
    \ int, description: str = \"\"\n) -> ProgressTicker"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a progress ticker.\n\nArgs:\n    callback: ProgressHandler |\
    \ None\n        Optional callback to be invoked with Progress updates.\n    num_total:\
    \ int\n        Total number of items to track progress for.\n    description:\
    \ str\n        Optional description to accompany the progress updates.\n\nReturns:\n\
    \    ProgressTicker\n        A ProgressTicker instance configured with the provided\
    \ callback, total, and description."
  code_example: null
  example_source: null
  line_start: 74
  line_end: 78
  dependencies: []
  called_by:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  - graphrag/index/operations/embed_text/strategies/mock.py::run
  - graphrag/index/operations/embed_text/strategies/openai.py::run
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
  - graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
- node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed
  file: tests/integration/language_model/test_factory.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate an embedding for the input text.\n\nArgs:\n  text: The input\
    \ text to generate the embedding for.\n  kwargs: Additional keyword arguments\
    \ passed to the embedding model.\n\nReturns:\n  list[float]: A list of floating-point\
    \ numbers representing the embedding.\n\nRaises:\n  This function does not raise\
    \ any exceptions...."
  code_example: null
  example_source: null
  line_start: 73
  line_end: 74
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_basic_search_configs
  file: tests/unit/config/utils.py
  name: assert_basic_search_configs
  signature: "def assert_basic_search_configs(\n    actual: BasicSearchConfig, expected:\
    \ BasicSearchConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two BasicSearchConfig objects have equal prompt and k values.\n\
    \nArgs:\n    actual: BasicSearchConfig to compare against expected\n    expected:\
    \ BasicSearchConfig to compare with actual\n\nReturns:\n    None\n\nRaises:\n\
    \    AssertionError: If actual.prompt != expected.prompt or actual.k != expected.k"
  code_example: null
  example_source: null
  line_start: 379
  line_end: 383
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.item_filter
  file: graphrag/storage/blob_pipeline_storage.py
  name: item_filter
  signature: 'def item_filter(item: dict[str, Any]) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Determine whether the given item matches the current file_filter or\
    \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
    \ The keys used by file_filter are read from this dict. If a key referenced by\
    \ file_filter is missing from item, a KeyError may be raised. Values should be\
    \ strings (or objects compatible with re.search).\n\nReturns:\n    bool: True\
    \ if no file_filter is defined. Otherwise, True if all key/value patterns in file_filter\
    \ match the corresponding fields in item using re.search; False otherwise.\n\n\
    Raises:\n    KeyError: If a key referenced by file_filter is missing from the\
    \ item."
  code_example: null
  example_source: null
  line_start: 134
  line_end: 140
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_vector
  file: graphrag/vector_stores/cosmosdb.py
  name: similarity_search_by_vector
  signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
    \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Perform a vector-based similarity search against the CosmosDB container.\n\
    \nArgs:\n  query_embedding: Embedding vector to search with\n  k: Number of top\
    \ results to return\n  kwargs: Additional keyword arguments for compatibility;\
    \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]:\
    \ Top-k results as VectorStoreSearchResult objects\n\nRaises:\n  ValueError: If\
    \ the container client is not initialized."
  code_example: null
  example_source: null
  line_start: 181
  line_end: 241
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/base.py::VectorStoreSearchResult
  - graphrag/vector_stores/cosmosdb.py::cosine_similarity
  called_by: []
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.pull_field
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: pull_field
  signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Pull a field from a list of strings by index.\n\nArgs:\n    index:\
    \ The position of the field to extract from fields.\n    fields: The list of string\
    \ fields from which to pull the value.\n\nReturns:\n    str | None: The trimmed\
    \ field at the given index if present; otherwise None.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 211
  line_end: 212
  dependencies: []
  called_by: []
- node_id: graphrag/logger/factory.py::LoggerFactory.get_logger_types
  file: graphrag/logger/factory.py
  name: get_logger_types
  signature: def get_logger_types(cls) -> list[str]
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"Get the registered logger implementations.\n\nArgs:\n    cls:\
    \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
    \ The list of registered logger implementation names.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 71
  line_end: 73
  dependencies: []
  called_by: []
- node_id: graphrag/utils/api.py::create_cache_from_config
  file: graphrag/utils/api.py
  name: create_cache_from_config
  signature: 'def create_cache_from_config(cache: CacheConfig, root_dir: str) -> PipelineCache'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a cache object from the given CacheConfig by delegating to CacheFactory.\n\
    \nArgs:\n  cache: CacheConfig The cache configuration to create the cache object\
    \ from. The configuration is dumped to a dict via model_dump(); the dictionary\
    \ must include a \"type\" key used to determine the concrete cache implementation.\n\
    \  root_dir: str The root directory to merge into the cache creation kwargs.\n\
    \nReturns:\n  PipelineCache: The created cache object.\n\nRaises:\n  Exception:\
    \ May raise any exception raised by the underlying CacheFactory during cache creation.\n\
    \nNotes:\n  This function uses cache.model_dump() to obtain the configuration,\
    \ reads the \"type\" field for the cache_type, and merges root_dir into the kwargs\
    \ before invoking CacheFactory."
  code_example: null
  example_source: null
  line_start: 273
  line_end: 280
  dependencies:
  - graphrag.cache.factory::CacheFactory
  called_by:
  - graphrag/index/run/run_pipeline.py::run_pipeline
- node_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType.__repr__
  file: graphrag/index/operations/summarize_descriptions/typing.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a string representation of this SummarizeStrategyType enum member.\n\
    \nArgs:\n    self: SummarizeStrategyType, the enum member to represent as a string.\n\
    \nReturns:\n    str: The enum member's value enclosed in double quotes."
  code_example: null
  example_source: null
  line_start: 46
  line_end: 48
  dependencies: []
  called_by: []
- node_id: tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  file: tests/unit/litellm_services/utils.py
  name: assert_max_num_values_per_period
  signature: "def assert_max_num_values_per_period(\n    periods: list[list[float]],\
    \ max_values_per_period: int\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert the maximum number of values per period.\n\nArgs:\n    periods:\
    \ list[list[float]] - a list of periods; each period is a list of time values\n\
    \    max_values_per_period: int - the maximum number of values allowed per period\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError - if any period contains more\
    \ values than max_values_per_period"
  code_example: null
  example_source: null
  line_start: 26
  line_end: 31
  dependencies: []
  called_by:
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_tpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
  - tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
  - tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
- node_id: graphrag/index/workflows/create_community_reports.py::_prep_nodes
  file: graphrag/index/workflows/create_community_reports.py
  name: _prep_nodes
  signature: 'def _prep_nodes(input: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Populate node descriptions and create NODE_DETAILS without filtering.\n\
    \nThis function fills missing descriptions with \"No Description\" and creates\
    \ a new\nNODE_DETAILS column by aggregating SHORT_ID, TITLE, DESCRIPTION, and\
    \ NODE_DEGREE\nfor each node. No rows are filtered; the operation mutates the\
    \ input DataFrame in\nplace and returns the same object.\n\nArgs:\n    input (pd.DataFrame):\
    \ Input nodes DataFrame. Must contain the columns\n        DESCRIPTION, SHORT_ID,\
    \ TITLE, and NODE_DEGREE (as defined by the data model).\n\nReturns:\n    pd.DataFrame:\
    \ The same input DataFrame, mutated in place with the new NODE_DETAILS column.\n\
    \nRaises:\n    KeyError: If any required column (DESCRIPTION, SHORT_ID, TITLE,\
    \ NODE_DEGREE) is\n        missing from the input DataFrame."
  code_example: null
  example_source: null
  line_start: 140
  line_end: 158
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
- node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__call__
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: __call__
  signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
    \ dict | None = None\n    ) -> ClaimExtractorResult"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Process a collection of input texts to extract claims and return the\
    \ structured results.\n\nArgs:\n  inputs: dict[str, Any] - The inputs containing\
    \ the texts to process and related fields such as input_text key, entity specs,\
    \ and claim descriptions.\n  prompt_variables: dict | None - Optional mapping\
    \ of prompt variables to customize extraction prompts and delimiters. If None,\
    \ defaults are used.\n\nReturns:\n  ClaimExtractorResult - The result object containing:\n\
    \    output: list[dict] - The cleaned claim dictionaries.\n    source_docs: dict[str,\
    \ str] - Mapping from document_id to the original text for each processed document.\n\
    \nRaises:\n  KeyError - If required input keys are missing from inputs."
  code_example: null
  example_source: null
  line_start: 87
  line_end: 133
  dependencies:
  - graphrag/index/operations/extract_covariates/claim_extractor.py::_clean_claim
  - graphrag/index/operations/extract_covariates/claim_extractor.py::_process_document
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_models
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_models
  signature: def _validate_models(self) -> None
  decorators: []
  raises:
  - LanguageModelConfigMissingError
  visibility: protected
  docstring: "Validate the models configuration.\n\nEnsure both a default chat model\
    \ and default embedding model have been defined. Other models may also be defined\
    \ but defaults are required for the time being as places of the code fallback\
    \ to default model configs instead of specifying a specific model.\n\nTODO: Don't\
    \ fallback to default models elsewhere in the code. Forcing code to specify a\
    \ model to use and allowing for any names for model configurations.\n\nArgs:\n\
    \    self: GraphRagConfig instance being validated.\n\nReturns:\n    None: This\
    \ method does not return a value.\n\nRaises:\n    LanguageModelConfigMissingError:\
    \ If the default chat model ID or the default embedding model ID is not present\
    \ in self.models."
  code_example: null
  example_source: null
  line_start: 80
  line_end: 96
  dependencies:
  - graphrag/config/errors.py::LanguageModelConfigMissingError
  called_by: []
- node_id: unified-search-app/app/ui/report_list.py::create_report_list_ui
  file: unified-search-app/app/ui/report_list.py
  name: create_report_list_ui
  signature: 'def create_report_list_ui(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Render the report list UI and update the selected report in the session\
    \ state based on user selection.\n\nArgs:\n    sv (SessionVariables): The session\
    \ state object containing community_reports and selected_report used to render\
    \ the UI and handle selection updates.\n\nReturns:\n    None: This function does\
    \ not return a value."
  code_example: null
  example_source: null
  line_start: 10
  line_end: 25
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType.__repr__
  file: graphrag/index/operations/embed_text/embed_text.py
  name: __repr__
  signature: def __repr__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Get a string representation of this TextEmbedStrategyType enum\
    \ member.\n\nArgs:\n    self: TextEmbedStrategyType, the enum member to represent\
    \ as a string.\n\nReturns:\n    str: The enum member's value enclosed in double\
    \ quotes.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 34
  line_end: 36
  dependencies: []
  called_by: []
- node_id: graphrag/cli/index.py::_register_signal_handlers
  file: graphrag/cli/index.py
  name: _register_signal_handlers
  signature: def _register_signal_handlers()
  decorators: []
  raises: []
  visibility: protected
  docstring: "Register signal handlers for graceful shutdown of the CLI.\n\nThis function\
    \ defines a signal handler that logs the received signal, cancels all asyncio\
    \ tasks, and logs that all tasks have been cancelled. It registers the handler\
    \ for SIGINT and, on non-Windows platforms, SIGHUP.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 25
  line_end: 39
  dependencies: []
  called_by:
  - graphrag/cli/index.py::_run_index
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::_wrapped_with_retries
  file: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py
  name: _wrapped_with_retries
  signature: 'def _wrapped_with_retries(**kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wrap the synchronous request function with retries using the configured\
    \ retry service.\n\nArgs:\n    kwargs: Keyword arguments passed to the underlying\
    \ synchronous request function. (type: Any)\n\nReturns:\n    Any: The value returned\
    \ by the underlying synchronous request function when called with the provided\
    \ kwargs.\n\nRaises:\n    Exception: Propagated from the underlying synchronous\
    \ function or the retry service."
  code_example: null
  example_source: null
  line_start: 46
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/embed_graph/embed_node2vec.py::embed_node2vec
  file: graphrag/index/operations/embed_graph/embed_node2vec.py
  name: embed_node2vec
  signature: "def embed_node2vec(\n    graph: nx.Graph | nx.DiGraph,\n    dimensions:\
    \ int = 1536,\n    num_walks: int = 10,\n    walk_length: int = 40,\n    window_size:\
    \ int = 2,\n    iterations: int = 3,\n    random_seed: int = 86,\n) -> NodeEmbeddings"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate node embeddings using Node2Vec.\n\nArgs:\n    graph: Graph\
    \ or DiGraph on which to compute node embeddings.\n    dimensions: Dimensionality\
    \ of the embeddings.\n    num_walks: Number of random walks to start at every\
    \ node.\n    walk_length: Length of each random walk.\n    window_size: Window\
    \ size parameter for embedding model.\n    iterations: Number of training iterations.\n\
    \    random_seed: Seed for the random number generator.\n\nReturns:\n    NodeEmbeddings:\
    \ Object containing embeddings (np.ndarray) and the corresponding list of node\
    \ identifiers (list[str])."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 43
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
- node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_start
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: pipeline_start
  signature: 'def pipeline_start(self, names: list[str]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback to signal when the entire pipeline starts.\n\n\
    Args:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
    \    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 14
  line_end: 15
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._delete_container
  file: graphrag/storage/blob_pipeline_storage.py
  name: _delete_container
  signature: def _delete_container(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Delete the container if it exists.\n\nThis protected method deletes\
    \ the container using the internal container name (self._container_name). Deletion\
    \ is conditional on existence, as determined by _container_exists(). The deletion\
    \ is performed via the BlobServiceClient stored on self._blob_service_client and\
    \ may raise Azure SDK exceptions if the operation fails.\n\nArgs:\n    None: This\
    \ method does not accept explicit parameters. It relies on internal state (self._container_name,\
    \ self._blob_service_client).\n\nReturns:\n    None: This method does not return\
    \ a value.\n\nRaises:\n    AzureError: Azure SDK exceptions may be raised during\
    \ the delete operation.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 87
  line_end: 90
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_container_exists
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomChatModel.__init__
  file: tests/integration/language_model/test_factory.py
  name: __init__
  signature: def __init__(self, **kwargs)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the instance with optional keyword arguments.\n\nThis __init__\
    \ accepts arbitrary keyword arguments (kwargs) but does not store or use them.\
    \ No state is initialized or modified, and no side effects occur.\n\nArgs:\n \
    \ kwargs: dict[str, Any] - keyword arguments provided to the initializer. They\
    \ are ignored.\n\nReturns:\n  None"
  code_example: null
  example_source: null
  line_start: 25
  line_end: 26
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_file_pipeline_storage.py::test_find
  file: tests/integration/storage/test_file_pipeline_storage.py
  name: test_find
  signature: def test_find()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test for FilePipelineStorage.find and basic get/set/delete\
    \ operations.\n\nThis test is asynchronous and uses await to verify the behavior\
    \ of FilePipelineStorage:\n- find: lists .txt files under a base directory matching\
    \ a pattern\n- get: reads file contents (including verifying the existence)\n\
    - set: creates a new file with specified content\n- delete: removes the file and\
    \ confirms it is deleted\n\nNote: Failures may raise assertions or other exceptions\
    \ during test execution.\n\nArgs:\n    None: The function does not accept any\
    \ input parameters. Type: None\n\nReturns:\n    None: The test does not return\
    \ a value. Type: None\n\nRaises:\n    AssertionError: If any assertion in the\
    \ test fails.\n    Exception: If any other error occurs during test execution."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 35
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::test_create_custom_embedding_llm
  file: tests/integration/language_model/test_factory.py
  name: test_create_custom_embedding_llm
  signature: def test_create_custom_embedding_llm()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test for creating a custom embedding LLM and validating\
    \ its methods.\n\nArgs:\n    None: The function takes no parameters.\n\nReturns:\n\
    \    None: This test does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any assertion in the test fails."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 97
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::to_optional_list
  file: graphrag/query/input/loaders/utils.py
  name: to_optional_list
  signature: "def to_optional_list(\n    data: Mapping[str, Any], column_name: str\
    \ | None, item_type: type | None = None\n) -> list | None"
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to an optional list.\n\nThis function retrieves\
    \ the value for column_name from data. If column_name is None or not present in\
    \ data, or if the value is None, it returns None. If the value is a numpy array,\
    \ it is converted to a Python list. If the value is a string, it is wrapped in\
    \ a single-element list. If the resulting value is not a list, a TypeError is\
    \ raised. If item_type is provided, every element in the list must be an instance\
    \ of item_type; otherwise a TypeError is raised.\n\nArgs:\n  data: Mapping[str,\
    \ Any]\n      Input data container.\n  column_name: str | None\n      The key\
    \ in data to retrieve. If None or not present, returns None. If present but value\
    \ is None, also returns None.\n  item_type: type | None\n      If provided, require\
    \ all list items to be instances of this type.\n\nReturns:\n  list | None\n  \
    \    The optional list value, or None if the key is missing or its value is None.\n\
    \nRaises:\n  TypeError\n      If the retrieved value cannot be converted to a\
    \ list or if any list item is not of the specified item_type."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 88
  dependencies: []
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_entities
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_covariates
  - graphrag/query/input/loaders/dfs.py::read_communities
  - graphrag/query/input/loaders/dfs.py::read_community_reports
  - graphrag/query/input/loaders/dfs.py::read_text_units
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
  file: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py
  name: with_cache
  signature: "def with_cache(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
    \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n    cache:\
    \ \"PipelineCache\",\n    request_type: Literal[\"chat\", \"embedding\"],\n  \
    \  cache_key_prefix: str,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Cache wrapper for Litellm request functions.\n\nArgs\n----\n \
    \   sync_fn: The synchronous chat/embedding request function to wrap.\n    async_fn:\
    \ The asynchronous chat/embedding request function to wrap.\n    model_config:\
    \ The configuration for the language model.\n    cache: The cache to use for storing\
    \ responses.\n    request_type: The type of request being made, either \"chat\"\
    \ or \"embedding\".\n    cache_key_prefix: The prefix to use for cache keys.\n\
    \nReturns\n-------\n    A tuple containing the wrapped synchronous and asynchronous\
    \ chat/embedding request functions.\n\nRaises\n------\n    Exceptions raised by\
    \ the underlying sync_fn/async_fn or by the cache operations.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 22
  line_end: 107
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
- node_id: graphrag/factory/factory.py::Factory.__init__
  file: graphrag/factory/factory.py
  name: __init__
  signature: def __init__(self)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize internal state for the Factory singleton instance on first\
    \ initialization.\n\nArgs\n----\nself: The Factory instance being initialized.\
    \ The internal state is created only on the first initialization due to the singleton\
    \ __new__-based pattern.\n\nReturns\n-------\nNone\n\nRaises\n-------\nNone\n\n\
    Attributes\n- _services: dict[str, Callable[..., T]] \u2014 registry mapping strategy\
    \ names to callables that return T.\n- _initialized: bool \u2014 initialization\
    \ flag."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 27
  dependencies: []
  called_by: []
- node_id: graphrag/utils/storage.py::delete_table_from_storage
  file: graphrag/utils/storage.py
  name: delete_table_from_storage
  signature: 'def delete_table_from_storage(name: str, storage: PipelineStorage) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Delete a table from storage.\n\nArgs:\n  name (str): The base name of\
    \ the parquet file to delete, without extension.\n  storage (PipelineStorage):\
    \ The storage backend to delete the parquet file from.\n\nReturns:\n  None\n\n\
    Raises:\n  Exception: Exceptions raised by the storage backend during the delete\
    \ operation may propagate."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 39
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_factory.py::CustomStorage.__init__
  file: tests/integration/storage/test_factory.py
  name: __init__
  signature: def __init__(self, **kwargs)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
    \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
    \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 113
  line_end: 114
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  file: graphrag/index/operations/chunk_text/chunk_text.py
  name: run_strategy
  signature: "def run_strategy(\n    strategy_exec: ChunkStrategy,\n    input: ChunkInput,\n\
    \    config: ChunkingConfig,\n    tick: ProgressTicker,\n) -> list[str | tuple[list[str]\
    \ | None, str, int]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the given chunking strategy on the input data and return the produced\
    \ chunks.\n\nArgs:\n    strategy_exec: ChunkStrategy\n        The strategy function\
    \ to execute to generate text chunks.\n    input: ChunkInput\n        The input\
    \ data to chunk. May be a string or a list of strings, or a list\n        of tuples\
    \ of (document_id, text content).\n    config: ChunkingConfig\n        Configuration\
    \ for chunking, including size, overlap, and encoding model.\n    tick: ProgressTicker\n\
    \        Progress ticker used to report progress.\n\nReturns:\n    list[str |\
    \ tuple[list[str] | None, str, int]]\n        A list of results. If the input\
    \ was a simple string, returns a list of\n        text_chunk strings. Otherwise\
    \ each element is either:\n        - a text_chunk string, or\n        - a tuple\
    \ (doc_ids, text_chunk, n_tokens) where doc_ids is a list of\n          document\
    \ ids corresponding to the source documents for that chunk,\n          text_chunk\
    \ is the chunk text, and n_tokens is the number of tokens\n          in the chunk."
  code_example: null
  example_source: null
  line_start: 82
  line_end: 111
  dependencies: []
  called_by:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_str
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_str
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple_same_doc
- node_id: graphrag/index/utils/dataframes.py::antijoin
  file: graphrag/index/utils/dataframes.py
  name: antijoin
  signature: 'def antijoin(df: pd.DataFrame, exclude: pd.DataFrame, column: str) ->
    pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return an anti-joined dataframe.\n\nArgs:\n    df (pd.DataFrame): The\
    \ DataFrame to apply the exclusion to.\n    exclude (pd.DataFrame): The DataFrame\
    \ containing rows to remove.\n    column (str): The join-on column.\n\nReturns:\n\
    \    pd.DataFrame: The rows from df whose value in column is not present in exclude[column]."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 31
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
  file: graphrag/language_model/providers/litellm/request_wrappers/with_retries.py
  name: with_retries
  signature: "def with_retries(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
    \ AsyncLitellmRequestFunc,\n    model_config: \"LanguageModelConfig\",\n) -> tuple[LitellmRequestFunc,\
    \ AsyncLitellmRequestFunc]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Wrap the synchronous and asynchronous request functions with retries.\n\
    \nThis function constructs a retry service from model_config and returns two wrappers:\n\
    - a synchronous wrapper that uses retry to invoke the provided sync_fn\n- an asynchronous\
    \ wrapper that uses aretry to invoke the provided async_fn\n\nRetry configuration\
    \ is driven by model_config fields: retry_strategy, max_retries, and max_retry_wait.\n\
    \nNotes:\n- The asynchronous wrapper uses aretry on the async_fn, while the synchronous\
    \ wrapper uses retry on the sync_fn.\n\nArgs:\n    sync_fn (LitellmRequestFunc):\
    \ The synchronous chat/embedding request function to wrap.\n    async_fn (AsyncLitellmRequestFunc):\
    \ The asynchronous chat/embedding request function to wrap.\n    model_config\
    \ (LanguageModelConfig): Configuration for the language model, including retry\
    \ parameters (retry_strategy, max_retries, max_retry_wait).\n\nReturns:\n    tuple[LitellmRequestFunc,\
    \ AsyncLitellmRequestFunc]: A tuple containing the wrapped synchronous and\n \
    \       asynchronous chat/embedding request functions.\n\nRaises:\n    Propagates\
    \ exceptions from the retry mechanism or the wrapped functions when retries are\
    \ exhausted or an unrecoverable error occurs."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 54
  dependencies:
  - graphrag/language_model/providers/litellm/services/retry/retry_factory.py::RetryFactory
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
- node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.acquire
  file: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  name: acquire
  signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
  decorators:
  - '@abstractmethod'
  - '@contextmanager'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "\"\"\"Acquire Rate Limiter.\n\nParameters\n    token_count (int): The\
    \ estimated number of tokens for the current request.\n\nReturns\n    Iterator[None]:\
    \ A context manager that yields None and does not return any value.\n\nRaises\n\
    \    NotImplementedError: RateLimiter subclasses must implement the acquire method.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 24
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.__init__
  file: graphrag/language_model/manager.py
  name: __init__
  signature: def __init__(self) -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Initialize the singleton LLM manager's internal state on first\
    \ instantiation.\n\nArgs:\n    self: The instance being initialized. Sets up internal\
    \ dictionaries for chat_models and embedding_models and marks the instance as\
    \ initialized to avoid reinitialization.\n\nReturns:\n    None: This method does\
    \ not return a value.\n\nRaises:\n    None: This method does not raise any exceptions.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 33
  line_end: 38
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_entity_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_entity_data
  signature: 'def get_entity_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a dataframe with entity data from the indexed data.\n\nReads\
    \ entity data from the configured table via the provided data source (config.entity_table).\
    \ The function prints the number of entity records and the dataset name as a side\
    \ effect and is cached with Streamlit's cache_data decorator using TTL from config.default_ttl.\n\
    \nArgs:\n    dataset: The dataset name to load.\n    _datasource: The data source\
    \ used to read the entity data from the indexed data.\n\nReturns:\n    pd.DataFrame:\
    \ A dataframe containing the entity data loaded from the indexed data.\n\nRaises:\n\
    \    Exception: If reading from the data source fails."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 29
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/query_variable.py::QueryVariable.key
  file: unified-search-app/app/state/query_variable.py
  name: key
  signature: def key(self) -> str
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Key property that returns the session_state key for this variable.\n\
    \nArgs:\n    self (QueryVariable): The instance of QueryVariable.\n\nReturns:\n\
    \    str: The key used to access the session_state dictionary for this variable."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 34
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.__init__
  file: graphrag/storage/file_pipeline_storage.py
  name: __init__
  signature: 'def __init__(self, **kwargs: Any) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a file-based storage backend.\n\nThis constructor prepares\
    \ the storage root by creating the base_dir if it does not exist and sets the\
    \ encoding used for file I/O.\n\nArgs:\n  base_dir (str): Directory path where\
    \ files are stored. Defaults to the empty string (uses the current working directory).\n\
    \  encoding (str): Text encoding for file operations. Defaults to \"utf-8\".\n\
    \nReturns:\n  None\n\nRaises:\n  OSError: If the root directory cannot be created\
    \ or accessed due to filesystem errors.\n  Other exceptions may be raised for\
    \ permission issues or unexpected filesystem failures."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 38
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/embed_text/embed_text.py::_create_vector_store
  file: graphrag/index/operations/embed_text/embed_text.py
  name: _create_vector_store
  signature: "def _create_vector_store(\n    vector_store_config: dict, index_name:\
    \ str, embedding_name: str | None = None\n) -> BaseVectorStore"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Create and configure a vector store from the provided configuration.\n\
    \nArgs:\n    vector_store_config (dict): Configuration for the vector store, including\
    \ the type, embeddings_schema, and other parameters forwarded to the vector store\
    \ constructor and connect().\n    index_name (str): The index name to assign if\
    \ not provided by the embedding config.\n    embedding_name (str | None): Optional\
    \ embedding name to select a specific embedding configuration from embeddings_schema.\n\
    \nReturns:\n    BaseVectorStore: A connected vector store instance created according\
    \ to the configuration.\n\nRaises:\n    Exception: If vector store creation or\
    \ connection fails due to misconfiguration or underlying storage errors."
  code_example: null
  example_source: null
  line_start: 186
  line_end: 217
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/factory.py::VectorStoreFactory
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::embed_text
- node_id: graphrag/language_model/response/base.pyi::BaseModelResponse.__init__
  file: graphrag/language_model/response/base.pyi
  name: __init__
  signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n   \
    \     parsed_response: _T | None = None,\n        history: list[Any] = ...,  #\
    \ default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
    \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
    \ bool | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initializes a BaseModelResponse with the given output, parsed_response,\
    \ and optional metadata.\n\nArgs:\n    output: BaseModelOutput\n        BaseModelOutput\
    \ instance containing the content and full_response.\n    parsed_response: _T\
    \ | None\n        The parsed response of type _T, or None.\n    history: list[Any]\n\
    \        History list; default provided by Pydantic.\n    tool_calls: list[Any]\n\
    \        Tool calls list; default provided by Pydantic.\n    metrics: Any | None\n\
    \        Metrics associated with the response; may be None.\n    cache_hit: bool\
    \ | None\n        Indicates whether a cache hit occurred; may be None.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 42
  line_end: 50
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__eq__
  file: graphrag/query/structured_search/drift_search/action.py
  name: __eq__
  signature: 'def __eq__(self, other: object) -> bool'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Check equality based on the query string.\n\nArgs:\n    other (object):\
    \ Another object to compare with.\n\nReturns:\n    bool: True if the other object\
    \ is a DriftAction with the same query, False otherwise."
  code_example: null
  example_source: null
  line_start: 223
  line_end: 237
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: get
  signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
    \ str | None = None\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously fetch items from the Cosmos DB container that match the\
    \ given key.\n\nThis coroutine supports two modes:\n\n- as_bytes is truthy: returns\
    \ matching items as Parquet-encoded data. The data are collected by querying items\
    \ whose id starts with the derived prefix, converted to a DataFrame, and then\
    \ serialized to Parquet using DataFrame.to_parquet(). The return value is Parquet\
    \ data only if in-memory writing is supported by the underlying to_parquet implementation;\
    \ otherwise the exact return type may vary depending on the runtime.\n- as_bytes\
    \ is None or False: returns the body of a single item as a JSON string.\n\nArgs:\n\
    \    key (str): The key used to query the container. If as_bytes is True, this\
    \ key is treated as a prefix for filtering by id.\n    as_bytes (bool | None):\
    \ If True, return matching items as Parquet-encoded data. If None or False, return\
    \ the body of a single item as a JSON string.\n    encoding (str | None): Encoding\
    \ to use for the returned data if applicable. This parameter is currently unused.\n\
    \nReturns:\n    Any: If as_bytes is True and in-memory Parquet writing is supported,\
    \ returns a Parquet-encoded bytes-like object representing the matching items.\
    \ If as_bytes is None or False, returns a JSON string of the item body for the\
    \ given key, or None if the database or container client is not initialized or\
    \ if an error occurs.\n\nRaises:\n    None: This coroutine handles exceptions\
    \ internally, logs a warning, and returns None on error."
  code_example: null
  example_source: null
  line_start: 205
  line_end: 239
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_text_str_empty
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_text_str_empty
  signature: def test_split_text_str_empty()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that TokenTextSplitter.split_text returns an empty list when the\
    \ input is an empty string.\n\nThis test initializes a TokenTextSplitter with\
    \ chunk_size=5 and chunk_overlap=2, calls split_text with an empty string, and\
    \ asserts that the result is [].\n\nReturns:\n    None: this test does not return\
    \ a value.\n\nRaises:\n    AssertionError: if the result is not an empty list."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 38
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.__init__
  file: graphrag/query/structured_search/basic_search/search.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n        system_prompt:\
    \ str | None = None,\n        response_type: str = \"multiple paragraphs\",\n\
    \        callbacks: list[QueryCallbacks] | None = None,\n        model_params:\
    \ dict[str, Any] | None = None,\n        context_builder_params: dict | None =\
    \ None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a BasicSearch instance for basic search orchestration (internal\
    \ API).\n\nArgs:\n    model (ChatModel): The language model interface used for\
    \ this basic search.\n    context_builder (BasicContextBuilder): The builder that\
    \ constructs the context for the search.\n    tokenizer (Tokenizer | None): Optional\
    \ tokenizer to use.\n    system_prompt (str | None): System prompt for the search.\
    \ If None, uses BASIC_SEARCH_SYSTEM_PROMPT.\n    response_type (str): Specifies\
    \ the format of the response. Defaults to \"multiple paragraphs\".\n    callbacks\
    \ (list[QueryCallbacks] | None): Optional list of query callbacks to invoke.\n\
    \    model_params (dict[str, Any] | None): Optional parameters to pass to the\
    \ model.\n    context_builder_params (dict | None): Optional parameters for the\
    \ context builder.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 30
  line_end: 50
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::embed
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: embed
  signature: 'def embed(chunk: list[str])'
  decorators: []
  raises: []
  visibility: public
  docstring: "Async helper to embed a batch of text chunks using the embedding model\
    \ with a concurrency guard.\n\nArgs:\n  chunk: A batch of text chunks to embed.\n\
    \nReturns:\n  numpy.ndarray: The embeddings for the input chunks as a 2D NumPy\
    \ array.\n\nRaises:\n  Exceptions raised by the embedding model (via model.aembed_batch)\
    \ may be propagated to the caller."
  code_example: null
  example_source: null
  line_start: 94
  line_end: 99
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::_execute
- node_id: graphrag/index/utils/dataframes.py::where_column_equals
  file: graphrag/index/utils/dataframes.py
  name: where_column_equals
  signature: 'def where_column_equals(df: pd.DataFrame, column: str, value: Any) ->
    pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a filtered DataFrame where a column equals a value.\n\nArgs:\n\
    \    df (pd.DataFrame): The DataFrame to filter.\n    column (str): The column\
    \ name to compare.\n    value (Any): The value to compare against.\n\nReturns:\n\
    \    pd.DataFrame: A DataFrame containing only rows where df[column] == value.\n\
    \nRaises:\n    KeyError: If the specified column is not in df."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 20
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
- node_id: tests/integration/storage/test_blob_pipeline_storage.py::test_find
  file: tests/integration/storage/test_blob_pipeline_storage.py
  name: test_find
  signature: def test_find()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test for BlobPipelineStorage.find and basic file operations.\n\
    \nThis test creates a BlobPipelineStorage instance, verifies that there are no\
    \ matching .txt files under the input base_dir, creates input/christmas.txt and\
    \ then confirms it is found, creates test.txt and confirms both files are listed,\
    \ reads the content of test.txt, and finally deletes test.txt and ensures it no\
    \ longer exists. The container is cleaned up at the end.\n\nReturns:\n    None\n\
    \nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 14
  line_end: 48
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.remove_embedding
  file: graphrag/language_model/manager.py
  name: remove_embedding
  signature: 'def remove_embedding(self, name: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Remove the EmbeddingsLLM instance registered under the given name.\n\
    \nArgs:\n    name: str \u2014 Unique identifier for the EmbeddingsLLM instance.\n\
    \nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 143
  line_end: 145
  dependencies: []
  called_by: []
- node_id: graphrag/logger/progress.py::ProgressTicker.__init__
  file: graphrag/logger/progress.py
  name: __init__
  signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
    \ int, description: str = \"\"\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a ProgressTicker with the provided callback, total items,\
    \ and optional description.\n\nArgs:\n    callback: ProgressHandler | None\n \
    \       A function to handle progress reports, or None to disable updates.\n \
    \   num_total: int\n        Total number of items to track.\n    description:\
    \ str\n        Optional description for the progress updates.\n\nReturns:\n  \
    \  None"
  code_example: null
  example_source: null
  line_start: 41
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.stream_search
  file: graphrag/query/structured_search/local_search/search.py
  name: stream_search
  signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n    ) -> AsyncGenerator"
  decorators: []
  raises: []
  visibility: public
  docstring: "Build local search context that fits a single context window and generate\
    \ answer for the user query.\n\nArgs:\n    query (str): The user query to process.\n\
    \    conversation_history (ConversationHistory | None): Optional conversation\
    \ history to incorporate into the search context.\n\nReturns:\n    AsyncGenerator[str,\
    \ None]: An asynchronous generator yielding strings representing chunks of the\
    \ generated answer.\n\nRaises:\n    Exception: If an error occurs during streaming\
    \ or within the model or callbacks."
  code_example: null
  example_source: null
  line_start: 132
  line_end: 163
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.connect
  file: graphrag/vector_stores/lancedb.py
  name: connect
  signature: 'def connect(self, **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Connect to LanceDB vector storage.\n\nArgs:\n    db_uri (str): The LanceDB\
    \ database URI to connect to. This value must be supplied via kwargs with the\
    \ key 'db_uri'.\n\nNotes:\n    - If self.index_name is set and a table with that\
    \ name exists in the connected database, the function will open that table and\
    \ assign it to self.document_collection.\n\nReturns:\n    None\n\nRaises:\n  \
    \  KeyError: If 'db_uri' is not provided in kwargs.\n    Exception: If the underlying\
    \ LanceDB library raises an exception during connect or table open."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 36
  dependencies: []
  called_by: []
- node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.__init__
  file: tests/integration/language_model/test_factory.py
  name: __init__
  signature: def __init__(self, **kwargs)
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
    \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
    \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None..."
  code_example: null
  example_source: null
  line_start: 67
  line_end: 68
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.__init__
  file: graphrag/vector_stores/base.py
  name: __init__
  signature: "def __init__(\n        self,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n\
    \        db_connection: Any | None = None,\n        document_collection: Any |\
    \ None = None,\n        query_filter: Any | None = None,\n        **kwargs: Any,\n\
    \    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the base vector store with the provided configuration and\
    \ optional resources.\n\nThis initializer assigns the given resources to instance\
    \ attributes and extracts\nconfiguration fields from vector_store_schema_config\
    \ to set the store's metadata\nand vector properties. It does not call super().__init__.\n\
    \nArgs:\n  vector_store_schema_config (VectorStoreSchemaConfig): The schema configuration\
    \ for the vector store. This is used to set index_name, id_field, text_field,\
    \ vector_field, attributes_field, and vector_size on the instance.\n  db_connection\
    \ (Any | None): Optional database connection.\n  document_collection (Any | None):\
    \ Optional document collection.\n  query_filter (Any | None): Optional query filter\
    \ to apply to queries.\n  kwargs (Any): Additional keyword arguments captured\
    \ for later use and stored in self.kwargs.\n\nReturns:\n  None\n\nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 42
  line_end: 60
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_query
  file: tests/smoke/test_fixtures.py
  name: __run_query
  signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
  decorators: []
  raises: []
  visibility: private
  docstring: "Run a uv run poe query command using the provided root and query_config\
    \ and return the subprocess result.\n\nArgs:\n  root: Path to the root directory\
    \ for the command. The path is resolved to an absolute POSIX string and passed\
    \ to --root.\n  query_config: dict[str, str]. Configuration for the query. Must\
    \ include:\n      method: The value for --method.\n      query: The value for\
    \ --query.\n      community_level: Optional; the value for --community-level.\
    \ If omitted, defaults to 2.\n\nReturns:\n  subprocess.CompletedProcess: The result\
    \ of subprocess.run invoked with capture_output=True and text=True.\n\nRaises:\n\
    \  None"
  code_example: null
  example_source: null
  line_start: 201
  line_end: 218
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/generator/community_reporter_role.py::generate_community_reporter_role
  file: graphrag/prompt_tune/generator/community_reporter_role.py
  name: generate_community_reporter_role
  signature: "def generate_community_reporter_role(\n    model: ChatModel, domain:\
    \ str, persona: str, docs: str | list[str]\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Generate a community reporter role for GraphRAG prompts.\n\nArgs:\n\
    \    model (ChatModel): The LLM to use for generation\n    domain (str): The domain\
    \ to generate a persona for\n    persona (str): The persona to generate a role\
    \ for\n    docs (str | list[str]): Documents to contextualize the persona; if\
    \ a list, these will be joined into a single string\n\nReturns:\n    str: The\
    \ generated domain prompt response content.\n\nRaises:\n    Exception: If the\
    \ underlying model call fails\n\"\"\""
  code_example: null
  example_source: null
  line_start: 12
  line_end: 35
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/language_model/protocol/base.py::ChatModel.achat_stream
  file: graphrag/language_model/protocol/base.py
  name: achat_stream
  signature: "def achat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Stream the given prompt via an asynchronous streaming interface. This\
    \ generator yields partial results over time as the model streams its response.\n\
    \nArgs:\n  prompt: The text to generate a response for.\n  history: The conversation\
    \ history. Optional prior messages that may influence generation.\n  **kwargs:\
    \ Additional keyword arguments (e.g., model parameters, streaming controls).\n\
    \nReturns:\n  AsyncGenerator[str, None]: An asynchronous generator that yields\
    \ strings representing portions of the response as they become available.\n\n\
    Raises:\n  Propagates exceptions raised by the underlying model call or streaming\
    \ backend (e.g., network errors, timeouts, invalid parameters).\n\nUsage:\n  async\
    \ for chunk in model.achat_stream(\"Hello, world!\", history=None):\n      print(chunk)"
  code_example: null
  example_source: null
  line_start: 115
  line_end: 131
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.check_query_doc_encodings
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: check_query_doc_encodings
  signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
    -> bool'
  decorators:
  - '@staticmethod'
  raises: []
  visibility: public
  docstring: "Check if the embeddings are compatible for direct comparison.\n\nThis\
    \ function enforces concrete compatibility criteria:\n- Both embeddings are non-None\n\
    - They have the same container type (type(query_embedding) == type(embedding))\n\
    - They have the same length (len(query_embedding) == len(embedding))\n- The inner\
    \ element types are the same (type(query_embedding[0]) == type(embedding[0]))\n\
    \nNote: The function expects sequences of homogeneous elements (e.g., lists of\
    \ floats, numpy arrays of floats). It assumes non-empty embeddings. If both embeddings\
    \ have length 0, attempting to access the first element (index 0) will raise IndexError.\n\
    \nArgs\n----\nquery_embedding : Any\n    Embedding of the query.\nembedding :\
    \ Any\n    Embedding to compare against.\n\nReturns\n-------\nbool: True if embeddings\
    \ are compatible, otherwise False.\n\nRaises\n------\nIndexError\n    If embeddings\
    \ are empty and the code attempts to access the first element.\nTypeError\n  \
    \  If inputs do not support len() or indexing, or are not indexable."
  code_example: null
  example_source: null
  line_start: 143
  line_end: 164
  dependencies: []
  called_by: []
- node_id: graphrag/config/errors.py::AzureApiVersionMissingError.__init__
  file: graphrag/config/errors.py
  name: __init__
  signature: 'def __init__(self, llm_type: str) -> None'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Init method for AzureApiVersionMissingError (internal API).\n\nThis\
    \ constructor formats the error message using the provided llm_type as: \"API\
    \ Version is required for {llm_type}. Please rerun graphrag init and set the api_version.\"\
    \ It initializes the base ValueError with that message. It returns None and does\
    \ not raise the exception by itself.\n\nArgs:\n    llm_type: The LLM type for\
    \ which the API Version is required.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 30
  line_end: 33
  dependencies: []
  called_by: []
- node_id: tests/integration/storage/test_file_pipeline_storage.py::test_child
  file: tests/integration/storage/test_file_pipeline_storage.py
  name: test_child
  signature: def test_child()
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nTest that a child FilePipelineStorage can be created from a\
    \ parent storage and used to perform basic file operations.\n\nReturns:\n    None\n\
    \        The function does not return a value.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 51
  line_end: 65
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
  called_by: []
- node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.decode
  file: graphrag/tokenizer/tokenizer.py
  name: decode
  signature: 'def decode(self, tokens: list[int]) -> str'
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
    \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from the\
    \ list of tokens.\n\nRaises:\n    NotImplementedError: If the decode method has\
    \ not been implemented by subclasses."
  code_example: null
  example_source: null
  line_start: 28
  line_end: 40
  dependencies: []
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::_run_rate_limiter
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: _run_rate_limiter
  signature: "def _run_rate_limiter(\n    rate_limiter: RateLimiter,\n    # Acquire\
    \ cost\n    input_queue: Queue[int | None],\n    # time value\n    output_queue:\
    \ Queue[float | None],\n)"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Run the RateLimiter and record timestamps for each acquired token count.\n\
    \nContinuously reads token_count from input_queue; when a token_count is None,\
    \ the function exits. For each non-None token_count, it enters the rate limiter\
    \ with acquire(token_count=token_count) and, after a successful acquisition, pushes\
    \ the current time (time.time()) to output_queue.\n\nArgs:\n    rate_limiter:\
    \ The RateLimiter instance used to enforce rate limits.\n    input_queue: Queue[int\
    \ | None]. Each non-None value represents the number of tokens to acquire; None\
    \ signals termination.\n    output_queue: Queue[float | None]. Timestamps (time.time())\
    \ are put here after acquisition; may contain None values.\n\nReturns:\n    None:\
    \ This function does not return a value.\n\nRaises:\n    Exceptions propagated\
    \ from the underlying RateLimiter or queue operations."
  code_example: null
  example_source: null
  line_start: 236
  line_end: 248
  dependencies: []
  called_by: []
- node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.child
  file: graphrag/storage/memory_pipeline_storage.py
  name: child
  signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None): Optional\
    \ name for the child storage. This parameter is accepted for API compatibility\
    \ but is ignored.\n\nReturns:\n    PipelineStorage: A new MemoryPipelineStorage\
    \ instance representing the child storage."
  code_example: null
  example_source: null
  line_start: 72
  line_end: 74
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_end
  file: graphrag/callbacks/query_callbacks.py
  name: on_reduce_response_end
  signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Handle the end of reduce operation.\n\nArgs:\n    reduce_response_output:\
    \ str\n        The output produced by the end of the reduce operation.\n\nReturns:\n\
    \    None: The function does not return a value."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 30
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.relate_actions
  file: graphrag/query/structured_search/drift_search/state.py
  name: relate_actions
  signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
    \ weight: float = 1.0\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Relate two actions in the graph.\n\nArgs:\n    self: The QueryState\
    \ instance.\n    parent: The parent DriftAction in the relation.\n    child: The\
    \ child DriftAction to be related to the parent.\n    weight: The weight of the\
    \ edge to add between the actions.\n\nReturns:\n    None. The function mutates\
    \ the internal graph by adding an edge with the specified weight.\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 28
  line_end: 32
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_cosmosdb.py::test_vector_store_operations
  file: tests/integration/vector_stores/test_cosmosdb.py
  name: test_vector_store_operations
  signature: def test_vector_store_operations()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test basic vector store operations with CosmosDB.\n\nArgs:\n    None:\
    \ The function does not accept any parameters.\n\nReturns:\n    None: The test\
    \ does not return a value.\n\nRises:\n    Exception: Exceptions may be raised\
    \ during test execution."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 76
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/base.py::VectorStoreDocument
  - graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
  called_by: []
- node_id: unified-search-app/app/ui/sidebar.py::create_side_bar
  file: unified-search-app/app/ui/sidebar.py
  name: create_side_bar
  signature: 'def create_side_bar(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a Streamlit sidebar panel in the app to configure dataset selection,\
    \ the number of suggested questions, and search options.\n\nThis function renders\
    \ the following UI components inside the Streamlit sidebar:\n- a selectbox labeled\
    \ \"Dataset\" to choose a dataset (options derived from sv.datasets.value and\
    \ displayed using dataset_name as the label)\n- a number input labeled \"Number\
    \ of suggested questions\" for the count\n- a subheader \"Search options:\" followed\
    \ by four toggles:\n  - \"Include basic RAG\"\n  - \"Include local search\"\n\
    \  - \"Include global search\"\n  - \"Include drift search\" \n\nBehavior notes:\n\
    - Uses sv as the source of keys and current values, and registers callbacks (on_change)\
    \ to update state when widgets change.\n- The function does not return a value\
    \ (returns None) and renders directly to the UI.\n\nAssumptions:\n- sv implements\
    \ the following attributes and structure:\n  - sv.datasets.value is iterable of\
    \ items with .key\n  - sv.dataset.key is the widget key for the dataset selectbox\n\
    \  - sv.suggested_questions.key is the key for the number input\n  - sv.include_basic_rag.key,\
    \ sv.include_local_search.key, sv.include_global_search.key, sv.include_drift_search.key\
    \ are keys for the toggles\n  - update_dataset, update_basic_rag, update_local_search,\
    \ update_global_search, update_drift_search are defined to handle changes\n\n\
    Raises:\n- AttributeError if any required sv attribute is missing."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 97
  dependencies: []
  called_by: []
- node_id: graphrag/index/text_splitting/check_token_limit.py::check_token_limit
  file: graphrag/index/text_splitting/check_token_limit.py
  name: check_token_limit
  signature: def check_token_limit(text, max_token)
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check whether the input text fits within the specified token limit.\n\
    \nArgs:\n    text (str): The input text to check against the token limit.\n  \
    \  max_token (int): The maximum number of tokens allowed for a single chunk.\n\
    \nReturns:\n    int: 1 if the text can be represented as a single chunk under\
    \ the limit, 0 otherwise.\n\nRaises:\n    Exception: If an error occurs during\
    \ tokenization/splitting using TokenTextSplitter.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 9
  line_end: 15
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage._read_file
  file: graphrag/storage/file_pipeline_storage.py
  name: _read_file
  signature: "def _read_file(\n        self,\n        path: str | Path,\n        as_bytes:\
    \ bool | None = False,\n        encoding: str | None = None,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Read the contents of a file asynchronously.\n\nArgs:\n    path: The\
    \ path to the file to read. (str | Path)\n    as_bytes: When True, read in binary\
    \ mode and return bytes; otherwise read in text mode. (bool | None)\n    encoding:\
    \ Encoding to use when reading as text. If None and as_bytes is False, uses self._encoding.\
    \ (str | None)\n\nReturns:\n    The contents of the file. Returns bytes if as_bytes\
    \ is True, otherwise a string.\n\nRaises:\n    Exceptions raised by the underlying\
    \ file I/O (e.g., open or read errors) may be propagated."
  code_example: null
  example_source: null
  line_start: 102
  line_end: 117
  dependencies: []
  called_by: []
- node_id: graphrag/factory/factory.py::Factory.keys
  file: graphrag/factory/factory.py
  name: keys
  signature: def keys(self) -> list[str]
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get a list of registered strategy names.\n\nArgs:\n    self: The\
    \ instance containing registered strategies.\n\nReturns:\n    list[str]: A list\
    \ of the registered strategy names.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 33
  line_end: 35
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.rank_incomplete_actions
  file: graphrag/query/structured_search/drift_search/state.py
  name: rank_incomplete_actions
  signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
    \ float] | None = None\n    ) -> list[DriftAction]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Rank all incomplete actions in the graph, optionally by a scorer.\n\
    \nArgs:\n    scorer: Callable[[DriftAction], float] | None. A function that takes\
    \ a DriftAction and returns a numeric score. If provided, actions are scored and\
    \ returned sorted by score in descending order. If None, actions are returned\
    \ in a random order.\n\nReturns:\n    list[DriftAction]: The incomplete actions,\
    \ either ranked by score or shuffled.\n\nRaises:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 59
  line_end: 77
  dependencies:
  - graphrag/query/structured_search/drift_search/state.py::find_incomplete_actions
  called_by: []
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_operations
  file: tests/integration/vector_stores/test_lancedb.py
  name: test_vector_store_operations
  signature: def test_vector_store_operations(self, sample_documents)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test basic vector store operations with LanceDB.\n\nArgs:\n    self:\
    \ The test case instance.\n    sample_documents: list[VectorStoreDocument] - Documents\
    \ used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\nRaises:\n\
    \    AssertionError: If any assertion in the test fails."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 125
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_end
  file: graphrag/callbacks/workflow_callbacks.py
  name: pipeline_end
  signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback to signal when the entire pipeline ends.\n\nParameters:\n\
    \    results: list[PipelineRunResult]. A list of PipelineRunResult objects representing\
    \ the results of the pipeline runs.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 23
  line_end: 25
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat_stream
  file: graphrag/language_model/providers/fnllm/models.py
  name: achat_stream
  signature: "def achat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nStream Chat with the Model using the given prompt.\n\nArgs:\n\
    \    prompt (str): The prompt to chat with.\n    history (list[str] | None): Optional\
    \ history to pass to the Model. If provided, the model will consider it.\n   \
    \ kwargs (dict[str, Any], optional): Additional keyword arguments to pass to the\
    \ Model.\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous generator\
    \ that yields non-None strings representing the response.\n\nRaises:\n    Propagates\
    \ exceptions raised by the underlying model call or streaming response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 97
  line_end: 117
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::_base_acompletion
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: _base_acompletion
  signature: 'def _base_acompletion(**kwargs: Any) -> ModelResponse | CustomStreamWrapper'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Merge base_args with provided keyword arguments and invoke the asynchronous\
    \ litellm acompletion.\n\nArgs:\n  kwargs: Any\n      Additional keyword arguments\
    \ to merge with base_args and pass to acompletion after removing the \"name\"\
    \ key if present.\n\nReturns:\n  ModelResponse | CustomStreamWrapper\n      The\
    \ result from the underlying acompletion call.\n\nRaises:\n  Exception\n     \
    \ Exceptions raised by the underlying acompletion."
  code_example: null
  example_source: null
  line_start: 103
  line_end: 109
  dependencies: []
  called_by: []
- node_id: graphrag/config/load_config.py::_search_for_config_in_root_dir
  file: graphrag/config/load_config.py
  name: _search_for_config_in_root_dir
  signature: 'def _search_for_config_in_root_dir(root: str | Path) -> Path | None'
  decorators: []
  raises:
  - FileNotFoundError
  visibility: protected
  docstring: "Resolve the config path from the given root directory.\n\nArgs:\n  \
    \  root: str | Path\n        The path to the root directory containing the config\
    \ file. Searches for a default config file (settings.{yaml,yml,json}).\n\nReturns:\n\
    \    Path | None: The Path to the config file if one exists in the root directory;\
    \ otherwise None.\n\nRaises:\n    FileNotFoundError: If the provided root is not\
    \ a directory."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 46
  dependencies: []
  called_by:
  - graphrag/config/load_config.py::_get_config_path
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.from_string
  file: graphrag/query/context_builder/conversation_history.py
  name: from_string
  signature: 'def from_string(value: str) -> "ConversationRole"'
  decorators:
  - '@staticmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Convert string to ConversationRole.\n\nArgs:\n    value: str.\
    \ The role as a string. Expected values are \"system\", \"user\", or \"assistant\"\
    .\n\nReturns:\n    ConversationRole. The corresponding ConversationRole enum member.\n\
    \nRaises:\n    ValueError: If value is not one of the supported roles.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 27
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/logger/progress.py::ProgressTicker.done
  file: graphrag/logger/progress.py
  name: done
  signature: def done(self) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Mark the progress as done.\n\nIf a callback was provided (self._callback\
    \ is not None), invoke it with a Progress object whose total_items equals the\
    \ configured total (self._num_total) and whose completed_items equals the same\
    \ value, indicating completion. The description from initialization (self._description)\
    \ is preserved.\n\nThis is a bound method; the self parameter is implicit and\
    \ not documented.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 62
  line_end: 71
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_relationship_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_relationship_data
  signature: 'def get_relationship_data(dataset: str, _datasource: Datasource) ->
    pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "Return a dataframe with entity-entity relationship data from the indexed-data.\n\
    \nReads relationship data from the configured table via the provided _datasource\
    \ (config.relationship_table). The function prints the number of relationship\
    \ records and the dataset name as a side effect and is cached with Streamlit's\
    \ cache_data decorator using TTL from config.default_ttl.\n\nArgs:\n    dataset:\
    \ str \u2014 The dataset name to load.\n    _datasource: Datasource \u2014 The\
    \ Datasource descriptor used to access the data from the configured relationship\
    \ table.\n\nReturns:\n    pd.DataFrame \u2014 DataFrame containing the relationship\
    \ data.\n\nRaises:\n    Exception \u2014 If reading from the data source fails."
  code_example: null
  example_source: null
  line_start: 33
  line_end: 38
  dependencies: []
  called_by: []
- node_id: graphrag/config/embeddings.py::create_index_name
  file: graphrag/config/embeddings.py
  name: create_index_name
  signature: "def create_index_name(\n    container_name: str, embedding_name: str,\
    \ validate: bool = True\n) -> str"
  decorators: []
  raises:
  - KeyError
  visibility: public
  docstring: "Create an index name for the embedding store.\n\nWithin any given vector\
    \ store, we can have multiple sets of embeddings organized into projects.\nThe\
    \ container_name parameter is used for this partitioning, and is added as a prefix\
    \ to the index name for differentiation.\n\nThe embedding_name is fixed, with\
    \ the available list defined in graphrag.index.config.embeddings\n\nNote that\
    \ we use dot notation in our names, but many vector stores do not support this\
    \ - so we convert to dashes.\n\nArgs:\n    container_name: The container name\
    \ used as a prefix for differentiation.\n    embedding_name: The embedding name\
    \ to include in the index name.\n    validate: Whether to validate embedding_name\
    \ against the allowed set before constructing the name.\n\nReturns:\n    str:\
    \ The constructed index name, with dots replaced by dashes.\n\nRaises:\n    KeyError:\
    \ If validate is True and embedding_name is not in all_embeddings."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 48
  dependencies: []
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::_get_index_name
  - graphrag/utils/api.py::get_embedding_store
  - tests/unit/utils/test_embeddings.py::test_create_index_name
  - tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_throws
  - tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_does_not_throw
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_noop_text_splitter
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_noop_text_splitter
  signature: def test_noop_text_splitter() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that NoopTextSplitter.split_text returns input unchanged.\n\nThis\
    \ test constructs a NoopTextSplitter and asserts that:\n- split_text(\"some text\"\
    ) yields [\"some text\"]\n- split_text([\"some\", \"text\"]) yields [\"some\"\
    , \"text\"]\n\nReturns:\n    None: This test does not return a value."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 23
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter
  called_by: []
- node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.emit
  file: graphrag/logger/blob_workflow_logger.py
  name: emit
  signature: def emit(self, record) -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Emit a log record to blob storage.\n\nCreates a JSON structure from\
    \ the given log record, including type (\"log\", \"warning\", or \"error\") based\
    \ on the record level, and the main message. Optional fields such as details,\
    \ cause (from exc_info), and stack (if present) are added if they exist. The resulting\
    \ payload is passed to _write_log for persistence in Azure Blob storage. If writing\
    \ fails with OSError or ValueError, those exceptions are not propagated; they\
    \ are handled by self.handleError(record). This method is part of the BlobWorkflowLogger\
    \ class and interacts with _write_log and the logic that reinitializes the blob\
    \ client when the block counter reaches the maximum.\n\nArgs:\n    record: logging.LogRecord\
    \ The log record to emit to blob storage.\n\nReturns:\n    None\n\nRaises:\n \
    \   OSError: Not raised; errors are caught and delegated to self.handleError(record)\
    \ instead.\n    ValueError: Not raised; errors are caught and delegated to self.handleError(record)\
    \ instead."
  code_example: null
  example_source: null
  line_start: 72
  line_end: 91
  dependencies:
  - graphrag/logger/blob_workflow_logger.py::_get_log_type
  - graphrag/logger/blob_workflow_logger.py::_write_log
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
  file: graphrag/language_model/providers/litellm/request_wrappers/with_logging.py
  name: with_logging
  signature: "def with_logging(\n    *,\n    sync_fn: LitellmRequestFunc,\n    async_fn:\
    \ AsyncLitellmRequestFunc,\n) -> tuple[LitellmRequestFunc, AsyncLitellmRequestFunc]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Wrap the provided synchronous and asynchronous Litellm request functions\
    \ with logging for exceptions.\n\nArgs\n----\n    sync_fn: LitellmRequestFunc\n\
    \        The synchronous chat/embedding request function to wrap.\n    async_fn:\
    \ AsyncLitellmRequestFunc\n        The asynchronous chat/embedding request function\
    \ to wrap.\n\nReturns\n-------\ntuple[LitellmRequestFunc, AsyncLitellmRequestFunc]\n\
    \        A tuple containing the wrapped synchronous and asynchronous chat/embedding\
    \ request functions.\n\nRaises\n------\nException\n        If either wrapped function\
    \ raises an exception, the exception is logged via logger.exception and re-raised."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 56
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.load_documents
  file: tests/integration/vector_stores/test_factory.py
  name: load_documents
  signature: def load_documents(self, documents, overwrite=True)
  decorators: []
  raises: []
  visibility: public
  docstring: "Load documents into the vector store.\n\nArgs:\n  documents (list[VectorStoreDocument]):\
    \ List of VectorStoreDocument objects to load into the vector store.\n  overwrite\
    \ (bool): If True, overwrite existing data in the vector store; otherwise, preserve\
    \ existing data.\n\nReturns:\n  None\n\nNotes:\n  - This base implementation is\
    \ a placeholder and intentionally does nothing. Subclasses should override to\
    \ provide concrete loading behavior.\n  - No input validation is performed in\
    \ this base method.\n  - If documents is empty, the method performs no action.\n\
    \  - Overwrite semantics are intended for the concrete implementation; callers\
    \ should ensure documents meet preconditions (e.g., required fields, IDs) as required\
    \ by the concrete store."
  code_example: null
  example_source: null
  line_start: 134
  line_end: 135
  dependencies: []
  called_by: []
- node_id: graphrag/logger/progress.py::progress_iterable
  file: graphrag/logger/progress.py
  name: progress_iterable
  signature: "def progress_iterable(\n    iterable: Iterable[T],\n    progress: ProgressHandler\
    \ | None,\n    num_total: int | None = None,\n    description: str = \"\",\n)\
    \ -> Iterable[T]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Wrap an iterable with a progress reporter. After each item is yielded,\
    \ the progress callback will be called with a Progress object describing the total\
    \ number of items and how many have been completed so far. If the callback is\
    \ None, no updates will be emitted.\n\nThe Progress object provides:\n- total_items:\
    \ total number of items (as given by num_total)\n- completed_items: number of\
    \ items yielded so far\n- description: optional description included with updates\n\
    \nNote: If num_total is None, the total will be inferred by consuming the iterable\
    \ (via list(iterable)), which may exhaust inputs that cannot be re-iterated. To\
    \ avoid this, pass a known num_total or ensure the iterable can be iterated multiple\
    \ times.\n\nArgs:\n    iterable (Iterable[T]): The input iterable to wrap. Each\
    \ item yielded is unchanged.\n    progress (ProgressHandler | None): Callback\
    \ invoked with a Progress instance after each item is yielded. If None, updates\
    \ are suppressed.\n    num_total (int | None): Total number of items. If None,\
    \ inferred by consuming the iterable (may exhaust it).\n    description (str):\
    \ Optional description to attach to progress updates.\n\nReturns:\n    Iterable[T]:\
    \ A generator that yields the same items as the input iterable, while updating\
    \ progress after each item is yielded."
  code_example: null
  example_source: null
  line_start: 81
  line_end: 95
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
- node_id: graphrag/query/indexer_adapters.py::embed_community_reports
  file: graphrag/query/indexer_adapters.py
  name: embed_community_reports
  signature: "def embed_community_reports(\n    reports_df: pd.DataFrame,\n    embedder:\
    \ EmbeddingModel,\n    source_col: str = \"full_content\",\n    embedding_col:\
    \ str = \"full_content_embedding\",\n) -> pd.DataFrame"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Embed a source column of the reports dataframe using the given embedder.\n\
    \nArgs:\n    reports_df (pd.DataFrame): The reports dataframe to embed. This function\
    \ may mutate the dataframe in place by adding a new embedding column if it does\
    \ not already exist.\n    embedder (EmbeddingModel): The model used to generate\
    \ embeddings from the content of the source column.\n    source_col (str): Name\
    \ of the column in reports_df that contains the text to embed. Defaults to \"\
    full_content\".\n    embedding_col (str): Name of the column to store the generated\
    \ embeddings. If this column does not exist, it will be created and populated.\
    \ Defaults to \"full_content_embedding\".\n\nReturns:\n    pd.DataFrame: The input\
    \ DataFrame, augmented with embedding_col. The same DataFrame object is returned\
    \ (in-place mutation when embedding_col is created).\n\nRaises:\n    ValueError:\
    \ If the source_col is missing from reports_df.\n\nExamples:\n    # Basic usage\n\
    \    df = embed_community_reports(reports_df, embedder)"
  code_example: null
  example_source: null
  line_start: 219
  line_end: 235
  dependencies: []
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_reports
- node_id: graphrag/index/update/communities.py::_update_and_merge_community_reports
  file: graphrag/index/update/communities.py
  name: _update_and_merge_community_reports
  signature: "def _update_and_merge_community_reports(\n    old_community_reports:\
    \ pd.DataFrame,\n    delta_community_reports: pd.DataFrame,\n    community_id_mapping:\
    \ dict,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update and merge old and delta community reports into a single DataFrame\
    \ aligned to the final columns.\n\nArgs:\n    old_community_reports: The old community\
    \ reports.\n    delta_community_reports: The delta community reports.\n    community_id_mapping:\
    \ The mapping from original delta community IDs to final IDs.\n\nReturns:\n  \
    \  pd.DataFrame: The updated community reports aligned to COMMUNITY_REPORTS_FINAL_COLUMNS.\n\
    \nRaises:\n    KeyError: If required columns such as 'community' or 'parent' are\
    \ missing from input DataFrames.\n    ValueError: If a column intended to be numeric\
    \ cannot be cast to int when applying the mapping.\n    TypeError: If inputs are\
    \ not DataFrames or the mapping is not a dict-like."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 151
  dependencies: []
  called_by:
  - graphrag/index/workflows/update_community_reports.py::_update_community_reports
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.item_filter
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: item_filter
  signature: 'def item_filter(item: dict[str, Any]) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Determine whether the given item matches the current file_filter or\
    \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
    \ The keys used by file_filter are read from this dict.\n\nReturns:\n    bool:\
    \ True if no file_filter is defined or if all key/value pairs in file_filter match\
    \ the corresponding fields in item using re.search.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 148
  line_end: 154
  dependencies: []
  called_by: []
- node_id: tests/smoke/test_fixtures.py::wrapper
  file: tests/smoke/test_fixtures.py
  name: wrapper
  signature: def wrapper(*args, **kwargs)
  decorators:
  - '@wraps(func)'
  raises: []
  visibility: public
  docstring: "Wrapper around the wrapped test function that forwards arguments and\
    \ cleans up after execution when not skipped.\n\nArgs:\n    args: Positional arguments\
    \ forwarded to the wrapped function.\n    kwargs: Keyword arguments forwarded\
    \ to the wrapped function; must include input_path used for cleanup.\n\nReturns:\n\
    \    Any: The return value of the wrapped function.\n\nRaises:\n    AssertionError:\
    \ The wrapped function's AssertionError is re-raised."
  code_example: null
  example_source: null
  line_start: 76
  line_end: 85
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_base_completions
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: _create_base_completions
  signature: "def _create_base_completions(\n    model_config: \"LanguageModelConfig\"\
    ,\n) -> tuple[FixedModelCompletion, AFixedModelCompletion]"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Wrap the base litellm completion function with the model configuration.\n\
    \nArgs:\n    model_config: The configuration for the language model.\n\nReturns:\n\
    \    A tuple containing the synchronous and asynchronous completion functions.\n\
    \nRaises:\n    ValueError"
  code_example: null
  example_source: null
  line_start: 48
  line_end: 111
  dependencies: []
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
- node_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: "def __call__(\n        self,\n        *,\n        request_id: str |\
    \ None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n    \
    \    # Optional params\n        dimensions: int | None = None,\n        encoding_format:\
    \ str | None = None,\n        timeout: int = 600,  # default to 10 minutes\n \
    \       # set api_base, api_version, api_key\n        api_base: str | None = None,\n\
    \        api_version: str | None = None,\n        api_key: str | None = None,\n\
    \        api_type: str | None = None,\n        caching: bool = False,\n      \
    \  user: str | None = None,\n        **kwargs: Any,\n    ) -> EmbeddingResponse"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Embedding function for a model configured elsewhere.\n\nCompute embeddings\
    \ for a batch of inputs using a pre-configured model (no model parameter is required).\
    \ This synchronous embedding function mirrors litellm.embedding but omits the\
    \ model argument, relying on the model configuration.\n\nArgs:\n  request_id (str\
    \ | None): Optional request identifier.\n  input (list): List of inputs to embed.\n\
    \  dimensions (int | None): Optional embedding dimensions to request. If None,\
    \ the model's default is used.\n  encoding_format (str | None): Optional encoding\
    \ format to return embeddings in. If None, the default format is used.\n  timeout\
    \ (int): Timeout in seconds for the request (default 600).\n  api_base (str |\
    \ None): Optional API base URL.\n  api_version (str | None): Optional API version.\n\
    \  api_key (str | None): Optional API key for authentication.\n  api_type (str\
    \ | None): Optional API type.\n  caching (bool): Enable or disable caching of\
    \ embeddings. Default is False.\n  user (str | None): Optional user identifier\
    \ for the request.\n  kwargs (Any): Additional keyword arguments passed to the\
    \ underlying embedding call.\n\nReturns:\n  EmbeddingResponse: The embedding response\
    \ object containing the embeddings and related metadata.\n\nRaises:\n  ValueError:\
    \ If input is not a list.\n  TimeoutError: If the embedding request times out.\n\
    \  Exception: If an error occurs during the embedding request."
  code_example: null
  example_source: null
  line_start: 159
  line_end: 178
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_compute_community_weights
  file: graphrag/query/context_builder/community_context.py
  name: _compute_community_weights
  signature: "def _compute_community_weights(\n    community_reports: list[CommunityReport],\n\
    \    entities: list[Entity] | None,\n    weight_attribute: str = \"occurrence\"\
    ,\n    normalize: bool = True,\n) -> list[CommunityReport]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Compute weights for communities based on the number of text units associated\
    \ with entities in each community.\n\nArgs:\n  community_reports: List[CommunityReport]\n\
    \      Reports for communities to assign weights to.\n  entities: list[Entity]\
    \ | None\n      Entities that reference community_ids and contain text_unit_ids.\
    \ Text units are aggregated by community_id across all entities.\n  weight_attribute:\
    \ str\n      Name of the attribute stored on each CommunityReport's attributes\
    \ dictionary to hold the computed weight. Defaults to \"occurrence\".\n  normalize:\
    \ bool\n      If True, normalize weights by the maximum weight across all reports\
    \ so weights lie in [0, 1].\n\nReturns:\n  list[CommunityReport]\n      The updated\
    \ list of CommunityReport objects with the computed weight stored under the specified\
    \ weight_attribute in report.attributes. Weights are normalized when normalize\
    \ is True."
  code_example: null
  example_source: null
  line_start: 189
  line_end: 225
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: tests/unit/config/utils.py::assert_embed_graph_configs
  file: tests/unit/config/utils.py
  name: assert_embed_graph_configs
  signature: "def assert_embed_graph_configs(\n    actual: EmbedGraphConfig, expected:\
    \ EmbedGraphConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that actual and expected EmbedGraphConfig instances have equal\
    \ values for their core fields.\n\nArgs:\n    actual: EmbedGraphConfig. The actual\
    \ EmbedGraphConfig instance produced by the code under test.\n    expected: EmbedGraphConfig.\
    \ The expected EmbedGraphConfig instance to compare against.\n\nReturns:\n   \
    \ None. The function does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any of the fields enabled, dimensions, num_walks, walk_length, window_size,\
    \ iterations, random_seed, or use_lcc differ between actual and expected."
  code_example: null
  example_source: null
  line_start: 186
  line_end: 196
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.mock_embedder
  file: tests/integration/vector_stores/test_lancedb.py
  name: mock_embedder
  signature: 'def mock_embedder(text: str) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "A simple text embedder used for testing that returns a fixed embedding\
    \ vector. The embedding is independent of the input text and always returns [0.1,\
    \ 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text (str): Input text to embed.\n\nReturns:\n\
    \    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4, 0.5].\n\nRaises:\n\
    \    None: This function does not raise any exceptions."
  code_example: null
  example_source: null
  line_start: 249
  line_end: 250
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/search.py::get_ids_per_key
  file: unified-search-app/app/ui/search.py
  name: get_ids_per_key
  signature: 'def get_ids_per_key(str_response: str, key: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get IDs associated with a given key from a string response.\n\nArgs:\n\
    \  str_response (str): The string to search for occurrences of the pattern 'key\
    \ (<numbers>)'.\n  key (str): The prefix text preceding the parenthesized list\
    \ of IDs.\n\nReturns:\n  List[str]: The IDs extracted from the parentheses after\
    \ the last matching occurrence of the key. If no occurrences are found, returns\
    \ an empty list.\n\nNotes:\n  - If multiple matches are found, only the IDs from\
    \ the last match are returned.\n  - IDs are strings and may contain whitespace;\
    \ you may trim each element if needed. The IDs are extracted from within the first\
    \ pair of parentheses following the key and are split on commas.\n  - The function\
    \ does not perform type checking and assumes inputs are strings; it does not raise\
    \ a TypeError on invalid types.\n\nPattern:\n  A regular expression that matches\
    \ a parenthesized list of digits separated by commas, optionally followed by ,\
    \ +more."
  code_example: null
  example_source: null
  line_start: 172
  line_end: 184
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::get_context_string
  file: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py
  name: get_context_string
  signature: "def get_context_string(\n    text_units: list[dict],\n    sub_community_reports:\
    \ list[dict] | None = None,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Concatenate structured data into a context string.\n\nArgs:\n    text_units\
    \ (list[dict]): List of text unit dictionaries to include in the context. Each\
    \ dictionary should have an \"id\" key with a non-empty value.\n    sub_community_reports\
    \ (list[dict] | None): Optional list of dictionaries for sub-community reports\
    \ to include at the top. Only reports containing a non-empty community id (defined\
    \ by schemas.COMMUNITY_ID) are considered.\n\nReturns:\n    str: The context string\
    \ built by optionally including a reports section followed by a sources section,\
    \ separated by blank lines.\n\nRaises:\n    None: This function does not raise\
    \ exceptions."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 55
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
- node_id: graphrag/query/context_builder/conversation_history.py::QATurn.__str__
  file: graphrag/query/context_builder/conversation_history.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return string representation of the QA turn.\n\nArgs:\n    self: QATurn\
    \ instance to stringify.\n\nReturns:\n    str: The string representation of the\
    \ QA turn. If there are assistant answers, the string is\n        \"Question:\
    \ <user_query.content>\\nAnswer: <answers>\"; otherwise, it's\n        \"Question:\
    \ <user_query.content>\"."
  code_example: null
  example_source: null
  line_start: 80
  line_end: 87
  dependencies:
  - graphrag/query/context_builder/conversation_history.py::get_answer_text
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::_generate_report
  file: graphrag/index/operations/summarize_communities/summarize_communities.py
  name: _generate_report
  signature: "def _generate_report(\n    runner: CommunityReportsStrategy,\n    callbacks:\
    \ WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy: dict,\n    community_id:\
    \ int,\n    community_level: int,\n    community_context: str,\n) -> CommunityReport\
    \ | None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Generate a report for a single community.\n\nArgs:\n  runner: The strategy\
    \ function used to generate the report for the community.\n  callbacks: Callbacks\
    \ to use during report generation.\n  cache: Cache instance used by the report\
    \ generation process.\n  strategy: Strategy configuration for the report generation.\n\
    \  community_id: Identifier of the community for which to generate the report.\n\
    \  community_level: Level of the community.\n  community_context: Context string\
    \ describing the community.\nReturns:\n  CommunityReport | None\n      The generated\
    \ CommunityReport, or None if no report was produced.\nRaises:\n  Exception: Propagates\
    \ exceptions raised by the underlying runner."
  code_example: null
  example_source: null
  line_start: 97
  line_end: 114
  dependencies: []
  called_by:
  - graphrag/index/operations/summarize_communities/summarize_communities.py::run_generate
- node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_start
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: workflow_start
  signature: 'def workflow_start(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
    \ The name of the workflow starting.\n    instance (object): The workflow instance\
    \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 20
  line_end: 21
  dependencies: []
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._get_prefix
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: _get_prefix
  signature: 'def _get_prefix(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"Get the prefix of the filename key.\"\n\nArgs:\n  key: The filename\
    \ key as a string. The prefix is the substring before the first dot.\n\nReturns:\n\
    \  str: The prefix portion of the key (substring before the first dot)."
  code_example: null
  example_source: null
  line_start: 337
  line_end: 339
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_local_search_configs
  file: tests/unit/config/utils.py
  name: assert_local_search_configs
  signature: "def assert_local_search_configs(\n    actual: LocalSearchConfig, expected:\
    \ LocalSearchConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two LocalSearchConfig objects have equal local search configuration\
    \ values.\n\nArgs:\n    actual: LocalSearchConfig to compare against expected.\n\
    \    expected: LocalSearchConfig to compare with actual.\n\nReturns:\n    None\n\
    \nRaises:\n    AssertionError: If any corresponding fields differ between actual\
    \ and expected."
  code_example: null
  example_source: null
  line_start: 315
  line_end: 326
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_update_index_output_base_dir
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_update_index_output_base_dir
  signature: def _validate_update_index_output_base_dir(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Validate the update index output base directory.\n\nArgs:\n  self (GraphRagConfig):\
    \ The instance of the graph rag configuration model containing the update_index_output\
    \ and root_dir attributes.\n\nReturns:\n  None. This method updates update_index_output.base_dir\
    \ to an absolute path derived from root_dir when the update_index_output.type\
    \ is file.\n\nRaises:\n  ValueError: If update_index_output.type is file and update_index_output.base_dir\
    \ is blank or whitespace."
  code_example: null
  example_source: null
  line_start: 209
  line_end: 217
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.get_user_turns
  file: graphrag/query/context_builder/conversation_history.py
  name: get_user_turns
  signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
  decorators: []
  raises: []
  visibility: public
  docstring: '"""Get the last user turns in the conversation history.\n\nArgs:\n    max_user_turns:
    Maximum number of user turns to include from history. If None, include all user
    turns. Default is 1.\n\nReturns:\n    list[str]: A list of user turn contents,
    in reverse chronological order (most recent first), up to max_user_turns.\n\nRaises:\n    None\n"""'
  code_example: null
  example_source: null
  line_start: 139
  line_end: 147
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__init__
  file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n  \
    \      extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
    \ | None = None,\n        max_report_length: int | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a CommunityReportsExtractor with the provided configuration.\n\
    \nArgs:\n  model_invoker: ChatModel\n    The model invoker used to run prompts.\n\
    \  extraction_prompt: str | None\n    Custom prompt to use for extraction. If\
    \ None, defaults to COMMUNITY_REPORT_PROMPT.\n  on_error: ErrorHandlerFn | None\n\
    \    Function to handle errors. If None, a no-op is used.\n  max_report_length:\
    \ int | None\n    Maximum length of the generated report. If None, defaults to\
    \ 1500.\n\nReturns:\n  None"
  code_example: null
  example_source: null
  line_start: 59
  line_end: 70
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::validate_blob_container_name
  file: graphrag/storage/blob_pipeline_storage.py
  name: validate_blob_container_name
  signature: 'def validate_blob_container_name(container_name: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Validate a blob container name against Azure rules.\n\nThis function\
    \ verifies the following constraints:\n- The name length is between 3 and 63 characters.\n\
    - The name starts with a letter or a number.\n- All characters are lowercase letters,\
    \ numbers, or hyphen.\n- The name does not contain consecutive hyphens.\n- The\
    \ name does not end with a hyphen.\n\nArgs:\n    container_name (str): The blob\
    \ container name to be validated.\n\nReturns:\n    bool: True if the container\
    \ name is valid.\n    ValueError: If the input is invalid, a ValueError instance\
    \ describing the reason is returned (note: the function does not raise exceptions;\
    \ invalid input signals are returned)."
  code_example: null
  example_source: null
  line_start: 315
  line_end: 365
  dependencies: []
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::join_path
  file: graphrag/storage/file_pipeline_storage.py
  name: join_path
  signature: 'def join_path(file_path: str, file_name: str) -> Path'
  decorators: []
  raises: []
  visibility: public
  docstring: "Join a base path with the relative components of a file name to form\
    \ a new pathlib.Path.\n\nThis function uses the parent directory and the file\
    \ name from file_name and joins them with file_path to produce the final path.\
    \ Note that if file_name is an absolute path, pathlib will discard the leading\
    \ path components of file_path and the result will correspond to the absolute\
    \ path described by file_name's components.\n\nArgs:\n    file_path (str): Base\
    \ path to join with the file's relative components (parent and name).\n    file_name\
    \ (str): Path-like string; its parent directory and file name are used for the\
    \ join.\n\nReturns:\n    pathlib.Path: The resulting Path object."
  code_example: null
  example_source: null
  line_start: 169
  line_end: 171
  dependencies: []
  called_by:
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed
  file: graphrag/language_model/providers/fnllm/models.py
  name: aembed
  signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Embed the given text using the Model.\n\nArgs:\n    text: The\
    \ text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\nReturns:\n\
    \    The embeddings of the text.\n\nRaises:\n    ValueError: If no embeddings\
    \ are found in the response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 195
  line_end: 212
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/create_final_text_units.py::_relationships
  file: graphrag/index/workflows/create_final_text_units.py
  name: _relationships
  signature: 'def _relationships(df: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Compute mapping of text units to the relationship IDs that reference\
    \ them.\n\nArgs:\n    df: pd.DataFrame containing the columns \"id\" and \"text_unit_ids\"\
    .\n\nReturns:\n    pd.DataFrame: DataFrame with columns \"id\" and \"relationship_ids\"\
    ; for each text_unit_id, relationship_ids is the list of unique ids referencing\
    \ that text unit.\n\nRaises:\n    KeyError: If the required columns \"id\" or\
    \ \"text_unit_ids\" are missing from df."
  code_example: null
  example_source: null
  line_start: 98
  line_end: 107
  dependencies: []
  called_by:
  - graphrag/index/workflows/create_final_text_units.py::create_final_text_units
- node_id: tests/unit/config/utils.py::assert_summarize_descriptions_configs
  file: tests/unit/config/utils.py
  name: assert_summarize_descriptions_configs
  signature: "def assert_summarize_descriptions_configs(\n    actual: SummarizeDescriptionsConfig,\
    \ expected: SummarizeDescriptionsConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Assert that two SummarizeDescriptionsConfig objects have identical\
    \ fields: prompt, max_length, strategy, and model_id.\n\nArgs:\n    actual (SummarizeDescriptionsConfig):\
    \ The actual config to compare.\n    expected (SummarizeDescriptionsConfig): The\
    \ expected config to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If any of the compared fields differ between actual and expected.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 272
  line_end: 278
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: unified-search-app/app/ui/full_graph.py::create_full_graph_ui
  file: unified-search-app/app/ui/full_graph.py
  name: create_full_graph_ui
  signature: 'def create_full_graph_ui(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create and render the full graph UI from the provided session variables.\n\
    \nArgs:\n    sv (SessionVariables): Container with entities, communities, and\
    \ graph_community_level used to construct and filter the graph.\n\nReturns:\n\
    \    alt.Chart: The Altair chart object representing the full graph UI. The function\
    \ also renders the chart via Streamlit."
  code_example: null
  example_source: null
  line_start: 12
  line_end: 56
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.search_by_id
  file: graphrag/vector_stores/base.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
    \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
    \ corresponding to the provided id."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 90
  dependencies: []
  called_by: []
- node_id: graphrag/index/utils/stable_lcc.py::normalize_node_names
  file: graphrag/index/utils/stable_lcc.py
  name: normalize_node_names
  signature: 'def normalize_node_names(graph: nx.Graph | nx.DiGraph) -> nx.Graph |
    nx.DiGraph'
  decorators: []
  raises: []
  visibility: public
  docstring: "Normalize node names for a graph by applying HTML unescaping, converting\
    \ to uppercase, and trimming whitespace on each node label.\n\nArgs:\n  graph\
    \ (nx.Graph | nx.DiGraph): Input graph whose node names will be normalized.\n\n\
    Returns:\n  nx.Graph | nx.DiGraph: The input graph with node names normalized\
    \ (uppercased, stripped of whitespace, and HTML entities unescaped)."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 67
  dependencies: []
  called_by:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
- node_id: tests/verbs/util.py::compare_outputs
  file: tests/verbs/util.py
  name: compare_outputs
  signature: "def compare_outputs(\n    actual: pd.DataFrame, expected: pd.DataFrame,\
    \ columns: list[str] | None = None\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compare the actual and expected dataframes, optionally specifying columns\
    \ to compare. This function uses pandas.testing.assert_series_equal to compare\
    \ columns and intentionally omits the id column from value checks. If a mismatch\
    \ is found, the function prints the Expected and Actual values for debugging before\
    \ raising an AssertionError.\n\nArgs:\n    actual: The actual DataFrame produced\
    \ by the workflow.\n    expected: The expected DataFrame against which to compare\
    \ the actual DataFrame.\n    columns: Optional list of column names to compare.\
    \ If None, all columns from expected are compared.\n\nReturns:\n    None\n\nRaises:\n\
    \    AssertionError: If the number of rows differs or any compared column's values\
    \ (excluding id) differ, or if a column listed in columns is not present in actual."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 86
  dependencies: []
  called_by:
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_communities.py::test_create_communities
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
- node_id: graphrag/index/operations/summarize_communities/strategies.py::_run_extractor
  file: graphrag/index/operations/summarize_communities/strategies.py
  name: _run_extractor
  signature: "def _run_extractor(\n    model: ChatModel,\n    community: str | int,\n\
    \    input: str,\n    level: int,\n    args: StrategyConfig,\n) -> CommunityReport\
    \ | None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Run the CommunityReportsExtractor to produce a CommunityReport from\
    \ the given input.\n\nArgs:\n  model (ChatModel): The chat model instance used\
    \ to perform extraction.\n  community (str | int): Identifier for the community\
    \ being processed.\n  input (str): The input text to extract information from.\n\
    \  level (int): The reporting level to assign to the resulting CommunityReport.\n\
    \  args (StrategyConfig): Strategy configuration containing optional keys such\
    \ as extraction_prompt and max_report_length.\n\nReturns:\n  CommunityReport |\
    \ None: The constructed CommunityReport, or None if no structured report is produced\
    \ or an error occurs during extraction."
  code_example: null
  example_source: null
  line_start: 46
  line_end: 85
  dependencies:
  - graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor
  - graphrag/index/operations/summarize_communities/typing.py::CommunityReport
  - graphrag/index/operations/summarize_communities/typing.py::Finding
  called_by:
  - graphrag/index/operations/summarize_communities/strategies.py::run_graph_intelligence
- node_id: tests/unit/config/utils.py::get_default_graphrag_config
  file: tests/unit/config/utils.py
  name: get_default_graphrag_config
  signature: 'def get_default_graphrag_config(root_dir: str | None = None) -> GraphRagConfig'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a GraphRagConfig instance configured with the default Graphrag\
    \ settings.\n\nArgs:\n    root_dir: Optional[str] - root directory to include\
    \ in the returned GraphRagConfig. If None, the root_dir key is not set.\n\nReturns:\n\
    \    GraphRagConfig: The GraphRagConfig created by merging graphrag_config_defaults\
    \ with DEFAULT_MODEL_CONFIG, and including root_dir when provided."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 65
  dependencies:
  - graphrag/config/models/graph_rag_config.py::GraphRagConfig
  called_by:
  - tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
  - tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
  - tests/unit/config/test_config.py::test_default_config
  - tests/unit/config/test_config.py::test_load_minimal_config
  - tests/unit/config/test_config.py::test_load_config_with_cli_overrides
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.child
  file: graphrag/storage/blob_pipeline_storage.py
  name: child
  signature: 'def child(self, name: str | None) -> "PipelineStorage"'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None): Optional\
    \ name for the child storage. If None, the current instance is returned.\n\nReturns:\n\
    \    PipelineStorage: The current instance when name is None; otherwise a new\
    \ BlobPipelineStorage configured with base_dir set to the path formed by joining\
    \ the current path prefix and name."
  code_example: null
  example_source: null
  line_start: 273
  line_end: 284
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.__init__
  file: graphrag/language_model/providers/litellm/services/retry/retry.py
  name: __init__
  signature: 'def __init__(self, /, **kwargs: Any)'
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: protected
  docstring: "Initialize a Retry subclass.\n\nArgs:\n  kwargs (Any): Arbitrary keyword\
    \ arguments for subclass initialization.\n\nReturns:\n  None\n\nRaises:\n  NotImplementedError:\
    \ If subclass does not implement __init__."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 17
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_prep.py::get_covariate_data
  file: unified-search-app/app/knowledge_loader/data_prep.py
  name: get_covariate_data
  signature: 'def get_covariate_data(dataset: str, _datasource: Datasource) -> pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=config.default_ttl)'
  raises: []
  visibility: public
  docstring: "\"\"\"Return a dataframe with covariate data from the indexed-data.\n\
    \nArgs:\n    dataset (str): The dataset identifier to load covariates for.\n \
    \   _datasource (Datasource): The data source to query for covariates.\n\nReturns:\n\
    \    pd.DataFrame: A DataFrame containing covariate data loaded for the specified\
    \ dataset.\n\nRaises:\n    Exception: If the underlying data source read operation\
    \ fails.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 42
  line_end: 47
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.aretry
  file: graphrag/language_model/providers/litellm/services/retry/retry.py
  name: aretry
  signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
    \        **kwargs: Any,\n    ) -> Any"
  decorators:
  - '@abstractmethod'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Retry an asynchronous function.\n\nArgs:\n  func (Callable[..., Awaitable[Any]]):\
    \ The asynchronous function to retry.\n  kwargs (Any): Additional keyword arguments\
    \ to pass to the function.\n\nReturns:\n  Any: The result of the awaited function.\n\
    \nRaises:\n  NotImplementedError: Subclasses must implement this method"
  code_example: null
  example_source: null
  line_start: 26
  line_end: 33
  dependencies: []
  called_by: []
- node_id: graphrag/language_model/manager.py::ModelManager.get_embedding_model
  file: graphrag/language_model/manager.py
  name: get_embedding_model
  signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"\nRetrieve the EmbeddingsLLM instance registered under the given\
    \ name.\n\nArgs:\n    name (str): Unique identifier for the EmbeddingsLLM instance.\n\
    \nReturns:\n    EmbeddingModel: The EmbeddingModel instance registered under the\
    \ name.\n\nRaises:\n    ValueError: If no EmbeddingsLLM is registered under the\
    \ name.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 92
  line_end: 103
  dependencies: []
  called_by: []
- node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_root_dir
  file: graphrag/config/models/graph_rag_config.py
  name: _validate_root_dir
  signature: def _validate_root_dir(self) -> None
  decorators: []
  raises:
  - FileNotFoundError
  visibility: protected
  docstring: "Validate the root directory.\n\nArgs:\n  self: GraphRagConfig. The instance\
    \ containing the root_dir attribute.\n\nReturns:\n  None. This method updates\
    \ self.root_dir to an absolute path of an existing directory, defaulting to the\
    \ current working directory if root_dir is blank.\n\nRaises:\n  FileNotFoundError:\
    \ If the resolved root_dir is not an existing directory."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 73
  dependencies: []
  called_by: []
- node_id: graphrag/logger/factory.py::create_blob_logger
  file: graphrag/logger/factory.py
  name: create_blob_logger
  signature: def create_blob_logger(**kwargs) -> logging.Handler
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a blob storage-based logger.\n\nArgs:\n    kwargs: The keyword\
    \ arguments for configuring the blob logger.\n        connection_string: The Azure\
    \ Blob Storage connection string.\n        container_name: The name of the blob\
    \ container.\n        base_dir: The base directory inside the container where\
    \ logs should be stored.\n        storage_account_blob_url: The URL of the blob\
    \ storage account used by the logger.\n\nReturns:\n    logging.Handler: A configured\
    \ BlobWorkflowLogger instance.\n\nRaises:\n    KeyError: If required keys (connection_string,\
    \ container_name, base_dir, storage_account_blob_url) are missing from kwargs."
  code_example: null
  example_source: null
  line_start: 99
  line_end: 108
  dependencies:
  - graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.has
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously determine whether the contents for the given filename\
    \ key exist in Cosmos DB storage.\n\nThis coroutine checks existence by querying\
    \ the Cosmos DB container. For parquet files, existence is determined by a prefix\
    \ match on the item id using STARTSWITH, since parquet data is stored with a prefixed\
    \ id. For non-parquet keys, existence is determined by an exact id match (c.id\
    \ == key).\n\nInitialization/readiness: If the storage has not been initialized\
    \ (database or container client not available), the method returns False without\
    \ performing I/O.\n\nAsync context: This is an async function and should be awaited\
    \ by callers. In this implementation it does not contain explicit await expressions;\
    \ the actual I/O occurs through the Cosmos client.\n\nArgs:\n    key (str): The\
    \ filename key to check for existence in Cosmos DB storage.\n\nReturns:\n    bool:\
    \ True if the item(s) exist in Cosmos DB storage, otherwise False. Parquet keys\
    \ return True if any item exists with an id starting with the generated prefix;\
    \ non-parquet keys return True only when exactly one matching item exists.\n\n\
    Raises:\n    Exception: Propagates exceptions raised by the underlying Cosmos\
    \ DB SDK during query/communication with the service."
  code_example: null
  example_source: null
  line_start: 281
  line_end: 296
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_get_prefix
  called_by: []
- node_id: graphrag/language_model/factory.py::ModelFactory.register_chat
  file: graphrag/language_model/factory.py
  name: register_chat
  signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
    -> None'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a ChatModel implementation in the registry.\n\nRegisters a\
    \ ChatModel implementation in the internal _chat_registry mapping for the specified\
    \ model_type.\n\nArgs:\n    cls: The ModelFactory class.\n    model_type: str\n\
    \        The unique identifier for the ChatModel implementation to register.\n\
    \    creator: Callable[..., ChatModel]\n        A callable that returns a ChatModel\
    \ instance when invoked.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 30
  line_end: 32
  dependencies: []
  called_by: []
- node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.search_by_id
  file: tests/integration/vector_stores/test_factory.py
  name: search_by_id
  signature: def search_by_id(self, id)
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nSearch for a document by id.\n\nArgs:\n    id (str): The identifier\
    \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
    \ corresponding to the provided id.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 146
  line_end: 149
  dependencies:
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: tests/integration/logging/test_factory.py::test_register_and_create_custom_logger
  file: tests/integration/logging/test_factory.py
  name: test_register_and_create_custom_logger
  signature: def test_register_and_create_custom_logger()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test registering and creating a custom logger type.


    This test registers a custom logger type named "custom" using LoggerFactory.register
    with a factory function, creates a logger via LoggerFactory.create_logger("custom",
    {}), and asserts that the factory was invoked and the returned logger is the expected
    instance. It also asserts that the created logger has the initialized attribute
    set to True and that "custom" is present in the list of registered logger types
    and is reported as supported.'
  code_example: null
  example_source: null
  line_start: 34
  line_end: 53
  dependencies: []
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.convert_reports_to_df
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: convert_reports_to_df
  signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
  decorators:
  - '@staticmethod'
  raises:
  - ValueError
  visibility: public
  docstring: "Convert a list of CommunityReport objects to a pandas DataFrame.\n\n\
    Args:\n    reports (list[CommunityReport]): List of CommunityReport objects.\n\
    \nReturns:\n    pd.DataFrame: DataFrame with report data.\n\nRaises:\n    ValueError:\
    \ If some reports are missing full content or full content embeddings."
  code_example: null
  example_source: null
  line_start: 101
  line_end: 140
  dependencies: []
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.search_by_id
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: search_by_id
  signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the first document with its id set to the provided id.\n\nArgs:\n\
    \    id (str): The identifier to assign to the first stored document before returning.\n\
    \nReturns:\n    VectorStoreDocument: The document whose id is set to the provided\
    \ id."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 63
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.register
  file: graphrag/vector_stores/factory.py
  name: register
  signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
    \ BaseVectorStore]\n    ) -> None"
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a custom vector store implementation.\n\nStores the provided\
    \ creator in the internal registry under the given vector_store_type. The registration\n\
    does not enforce any factory semantics; the creator is stored as-is and will be\
    \ invoked at runtime by\nVectorStoreFactory.create_vector_store with vector_store_schema_config\
    \ and any additional keyword arguments.\n\nArgs:\n    vector_store_type (str):\
    \ The type identifier for the vector store.\n    creator (Callable[..., BaseVectorStore]):\
    \ A class or callable that creates an instance of BaseVectorStore. The creator\n\
    \        is registered as-is and is invoked later with vector_store_schema_config\
    \ and kwargs.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 36
  line_end: 49
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__str__
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: __str__
  signature: def __str__(self) -> str
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return string representation of the regex extractor, used for cache\
    \ key generation.\n\nArgs:\n    self: The instance of the extractor.\n\nReturns:\n\
    \    str: The cache key string encoding the extractor's configuration, built from\
    \ exclude_nouns, max_word_length, and word_delimiter.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 121
  line_end: 123
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/layout_graph/umap.py::_filter_raw_embeddings
  file: graphrag/index/operations/layout_graph/umap.py
  name: _filter_raw_embeddings
  signature: 'def _filter_raw_embeddings(embeddings: NodeEmbeddings) -> NodeEmbeddings'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Filter out None entries from the input node embeddings mapping.\n\n\
    Args:\n    embeddings: NodeEmbeddings - Mapping of node identifiers to embedding\
    \ vectors; may contain None values.\n\nReturns:\n    NodeEmbeddings - A new mapping\
    \ with all entries whose embeddings are not None."
  code_example: null
  example_source: null
  line_start: 72
  line_end: 77
  dependencies: []
  called_by:
  - graphrag/index/operations/layout_graph/umap.py::run
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat_stream
  file: graphrag/language_model/providers/fnllm/models.py
  name: chat_stream
  signature: "def chat_stream(\n        self, prompt: str, history: list | None =\
    \ None, **kwargs\n    ) -> Generator[str, None]"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
    \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
    \ Additional arguments to pass to the Model.\n\nReturns:\n    Generator[str, None]:\
    \ A generator that yields strings representing the response.\n\nRaises:\n    NotImplementedError:\
    \ chat_stream is not supported for synchronous execution."
  code_example: null
  example_source: null
  line_start: 336
  line_end: 351
  dependencies: []
  called_by: []
- node_id: graphrag/prompt_tune/generator/domain.py::generate_domain
  file: graphrag/prompt_tune/generator/domain.py
  name: generate_domain
  signature: 'def generate_domain(model: ChatModel, docs: str | list[str]) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Generate an LLM persona to use for GraphRAG prompts.\n\nArgs:\n\
    \    model (ChatModel): The LLM to use for generation\n    docs (str | list[str]):\
    \ The domain to generate a persona for\n\nReturns:\n    str: The generated domain\
    \ prompt response.\n\nRaises:\n    Exception: If the underlying model call fails.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 10
  line_end: 27
  dependencies: []
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.env
  file: graphrag/config/environment_reader.py
  name: env
  signature: def env(self)
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "\"\"\"Get the environment object.\n\nReturns:\n    Env: The environment\
    \ object stored on this reader.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 37
  line_end: 39
  dependencies: []
  called_by: []
- node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_pipeline
  file: graphrag/index/workflows/factory.py
  name: register_pipeline
  signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
  decorators:
  - '@classmethod'
  raises: []
  visibility: public
  docstring: "Register a new pipeline method as a list of workflow names.\n\nArgs:\n\
    \    name: The name of the pipeline to register.\n    workflows: A list of workflow\
    \ names that constitute the pipeline.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 35
  line_end: 37
  dependencies: []
  called_by: []
- node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_text
  file: graphrag/vector_stores/base.py
  name: similarity_search_by_text
  signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
    \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
  decorators:
  - '@abstractmethod'
  raises: []
  visibility: public
  docstring: "\"\"\"Perform ANN search by text.\n\nArgs:\n    self: The instance of\
    \ the class.\n    text: str The input text to search for similar documents.\n\
    \    text_embedder: TextEmbedder The callable used to compute an embedding for\
    \ the input text.\n    k: int The number of top results to return.\n    **kwargs:\
    \ Any Additional keyword arguments.\n\nReturns:\n    list[VectorStoreSearchResult]:\
    \ A list of matching VectorStoreSearchResult objects.\n\nRaises:\n    NotImplementedError:\
    \ If the method is not implemented.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 79
  line_end: 82
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
  file: graphrag/index/operations/chunk_text/strategies.py
  name: get_encoding_fn
  signature: def get_encoding_fn(encoding_name)
  decorators: []
  raises: []
  visibility: public
  docstring: "Get encoding functions for a given encoding model.\n\nArgs:\n- encoding_name:\
    \ str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
    \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
    \ list[int]] - Encodes input text into token ids using the selected encoding;\
    \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
    \ str] - Decodes a list of token ids back into a string using the selected encoding.\n\
    \nRaises:\n- Exception: Propagates exceptions raised by tiktoken.get_encoding\
    \ when an invalid encoding_name is provided."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 32
  dependencies: []
  called_by:
  - graphrag/index/operations/chunk_text/strategies.py::run_tokens
  - graphrag/index/workflows/create_base_text_units.py::chunker
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_encode
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_decode
- node_id: graphrag/config/load_config.py::_parse_env_variables
  file: graphrag/config/load_config.py
  name: _parse_env_variables
  signature: 'def _parse_env_variables(text: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Parse environment variables in the configuration text.\n\nArgs:\n\
    \    text: The configuration text.\n\nReturns:\n    The configuration text with\
    \ environment variables parsed.\n\nRaises:\n    KeyError: If an environment variable\
    \ is not found.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 49
  line_end: 67
  dependencies: []
  called_by:
  - graphrag/config/load_config.py::load_config
- node_id: tests/integration/storage/test_file_pipeline_storage.py::test_get_creation_date
  file: tests/integration/storage/test_file_pipeline_storage.py
  name: test_get_creation_date
  signature: def test_get_creation_date()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that FilePipelineStorage.get_creation_date returns a correctly\
    \ formatted creation timestamp for a blob. The test uses the fixture file tests/fixtures/text/input/dulce.txt\
    \ and asserts that the returned string matches the format '%Y-%m-%d %H:%M:%S %z'.\n\
    \nArgs:\n  None\n\nReturns:\n  str - A timestamp string formatted as '%Y-%m-%d\
    \ %H:%M:%S %z' as produced by FilePipelineStorage.get_creation_date for the fixture\
    \ file.\n\nRaises:\n  ValueError: If the returned creation_date string cannot\
    \ be parsed using the expected format '%Y-%m-%d %H:%M:%S %z'."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 48
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
  called_by: []
- node_id: graphrag/index/operations/extract_graph/extract_graph.py::run_strategy
  file: graphrag/index/operations/extract_graph/extract_graph.py
  name: run_strategy
  signature: def run_strategy(row)
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Run a strategy on a single input row to extract graph data.\n\n\
    Args:\n    row: A row from the input DataFrame containing the values for text_column\
    \ and id_column. The text is read from row[text_column] and the id from row[id_column].\n\
    \nReturns:\n    A list with three elements: the entities, relationships, and graph\
    \ returned by the strategy execution for this row.\n\nRaises:\n    Exceptions\
    \ raised by the underlying strategy execution (strategy_exec) are propagated.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 50
  line_end: 61
  dependencies:
  - graphrag/index/operations/extract_graph/typing.py::Document
  called_by: []
- node_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc.__call__
  file: graphrag/language_model/providers/litellm/types.py
  name: __call__
  signature: 'def __call__(self, /, **kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Synchronous request function.\n\nRepresents either a chat completion\
    \ or embedding function. The implementation forwards all provided keyword arguments\
    \ to the underlying request function, enabling flexible use with different backends.\n\
    \nArgs:\n    kwargs: Arbitrary keyword arguments forwarded to the underlying request\
    \ function. Specific accepted keys depend on the concrete impl...\n\nReturns:\n\
    \    Any: The result of the underlying request function.\n\nRaises:\n    Exception:\
    \ If the underlying request function raises an exception, it will propagate to\
    \ the caller."
  code_example: null
  example_source: null
  line_start: 220
  line_end: 222
  dependencies: []
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_is_included
  file: graphrag/query/context_builder/community_context.py
  name: _is_included
  signature: 'def _is_included(report: CommunityReport) -> bool'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Determine whether the given CommunityReport should be included in the\
    \ context based on its rank.\n\nArgs:\n    report (CommunityReport): The community\
    \ report to evaluate for inclusion.\n\nReturns:\n    bool: True if report.rank\
    \ is not None and report.rank >= min_community_rank, otherwise False."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 52
  dependencies: []
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: graphrag/index/workflows/update_clean_state.py::run_workflow
  file: graphrag/index/workflows/update_clean_state.py
  name: run_workflow
  signature: "def run_workflow(  # noqa: RUF029\n    _config: GraphRagConfig,\n  \
    \  context: PipelineRunContext,\n) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Clean the state after the update.\n\nArgs:\n    _config (GraphRagConfig):\
    \ GraphRag configuration.\n    context (PipelineRunContext): Runtime context for\
    \ the workflow execution.\n\nReturns:\n    WorkflowFunctionOutput: Output object\
    \ for the workflow function; result is None."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 31
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  called_by: []
- node_id: tests/smoke/test_fixtures.py::decorator
  file: tests/smoke/test_fixtures.py
  name: decorator
  signature: def decorator(func)
  decorators: []
  raises: []
  visibility: public
  docstring: 'Decorator factory that wraps a test function to forward all positional
    and keyword arguments to the wrapped function and to perform post-execution cleanup
    of test artefacts.


    Parameters:

    - skip (bool): If True, skip performing cleanup after the wrapped function returns.
    Defaults to False.


    Returns:

    - Callable[[Callable[..., Any]], Callable[..., Any]]: A decorator that can be
    applied to a test function. The decorated function will forward all positional
    and keyword arguments to the wrapped function and, after execution, clean up the
    output and cache directories under the root path derived from the input_path keyword
    argument, unless skip is True.


    Notes:

    - The cleanup targets are root/output and root/cache, where root is obtained from
    kwargs["input_path"].

    - input_path must be provided in the call to the decorated function, as it is
    used to determine the cleanup root.

    - All positional and keyword arguments are forwarded to the wrapped function.
    Any AssertionError raised by the wrapped function is propagated.'
  code_example: null
  example_source: null
  line_start: 74
  line_end: 87
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/ui/sidebar.py::update_dataset
  file: unified-search-app/app/ui/sidebar.py
  name: update_dataset
  signature: 'def update_dataset(sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Update dataset from the dropdown and reinitialize related UI state.\n\
    \nArgs:\n    sv: SessionVariables\n        Container holding session-related configuration,\
    \ including dataset metadata and the keys used to read UI state from st.session_state.\
    \ In particular, sv.dataset.key is used to retrieve the selected dataset value.\n\
    \nReturns:\n    None\n\nSide effects:\n- Clears the Streamlit cache using st.cache_data.clear().\n\
    - Ensures st.session_state.response_lengths exists; resets it to an empty list.\n\
    - Loads the selected dataset via load_dataset(value, sv), where value is obtained\
    \ as value = st.session_state[sv.dataset.key].\n\nNotes:\n- Exceptions are not\
    \ handled within this function; they propagate to the caller."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 25
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/model.py::load_text_units
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_text_units
  signature: 'def load_text_units(dataset: str, _datasource: Datasource) -> pd.DataFrame'
  decorators:
  - '@st.cache_data(ttl=default_ttl)'
  raises: []
  visibility: public
  docstring: "Load text units from the specified dataset and data source.\n\nThis\
    \ function delegates to get_text_unit_data to retrieve a DataFrame containing\
    \ text unit records. It returns a DataFrame, not a list of objects.\n\nArgs:\n\
    \    dataset: str The dataset identifier.\n    _datasource: Datasource The data\
    \ source to read text units from.\n\nReturns:\n    pd.DataFrame: A DataFrame containing\
    \ the text unit records from the indexed data.\n    The exact columns reflect\
    \ the text unit schema defined by get_text_unit_data.\n\nRaises:\n    Exception:\
    \ If reading from the data source or processing fails."
  code_example: null
  example_source: null
  line_start: 70
  line_end: 72
  dependencies: []
  called_by:
  - unified-search-app/app/knowledge_loader/model.py::load_model
- node_id: tests/mock_provider.py::MockChatLLM.chat_stream
  file: tests/mock_provider.py
  name: chat_stream
  signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
    \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
  decorators: []
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Not yet implemented: chat_stream for the mock provider.\n\nThis mock\
    \ chat_stream is not implemented and will raise NotImplementedError if called.\
    \ When implemented, it would yield strings from an internal responses list, in\
    \ order, ignoring the prompt and history.\n\nArgs:\n    prompt (str): The input\
    \ prompt to process. This mock ignores the prompt data.\n    history (list | None):\
    \ Optional conversation history. This mock ignores history.\n    **kwargs: Additional\
    \ keyword arguments forwarded to the underlying chat handler.\n\nReturns:\n  \
    \  None\n    Type: None\n\nRaises:\n    NotImplementedError"
  code_example: null
  example_source: null
  line_start: 87
  line_end: 94
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/app_logic.py::run_basic_search
  file: unified-search-app/app/app_logic.py
  name: run_basic_search
  signature: "def run_basic_search(\n    query: str,\n    sv: SessionVariables,\n\
    ) -> SearchResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run basic search.\n\nArgs:\n    query: str\n        The search query\
    \ string used to perform the basic search.\n    sv: SessionVariables\n       \
    \ The SessionVariables instance containing configuration and state for the current\
    \ session.\n\nReturns:\n    SearchResult\n        The result of the basic search,\
    \ including the search_type (Basic), the textual response, and the context data.\n\
    \nRaises:\n    Exception\n        If an error occurs during the basic search process\
    \ (e.g., API call failure)."
  code_example: null
  example_source: null
  line_start: 307
  line_end: 351
  dependencies:
  - graphrag.api::basic_search
  called_by:
  - unified-search-app/app/app_logic.py::run_all_searches
- node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.load_documents
  file: graphrag/vector_stores/azure_ai_search.py
  name: load_documents
  signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
    \ overwrite: bool = True\n    ) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load documents into an Azure AI Search index.\n\nArgs:\n    documents\
    \ (list[VectorStoreDocument]): Documents to load into the index. Each document\
    \ has id, vector, text, and attributes. Only documents with a non-null vector\
    \ are uploaded.\n    overwrite (bool): If True, delete and recreate the index\
    \ before loading.\n\nReturns:\n    None: This method does not return a value.\n\
    \nRaises:\n    Exception: If an error occurs during index creation or document\
    \ upload (propagates from underlying Azure SDK calls)."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 150
  dependencies: []
  called_by: []
- node_id: graphrag/cli/initialize.py::initialize_project_at
  file: graphrag/cli/initialize.py
  name: initialize_project_at
  signature: 'def initialize_project_at(path: Path, force: bool) -> None'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Initialize the project at the given path.\n\nArgs:\n    path: The path\
    \ at which to initialize the project.\n    force: Whether to force initialization\
    \ even if the project already exists.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
    \ If the project already exists and force is False."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 95
  dependencies: []
  called_by:
  - graphrag/cli/main.py::_initialize_cli
- node_id: tests/mock_provider.py::MockEmbeddingLLM.__init__
  file: tests/mock_provider.py
  name: __init__
  signature: 'def __init__(self, **kwargs: Any)'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a mock embedding LLM provider.\n\nArgs:\n    kwargs: Additional\
    \ keyword arguments (ignored).\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 100
  line_end: 103
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  called_by: []
- node_id: graphrag/language_model/response/base.pyi::ModelResponse.output
  file: graphrag/language_model/response/base.pyi
  name: output
  signature: def output(self) -> ModelOutput
  decorators:
  - '@property'
  raises: []
  visibility: public
  docstring: "Return the ModelOutput for this response.\n\nArgs:\n    self: The response\
    \ object.\n\nReturns:\n    ModelOutput: The output associated with this response.\
    \ The returned object provides:\n        content: str - The textual content of\
    \ the output.\n        full_response: dict[str, Any] | None - The complete JSON\
    \ response from the LLM provider, or None if not available."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 18
  dependencies: []
  called_by: []
- node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_end
  file: graphrag/callbacks/workflow_callbacks.py
  name: workflow_end
  signature: 'def workflow_end(self, name: str, instance: object) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Execute this callback when a workflow ends.\n\nArgs:\n    name:\
    \ str\n        The name of the workflow.\n    instance: object\n        The workflow\
    \ instance object.\n\nReturns:\n    None...\n\"\"\""
  code_example: null
  example_source: null
  line_start: 31
  line_end: 33
  dependencies: []
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._blobname
  file: graphrag/storage/blob_pipeline_storage.py
  name: _blobname
  signature: 'def _blobname(blob_name: str) -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Normalize a blob name by removing the internal path prefix and any leading\
    \ slash.\n\nArgs:\n    blob_name: The original blob name as a string.\n\nReturns:\n\
    \    The blob name with the path prefix (self._path_prefix) removed if present,\
    \ and any leading slash stripped.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 127
  line_end: 132
  dependencies: []
  called_by: []
- node_id: unified-search-app/app/state/query_variable.py::QueryVariable.value
  file: unified-search-app/app/state/query_variable.py
  name: value
  signature: 'def value(self, value: Any) -> None'
  decorators:
  - '@value.setter'
  raises: []
  visibility: public
  docstring: "Value setter for the QueryVariable. Sets the session state value and\
    \ updates the corresponding URL query parameter with the lowercase string representation\
    \ of the value.\n\nArgs:\n    value: The new value to assign to the session variable.\
    \ Can be of any type.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 42
  line_end: 45
  dependencies: []
  called_by: []
- node_id: graphrag/index/operations/layout_graph/umap.py::compute_umap_positions
  file: graphrag/index/operations/layout_graph/umap.py
  name: compute_umap_positions
  signature: "def compute_umap_positions(\n    embedding_vectors: np.ndarray,\n  \
    \  node_labels: list[str],\n    node_categories: list[int] | None = None,\n  \
    \  node_sizes: list[int] | None = None,\n    min_dist: float = 0.75,\n    n_neighbors:\
    \ int = 5,\n    spread: int = 1,\n    metric: str = \"euclidean\",\n    n_components:\
    \ int = 2,\n    random_state: int = 86,\n) -> list[NodePosition]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Project embedding vectors down to 2D/3D coordinates using UMAP.\n\n\
    Args:\n  embedding_vectors: Embedding vectors to project, provided as a numpy\
    \ array.\n  node_labels: Labels for each node.\n  node_categories: Optional per-node\
    \ category identifiers. If None, defaults to 1 for all nodes.\n  node_sizes: Optional\
    \ per-node sizes. If None, defaults to 1 for all nodes.\n  min_dist: UMAP min_dist\
    \ hyperparameter controlling the minimum distance between embedded points.\n \
    \ n_neighbors: UMAP n_neighbors hyperparameter controlling the local connectivity.\n\
    \  spread: UMAP spread parameter influencing the layout.\n  metric: Distance metric\
    \ used by UMAP (e.g., \"euclidean\").\n  n_components: Number of output dimensions\
    \ (2 or 3) for the embedding.\n  random_state: Seed for random number generation\
    \ to ensure reproducibility.\n\nReturns:\n  list[NodePosition]: A list of NodePosition\
    \ objects corresponding to each input label. Each NodePosition includes coordinates\
    \ (x, y) for 2D layouts or x, y, z for 3D layouts, along with label, cluster (from\
    \ node_categories or 1), and size (from node_sizes or 1).\n\nRaises:\n  ImportError:\
    \ If the umap package is not installed or cannot be imported."
  code_example: null
  example_source: null
  line_start: 80
  line_end: 132
  dependencies:
  - graphrag/index/operations/layout_graph/typing.py::NodePosition
  called_by:
  - graphrag/index/operations/layout_graph/umap.py::run
- node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.encode
  file: graphrag/tokenizer/litellm_tokenizer.py
  name: encode
  signature: 'def encode(self, text: str) -> list[int]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Encode the given text into a list of tokens using the configured Litellm\
    \ model.\n\nArgs:\n    self (LitellmTokenizer): The instance of LitellmTokenizer.\
    \ The encoding model is determined by the model_name attribute.\n    text (str):\
    \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
    \ the encoded text.\n\nRaises:\n    Exception: If encoding fails due to underlying\
    \ encoder errors or model issues."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 34
  dependencies: []
  called_by: []
- node_id: tests/unit/config/utils.py::assert_text_embedding_configs
  file: tests/unit/config/utils.py
  name: assert_text_embedding_configs
  signature: "def assert_text_embedding_configs(\n    actual: TextEmbeddingConfig,\
    \ expected: TextEmbeddingConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two TextEmbeddingConfig objects are equal for all relevant\
    \ fields.\n\nArgs:\n    actual: TextEmbeddingConfig - The actual configuration\
    \ to validate.\n    expected: TextEmbeddingConfig - The expected configuration\
    \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError - If\
    \ any of the compared fields do not match."
  code_example: null
  example_source: null
  line_start: 199
  line_end: 207
  dependencies: []
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/utils/api.py::truncate
  file: graphrag/utils/api.py
  name: truncate
  signature: 'def truncate(text: str, max_length: int) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Truncate a string to a maximum length.\n\nIf the input text length is\
    \ greater than max_length, return the first max_length characters followed by\
    \ \"...[truncated]\". Otherwise, return the input text unchanged.\n\nArgs:\n \
    \   text: str\n        The text to truncate.\n    max_length: int\n        The\
    \ maximum allowed length of the text.\n\nReturns:\n    str\n        The possibly\
    \ truncated string. If truncation occurred, the returned string ends with \"...[truncated]\"\
    .\n\nRaises:\n    TypeError: If text is not a string or max_length is not an integer."
  code_example: null
  example_source: null
  line_start: 283
  line_end: 287
  dependencies: []
  called_by:
  - graphrag/api/query.py::global_search
  - graphrag/api/query.py::multi_index_global_search
  - graphrag/api/query.py::local_search
  - graphrag/api/query.py::multi_index_local_search
  - graphrag/api/query.py::drift_search
  - graphrag/api/query.py::multi_index_drift_search
  - graphrag/api/query.py::basic_search
- node_id: graphrag/index/input/factory.py::create_input
  file: graphrag/index/input/factory.py
  name: create_input
  signature: "def create_input(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
    ) -> pd.DataFrame"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Instantiate input data for a pipeline.\n\nArgs:\n    config: InputConfig\
    \ containing input configuration (such as file_type and metadata) and storage\
    \ base_dir information.\n    storage: PipelineStorage used to access the input\
    \ data.\n\nReturns:\n    pandas.DataFrame: The loaded input data as a DataFrame.\
    \ If config.metadata is provided and all metadata columns exist in the DataFrame,\
    \ a new column named \"metadata\" is added containing a JSON object per row built\
    \ from the metadata columns, and the original metadata columns are converted to\
    \ strings.\n\nRaises:\n    ValueError: If one or more metadata columns listed\
    \ in config.metadata are not found in the DataFrame, or if the input type specified\
    \ in config.file_type is unknown."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 56
  dependencies: []
  called_by:
  - graphrag/index/workflows/load_input_documents.py::load_input_documents
  - graphrag/index/workflows/load_update_documents.py::load_update_documents
  - graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
  - tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
- node_id: graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
  file: graphrag/index/operations/compute_edge_combined_degree.py
  name: join_to_degree
  signature: 'def join_to_degree(df: pd.DataFrame, column: str) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Join the input DataFrame with the node degree information for the specified\
    \ column and return the result augmented with a degree column.\n\nArgs:\n  df:\
    \ The input DataFrame to augment.\n  column: The column name in df used to align\
    \ with the node degree data.\n\nReturns:\n  A DataFrame with an additional column\
    \ named '<column>_degree' containing the degree values; missing degrees are filled\
    \ with 0."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 31
  dependencies:
  - graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
  called_by:
  - graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
- node_id: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_graph_intelligence
  file: graphrag/index/operations/extract_graph/graph_intelligence_strategy.py
  name: run_graph_intelligence
  signature: "def run_graph_intelligence(\n    docs: list[Document],\n    entity_types:\
    \ EntityTypes,\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> EntityExtractionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the graph intelligence entity extraction strategy.\n\nArgs:\n  \
    \  docs: list[Document] - The input documents to process.\n    entity_types: EntityTypes\
    \ - The types of entities to extract.\n    cache: PipelineCache - Cache to use\
    \ for the language model and computations.\n    args: StrategyConfig - Strategy\
    \ configuration, including llm settings and extraction prompts.\n\nReturns:\n\
    \    EntityExtractionResult - The extraction results.\n\nRaises:\n    Exceptions\
    \ propagated from LanguageModelConfig initialization, ModelManager.get_or_create_chat_model,\
    \ or run_extract_graph."
  code_example: null
  example_source: null
  line_start: 26
  line_end: 42
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/language_model/manager.py::ModelManager
  called_by: []
- node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: test_run_extract_graph_single_document_correct_entities_returned
  signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
  decorators: []
  raises: []
  visibility: public
  docstring: 'Tests that run_extract_graph returns the expected entity titles for
    a single document.


    Args:

    - self: The test method instance (TestRunChain).


    Returns:

    - None: The test does not return a value.


    Raises:

    - AssertionError: If the assertion verifying the returned entities fails.'
  code_example: null
  example_source: null
  line_start: 15
  line_end: 45
  dependencies:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/index/operations/extract_graph/typing.py::Document
  - tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  called_by: []
- node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: test_run_extract_graph_multiple_documents_correct_entities_returned
  signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
    \        self,\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Tests that run_extract_graph returns the expected entity titles for\
    \ multiple documents.\n\nArgs:\n    self (TestRunChain): The test method instance.\n\
    \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
    \ If the assertion verifying the returned entities fails."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 83
  dependencies:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/index/operations/extract_graph/typing.py::Document
  - tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  called_by: []
- node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: test_run_extract_graph_multiple_documents_correct_edges_returned
  signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Verify that run_extract_graph returns a graph with the correct edges\
    \ when given multiple documents.\n\nArgs:\n    self (TestRunChain): The test method\
    \ instance.\n\nReturns:\n    None: The test does not return a value.\n\nRaises:\n\
    \    AssertionError: If the expected edges are not found in the graph."
  code_example: null
  example_source: null
  line_start: 85
  line_end: 125
  dependencies:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/index/operations/extract_graph/typing.py::Document
  - tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  called_by: []
- node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
  signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
    \        self,\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that run_extract_graph maps entity source_ids across multiple documents\
    \ correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
    \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any assertion fails."
  code_example: null
  example_source: null
  line_start: 127
  line_end: 174
  dependencies:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/index/operations/extract_graph/typing.py::Document
  - tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  called_by: []
- node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
  signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
    \        self,\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that run_extract_graph maps edge source_ids across multiple documents\
    \ correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
    \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any assertion fails."
  code_example: null
  example_source: null
  line_start: 176
  line_end: 218
  dependencies:
  - graphrag/index/operations/extract_graph/graph_intelligence_strategy.py::run_extract_graph
  - graphrag/index/operations/extract_graph/typing.py::Document
  - tests/unit/indexing/verbs/helpers/mock_llm.py::create_mock_llm
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_graph_intelligence
  file: graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py
  name: run_graph_intelligence
  signature: "def run_graph_intelligence(\n    id: str | tuple[str, str],\n    descriptions:\
    \ list[str],\n    cache: PipelineCache,\n    args: StrategyConfig,\n) -> SummarizedDescriptionResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the graph intelligence entity extraction strategy using a language\
    \ model to summarize the provided descriptions.\n\nArgs:\n    id: str | tuple[str,\
    \ str]\n        Identifier for the target item; could be a string or a pair of\
    \ strings.\n    descriptions: list[str]\n        The descriptions to summarize.\n\
    \    cache: PipelineCache\n        Cache to use for the language model and computations.\n\
    \    args: StrategyConfig\n        Strategy configuration, including llm settings\
    \ and summarization prompts.\n\nReturns:\n    SummarizedDescriptionResult\n  \
    \      The summarized description result containing the id and generated description."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 38
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/summarize_descriptions/graph_intelligence_strategy.py::run_summarize_descriptions
  - graphrag/language_model/manager.py::ModelManager
  called_by: []
- node_id: tests/unit/config/test_config.py::test_missing_openai_required_api_key
  file: tests/unit/config/test_config.py
  name: test_missing_openai_required_api_key
  signature: def test_missing_openai_required_api_key() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that missing required API keys for OpenAI models raise ValidationError.


    This test builds a model configuration lacking API keys for OpenAIChat and asserts
    that create_graphrag_config raises ValidationError. It then changes the chat model
    type to OpenAIEmbedding and asserts that a ValidationError is raised again, this
    time due to a missing API key for the embedding model.'
  code_example: null
  example_source: null
  line_start: 24
  line_end: 42
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_missing_azure_api_key
  file: tests/unit/config/test_config.py
  name: test_missing_azure_api_key
  signature: def test_missing_azure_api_key() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
    \ is configured with APIKey authentication but no API key is provided, and that\
    \ switching to AzureManagedIdentity does not raise an error.\n\nReturns:\n   \
    \ None\n\nRaises:\n    pydantic.ValidationError: If the model configuration is\
    \ invalid (e.g., missing API key for an Azure OpenAI Chat model with APIKey authentication)."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 65
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_conflicting_auth_type
  file: tests/unit/config/test_config.py
  name: test_conflicting_auth_type
  signature: def test_conflicting_auth_type() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a conflicting authentication type raises ValidationError when\
    \ a model configuration specifies AzureManagedIdentity for an OpenAIChat model.\n\
    \nReturns:\n    None\n\nRaises:\n    ValidationError: If the models configuration\
    \ contains a conflicting auth_type."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 79
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_conflicting_azure_api_key
  file: tests/unit/config/test_config.py
  name: test_conflicting_azure_api_key
  signature: def test_conflicting_azure_api_key() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that configuring an Azure OpenAI Chat model with Azure Managed\
    \ Identity and an API key raises a ValidationError.\n\nReturns:\n    None\n\n\
    Raises:\n    ValidationError: If the models configuration includes an api_key\
    \ while auth_type is AzureManagedIdentity for an Azure OpenAI Chat model, making\
    \ the config invalid."
  code_example: null
  example_source: null
  line_start: 82
  line_end: 97
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_missing_azure_api_base
  file: tests/unit/config/test_config.py
  name: test_missing_azure_api_base
  signature: def test_missing_azure_api_base() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
    \ configuration is missing the required api_base field.\n\nParameters:\n    None\n\
    \nReturns:\n    None\n\nRaises:\n    pydantic.ValidationError: If the model configuration\
    \ is invalid due to a missing api_base in an Azure OpenAI Chat model."
  code_example: null
  example_source: null
  line_start: 110
  line_end: 120
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_missing_azure_api_version
  file: tests/unit/config/test_config.py
  name: test_missing_azure_api_version
  signature: def test_missing_azure_api_version() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a ValidationError is raised when an Azure OpenAI Chat model\
    \ configuration is missing the required api_version field.\n\nArgs:\n    None:\
    \ This test does not take any parameters.\n\nReturns:\n    None: This test does\
    \ not return a value.\n\nRaises:\n    pydantic.ValidationError: If the model configuration\
    \ is invalid due to a missing api_version in an Azure OpenAI Chat model."
  code_example: null
  example_source: null
  line_start: 123
  line_end: 133
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: tests/unit/indexing/test_init_content.py::test_init_yaml
  file: tests/unit/indexing/test_init_content.py
  name: test_init_yaml
  signature: def test_init_yaml()
  decorators: []
  raises: []
  visibility: public
  docstring: "Load configuration parameters into a plain dictionary suitable for subsequent\
    \ GraphRagConfig validation.\n\nArgs:\n    values: dict[str, Any] | None\n   \
    \     Configuration values to pass into the GraphRagConfig validation step. If\
    \ None, defaults are applied.\n    root_dir: str | None\n        Root directory\
    \ for the project; used to resolve relative paths within the configuration.\n\n\
    Returns:\n    dict[str, Any]\n        A dictionary of configuration values ready\
    \ to be consumed by GraphRagConfig.model_validate to produce a GraphRagConfig\
    \ instance.\n\nExamples:\n    data = {...}  # your input dictionary\n    config\
    \ = create_graphrag_config(data)\n    GraphRagConfig.model_validate(config, strict=True)"
  code_example: null
  example_source: null
  line_start: 14
  line_end: 17
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  called_by: []
- node_id: graphrag/index/input/csv.py::load_csv
  file: graphrag/index/input/csv.py
  name: load_csv
  signature: "def load_csv(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
    ) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load csv inputs from a directory.\n\nParameters:\n    config: InputConfig\n\
    \        Configuration for the CSV input, including encoding and storage settings.\n\
    \    storage: PipelineStorage\n        Storage backend used to retrieve CSV files\
    \ and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
    \ containing the data from loaded CSV files with any\n        additional processing\
    \ applied (data columns, and creation_date).\n\nRaises:\n    Exception\n     \
    \   Propagates exceptions from the underlying storage or pandas operations; per-file\n\
    \        failures are logged and skipped by the loader used to load files."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 43
  dependencies:
  - graphrag/index/input/util.py::load_files
  called_by: []
- node_id: graphrag/index/input/json.py::load_json
  file: graphrag/index/input/json.py
  name: load_json
  signature: "def load_json(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
    ) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load json inputs from a directory.\n\nArgs:\n    config: InputConfig\n\
    \        Configuration for the JSON input, including encoding and storage settings.\n\
    \    storage: PipelineStorage\n        Storage backend used to retrieve JSON files\
    \ and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
    \ containing the data from loaded JSON files with any\n        additional processing\
    \ applied (including adding per-file group keys as columns\n        and a creation_date\
    \ column).\n\nRaises:\n    json.JSONDecodeError\n        If a JSON payload cannot\
    \ be decoded from the loaded text."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 47
  dependencies:
  - graphrag/index/input/util.py::load_files
  called_by: []
- node_id: graphrag/index/input/text.py::load_text
  file: graphrag/index/input/text.py
  name: load_text
  signature: "def load_text(\n    config: InputConfig,\n    storage: PipelineStorage,\n\
    ) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load text inputs from a directory.\n\nArgs:\n    config: InputConfig\n\
    \        Configuration for the text input, including encoding and storage settings.\n\
    \    storage: PipelineStorage\n        Storage backend used to retrieve text files\
    \ and metadata.\n\nReturns:\n    pd.DataFrame\n        Concatenated DataFrame\
    \ containing the data from loaded text files with any\n        additional processing\
    \ applied.\n\nRaises:\n    None\n        The per-file loading failures are logged\
    \ and skipped by the internal loader\n        (load_files) rather than raised\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 35
  dependencies:
  - graphrag/index/input/util.py::load_files
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
  file: graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py
  name: is_valid_entity
  signature: 'def is_valid_entity(entity: tuple[str, str], tokens: list[str]) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Check if the given entity is valid with respect to the provided tokens.\n\
    \nArgs:\n    entity: tuple[str, str] - The entity as (text, label). The label\
    \ indicates the category of the entity, e.g., CARDINAL or ORDINAL.\n    tokens:\
    \ list[str] - The tokens associated with the entity used to determine validity.\n\
    \nReturns:\n    bool - True if the entity is valid according to the validation\
    \ rules; otherwise False.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 20
  line_end: 25
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
  called_by:
  - graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
  - graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
- node_id: unified-search-app/app/app_logic.py::run_generate_questions
  file: unified-search-app/app/app_logic.py
  name: run_generate_questions
  signature: 'def run_generate_questions(query: str, sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Run global search to generate questions for the dataset.\n\nArgs:\n\
    \  query (str): The search query string used to generate questions from the global\
    \ search.\n  sv (SessionVariables): The SessionVariables instance containing configuration\
    \ and state for the current session.\n\nReturns:\n  tuple[SearchResult, ...]:\
    \ The results of the global search question generation tasks, in the order they\
    \ were added.\n\nRaises:\n  Exception: Exceptions raised by the inner coroutines\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 106
  line_end: 119
  dependencies:
  - unified-search-app/app/app_logic.py::run_global_search_question_generation
  called_by: []
- node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
  file: graphrag/index/operations/extract_covariates/extract_covariates.py
  name: run_extract_claims
  signature: "def run_extract_claims(\n    input: str | Iterable[str],\n    entity_types:\
    \ list[str],\n    resolved_entities_map: dict[str, str],\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    strategy_config: dict[str, Any],\n) -> CovariateExtractionResult"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Run the Claim extraction chain to derive covariates from input text.\n\
    \nArgs:\n    input: str | Iterable[str]\n        The input text or collection\
    \ of texts to process.\n    entity_types: list[str]\n        The entity types\
    \ to consider when extracting claims.\n    resolved_entities_map: dict[str, str]\n\
    \        Mapping of resolved entities for the extraction process.\n    callbacks:\
    \ WorkflowCallbacks\n        Callbacks used during model invocation and extraction.\n\
    \    cache: PipelineCache\n        Cache to use for model invocations.\n    strategy_config:\
    \ dict[str, Any]\n        Strategy configuration for the extraction, including\
    \ llm settings,\n        prompts, and claim description.\n\nReturns:\n    CovariateExtractionResult\n\
    \        The extraction result containing covariates derived from the claims.\n\
    \nRaises:\n    ValueError\n        If claim_description is missing from strategy_config\
    \ (i.e., claim_description\n        is required for claim extraction)."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 137
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor
  - graphrag/index/operations/extract_covariates/extract_covariates.py::create_covariate
  - graphrag/index/operations/extract_covariates/typing.py::CovariateExtractionResult
  - graphrag/language_model/manager.py::ModelManager
  called_by:
  - graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
- node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
  file: graphrag/language_model/providers/fnllm/utils.py
  name: get_openai_model_parameters_from_dict
  signature: 'def get_openai_model_parameters_from_dict(config: dict[str, Any]) ->
    dict[str, Any]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the OpenAI API parameters from a configuration dictionary, adjusting\
    \ for reasoning model differences.\n\nArgs:\n    config: dict[str, Any] - Configuration\
    \ dictionary containing model and related parameter fields used to derive OpenAI\
    \ API parameters.\n\nReturns:\n    dict[str, Any] - Dictionary of OpenAI API parameters\
    \ derived from the input config. Includes 'n' and either:\n        - For reasoning\
    \ models: 'max_completion_tokens' and 'reasoning_effort'\n        - For non-reasoning\
    \ models: 'max_tokens', 'temperature', 'frequency_penalty', 'presence_penalty',\
    \ 'top_p'\n      If 'response_format' is provided in config, it is included as\
    \ 'response_format' in the result.\n\nRaises:\n    KeyError - If required keys\
    \ (such as 'model') are missing from config."
  code_example: null
  example_source: null
  line_start: 147
  line_end: 165
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::is_reasoning_model
  called_by:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  - graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
  - graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
  - graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n  \
    \      max_word_length: int,\n        word_delimiter: str,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a regular-expression-based noun phrase extractor for English.\n\
    \nNOTE: This is the extractor used in the first benchmarking of LazyGraphRAG but\
    \ it only works for English. It is much faster but likely less accurate than the\
    \ syntactic parser-based extractor. TODO: Reimplement this using SpaCy to remove\
    \ TextBlob dependency.\n\nArgs:\n    exclude_nouns: list[str] \u2014 List of nouns\
    \ to exclude from extraction.\n    max_word_length: int \u2014 Maximum length\
    \ (in characters) of each extracted word.\n    word_delimiter: str \u2014 Delimiter\
    \ for joining words.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 23
  line_end: 58
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/resource_loader.py::download_if_not_exists
  called_by: []
- node_id: unified-search-app/app/ui/search.py::format_suggested_questions
  file: unified-search-app/app/ui/search.py
  name: format_suggested_questions
  signature: 'def format_suggested_questions(questions: str)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Format suggested questions to the UI.\n\nArgs:\n    questions: str\n\
    \        A string containing suggested questions. The function removes square-bracketed\
    \ citations (patterns like [ ... ]) and then converts the remaining text into\
    \ an array by parsing a numbered list.\n\nReturns:\n    list[str]\n        A list\
    \ of extracted questions obtained from the remaining numbered-list text."
  code_example: null
  example_source: null
  line_start: 151
  line_end: 155
  dependencies:
  - unified-search-app/app/ui/search.py::convert_numbered_list_to_array
  called_by: []
- node_id: graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  file: graphrag/index/operations/chunk_text/chunk_text.py
  name: load_strategy
  signature: 'def load_strategy(strategy: ChunkStrategyType) -> ChunkStrategy'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load the strategy method for the given chunking strategy.\n\nArgs:\n\
    \    strategy (ChunkStrategyType): The type of chunk strategy to load. If ChunkStrategyType.tokens,\
    \ the tokens strategy is returned. If ChunkStrategyType.sentence, NLP resources\
    \ are bootstrapped and the sentences strategy is returned.\n\nReturns:\n    ChunkStrategy:\
    \ The loaded strategy callable corresponding to the provided strategy type.\n\n\
    Raises:\n    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 114
  line_end: 130
  dependencies:
  - graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
  called_by:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_tokens
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_sentence
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_none
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: setup_method
  signature: def setup_method(self, method)
  decorators: []
  raises: []
  visibility: public
  docstring: "Set up the test environment before each test by invoking bootstrap().\n\
    \nArgs:\n    method: object\n        The test method currently being executed.\
    \ Provided by the test framework.\n\nReturns:\n    None\n        Type: None\n\n\
    Raises:\n    ImportError: If the bootstrap process fails during initialization\
    \ of required resources."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 18
  dependencies:
  - graphrag/index/operations/chunk_text/bootstrap.py::bootstrap
  called_by: []
- node_id: tests/unit/config/utils.py::assert_extract_graph_nlp_configs
  file: tests/unit/config/utils.py
  name: assert_extract_graph_nlp_configs
  signature: "def assert_extract_graph_nlp_configs(\n    actual: ExtractGraphNLPConfig,\
    \ expected: ExtractGraphNLPConfig\n) -> None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two ExtractGraphNLPConfig objects are equal for all relevant\
    \ fields.\n\nArgs:\n    actual: ExtractGraphNLPConfig - The actual configuration\
    \ to validate.\n    expected: ExtractGraphNLPConfig - The expected configuration\
    \ to compare against.\n\nReturns:\n    None\n\nRaises:\n    AssertionError - If\
    \ any of the compared fields do not match...."
  code_example: null
  example_source: null
  line_start: 252
  line_end: 257
  dependencies:
  - tests/unit/config/utils.py::assert_text_analyzer_configs
  called_by:
  - tests/unit/config/utils.py::assert_graphrag_configs
- node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_id
  file: graphrag/query/input/retrieval/entities.py
  name: get_entity_by_id
  signature: 'def get_entity_by_id(entities: dict[str, Entity], value: str) -> Entity
    | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get entity by id.\n\nArgs:\n    entities: dict[str, Entity]. Mapping\
    \ of entity IDs to Entity objects.\n    value: str. The id value to look up. If\
    \ value is a valid UUID, also try the same value with dashes removed.\n\nReturns:\n\
    \    Entity | None. The matching Entity if found, otherwise None."
  code_example: null
  example_source: null
  line_start: 15
  line_end: 20
  dependencies:
  - graphrag/query/input/retrieval/entities.py::is_valid_uuid
  called_by:
  - graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
  - tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_id
- node_id: graphrag/query/input/retrieval/entities.py::get_entity_by_key
  file: graphrag/query/input/retrieval/entities.py
  name: get_entity_by_key
  signature: "def get_entity_by_key(\n    entities: Iterable[Entity], key: str, value:\
    \ str | int\n) -> Entity | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get an entity by key from a collection.\n\nArgs:\n    entities: Iterable[Entity].\
    \ The collection of Entity objects to search.\n    key: str. The attribute name\
    \ on the Entity to compare.\n    value: str or int. The value to compare against\
    \ the attribute. If value is a string UUID, also compare the same UUID with dashes\
    \ removed.\n\nReturns:\n    Entity | None: The first matching Entity if found,\
    \ otherwise None.\n\nRaises:\n    AttributeError: If any entity in the iterable\
    \ does not have the attribute named by 'key'."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 37
  dependencies:
  - graphrag/query/input/retrieval/entities.py::is_valid_uuid
  called_by:
  - graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
  - tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_key
- node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
  file: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  name: _extract_edges
  signature: "def _extract_edges(\n    nodes_df: pd.DataFrame,\n    normalize_edge_weights:\
    \ bool = True,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Extract edges between nodes by linking nouns that co-occur in the same\
    \ text unit.\n\nNodes that appear in the same text unit are connected. If normalize_edge_weights\
    \ is True, PMI-based weights are computed via calculate_pmi_edge_weights.\n\n\
    Args:\n  nodes_df (pd.DataFrame): DataFrame containing node information with columns\
    \ including id, title, frequency, and text_unit_ids.\n  normalize_edge_weights\
    \ (bool): If True, PMI-based weights are computed instead of raw counts. Default:\
    \ True.\n\nReturns:\n  pd.DataFrame: Edges DataFrame with columns [source, target,\
    \ weight, text_unit_ids]."
  code_example: null
  example_source: null
  line_start: 92
  line_end: 140
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
  called_by:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
- node_id: graphrag/index/utils/graphs.py::calculate_rrf_edge_weights
  file: graphrag/index/utils/graphs.py
  name: calculate_rrf_edge_weights
  signature: "def calculate_rrf_edge_weights(\n    nodes_df: pd.DataFrame,\n    edges_df:\
    \ pd.DataFrame,\n    node_name_col=\"title\",\n    node_freq_col=\"freq\",\n \
    \   edge_weight_col=\"weight\",\n    edge_source_col=\"source\",\n    edge_target_col=\"\
    target\",\n    rrf_smoothing_factor: int = 60,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate reciprocal rank fusion (RRF) edge weights as a combination\
    \ of PMI weight and combined freq of source and target.\n\nArgs:\n    nodes_df\
    \ (pd.DataFrame): DataFrame containing node information with at least the columns\
    \ specified by node_name_col and node_freq_col.\n    edges_df (pd.DataFrame):\
    \ DataFrame containing edge information with at least the columns specified by\
    \ edge_weight_col, edge_source_col, and edge_target_col.\n    node_name_col (str):\
    \ Column in nodes_df that identifies the node name. Default \"title\".\n    node_freq_col\
    \ (str): Column in nodes_df that indicates node frequency. Default \"freq\".\n\
    \    edge_weight_col (str): Column in edges_df that holds edge weights (PMI) before\
    \ RRF adjustment. Default \"weight\".\n    edge_source_col (str): Column in edges_df\
    \ that indicates the source node. Default \"source\".\n    edge_target_col (str):\
    \ Column in edges_df that indicates the target node. Default \"target\".\n   \
    \ rrf_smoothing_factor (int): Smoothing factor used in reciprocal rank fusion.\
    \ Default 60.\n\nReturns:\n    pd.DataFrame: Edge dataframe with edge weights\
    \ updated using the RRF formula. It first computes PMI-based weights, ranks them,\
    \ and then combines the ranks to produce the new weight. The resulting DataFrame\
    \ retains edge information and the updated weight column.\n\nRaises:\n    Not\
    \ documented in the provided data."
  code_example: null
  example_source: null
  line_start: 204
  line_end: 235
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_pmi_edge_weights
  called_by: []
- node_id: tests/notebook/test_notebooks.py::test_notebook
  file: tests/notebook/test_notebooks.py
  name: test_notebook
  signature: 'def test_notebook(notebook_path: Path)'
  decorators:
  - '@pytest.mark.parametrize("notebook_path", notebooks_list)'
  raises: []
  visibility: public
  docstring: "Test that a notebook executes without errors.\n\nArgs:\n    notebook_path:\
    \ Path to the notebook file to test.\n\nReturns:\n    None\n\nRaises:\n    subprocess.CalledProcessError:\
    \ If the nbconvert command fails to execute."
  code_example: null
  example_source: null
  line_start: 47
  line_end: 48
  dependencies:
  - tests/notebook/test_notebooks.py::_notebook_run
  called_by: []
- node_id: graphrag/index/utils/graphs.py::calculate_graph_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_graph_modularity
  signature: "def calculate_graph_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
    \ = True,\n) -> float"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate modularity of the whole graph.\n\nArgs:\n    graph (nx.Graph):\
    \ The input graph.\n    max_cluster_size (int): Maximum cluster size for the root-level\
    \ clustering produced by Hierarchical Leiden.\n    random_seed (int): Seed for\
    \ random number generation.\n    use_root_modularity (bool): If True, compute\
    \ modularity using root-level clustering; otherwise compute using leaf-level clustering.\n\
    \nReturns:\n    float: The modularity score for the graph with respect to the\
    \ chosen clustering."
  code_example: null
  example_source: null
  line_start: 46
  line_end: 59
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_leaf_modularity
  - graphrag/index/utils/graphs.py::calculate_root_modularity
  called_by:
  - graphrag/index/utils/graphs.py::calculate_modularity
- node_id: graphrag/index/utils/graphs.py::calculate_lcc_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_lcc_modularity
  signature: "def calculate_lcc_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
    \ = True,\n) -> float"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate modularity of the largest connected component of the graph.\n\
    \nArgs:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int): Maximum\
    \ cluster size for the root/leaf hierarchical clustering used to compute modularity.\n\
    \    random_seed (int): Seed for random number generation.\n    use_root_modularity\
    \ (bool): If True, compute modularity using root-level clustering; otherwise compute\
    \ using leaf-level clustering.\n\nReturns:\n    float: The modularity value of\
    \ the largest connected component of the input graph."
  code_example: null
  example_source: null
  line_start: 62
  line_end: 76
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_leaf_modularity
  - graphrag/index/utils/graphs.py::calculate_root_modularity
  called_by:
  - graphrag/index/utils/graphs.py::calculate_modularity
- node_id: graphrag/index/utils/graphs.py::calculate_weighted_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_weighted_modularity
  signature: "def calculate_weighted_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    min_connected_component_size:\
    \ int = 10,\n    use_root_modularity: bool = True,\n) -> float"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate weighted modularity of all connected components with size\
    \ greater than min_connected_component_size.\n\nModularity = sum(component_modularity\
    \ * component_size) / total_nodes.\nModularity for the overall calculation is\
    \ obtained by weighting each component's modularity by its size and normalizing\
    \ by the total number of nodes in all considered components.\n\nArgs:\n  graph\
    \ (nx.Graph): The input graph.\n  max_cluster_size (int): Maximum cluster size\
    \ for the modularity computations per component.\n  random_seed (int): Seed for\
    \ random number generation.\n  min_connected_component_size (int): Components\
    \ with size less than or equal to this value are ignored; if no components pass\
    \ this threshold, the entire graph is used.\n  use_root_modularity (bool): If\
    \ True, compute modularity using root-level clustering; otherwise compute leaf-level\
    \ clustering.\n\nReturns:\n  float: The weighted modularity value."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 114
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_leaf_modularity
  - graphrag/index/utils/graphs.py::calculate_root_modularity
  called_by:
  - graphrag/index/utils/graphs.py::calculate_modularity
- node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: _process_results
  signature: "def _process_results(\n        self,\n        results: dict[int, str],\n\
    \        tuple_delimiter: str,\n        record_delimiter: str,\n    ) -> nx.Graph"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Parse the result string to create an undirected unipartite graph.\n\n\
    Args:\n    results (dict[int, str]): dict of results from the extraction chain\n\
    \    tuple_delimiter (str): delimiter between tuples in an output record, default\
    \ is '<|>'\n    record_delimiter (str): delimiter between records, default is\
    \ '##'\n\nReturns:\n    nx.Graph: The undirected graph constructed from the results."
  code_example: null
  example_source: null
  line_start: 179
  line_end: 290
  dependencies:
  - graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_descriptions
  - graphrag/index/operations/extract_graph/graph_extractor.py::_unpack_source_ids
  - graphrag/index/utils/string.py::clean_str
  called_by: []
- node_id: graphrag/index/workflows/prune_graph.py::prune_graph
  file: graphrag/index/workflows/prune_graph.py
  name: prune_graph
  signature: "def prune_graph(\n    entities: pd.DataFrame,\n    relationships: pd.DataFrame,\n\
    \    pruning_config: PruneGraphConfig,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prune a full graph based on graph statistics.\n\nArgs:\n    entities\
    \ (pd.DataFrame): DataFrame of entity nodes used to construct the graph. Must\
    \ include a 'title' column to identify nodes.\n    relationships (pd.DataFrame):\
    \ DataFrame of relationships/edges. Must include 'source' and 'target' columns.\
    \ May include a 'weight' column used as an edge attribute during pruning.\n  \
    \  pruning_config (PruneGraphConfig): Configuration object containing pruning\
    \ parameters such as min_node_freq, max_node_freq_std, min_node_degree, max_node_degree_std,\
    \ min_edge_weight_pct, remove_ego_nodes, and lcc_only.\n\nReturns:\n    tuple[pd.DataFrame,\
    \ pd.DataFrame]: A tuple containing the pruned entities and pruned relationships\
    \ as DataFrames. These DataFrames are subsets of the input DataFrames corresponding\
    \ to the pruned graph.\n\nRaises:\n    Propagates exceptions from the underlying\
    \ graph construction and pruning operations (e.g., due to invalid input data or\
    \ missing required columns)."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 82
  dependencies:
  - graphrag/index/operations/create_graph.py::create_graph
  - graphrag/index/operations/graph_to_dataframes.py::graph_to_dataframes
  called_by: []
- node_id: graphrag/index/operations/layout_graph/zero.py::run
  file: graphrag/index/operations/layout_graph/zero.py
  name: run
  signature: "def run(\n    graph: nx.Graph,\n    on_error: ErrorHandlerFn,\n) ->\
    \ GraphLayout"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute a zero-coordinate GraphLayout for the given graph, optionally\
    \ using per-node cluster/category and size hints, and fall back to a default layout\
    \ if an error occurs.\n\nArgs:\n  graph: nx.Graph\n      The input graph. Nodes\
    \ may have attributes such as cluster (or community) and degree (or size) that\
    \ are used to influence the layout.\n\n  on_error: ErrorHandlerFn\n      Callback\
    \ invoked when an error occurs. It is called with the exception, a formatted traceback\
    \ string, and None.\n\nReturns:\n  GraphLayout\n      A layout (list of NodePosition)\
    \ for all nodes in the graph. Coordinates are initialized to zero; cluster and\
    \ size metadata are derived from node attributes when available. If an error occurs\
    \ during layout computation, a fallback layout with all nodes at (0,0) is returned.\n\
    \nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 24
  line_end: 60
  dependencies:
  - graphrag/index/operations/layout_graph/typing.py::NodePosition
  - graphrag/index/operations/layout_graph/zero.py::get_zero_positions
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
  file: graphrag/query/context_builder/community_context.py
  name: _convert_report_context_to_df
  signature: "def _convert_report_context_to_df(\n    context_records: list[list[str]],\n\
    \    header: list[str],\n    weight_column: str | None = None,\n    rank_column:\
    \ str | None = None,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Convert report context records to a pandas DataFrame and sort by weight\
    \ and rank if provided.\n\nArgs:\n    context_records (list[list[str]]): The report\
    \ context records to convert. Each inner list represents a row.\n    header (list[str]):\
    \ Column names for the resulting DataFrame.\n    weight_column (str | None): Name\
    \ of the column to use for weighting. If not None, the column will be cast to\
    \ float and used for sorting in descending order.\n    rank_column (str | None):\
    \ Name of the column to use for ranking. If not None, the column will be cast\
    \ to float and used for sorting in descending order.\n\nReturns:\n    pd.DataFrame:\
    \ A DataFrame constructed from the context records with the provided header. If\
    \ context_records is empty, an empty DataFrame is returned. The DataFrame is sorted\
    \ in-place by the specified weight and rank columns when provided.\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 246
  line_end: 264
  dependencies:
  - graphrag/query/context_builder/community_context.py::_rank_report_context
  called_by:
  - graphrag/query/context_builder/community_context.py::_cut_batch
- node_id: graphrag/index/run/run_pipeline.py::_copy_previous_output
  file: graphrag/index/run/run_pipeline.py
  name: _copy_previous_output
  signature: "def _copy_previous_output(\n    storage: PipelineStorage,\n    copy_storage:\
    \ PipelineStorage,\n)"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Copy parquet outputs from the source storage to the copy storage asynchronously.\n\
    \nThis async function locates all parquet files in the source storage (matching\
    \ the pattern \".parquet\" at the end of the name), derives a base name by removing\
    \ the \".parquet\" extension from the path, loads the corresponding table from\
    \ the source storage, and writes it to the copy storage under the same base name.\n\
    \nArgs:\n    storage (PipelineStorage): The storage backend to read parquet files\
    \ from.\n    copy_storage (PipelineStorage): The storage backend to which parquet\
    \ files will be written.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If\
    \ a required parquet file cannot be found or if base name extraction yields an\
    \ invalid name. The underlying load operation may raise ValueError.\n    Exception:\
    \ Exceptions raised by the storage backend or parquet reader/writer during the\
    \ load or write operations may propagate to the caller.\n\nNotes:\n    - base_name\
    \ is derived by removing the \".parquet\" extension from the discovered file path.\
    \ If a file name contains multiple occurrences of \".parquet\", all occurrences\
    \ will be removed, which may lead to an unexpected base name in rare cases.\n\
    \    - This function processes all parquet files found in storage; if multiple\
    \ files share the same base name, later files may overwrite earlier ones in copy_storage."
  code_example: null
  example_source: null
  line_start: 160
  line_end: 167
  dependencies:
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/run/run_pipeline.py::run_pipeline
- node_id: graphrag/index/update/incremental_index.py::get_delta_docs
  file: graphrag/index/update/incremental_index.py
  name: get_delta_docs
  signature: "def get_delta_docs(\n    input_dataset: pd.DataFrame, storage: PipelineStorage\n\
    ) -> InputDelta"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the delta between the input dataset and the final documents\
    \ stored in the pipeline storage.\n\nThis asynchronous function compares the input_dataset\
    \ against the documents currently stored in storage and returns the delta as an\
    \ InputDelta with new_inputs and deleted_inputs.\n\nNotes\n    - new_inputs corresponds\
    \ to documents in input_dataset whose titles are not present in the stored final\
    \ documents.\n    - deleted_inputs corresponds to documents present in the stored\
    \ final documents but not present in input_dataset.\n\nParameters\n    input_dataset\
    \ (pd.DataFrame): The input dataset containing documents to be indexed.\n    storage\
    \ (PipelineStorage): The Pipeline storage where final documents are stored.\n\n\
    Returns\n    InputDelta\n        The input delta containing:\n        new_inputs\
    \ (pd.DataFrame): The new documents to add (rows from input_dataset not present\
    \ in storage).\n        deleted_inputs (pd.DataFrame): The documents to remove\
    \ (rows from storage not present in input_dataset).\n\nRaises\n    Exception\n\
    \        Exceptions raised by the storage backend or parquet reader during the\
    \ load operation."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 63
  dependencies:
  - graphrag/utils/storage.py::load_table_from_storage
  called_by:
  - graphrag/index/workflows/load_update_documents.py::load_update_documents
- node_id: graphrag/index/update/incremental_index.py::concat_dataframes
  file: graphrag/index/update/incremental_index.py
  name: concat_dataframes
  signature: "def concat_dataframes(\n    name: str,\n    previous_storage: PipelineStorage,\n\
    \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n)\
    \ -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Concatenate dataframes.\n\nConcatenate old and delta documents: load\
    \ from previous_storage and delta_storage, append delta to old after assigning\
    \ sequential human_readable_id values to delta rows, and write the final dataframe\
    \ to output_storage.\n\nParameters\nname: str\n    Base name for the parquet file\
    \ to load and to which the final dataframe will be written as {name}.parquet.\n\
    previous_storage: PipelineStorage\n    Storage backend containing the existing\
    \ (old) documents.\ndelta_storage: PipelineStorage\n    Storage backend containing\
    \ the delta (new) documents.\noutput_storage: PipelineStorage\n    Storage backend\
    \ where the final concatenated dataframe will be written.\n\nReturns\npd.DataFrame\n\
    \    The concatenated DataFrame containing old and delta documents.\n\nRaises\n\
    ValueError\n    Could not find {name}.parquet in storage!\nException\n    Exceptions\
    \ raised by the storage backend or parquet reader during the load or write operations\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 66
  line_end: 83
  dependencies:
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_final_documents.py::run_workflow
- node_id: graphrag/index/workflows/create_final_documents.py::run_workflow
  file: graphrag/index/workflows/create_final_documents.py
  name: run_workflow
  signature: "def run_workflow(\n    _config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Runs the final documents transformation workflow.\n\nThis async function\
    \ loads the documents and text_units tables from storage via the given context,\
    \ creates the final documents with create_final_documents, writes the resulting\
    \ documents table back to storage, and returns a WorkflowFunctionOutput containing\
    \ the produced DataFrame.\n\nArgs:\n  _config: GraphRagConfig\n      GraphRag\
    \ configuration used by the workflow.\n  context: PipelineRunContext\n      Runtime\
    \ context including the output storage handle.\n\nReturns:\n  WorkflowFunctionOutput\n\
    \      The workflow output whose result is the final documents DataFrame.\n\n\
    Raises:\n  Exception\n      Exceptions raised by the storage backend or by the\
    \ create_final_documents\n      operation may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 33
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_final_documents.py::create_final_documents
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
- node_id: graphrag/index/workflows/update_text_units.py::_update_text_units
  file: graphrag/index/workflows/update_text_units.py
  name: _update_text_units
  signature: "def _update_text_units(\n    previous_storage: PipelineStorage,\n  \
    \  delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n   \
    \ entity_id_mapping: dict,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronously update and merge text units from storage and write the\
    \ result to the output storage.\n\nArgs:\n    previous_storage: PipelineStorage\n\
    \        The storage containing the old text units.\n    delta_storage: PipelineStorage\n\
    \        The storage containing the delta text units to apply.\n    output_storage:\
    \ PipelineStorage\n        The storage where the merged text units will be written.\n\
    \    entity_id_mapping: dict\n        Mapping from old entity ids to new ids to\
    \ apply to delta_text_units.\n\nReturns:\n    pd.DataFrame\n        The updated\
    \ text units.\n\nRaises:\n    ValueError\n        Could not find text_units.parquet\
    \ in storage.\n    Exception\n        Exceptions raised by the storage backend\
    \ or parquet reader during the load or write operations.\n    KeyError\n     \
    \   If required columns are missing from the input dataframes (e.g., 'entity_ids'\
    \ or 'human_readable_id') during the update/merge process."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 57
  dependencies:
  - graphrag/index/workflows/update_text_units.py::_update_and_merge_text_units
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_text_units.py::run_workflow
- node_id: tests/verbs/util.py::update_document_metadata
  file: tests/verbs/util.py
  name: update_document_metadata
  signature: 'def update_document_metadata(metadata: list[str], context: PipelineRunContext)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously load the documents table from storage, create a new metadata\
    \ column containing per-row dictionaries built from the selected metadata columns,\
    \ and write the updated table back to storage.\n\nThis function loads the documents\
    \ table from the provided storage backend, builds a dictionary for each row from\
    \ the specified metadata columns, assigns it to a new 'metadata' column, and persists\
    \ the changes back to storage.\n\nArgs:\n  metadata: List[str] - Names of the\
    \ columns to include in each per-row metadata dictionary.\n  context: PipelineRunContext\
    \ - Runtime context containing the output_storage used to load and write the documents\
    \ table.\n\nReturns:\n  None\n\nRaises:\n  Exception: Exceptions raised by the\
    \ storage backend during load or write operations may propagate."
  code_example: null
  example_source: null
  line_start: 89
  line_end: 95
  dependencies:
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
- node_id: unified-search-app/app/app_logic.py::load_dataset
  file: unified-search-app/app/app_logic.py
  name: load_dataset
  signature: 'def load_dataset(dataset: str, sv: SessionVariables)'
  decorators: []
  raises: []
  visibility: public
  docstring: 'Load the selected dataset and initialize related session state. This
    function updates several fields on the session variables container (sv) and, when
    possible, loads the corresponding data source and knowledge model.


    Args:

    - dataset (str): The dataset key to load. This is a key from sv.datasets.value,
    not a UI element.

    - sv (SessionVariables): The session variables container that will be updated
    in place with the selected dataset''s metadata and configuration, including sv.dataset.value,
    sv.dataset_config.value, sv.datasource.value, and sv.graphrag_config.value. If
    a matching dataset configuration is found, a data source is created from its path
    and the graphrag settings are read from that source; the function then calls load_knowledge_model(sv)
    to populate the knowledge model.


    Returns:

    - None


    Notes:

    - If the dataset key is not found in sv.datasets.value (i.e., sv.dataset_config.value
    becomes None), the function will not create a data source, will not read settings,
    and will not load the knowledge model.

    - This function may raise exceptions originating from create_datasource or read_settings
    (depending on the dataset path and settings file) or from load_knowledge_model,
    if any of those operations fail.

    - The dataset parameter refers to a dataset key, not a user interface element.'
  code_example: null
  example_source: null
  line_start: 51
  line_end: 60
  dependencies:
  - unified-search-app/app/app_logic.py::load_knowledge_model
  called_by:
  - unified-search-app/app/app_logic.py::initialize
- node_id: graphrag/index/workflows/update_covariates.py::_update_covariates
  file: graphrag/index/workflows/update_covariates.py
  name: _update_covariates
  signature: "def _update_covariates(\n    previous_storage: PipelineStorage,\n  \
    \  delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n) ->\
    \ None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update the covariates output by merging existing covariates with the\
    \ delta covariates and writing the result to storage.\n\nArgs:\n    previous_storage:\
    \ The storage containing the previous covariates.\n    delta_storage: The storage\
    \ containing the delta covariates to apply.\n    output_storage: The storage to\
    \ write the merged covariates to.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
    \ Could not find covariates.parquet in storage!\n    Exception: Exceptions raised\
    \ by the storage backend or parquet reader during load or write operations."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 55
  dependencies:
  - graphrag/index/workflows/update_covariates.py::_merge_covariates
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_covariates.py::run_workflow
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: find
  signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n \
    \       base_dir: str | None = None,\n        file_filter: dict[str, Any] | None\
    \ = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str, Any]]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Find documents in a Cosmos DB container using a file pattern regex and\
    \ optional file filter.\n\nArgs:\n    file_pattern (re.Pattern[str]): The file\
    \ pattern to use.\n    base_dir (str | None): The name of the base directory (not\
    \ used in Cosmos DB context).\n    file_filter (dict[str, Any] | None): A dictionary\
    \ of key-value pairs to filter the documents.\n    max_count (int): The maximum\
    \ number of documents to return. If -1, all documents are returned.\n\nReturns:\n\
    \    Iterator[tuple[str, dict[str, Any]]]: An iterator of document IDs and their\
    \ corresponding regex matches."
  code_example: null
  example_source: null
  line_start: 120
  line_end: 203
  dependencies:
  - graphrag/storage/cosmosdb_pipeline_storage.py::_create_progress_status
  - graphrag/storage/cosmosdb_pipeline_storage.py::item_filter
  called_by: []
- node_id: tests/smoke/test_fixtures.py::TestIndexer.test_fixture
  file: tests/smoke/test_fixtures.py
  name: test_fixture
  signature: "def test_fixture(\n        self,\n        input_path: str,\n       \
    \ input_file_type: str,\n        workflow_config: dict[str, dict[str, Any]],\n\
    \        query_config: list[dict[str, str]],\n    )"
  decorators:
  - '@cleanup(skip=debug)'
  - "@mock.patch.dict(\n        os.environ,\n        {\n            **os.environ,\n\
    \            \"BLOB_STORAGE_CONNECTION_STRING\": WELL_KNOWN_AZURITE_CONNECTION_STRING,\n\
    \            \"LOCAL_BLOB_STORAGE_CONNECTION_STRING\": WELL_KNOWN_AZURITE_CONNECTION_STRING,\n\
    \            \"AZURE_AI_SEARCH_URL_ENDPOINT\": os.getenv(\"AZURE_AI_SEARCH_URL_ENDPOINT\"\
    ),\n            \"AZURE_AI_SEARCH_API_KEY\": os.getenv(\"AZURE_AI_SEARCH_API_KEY\"\
    ),\n        },\n        clear=True,\n    )"
  - '@pytest.mark.timeout(2000)'
  raises: []
  visibility: public
  docstring: "Prepare Azurite test data for the fixtures. This coroutine uses the\
    \ azure configuration to create or reset a blob storage container, uploads test\
    \ data from the local input directory (txt and csv files), and returns a callable\
    \ that will delete the container when invoked.\n\nArgs:\n  input_path: Path on\
    \ disk containing test input data. The function looks for an input subdirectory\
    \ with .txt and .csv files to upload.\n  azure: Dictionary with Azure/Azurite\
    \ configuration. Expected keys include: ...\n\nReturns:\n  Callable[[], None]:\
    \ A callable that will delete the container when invoked.\n\nRaises:\n  Exceptions\
    \ related to Azure/Azurite operations (not specified in the provided data)."
  code_example: null
  example_source: null
  line_start: 233
  line_end: 268
  dependencies:
  - tests/smoke/test_fixtures.py::__assert_indexer_outputs
  - tests/smoke/test_fixtures.py::__run_indexer
  - tests/smoke/test_fixtures.py::__run_query
  - tests/smoke/test_fixtures.py::prepare_azurite_data
  called_by: []
- node_id: graphrag/index/run/utils.py::get_update_storages
  file: graphrag/index/run/utils.py
  name: get_update_storages
  signature: "def get_update_storages(\n    config: GraphRagConfig, timestamp: str\n\
    ) -> tuple[PipelineStorage, PipelineStorage, PipelineStorage]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get storage objects for the update index run.\n\nThe function creates\
    \ storage objects from the provided config:\n- output_storage: created from config.output\n\
    - update_storage: created from config.update_index_output\n- timestamped_storage:\
    \ derived by applying the given timestamp to update_storage\n- delta_storage:\
    \ timestamped_storage.child(\"delta\")\n- previous_storage: timestamped_storage.child(\"\
    previous\")\n\nReturns:\n    tuple[PipelineStorage, PipelineStorage, PipelineStorage]:\
    \ A tuple containing\n    output_storage, previous_storage, delta_storage respectively.\n\
    \nArgs:\n    config: GraphRagConfig The configuration containing storage settings\
    \ to use for output and update index outputs.\n    timestamp: str The timestamp\
    \ used to namespace the update index storage.\n\nRaises:\n    ValueError: If the\
    \ storage type is not registered when creating a storage from config."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 61
  dependencies:
  - graphrag/utils/api.py::create_storage_from_config
  called_by:
  - graphrag/index/workflows/update_communities.py::run_workflow
  - graphrag/index/workflows/update_community_reports.py::run_workflow
  - graphrag/index/workflows/update_covariates.py::run_workflow
  - graphrag/index/workflows/update_entities_relationships.py::run_workflow
  - graphrag/index/workflows/update_final_documents.py::run_workflow
  - graphrag/index/workflows/update_text_embeddings.py::run_workflow
  - graphrag/index/workflows/update_text_units.py::run_workflow
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
  file: graphrag/config/environment_reader.py
  name: envvar_prefix
  signature: 'def envvar_prefix(self, prefix: KeyValue)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Set the environment variable prefix.\n\nThe provided prefix is normalized\
    \ via read_key and then transformed to uppercase with a trailing underscore, and\
    \ applied to the underlying Env instance using Env.prefixed.\n\nArgs:\n    prefix\
    \ (KeyValue): The key to use as the environment variable prefix. It can be a string\
    \ or Enum. Strings are converted to lowercase; Enum values are converted to their\
    \ value and lowercased. The result is suffixed with an underscore and uppercased\
    \ before being applied as the prefix.\n\nReturns:\n    Env: An environment object\
    \ with the specified prefix applied."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 58
  dependencies:
  - graphrag/config/environment_reader.py::read_key
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.str
  file: graphrag/config/environment_reader.py
  name: str
  signature: "def str(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
    \ | None = None,\n        default_value: str | None = None,\n    ) -> str | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read a configuration value from the current section or environment.\n\
    \nArgs:\n    key: KeyValue\n        The key to read. It can be a string or an\
    \ Enum. It is normalized to a string using read_key.\n    env_key: EnvKeySet |\
    \ None\n        Optional environment variable name(s) to check if the key is not\
    \ found in the current section. If None, the environment key used is the key.\n\
    \    default_value: str | None\n        Default value to return if the key is\
    \ not found in the current section or environment.\n\nReturns:\n    Any | None\n\
    \    The value read from the current section (returned as stored, which may be\
    \ non-string) or from the environment (typically a string), or None if not found."
  code_example: null
  example_source: null
  line_start: 78
  line_end: 91
  dependencies:
  - graphrag/config/environment_reader.py::_read_env
  - graphrag/config/environment_reader.py::read_key
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.int
  file: graphrag/config/environment_reader.py
  name: int
  signature: "def int(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
    \ | None = None,\n        default_value: int | None = None,\n    ) -> int | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read an integer configuration value.\n\nArgs:\n  key: KeyValue\n   \
    \   The key to read. It is normalized to a string using read_key.\n  env_key:\
    \ EnvKeySet | None\n      Optional environment variable name(s) to check if the\
    \ key is not found in the current section. If None, the environment key used is\
    \ the key.\n  default_value: int | None\n      The default value to return if\
    \ the key is not found in either the current section or environment.\n\nReturns:\n\
    \  int | None\n      The resulting integer value, or None if no value is found\
    \ and no default_value is provided.\n\nRaises:\n  AttributeError\n      If the\
    \ provided key cannot be normalized to a string via read_key."
  code_example: null
  example_source: null
  line_start: 93
  line_end: 105
  dependencies:
  - graphrag/config/environment_reader.py::_read_env
  - graphrag/config/environment_reader.py::read_key
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.bool
  file: graphrag/config/environment_reader.py
  name: bool
  signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
    \ | None = None,\n        default_value: bool | None = None,\n    ) -> bool |\
    \ None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read a boolean configuration value.\n\nArgs:\n  key: KeyValue\n    \
    \  The key to read. It is normalized to a string using read_key.\n  env_key: EnvKeySet\
    \ | None\n      Optional environment variable name(s) to check if the key is not\
    \ found in the current section. If None, the environment key used is the key.\n\
    \  default_value: bool | None\n      The default value to return if the key is\
    \ not found in either the section or environment.\nReturns:\n  bool | None\n \
    \     The boolean value read from the configuration, or None if not found and\
    \ no default is provided.\nRaises:\n  AttributeError\n      If value is not a\
    \ string and Enum (as raised by read_key)."
  code_example: null
  example_source: null
  line_start: 107
  line_end: 120
  dependencies:
  - graphrag/config/environment_reader.py::_read_env
  - graphrag/config/environment_reader.py::read_key
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.float
  file: graphrag/config/environment_reader.py
  name: float
  signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
    \ EnvKeySet | None = None,\n        default_value: float | None = None,\n    )\
    \ -> float | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Read a float configuration value.\n\nArgs:\n    key: KeyValue\n\
    \        The key to read. It can be a string or an Enum. It is normalized to a\
    \ string using read_key.\n    env_key: EnvKeySet | None\n        Optional environment\
    \ variable name(s) to check if the key is not found in the current section. If\
    \ None, the environment key used is the key.\n    default_value: float | None\n\
    \        The default value to return if the key is not found in either the current\
    \ section or environment.\n\nReturns:\n    float | None\n        The configured\
    \ float value, or None if not found and no default is provided.\n\nRaises:\n \
    \   AttributeError\n        If the provided key cannot be normalized to a string.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 122
  line_end: 134
  dependencies:
  - graphrag/config/environment_reader.py::_read_env
  - graphrag/config/environment_reader.py::read_key
  called_by: []
- node_id: graphrag/config/environment_reader.py::EnvironmentReader.list
  file: graphrag/config/environment_reader.py
  name: list
  signature: "def list(\n        self,\n        key: KeyValue,\n        env_key: EnvKeySet\
    \ | None = None,\n        default_value: list | None = None,\n    ) -> list |\
    \ None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Parse a list configuration value.\n\nArgs:\n  key: KeyValue\n      The\
    \ key to read. It can be a string or an Enum. It is normalized to a string using\
    \ read_key.\n  env_key: EnvKeySet | None\n      Optional environment variable\
    \ name(s) to check if the key is not found in the current section. If None, the\
    \ environment key used is the key.\n  default_value: list | None\n      The default\
    \ value to return if the key is not found and no environment value is available.\n\
    \nReturns:\n  list | None\n      The parsed list from the configuration, or default_value\
    \ if not found.\n\nRaises:\n  AttributeError\n      If value is not a string and\
    \ ..."
  code_example: null
  example_source: null
  line_start: 136
  line_end: 155
  dependencies:
  - graphrag/config/environment_reader.py::read_key
  - graphrag/config/environment_reader.py::str
  called_by: []
- node_id: graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
  file: graphrag/query/context_builder/rate_relevancy.py
  name: rate_relevancy
  signature: "def rate_relevancy(\n    query: str,\n    description: str,\n    model:\
    \ ChatModel,\n    tokenizer: Tokenizer,\n    rate_query: str = RATE_QUERY,\n \
    \   num_repeats: int = 1,\n    semaphore: asyncio.Semaphore | None = None,\n \
    \   **model_params: Any,\n) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Rate the relevancy between the query and description on a scale of 0\
    \ to 10.\n\nArgs:\n    query (str): the query (or question) to rate against\n\
    \    description (str): the community description to rate, it can be the community\
    \ title, summary, or the full content.\n    model (ChatModel): LLM model to use\
    \ for rating\n    tokenizer (Tokenizer): tokenizer\n    rate_query (str): prompt\
    \ template used to format the system message with the given description and question\n\
    \    num_repeats (int): number of times to repeat the rating process for the same\
    \ community (default: 1)\n    semaphore (asyncio.Semaphore | None): asyncio.Semaphore\
    \ to limit the number of concurrent LLM calls (default: None)\n    model_params\
    \ (dict[str, Any]): additional arguments to pass to the LLM model\nReturns:\n\
    \    dict[str, Any]: a dictionary containing the final rating and related metadata:\n\
    \        rating (int): the final chosen rating\n        ratings (list[int]): list\
    \ of individual ratings collected\n        llm_calls (int): number of LLM calls\
    \ performed\n        prompt_tokens (int): total number of tokens used for prompts\n\
    \        output_tokens (int): total number of tokens produced by the model\nRaises:\n\
    \    Exception: if an error occurs during model invocation or response parsing\
    \ that propagates from the underlying LLM API"
  code_example: null
  example_source: null
  line_start: 21
  line_end: 77
  dependencies:
  - graphrag/query/llm/text_utils.py::try_parse_json_object
  called_by:
  - graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
- node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.search
  file: graphrag/query/structured_search/drift_search/action.py
  name: search
  signature: 'def search(self, search_engine: Any, global_query: str, scorer: Any
    = None)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute an asynchronous search using the search engine, and update the\
    \ action with the results.\n\nIf a scorer is provided, compute the score for the\
    \ action.\n\nArgs:\n    search_engine (Any): The search engine to execute the\
    \ query.\n    global_query (str): The global query string.\n    scorer (Any, optional):\
    \ Scorer to compute scores for the action.\n\nReturns:\n    DriftAction: The updated\
    \ action with the search results.\n\nRaises:\n    Exception: If an error occurs\
    \ during search execution or processing."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 99
  dependencies:
  - graphrag/query/llm/text_utils.py::try_parse_json_object
  - graphrag/query/structured_search/drift_search/action.py::compute_score
  called_by: []
- node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
  file: graphrag/query/structured_search/global_search/search.py
  name: _parse_search_response
  signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
    Any]]'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Parse the search response json and return a list of key points.\n\n\
    Parameters\n----------\nsearch_response : str\n    The search response json string.\
    \ Expected to contain a top-level object with a\n    \"points\" field that is\
    \ a list of key point objects. Each point should include\n    a \"description\"\
    \ and a \"score\".\n\nReturns\n-------\nlists[dict[str, Any]]\n    A list of key\
    \ points, where each point is a dictionary with keys \"answer\" and\n    \"score\"\
    . The value for \"answer\" comes from the point's \"description\". The value\n\
    \    for \"score\" is the integer value of the point's \"score\". If the input\
    \ cannot be\n    parsed or does not contain a valid \"points\" list, a single\
    \ default point is returned:\n    {\"answer\": \"\", \"score\": 0}.\n\nNotes\n\
    -----\nNo exceptions are raised by this function. Parsing errors or missing data\
    \ result in the\ndefault return above."
  code_example: null
  example_source: null
  line_start: 266
  line_end: 294
  dependencies:
  - graphrag/query/llm/text_utils.py::try_parse_json_object
  called_by: []
- node_id: unified-search-app/app/ui/search.py::format_response_hyperlinks
  file: unified-search-app/app/ui/search.py
  name: format_response_hyperlinks
  signature: 'def format_response_hyperlinks(str_response: str, search_type: str =
    "")'
  decorators: []
  raises: []
  visibility: public
  docstring: "Format response to show hyperlinks inside the response UI.\n\nArgs:\n\
    \    str_response (str): The response string to process.\n    search_type (str):\
    \ The search type value; used to build the hyperlink href. Optional.\nReturns:\n\
    \    str: The response string with the matched citation numbers converted to hyperlinks."
  code_example: null
  example_source: null
  line_start: 100
  line_end: 118
  dependencies:
  - unified-search-app/app/ui/search.py::format_response_hyperlinks_by_key
  called_by:
  - unified-search-app/app/ui/search.py::display_search_result
- node_id: graphrag/index/utils/derive_from_rows.py::execute_row_protected
  file: graphrag/index/utils/derive_from_rows.py
  name: execute_row_protected
  signature: "def execute_row_protected(\n            row: tuple[Hashable, pd.Series],\n\
    \        ) -> ItemType | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Execute the provided row transformation in a protected asynchronous\
    \ context using a shared semaphore.\n\nThis wrapper acquires the shared semaphore\
    \ before invoking the underlying transform (execute) on the given row and returns\
    \ its result. It does not perform any casting; the return value is whatever execute(row)\
    \ returns (which may be None).\n\nArgs:\n    row (tuple[Hashable, pd.Series]):\
    \ A row, where row[1] is the pd.Series passed to the transform.\n\nReturns:\n\
    \    ItemType | None: The result of execute(row); may be None if the underlying\
    \ transform returns None.\n\nRaises:\n    Propagates any exception raised by execute(row);\
    \ exceptions may bubble up to upstream callers."
  code_example: null
  example_source: null
  line_start: 109
  line_end: 113
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::execute
  called_by:
  - graphrag/index/utils/derive_from_rows.py::gather
- node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
  file: graphrag/index/text_splitting/text_splitting.py
  name: split_text
  signature: 'def split_text(self, text: str | list[str]) -> list[str]'
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Split text into chunks using a token-based tokenizer.\n\nIf text is\
    \ a list of strings, it is joined with spaces to form a single string before splitting.\
    \ If the input is NaN or an empty string, an empty list is returned.\n\nArgs:\n\
    \    text: str | list[str]\n        The input text to split. If a list of strings\
    \ is provided, they are joined with spaces prior to splitting.\n\nReturns:\n \
    \   list[str]\n        The list of chunked text strings produced.\n\nRaises:\n\
    \    TypeError: If a non-string value is encountered during processing."
  code_example: null
  example_source: null
  line_start: 99
  line_end: 116
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_single_text_on_tokens
  signature: def test_split_single_text_on_tokens()
  decorators: []
  raises: []
  visibility: public
  docstring: "Split a single text into chunks using the provided tokenizer.\n\nArgs:\n\
    \    text: str The input text to split into chunks.\n    tokenizer: TokenChunkerOptions\
    \ The tokenizer configuration used to encode the text into tokens and decode chunks.\
    \ It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\nReturns:\n\
    \    list[str] The list of chunked text strings produced.\n\nRaises:\n    Exception:\
    \ If the underlying tokenizer raises an error during encoding or decoding operations."
  code_example: null
  example_source: null
  line_start: 83
  line_end: 110
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
  - graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_single_text_on_tokens_no_overlap
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_single_text_on_tokens_no_overlap
  signature: def test_split_single_text_on_tokens_no_overlap()
  decorators: []
  raises: []
  visibility: public
  docstring: "Split a single text into chunks using the provided tokenizer.\n\nArgs:\n\
    \    text (str): The input text to split into chunks.\n    tokenizer (TokenChunkerOptions):\
    \ The tokenizer configuration used to encode the text into tokens and decode chunks.\
    \ It must provide encode, decode, tokens_per_chunk, and chunk_overlap.\n\nReturns:\n\
    \    list[str]: The list of chunked text strings produced.\n\nRaises:\n    Exception:\
    \ If the underlying tokenizer raises an error during encoding or decoding operations."
  code_example: null
  example_source: null
  line_start: 132
  line_end: 170
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
  - graphrag/index/text_splitting/text_splitting.py::split_single_text_on_tokens
  - tests/unit/indexing/text_splitting/test_text_splitting.py::encode
  called_by: []
- node_id: graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
  file: graphrag/query/input/retrieval/text_units.py
  name: get_candidate_text_units
  signature: "def get_candidate_text_units(\n    selected_entities: list[Entity],\n\
    \    text_units: list[TextUnit],\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get all text units that are associated to selected entities.\n\nArgs:\n\
    \    selected_entities (list[Entity]): Entities whose text_unit_ids (if any) are\
    \ used to select text units.\n    text_units (list[TextUnit]): The pool of TextUnit\
    \ objects to search.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the\
    \ text units associated with the selected entities. The DataFrame is produced\
    \ by converting the selected TextUnit objects via to_text_unit_dataframe; if no\
    \ such text units are found, an empty DataFrame is returned."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 24
  dependencies:
  - graphrag/query/input/retrieval/text_units.py::to_text_unit_dataframe
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
- node_id: graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
  file: graphrag/language_model/providers/litellm/get_cache_key.py
  name: get_cache_key
  signature: "def get_cache_key(\n    model_config: \"LanguageModelConfig\",\n   \
    \ prefix: str,\n    messages: str | None = None,\n    input: str | None = None,\n\
    \    **kwargs: Any,\n) -> str"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Generate a cache key based on the model configuration, input arguments,\
    \ and optional name.\n\nModeled after the fnllm cache key generation.\nhttps://github.com/microsoft/essex-toolkit/blob/23d3077b65c0e8f1d89c397a2968fe570a25f790/python/fnllm/fnllm/caching/base.py#L50\n\
    \nArgs:\n    model_config: The configuration of the language model.\n    prefix:\
    \ A prefix for the cache key.\n    messages: Optional messages input for the cache\
    \ key.\n    input: Optional single input for the cache key.\n    **kwargs: Additional\
    \ model input parameters. May include 'name', which, if present, is appended to\
    \ the prefix after hashing.\n\nReturns:\n    str: The generated cache key in the\
    \ form '{prefix}_{data_hash}_v{version}'. Note that the provided 'name' (via kwargs)\
    \ is appended to the prefix after computing the data hash, not before.\n\nRaises:\n\
    \    ValueError: If both 'messages' and 'input' are provided. The exact message\
    \ is: \"Only one of 'messages' or 'input' should be provided.\"\n    ValueError:\
    \ If neither 'messages' nor 'input' is provided. The exact message is: \"Either\
    \ 'messages' or 'input' must be provided.\""
  code_example: null
  example_source: null
  line_start: 33
  line_end: 78
  dependencies:
  - graphrag/language_model/providers/litellm/get_cache_key.py::_get_parameters
  - graphrag/language_model/providers/litellm/get_cache_key.py::_hash
  called_by:
  - graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache
  - graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache_async
- node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::create_datasource
  file: unified-search-app/app/knowledge_loader/data_sources/loader.py
  name: create_datasource
  signature: 'def create_datasource(dataset_folder: str) -> Datasource'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a datasource that reads from a local or blob storage parquet\
    \ file.\n\nArgs:\n    dataset_folder: Path to the dataset folder to load data\
    \ from.\n\nReturns:\n    Datasource: A datasource instance. If blob_account_name\
    \ is set, a BlobDatasource is returned;\n        otherwise, a LocalDatasource\
    \ configured with the base path derived from dataset_folder and local_data_root.\n\
    \nRaises:\n    Exceptions that may be raised by the underlying BlobDatasource\
    \ or LocalDatasource constructors."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 49
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_dataset_listing
  file: unified-search-app/app/knowledge_loader/data_sources/loader.py
  name: load_dataset_listing
  signature: def load_dataset_listing() -> list[DatasetConfig]
  decorators: []
  raises: []
  visibility: public
  docstring: "Load dataset listing file from blob storage or local data.\n\nThis function\
    \ takes no parameters and returns a list of DatasetConfig instances parsed from\
    \ the listing file. When blob storage is configured (blob_account_name is set),\
    \ the function loads the listing from blob storage and, on error, prints the issue\
    \ and returns an empty list (no exception is raised).\n\nWhen blob storage is\
    \ not configured, the function loads the listing from the local filesystem using\
    \ the path derived from local_data_root and LISTING_FILE, parses the JSON content,\
    \ and converts each listing item into a DatasetConfig instance. Errors during\
    \ local loading may propagate to the caller.\n\nReturns:\n    list[DatasetConfig]:\
    \ A list of DatasetConfig instances created from the listing entries. May be empty\
    \ if loading from blob storage fails or no data is found.\n\nRaises:\n    FileNotFoundError:\
    \ If the local listing file cannot be found when blob storage is not used.\n \
    \   json.JSONDecodeError: If the listing JSON content is invalid when loaded from\
    \ local path.\n    TypeError: If a listing item does not provide the required\
    \ fields for DatasetConfig."
  code_example: null
  example_source: null
  line_start: 52
  line_end: 69
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/loader.py::load_prompts
  file: unified-search-app/app/knowledge_loader/data_sources/loader.py
  name: load_prompts
  signature: 'def load_prompts(dataset: str) -> dict[str, str]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Return the prompts configuration for a specific dataset.\n\nIf\
    \ a blob account name is configured, the prompts are loaded from blob storage;\
    \ otherwise\nthey are loaded from local storage.\n\nArgs:\n    dataset (str):\
    \ The dataset name to load prompts for.\n\nReturns:\n    dict[str, str]: The prompts\
    \ configuration for the specified dataset.\n\nRaises:\n    Exception: Propagated\
    \ exceptions from underlying loading functions (load_blob_prompt_config\n    \
    \    or load_local_prompt_config) may be raised.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 72
  line_end: 78
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/loader.py::_get_base_path
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::to_str
  file: graphrag/query/input/loaders/utils.py
  name: to_str
  signature: 'def to_str(data: Mapping[str, Any], column_name: str | None) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert and validate a value to a string.\n\nArgs:\n  data (Mapping[str,\
    \ Any]): The mapping that contains column values.\n  column_name (str | None):\
    \ The name of the column to retrieve, or None.\n\nReturns:\n  str: The string\
    \ representation of the value.\n\nRaises:\n  ValueError: If column_name is None\
    \ or the column is missing from data."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 40
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_entities
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_covariates
  - graphrag/query/input/loaders/dfs.py::read_communities
  - graphrag/query/input/loaders/dfs.py::read_community_reports
  - graphrag/query/input/loaders/dfs.py::read_text_units
- node_id: graphrag/query/input/loaders/utils.py::to_optional_str
  file: graphrag/query/input/loaders/utils.py
  name: to_optional_str
  signature: 'def to_optional_str(data: Mapping[str, Any], column_name: str | None)
    -> str | None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Convert and validate a value to an optional string.\n\nRetrieve the\
    \ value for column_name from data and convert it to a string if it is not None.\
    \ If the value is None, returns None. If column_name is None or not present in\
    \ data, raises ValueError.\n\nArgs:\n    data (Mapping[str, Any]): Input data\
    \ mapping containing column values.\n    column_name (str | None): The name of\
    \ the column to retrieve. If None or not present in data, a ValueError will be\
    \ raised.\n\nReturns:\n    str | None: The string representation of the value,\
    \ or None if the value is None.\n\nRaises:\n    ValueError: If column_name is\
    \ None or column_name is not found in data."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 46
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_entities
  - graphrag/query/input/loaders/dfs.py::read_relationships
  - graphrag/query/input/loaders/dfs.py::read_covariates
  - graphrag/query/input/loaders/dfs.py::read_communities
  - graphrag/query/input/loaders/dfs.py::read_community_reports
- node_id: graphrag/query/input/loaders/utils.py::to_list
  file: graphrag/query/input/loaders/utils.py
  name: to_list
  signature: "def to_list(\n    data: Mapping[str, Any], column_name: str | None,\
    \ item_type: type | None = None\n) -> list"
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to a list.\n\nThis function retrieves the\
    \ value for the given column from data using _get_value with required=True. If\
    \ the value is a numpy.ndarray, it is converted to a Python list before validation.\
    \ If the resulting value is not a list, a TypeError is raised. If item_type is\
    \ provided, all items in the list must be instances of that type; otherwise a\
    \ TypeError is raised.\n\nArgs:\n    data (Mapping[str, Any]): The mapping that\
    \ contains column values.\n    column_name (str | None): The name of the column\
    \ to retrieve, or None. If None or missing, _get_value will raise ValueError.\n\
    \    item_type (type | None): Optional type that each item in the resulting list\
    \ must be.\n\nReturns:\n    list: The converted and validated list.\n\nRaises:\n\
    \    TypeError: If the value is not a list, or any list item is not of the specified\
    \ item_type.\n    ValueError: If column_name is None or the column is missing\
    \ from data (via _get_value)."
  code_example: null
  example_source: null
  line_start: 49
  line_end: 64
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by:
  - graphrag/query/input/loaders/dfs.py::read_communities
- node_id: graphrag/query/input/loaders/utils.py::to_int
  file: graphrag/query/input/loaders/utils.py
  name: to_int
  signature: 'def to_int(data: Mapping[str, Any], column_name: str | None) -> int'
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to an int. The value is retrieved from\
    \ data via _get_value(data, column_name, required=True). If the retrieved value\
    \ is a Python float, it is truncated to an integer. After conversion, if the value\
    \ is not a Python int, a TypeError is raised.\n\nArgs:\n    data (Mapping[str,\
    \ Any]): The mapping that contains column values.\n    column_name (str | None):\
    \ The name of the column to retrieve from data. If None or the column is missing,\
    \ _get_value will raise a ValueError.\n\nReturns:\n    int: The value converted\
    \ to int.\n\nRaises:\n    ValueError: If column_name is None or the column_name\
    \ is not present in data (as raised by _get_value with required=True).\n    TypeError:\
    \ If the retrieved value cannot be interpreted as an int after any necessary conversion.\
    \ Note that numpy integer types (e.g., np.int64) are not treated as ints and will\
    \ trigger this error."
  code_example: null
  example_source: null
  line_start: 91
  line_end: 99
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::to_float
  file: graphrag/query/input/loaders/utils.py
  name: to_float
  signature: 'def to_float(data: Mapping[str, Any], column_name: str | None) -> float'
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to a float.\n\nArgs:\n    data: The mapping\
    \ that contains column values.\n    column_name: The name of the column to retrieve,\
    \ or None.\n\nReturns:\n    float: The value as a float.\n\nRaises:\n    TypeError:\
    \ If the retrieved value is not a float."
  code_example: null
  example_source: null
  line_start: 117
  line_end: 123
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by: []
- node_id: graphrag/query/input/loaders/utils.py::to_dict
  file: graphrag/query/input/loaders/utils.py
  name: to_dict
  signature: "def to_dict(\n    data: Mapping[str, Any],\n    column_name: str | None,\n\
    \    key_type: type | None = None,\n    value_type: type | None = None,\n) ->\
    \ dict"
  decorators: []
  raises:
  - TypeError
  visibility: public
  docstring: "Convert and validate a value to a dict.\n\nThis function retrieves a\
    \ value for the given column from data using _get_value with required=True and\
    \ validates that the value is a dictionary. If key_type is provided, all keys\
    \ must be instances of that type. If value_type is provided, all values must be\
    \ instances of that type. Returns the dict if valid.\n\nArgs:\n  data: Mapping[str,\
    \ Any]\n      The mapping that contains column values.\n  column_name: str | None\n\
    \      The name of the column to retrieve, or None.\n  key_type: type | None\n\
    \      If provided, require all dict keys to be instances of this type.\n  value_type:\
    \ type | None\n      If provided, require all dict values to be instances of this\
    \ type.\n\nReturns:\n  dict: The validated dictionary retrieved from data.\n\n\
    Raises:\n  ValueError: If column_name is None or not in data.\n  TypeError: If\
    \ the value is not a dict, or if any keys/values do not match the provided types."
  code_example: null
  example_source: null
  line_start: 138
  line_end: 159
  dependencies:
  - graphrag/query/input/loaders/utils.py::_get_value
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_prompt_config
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: load_blob_prompt_config
  signature: "def load_blob_prompt_config(\n    dataset: str,\n    account_name: str\
    \ | None = blob_account_name,\n    container_name: str | None = blob_container_name,\n\
    ) -> dict[str, str]"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Load blob prompt configuration for a dataset from Azure Blob Storage.\n\
    \nArgs:\n    dataset: The dataset name to load prompts for.\n    account_name:\
    \ The Azure storage account name. If None, no prompts are loaded.\n    container_name:\
    \ The blob container name. If None, no prompts are loaded.\n\nReturns:\n    dict[str,\
    \ str]: A mapping from prompt map name to its content loaded from the blob storage.\n\
    \nRaises:\n    Exception: Propagated exceptions from underlying Azure Blob Storage\
    \ operations.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 38
  line_end: 57
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: load_blob_file
  signature: "def load_blob_file(\n    dataset: str | None,\n    file: str | None,\n\
    \    account_name: str | None = blob_account_name,\n    container_name: str |\
    \ None = blob_container_name,\n) -> BytesIO"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load blob file from container.\n\nArgs:\n    dataset: The dataset prefix\
    \ to use when constructing the blob path. If None, only the file name is used.\n\
    \    file: The blob file name to load. If dataset is provided, the blob path will\
    \ be \"<dataset>/<file>\".\n    account_name: The Azure storage account name.\
    \ Defaults to blob_account_name.\n    container_name: The Azure Blob container\
    \ name. Defaults to blob_container_name.\n\nReturns:\n    BytesIO: An in-memory\
    \ binary stream containing the blob data read into the stream. If account_name\
    \ or container_name is None, an empty BytesIO is returned.\n\nRaises:\n    Exception:\
    \ May raise exceptions from Azure Blob Storage operations (authentication, network,\
    \ or other errors) during container retrieval or blob download."
  code_example: null
  example_source: null
  line_start: 60
  line_end: 78
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::_get_container
  called_by:
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
- node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
  file: graphrag/index/operations/embed_text/embed_text.py
  name: _text_embed_in_memory
  signature: "def _text_embed_in_memory(\n    input: pd.DataFrame,\n    callbacks:\
    \ WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column: str,\n    strategy:\
    \ dict,\n)"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Embed a piece of text into a vector space in memory using the specified\
    \ embedding strategy.\n\nArgs:\n    input: DataFrame containing the text data\
    \ to embed\n    callbacks: WorkflowCallbacks used during embedding\n    cache:\
    \ PipelineCache used for caching intermediary results\n    embed_column: Name\
    \ of the column in input containing the text to embed\n    strategy: Dictionary\
    \ describing the embedding strategy to use (must include a \"type\" key)\n\nReturns:\n\
    \    embeddings: The embeddings produced by the embedding strategy\n\nRaises:\n\
    \    ValueError: If an unknown strategy is provided\n    KeyError: If embed_column\
    \ is not found in the input dataframe"
  code_example: null
  example_source: null
  line_start: 81
  line_end: 95
  dependencies:
  - graphrag/index/operations/embed_text/embed_text.py::load_strategy
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::embed_text
- node_id: graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
  file: graphrag/index/operations/embed_text/embed_text.py
  name: _text_embed_with_vector_store
  signature: "def _text_embed_with_vector_store(\n    input: pd.DataFrame,\n    callbacks:\
    \ WorkflowCallbacks,\n    cache: PipelineCache,\n    embed_column: str,\n    strategy:\
    \ dict[str, Any],\n    vector_store: BaseVectorStore,\n    vector_store_config:\
    \ dict,\n    id_column: str = \"id\",\n    title_column: str | None = None,\n)"
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Embed text from a DataFrame into a vector store using a specified embedding\
    \ strategy and load the resulting vectors into the provided vector store.\n\n\
    Args:\n    input (pd.DataFrame): Input DataFrame containing the data to embed;\
    \ must include the embed_column and id_column, and may include the title column.\n\
    \    callbacks (WorkflowCallbacks): Callbacks used during embedding.\n    cache\
    \ (PipelineCache): Cache object used by the embedding strategy.\n    embed_column\
    \ (str): Name of the DataFrame column containing the text to embed (or lists of\
    \ texts per row).\n    strategy (dict[str, Any]): Embedding strategy configuration,\
    \ including the type of strategy to load.\n    vector_store (BaseVectorStore):\
    \ Vector store where embeddings will be loaded.\n    vector_store_config (dict):\
    \ Configuration for the vector store (e.g., batch_size, overwrite).\n    id_column\
    \ (str): Name of the DataFrame column containing the identifier for each row.\n\
    \    title_column (str | None): Optional column name to use as the title; defaults\
    \ to embed_column when None.\n\nReturns:\n    list[Any]: Aggregated embeddings\
    \ produced by the embedding strategy across all batches.\n\nRaises:\n    ValueError:\
    \ If required columns are missing from the input DataFrame or if the necessary\
    \ columns cannot be found (embed_column, id_column, and title_column as applicable)."
  code_example: null
  example_source: null
  line_start: 98
  line_end: 183
  dependencies:
  - graphrag/index/operations/embed_text/embed_text.py::load_strategy
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::embed_text
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
  file: graphrag/language_model/providers/fnllm/models.py
  name: chat
  signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs) ->
    ModelResponse'
  decorators: []
  raises: []
  visibility: public
  docstring: "Chat with the Model using the given prompt.\n\nParameters:\n    prompt\
    \ (str): The prompt to chat with.\n    history (list | None): The conversation\
    \ history to include in the chat, or None for no history.\n    kwargs: Additional\
    \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
    \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
    \ model call are propagated to the caller."
  code_example: null
  example_source: null
  line_start: 119
  line_end: 131
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::achat
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
  file: graphrag/language_model/providers/fnllm/models.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
    \ (list[str]): The texts to embed.\n    kwargs (dict[str, Any]): Additional arguments\
    \ to pass to the LLM.\n\nReturns:\n    list[list[float]]: The embeddings for the\
    \ input texts.\n\nRaises:\n    ValueError: If no embeddings are found in the response.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 214
  line_end: 226
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::aembed_batch
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
  file: graphrag/language_model/providers/fnllm/models.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
    \ The text to embed.\n    kwargs (dict[str, Any]): Additional arguments to pass\
    \ to the Model.\n\nReturns:\n    list[float]: The embeddings of the text.\n\n\
    Raises:\n    ValueError: If no embeddings are found in the response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 228
  line_end: 240
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::aembed
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
  file: graphrag/language_model/providers/fnllm/models.py
  name: chat
  signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs) ->
    ModelResponse'
  decorators: []
  raises: []
  visibility: public
  docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt (str):\
    \ The prompt to chat with.\n    history (list | None): The conversation history\
    \ to include in the chat, or None for no history.\n    kwargs: Additional keyword\
    \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
    \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
    \ model call are propagated."
  code_example: null
  example_source: null
  line_start: 322
  line_end: 334
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::achat
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
  file: graphrag/language_model/providers/fnllm/models.py
  name: embed_batch
  signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
    \ (list[str]): The texts to embed.\n    kwargs: Additional arguments to pass to\
    \ the Model.\n\nReturns:\n    list[list[float]]: The embeddings for the input\
    \ texts.\n\nRaises:\n    ValueError: If no embeddings are found in the response.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 417
  line_end: 429
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::aembed_batch
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
  file: graphrag/language_model/providers/fnllm/models.py
  name: embed
  signature: 'def embed(self, text: str, **kwargs) -> list[float]'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
    \ The text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
    \nReturns:\n    list[float]: The embeddings of the text.\n\nRaises:\n    ValueError:\
    \ If no embeddings are found in the response.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 431
  line_end: 443
  dependencies:
  - graphrag/language_model/providers/fnllm/models.py::aembed
  - graphrag/language_model/providers/fnllm/utils.py::run_coroutine_sync
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _drop_community_level
  signature: 'def _drop_community_level(df: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Drop the community level column from the dataframe.\n\nArgs:\n    df\
    \ (pd.DataFrame): The DataFrame from which to drop the community level column.\n\
    \nReturns:\n    pd.DataFrame: The DataFrame with the COMMUNITY_LEVEL column dropped.\n\
    \nRaises:\n    KeyError: If the COMMUNITY_LEVEL column does not exist in df."
  code_example: null
  example_source: null
  line_start: 264
  line_end: 266
  dependencies:
  - graphrag/index/utils/dataframes.py::drop_columns
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
- node_id: graphrag/index/operations/prune_graph.py::prune_graph
  file: graphrag/index/operations/prune_graph.py
  name: prune_graph
  signature: "def prune_graph(\n    graph: nx.Graph,\n    min_node_freq: int = 1,\n\
    \    max_node_freq_std: float | None = None,\n    min_node_degree: int = 1,\n\
    \    max_node_degree_std: float | None = None,\n    min_edge_weight_pct: float\
    \ = 40,\n    remove_ego_nodes: bool = False,\n    lcc_only: bool = False,\n) ->\
    \ nx.Graph"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prune graph by removing nodes that are out of frequency/degree ranges\
    \ and edges with low weights.\n\nArgs:\n    graph (nx.Graph): The graph to prune.\n\
    \    min_node_freq (int): Minimum node frequency threshold; nodes with frequency\
    \ below this value are removed.\n    max_node_freq_std (float | None): If provided,\
    \ upper threshold is mean + max_node_freq_std * std of node frequencies; nodes\
    \ with frequency above this threshold are removed.\n    min_node_degree (int):\
    \ Minimum degree threshold; nodes with degree below this value are removed.\n\
    \    max_node_degree_std (float | None): If provided, upper threshold is mean\
    \ + max_node_degree_std * std of node degrees; nodes with degree above this threshold\
    \ are removed.\n    min_edge_weight_pct (float): Percentile for edge weights;\
    \ edges with weight below this percentile are removed.\n    remove_ego_nodes (bool):\
    \ If True, remove the ego node (the node with the highest degree) before pruning.\n\
    \    lcc_only (bool): If True, return only the largest connected component of\
    \ the pruned graph.\n\nReturns:\n    nx.Graph: The pruned graph. If lcc_only is\
    \ True, returns the largest connected component of the pruned graph."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 83
  dependencies:
  - graphrag/index/operations/prune_graph.py::_get_upper_threshold_by_std
  called_by:
  - graphrag/index/workflows/prune_graph.py::run_workflow
- node_id: tests/unit/indexing/test_init_content.py::test_init_yaml_uncommented
  file: tests/unit/indexing/test_init_content.py
  name: test_init_yaml_uncommented
  signature: def test_init_yaml_uncommented()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that uncommenting the YAML in INIT_YAML produces a valid GraphRagConfig.\n\
    \nReturns:\n    None (type: None)\n\nRaises:\n    ValidationError: If the configuration\
    \ values do not satisfy pydantic validation."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 31
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - tests/unit/indexing/test_init_content.py::uncomment_line
  called_by: []
- node_id: tests/verbs/test_pipeline_state.py::test_pipeline_state
  file: tests/verbs/test_pipeline_state.py
  name: test_pipeline_state
  signature: def test_pipeline_state()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the pipeline run context state can be updated by workflows.\n\
    \nTwo workflows are registered and executed in sequence, and the test asserts\
    \ the state's count becomes 2.\n\nReturns:\n    None\n        This test does not\
    \ return a value."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 41
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/run/utils.py::create_run_context
  called_by: []
- node_id: tests/verbs/test_pipeline_state.py::test_pipeline_existing_state
  file: tests/verbs/test_pipeline_state.py
  name: test_pipeline_existing_state
  signature: def test_pipeline_existing_state()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that an existing state value in the pipeline run context can be\
    \ updated by a workflow.\n\nOnly workflow_2 is registered and executed; the test\
    \ initializes the run context with state={\"count\": 4}, runs the pipeline, and\
    \ asserts the final state count is 5.\n\nReturns:\n    None\n        This test\
    \ does not return a value.\n\nRaises:\n    AssertionError\n        If the final\
    \ state count is not 5."
  code_example: null
  example_source: null
  line_start: 44
  line_end: 54
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/run/utils.py::create_run_context
  called_by: []
- node_id: graphrag/index/workflows/update_communities.py::_update_communities
  file: graphrag/index/workflows/update_communities.py
  name: _update_communities
  signature: "def _update_communities(\n    previous_storage: PipelineStorage,\n \
    \   delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n) ->\
    \ dict"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update the communities output.\n\nArgs:\n  previous_storage: PipelineStorage\n\
    \      Storage containing the existing/previous communities.\n  delta_storage:\
    \ PipelineStorage\n      Storage containing the delta (updated) communities.\n\
    \  output_storage: PipelineStorage\n      Storage to write the merged communities\
    \ to.\n\nReturns:\n  dict\n      Mapping from original delta community IDs to\
    \ the new IDs assigned during the merge.\n\nRaises:\n  ValueError\n      Could\
    \ not find {name}.parquet in storage!\n  Exception\n      Exceptions raised by\
    \ the storage backend or parquet reader during the load operation.\n  Exception\n\
    \      Exceptions raised by the storage backend during the write operation may\
    \ propagate."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 53
  dependencies:
  - graphrag/index/update/communities.py::_update_and_merge_communities
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_communities.py::run_workflow
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_binning
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_binning
  signature: def test_binning()
  decorators: []
  raises: []
  visibility: public
  docstring: "Bin time values into consecutive time-based intervals.\n\nArgs:\n  \
    \  time_values: list[float] - Input time values to bin.\n    time_interval: int\
    \ - Size of each time interval.\n\nReturns:\n    list[list[float]] - A list of\
    \ bins, where each inner list contains the values that fall into the corresponding\
    \ time interval. The i-th bin contains values in the interval [i * time_interval,\
    \ (i + 1) * time_interval).\n\nRaises:\n    None..."
  code_example: null
  example_source: null
  line_start: 35
  line_end: 46
  dependencies:
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_default
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_get_num_total_default
  signature: def test_get_num_total_default()
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the total number of elements in a DataFrame column.\n\nArgs:\n\
    \    output: pandas.DataFrame The DataFrame containing the target column.\n  \
    \  column: str The name of the column to process.\n\nReturns:\n    int The total\
    \ number of elements in the specified column; strings contribute 1 each, non-string\
    \ entries contribute their length."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 27
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_get_num_total_array
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_get_num_total_array
  signature: def test_get_num_total_array()
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the total number of elements in a DataFrame column, counting\
    \ strings as a single element and non-string entries by their length.\n\nArgs:\n\
    \    output (pd.DataFrame): The DataFrame containing the target column.\n    column\
    \ (str): The name of the column to process.\n\nReturns:\n    int: The total number\
    \ of elements in the specified column; strings contribute 1 each, non-string entries\
    \ contribute their length."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 34
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
  called_by: []
- node_id: tests/verbs/util.py::create_test_context
  file: tests/verbs/util.py
  name: create_test_context
  signature: 'def create_test_context(storage: list[str] | None = None) -> PipelineRunContext'
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a test context with test tables loaded into storage.\n\nArgs:\n\
    \    storage: list[str] | None\n        A list of test table names to load from\
    \ test data and write into the\n        context's output storage. If None, only\
    \ the documents table is loaded and stored.\n\nReturns:\n    PipelineRunContext\n\
    \        The initialized pipeline run context with the test data loaded into its\n\
    \        output storage.\n\nRaises:\n    Exception: Exceptions raised by load_test_table\
    \ or write_table_to_storage may propagate."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 50
  dependencies:
  - graphrag/index/run/utils.py::create_run_context
  - graphrag/utils/storage.py::write_table_to_storage
  - tests/verbs/util.py::load_test_table
  called_by:
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  - tests/verbs/test_create_communities.py::test_create_communities
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
  - tests/verbs/test_create_final_documents.py::test_create_final_documents
  - tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
  - tests/verbs/test_extract_covariates.py::test_extract_covariates
  - tests/verbs/test_extract_graph.py::test_extract_graph
  - tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
  - tests/verbs/test_finalize_graph.py::_prep_tables
  - tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
  - tests/verbs/test_prune_graph.py::test_prune_graph
- node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
  file: graphrag/config/models/vector_store_schema_config.py
  name: _validate_schema
  signature: def _validate_schema(self) -> None
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "\"\"\"Validate the schema.\n\nArgs:\n    self (VectorStoreSchemaConfig):\
    \ The instance containing schema field names to validate.\n\nReturns:\n    None\n\
    \nRaises:\n    ValueError: If any of the id_field, vector_field, text_field, or\
    \ attributes_field contains an unsafe or invalid field name.\n\"\""
  code_example: null
  example_source: null
  line_start: 50
  line_end: 60
  dependencies:
  - graphrag/config/models/vector_store_schema_config.py::is_valid_field_name
  called_by: []
- node_id: graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
  file: graphrag/query/input/retrieval/community_reports.py
  name: get_candidate_communities
  signature: "def get_candidate_communities(\n    selected_entities: list[Entity],\n\
    \    community_reports: list[CommunityReport],\n    include_community_rank: bool\
    \ = False,\n    use_community_summary: bool = False,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get all communities that are related to selected entities.\n\nThis function\
    \ collects all community IDs from the provided selected entities, filters the\
    \ given\ncommunity_reports to those IDs, and returns a DataFrame produced by to_community_report_dataframe\n\
    using the specified options.\n\nArgs:\n  selected_entities: The selected entities\
    \ for which to retrieve candidate communities.\n  community_reports: The pool\
    \ of CommunityReport objects to search.\n  include_community_rank: Whether to\
    \ include a rank column in the output.\n  use_community_summary: Whether to include\
    \ the summary field instead of full content.\n\nReturns:\n  pd.DataFrame: A DataFrame\
    \ representing the candidate communities related to the selected entities.\n\n\
    Raises:\n  None"
  code_example: null
  example_source: null
  line_start: 14
  line_end: 36
  dependencies:
  - graphrag/query/input/retrieval/community_reports.py::to_community_report_dataframe
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
- node_id: graphrag/index/input/text.py::load_file
  file: graphrag/index/input/text.py
  name: load_file
  signature: 'def load_file(path: str, group: dict | None = None) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Load a text input from storage and return it as a DataFrame containing\
    \ metadata.\n\nArgs:\n  path (str): Path to the text file.\n  group (dict | None):\
    \ Optional grouping metadata to merge with the item. If None, an empty dict is\
    \ used.\n\nReturns:\n  pd.DataFrame: A DataFrame containing a single row with\
    \ the loaded text and metadata, including id, title, and creation_date.\n\nRaises:\n\
    \  KeyError: If a key from the hashcode used for hashing is not present in the\
    \ item."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 33
  dependencies:
  - graphrag/index/utils/hashing.py::gen_sha512_hash
  called_by: []
- node_id: graphrag/index/input/util.py::process_data_columns
  file: graphrag/index/input/util.py
  name: process_data_columns
  signature: "def process_data_columns(\n    documents: pd.DataFrame, config: InputConfig,\
    \ path: str\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Process configured data columns of a DataFrame by augmenting it with\
    \ id, text, and title columns according to the provided configuration. Warnings\
    \ are logged if a configured text or title column is not found in the data.\n\n\
    The function mutates the input DataFrame and returns it.\n\nArgs:\n  documents\
    \ (pd.DataFrame): DataFrame to augment with id, text, and title columns as configured.\n\
    \  config (InputConfig): Configuration containing optional text_column and title_column.\
    \ If text_column is provided, a text column will be created from that column if\
    \ present; otherwise a warning is logged and no text column is created. If text_column\
    \ is None, no text column is created.\n  path (str): File path used for logging\
    \ warnings and as the default title when no title column is specified.\n\nReturns:\n\
    \  pd.DataFrame: The input DataFrame augmented with id, text, and title columns\
    \ as configured.\n\nRaises:\n  Exception: If hashing or data processing fails\
    \ due to underlying data issues or library operations. The exact exception depends\
    \ on the hashing function or pandas operations."
  code_example: null
  example_source: null
  line_start: 56
  line_end: 86
  dependencies:
  - graphrag/index/utils/hashing.py::gen_sha512_hash
  called_by:
  - graphrag/index/input/csv.py::load_file
  - graphrag/index/input/json.py::load_file
- node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::extract
  file: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  name: extract
  signature: def extract(row)
  decorators: []
  raises: []
  visibility: public
  docstring: "Extract noun phrases from a row's text using a cache-backed analyzer.\n\
    \nArgs:\n  row: dict[str, Any] or pandas.Series: A mapping with a \"text\" key\
    \ containing the input text to analyze.\n\nReturns:\n  list[str]: The noun phrases\
    \ extracted from the text, or the cached result if available.\n\nRaises:\n  KeyError:\
    \ If the input row does not contain the required \"text\" key, or if a required\
    \ key is missing during hashing in gen_sha512_hash."
  code_example: null
  example_source: null
  line_start: 59
  line_end: 67
  dependencies:
  - graphrag/index/utils/hashing.py::gen_sha512_hash
  called_by: []
- node_id: graphrag/index/utils/stable_lcc.py::_stabilize_graph
  file: graphrag/index/utils/stable_lcc.py
  name: _stabilize_graph
  signature: 'def _stabilize_graph(graph: nx.Graph) -> nx.Graph'
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Ensure an undirected graph with the same relationships will always\
    \ be read the same way.\n\nArgs:\n    graph: nx.Graph The input graph. May be\
    \ directed or undirected; the function will preserve the directedness and return\
    \ a new graph with deterministic ordering of nodes and edges.\n\nReturns:\n  \
    \  nx.Graph The stabilized graph. If the input graph is directed, the returned\
    \ graph is a nx.DiGraph; otherwise, a nx.Graph.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 23
  line_end: 61
  dependencies:
  - graphrag/index/utils/stable_lcc.py::_get_edge_key
  - graphrag/index/utils/stable_lcc.py::_sort_source_target
  called_by:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_basic_functionality
  signature: def test_basic_functionality(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test basic sentence splitting without metadata.\n\nVerifies that a single\
    \ input document containing two sentences is split into two TextChunk objects\
    \ with the expected text_chunk values, and that each chunk references the first\
    \ document (source_doc_indices == [0]). Also ensures the progress ticker is invoked\
    \ exactly once with the value 1.\n\nArgs:\n  self (TestRunSentences): The test\
    \ case instance.\n\nReturns:\n  None\n\nRaises:\n  None\n\nExamples:\n  Input:\n\
    \    [\"This is a test. Another sentence.\"]\n  Expected:\n    - Two chunks:\n\
    \        - chunks[0].text_chunk == \"This is a test.\"\n        - chunks[1].text_chunk\
    \ == \"Another sentence.\"\n      - All chunks have source_doc_indices == [0]\n\
    \      - tick.assert_called_once_with(1)\n  Notes:\n    No metadata is attached\
    \ to the produced chunks in this test."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 30
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/strategies.py::run_sentences
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_multiple_documents
  signature: def test_multiple_documents(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test processing multiple input documents into separate chunks and verify\
    \ correct chunk origins and progress tick behavior.\n\nThe test provides two input\
    \ documents: \\\"First. Document.\\\" and \\\"Second. Doc.\\\" which should be\
    \ chunked into four sentences (two per document). Each resulting TextChunk should\
    \ have its source_doc_indices set to the index of its originating document (the\
    \ first two chunks originate from document 0, the last two from document 1). The\
    \ test also asserts that the progress tick is invoked once for each input document\
    \ (two total).\n\nArgs:\n    self: The test case instance.\n\nReturns:\n    None\n\
    \nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 32
  line_end: 41
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/strategies.py::run_sentences
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_mixed_whitespace_handling
  signature: def test_mixed_whitespace_handling(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Chunks text into multiple parts by sentence.\n\nArgs:\n  input: list[str]\
    \ - list of input documents to chunk into sentences.\n  _config: ChunkingConfig\
    \ - chunking configuration (unused by this strategy).\n  tick: ProgressTicker\
    \ - progress reporter; invoked to indicate progress after processing each input\
    \ document.\n\nReturns:\n  Iterable[TextChunk] - yields TextChunk objects for\
    \ each sentence, with text_chunk set to the sentence and source_doc_indices containing\
    \ the index of the source document.\n\nRaises:"
  code_example: null
  example_source: null
  line_start: 43
  line_end: 48
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/strategies.py::run_sentences
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: _get_splitter
  signature: "def _get_splitter(\n    config: LanguageModelConfig, batch_max_tokens:\
    \ int\n) -> TokenTextSplitter"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get a TokenTextSplitter configured for the given language model configuration.\n\
    \nArgs:\n    config: LanguageModelConfig describing the model configuration used\
    \ to select the tokenizer.\n    batch_max_tokens: int representing the maximum\
    \ number of tokens per chunk.\n\nReturns:\n    TokenTextSplitter: A text splitter\
    \ initialized with a tokenizer built from the provided config and chunk_size equal\
    \ to batch_max_tokens.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 79
  line_end: 85
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: __init__
  signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n  \
    \      max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
    \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a SummarizeExtractor with the given model invoker and configuration.\n\
    \nArgs:\n    model_invoker (ChatModel): The model invoker used to run prompts.\n\
    \    max_summary_length (int): Maximum length of the summary to produce.\n   \
    \ max_input_tokens (int): Maximum number of input tokens to consider for summarization.\n\
    \    summarization_prompt (str | None): Custom prompt to use for summarization.\
    \ If None, defaults to SUMMARIZE_PROMPT.\n    on_error (ErrorHandlerFn | None):\
    \ Optional error handler. If None, a no-op is used.\n\nReturns:\n    None: The\
    \ constructor does not return a value."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 52
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
  file: graphrag/index/text_splitting/text_splitting.py
  name: __init__
  signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None =\
    \ None,\n        **kwargs: Any,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Init method for TokenTextSplitter with an optional tokenizer.\n\nThis\
    \ initializer sets the tokenizer to use for tokenization. If no tokenizer is provided,\n\
    a default tokenizer is obtained via get_tokenizer(). Any additional keyword arguments\
    \ are\nforwarded to the base class initializer via super().__init__(**kwargs).\n\
    \nArgs:\n    tokenizer (Tokenizer | None): Tokenizer to use for tokenization.\
    \ If None, a default tokenizer\n        is obtained via get_tokenizer().\n   \
    \ **kwargs (Any): Additional keyword arguments forwarded to the base class initializer.\n\
    \nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 86
  line_end: 93
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
  file: graphrag/prompt_tune/generator/extract_graph_prompt.py
  name: create_extract_graph_prompt
  signature: "def create_extract_graph_prompt(\n    entity_types: str | list[str]\
    \ | None,\n    docs: list[str],\n    examples: list[str],\n    language: str,\n\
    \    max_token_count: int,\n    tokenizer: Tokenizer | None = None,\n    json_mode:\
    \ bool = False,\n    output_path: Path | None = None,\n    min_examples_required:\
    \ int = 2,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"\nCreate a prompt for entity extraction.\n\nArgs:\n    entity_types\
    \ (str | list[str] | None): The entity types to extract.\n    docs (list[str]):\
    \ The list of documents to extract entities from.\n    examples (list[str]): The\
    \ list of examples to use for entity extraction.\n    language (str): The language\
    \ of the inputs and outputs.\n    max_token_count (int): The maximum number of\
    \ tokens to use for the prompt.\n    tokenizer (Tokenizer | None): The tokenizer\
    \ to use for encoding and decoding text. If None, a default tokenizer will be\
    \ used.\n    json_mode (bool): Whether to use JSON mode for the prompt. Default\
    \ is False.\n    output_path (Path | None): The path to write the prompt to. Default\
    \ is None.\n    min_examples_required (int): The minimum number of examples required.\
    \ Default is 2.\n\nReturns:\n    str: The entity extraction prompt.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 21
  line_end: 109
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
  file: graphrag/query/context_builder/conversation_history.py
  name: build_context
  signature: "def build_context(\n        self,\n        tokenizer: Tokenizer | None\
    \ = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
    \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
    \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
    \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare conversation history as context data for system prompt.\n\n\
    Parameters\n----------\ntokenizer : Tokenizer | None\n    The tokenizer to use.\
    \ If None, a default tokenizer retrieved by get_tokenizer() will be used.\ninclude_user_turns_only\
    \ : bool\n    If True, only user queries (not assistant responses) will be included\
    \ in the context. Default is True.\nmax_qa_turns : int | None\n    Maximum number\
    \ of QA turns to include in the context. If None, there is no explicit limit.\
    \ Default is 5.\nmax_context_tokens : int\n    Maximum number of tokens allowed\
    \ for the context data. Default is 8000.\nrecency_bias : bool\n    If True, reverse\
    \ the order of the conversation history to prioritize the most recent QA turn.\
    \ Default is True.\ncolumn_delimiter : str\n    Delimiter to use for separating\
    \ columns in the context data. Default is \"|\".\ncontext_name : str\n    Name\
    \ of the context, default is \"Conversation History\".\n\nReturns\n-------\ntuple[str,\
    \ dict[str, pd.DataFrame]]\n    A tuple containing:\n    - context_text: the context\
    \ data text (string) including a header line and the QA turns formatted as a table.\n\
    \    - context_dict: a mapping from the lowercase context name (context_name.lower())\
    \ to a DataFrame containing the turns included in the context. The key is consistently\
    \ lowercase in both empty and non-empty cases."
  code_example: null
  example_source: null
  line_start: 149
  line_end: 213
  dependencies:
  - graphrag/query/context_builder/conversation_history.py::to_qa_turns
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/context_builder/local_context.py::build_entity_context
  file: graphrag/query/context_builder/local_context.py
  name: build_entity_context
  signature: "def build_entity_context(\n    selected_entities: list[Entity],\n  \
    \  tokenizer: Tokenizer | None = None,\n    max_context_tokens: int = 8000,\n\
    \    include_entity_rank: bool = True,\n    rank_description: str = \"number of\
    \ relationships\",\n    column_delimiter: str = \"|\",\n    context_name=\"Entities\"\
    ,\n) -> tuple[str, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare entity data table as context data for system prompt.\n\nArgs:\n\
    \    selected_entities (list[Entity]): Entities to include in the context.\n \
    \   tokenizer (Tokenizer | None): Tokenizer to measure token counts; if None,\
    \ get_tokenizer() is used.\n    max_context_tokens (int): Maximum allowed tokens\
    \ for the generated context text. Parsing stops when adding a new entity would\
    \ exceed this limit.\n    include_entity_rank (bool): Whether to include the entity's\
    \ rank in the context row.\n    rank_description (str): Label for the rank column\
    \ (e.g., \"number of relationships\").\n    column_delimiter (str): Delimiter\
    \ to join fields in the context text.\n    context_name (str): Name of the context\
    \ section used in the header, default \"Entities\".\n\nReturns:\n    tuple[str,\
    \ pd.DataFrame]: A tuple containing:\n        - current_context_text: The textual\
    \ context including header and entity rows (up to max_context_tokens).\n     \
    \   - record_df: A DataFrame of the context records (excluding the header). If\
    \ no entities were processed, this is an empty DataFrame."
  code_example: null
  example_source: null
  line_start: 30
  line_end: 90
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
- node_id: graphrag/query/context_builder/local_context.py::build_covariates_context
  file: graphrag/query/context_builder/local_context.py
  name: build_covariates_context
  signature: "def build_covariates_context(\n    selected_entities: list[Entity],\n\
    \    covariates: list[Covariate],\n    tokenizer: Tokenizer | None = None,\n \
    \   max_context_tokens: int = 8000,\n    column_delimiter: str = \"|\",\n    context_name:\
    \ str = \"Covariates\",\n) -> tuple[str, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare covariate data tables as context data for system prompt.\n\n\
    Args:\n  selected_entities (list[Entity]): Entities to include in the covariate\
    \ context.\n  covariates (list[Covariate]): Covariates from which to build context\
    \ for the entities.\n  tokenizer (Tokenizer | None): Tokenizer to count tokens;\
    \ if None, get_tokenizer() is used.\n  max_context_tokens (int): Maximum allowed\
    \ tokens for the generated context text. Parsing stops when adding a new covariate\
    \ would exceed this limit.\n  column_delimiter (str): Delimiter used to join fields\
    \ in the context rows (default \"|\").\n  context_name (str): Name used to label\
    \ the context section in the output (default \"Covariates\").\n\nReturns:\n  tuple[str,\
    \ pd.DataFrame]: A tuple containing:\n    - current_context_text (str): The assembled\
    \ context text including a header and covariate rows.\n    - record_df (pd.DataFrame):\
    \ A DataFrame of the covariate records (excluding the header). If no covariates\
    \ were added, this is an empty DataFrame."
  code_example: null
  example_source: null
  line_start: 93
  line_end: 155
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
- node_id: graphrag/query/context_builder/source_context.py::build_text_unit_context
  file: graphrag/query/context_builder/source_context.py
  name: build_text_unit_context
  signature: "def build_text_unit_context(\n    text_units: list[TextUnit],\n    tokenizer:\
    \ Tokenizer | None = None,\n    column_delimiter: str = \"|\",\n    shuffle_data:\
    \ bool = True,\n    max_context_tokens: int = 8000,\n    context_name: str = \"\
    Sources\",\n    random_state: int = 86,\n) -> tuple[str, dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare text-unit data table as context data for system prompt.\n\n\
    Args:\n    text_units: list[TextUnit]\n        Text units to include in the context.\n\
    \    tokenizer: Tokenizer | None\n        Tokenizer used to count tokens; if None,\
    \ a tokenizer is retrieved with get_tokenizer().\n    column_delimiter: str\n\
    \        Delimiter used to separate fields in the context rows.\n    shuffle_data:\
    \ bool\n        Whether to shuffle text_units before building the context.\n \
    \   max_context_tokens: int\n        Maximum number of tokens allowed for the\
    \ generated context.\n    context_name: str\n        Name of the context section\
    \ included in the header.\n    random_state: int\n        Seed used to shuffle\
    \ when shuffle_data is True.\nReturns:\n    tuple[str, dict[str, pd.DataFrame]]\n\
    \        The generated text context and a dictionary mapping the context name\
    \ (lowercased) to a pandas DataFrame containing the context rows."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 79
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
- node_id: graphrag/query/factory.py::get_drift_search_engine
  file: graphrag/query/factory.py
  name: get_drift_search_engine
  signature: "def get_drift_search_engine(\n    config: GraphRagConfig,\n    reports:\
    \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
    \    relationships: list[Relationship],\n    description_embedding_store: BaseVectorStore,\n\
    \    response_type: str,\n    local_system_prompt: str | None = None,\n    reduce_system_prompt:\
    \ str | None = None,\n    callbacks: list[QueryCallbacks] | None = None,\n) ->\
    \ DRIFTSearch"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Create a local drift search engine based on data + configuration.\n\
    \nArgs:\n    config: GraphRagConfig\n        GraphRag configuration object containing\
    \ drift_search settings used to configure the search engine and models.\n    reports:\
    \ list[CommunityReport]\n        Community reports to be used by the search context.\n\
    \    text_units: list[TextUnit]\n        Text units to be included in the search\
    \ context.\n    entities: list[Entity]\n        Entities to be included in the\
    \ search context.\n    relationships: list[Relationship]\n        Relationships\
    \ to be included in the search context.\n    description_embedding_store: BaseVectorStore\n\
    \        Vector store of text embeddings for entity descriptions.\n    response_type:\
    \ str\n        Type of response to generate.\n    local_system_prompt: str | None\n\
    \        Optional system prompt to be used locally for prompt construction.\n\
    \    reduce_system_prompt: str | None\n        Optional reduced system prompt\
    \ for shorter prompts.\n    callbacks: list[QueryCallbacks] | None\n        Optional\
    \ query callbacks to handle search events.\n\nReturns:\n    DRIFTSearch\n    \
    \    A configured DRIFTSearch instance ready to execute drift-based searches.\n\
    \nRaises:\n    None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 195
  line_end: 247
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder
  - graphrag/query/structured_search/drift_search/search.py::DRIFTSearch
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/api/query.py::drift_search_streaming
- node_id: graphrag/query/llm/text_utils.py::chunk_text
  file: graphrag/query/llm/text_utils.py
  name: chunk_text
  signature: 'def chunk_text(text: str, max_tokens: int, tokenizer: Tokenizer | None
    = None)'
  decorators: []
  raises: []
  visibility: public
  docstring: "Chunk text by token length.\n\nArgs:\n    text (str): The input text\
    \ to chunk.\n    max_tokens (int): Maximum number of tokens per chunk.\n    tokenizer\
    \ (Tokenizer | None): Tokenizer to use for encoding/decoding. If None, a default\
    \ tokenizer is obtained via get_tokenizer(encoding_model=defs.ENCODING_MODEL).\n\
    \nReturns:\n    Iterator[str]: An iterator that yields chunk strings, each created\
    \ by decoding token sequences with at most max_tokens tokens.\n\nRaises:\n   \
    \ ValueError: If max_tokens < 1."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 42
  dependencies:
  - graphrag/query/llm/text_utils.py::batched
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
  file: graphrag/query/question_gen/base.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer |\
    \ None = None,\n        model_params: dict[str, Any] | None = None,\n        context_builder_params:\
    \ dict[str, Any] | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a Base Question Gen with the provided model and context builder.\n\
    \nArgs:\n    model (ChatModel): The language model interface used for this base\
    \ question generator.\n    context_builder (GlobalContextBuilder | LocalContextBuilder):\
    \ The builder that constructs the context for the questions.\n    tokenizer (Tokenizer\
    \ | None): Optional tokenizer to use. If None, a tokenizer appropriate for the\
    \ model will be selected.\n    model_params (dict[str, Any] | None): Optional\
    \ parameters for the language model. If None, defaults to an empty dict.\n   \
    \ context_builder_params (dict[str, Any] | None): Optional parameters for the\
    \ context builder. If None, defaults to an empty dict.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 33
  line_end: 45
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/base.py::BaseSearch.__init__
  file: graphrag/query/structured_search/base.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
    \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None =\
    \ None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a BaseSearch instance with the provided model and context\
    \ builder.\n\nArgs:\n    model (ChatModel): The language model interface used\
    \ for this base search.\n    context_builder (T): The builder that constructs\
    \ the context for the search.\n    tokenizer (Tokenizer | None): Optional tokenizer\
    \ to use. If None, a tokenizer is selected via get_tokenizer().\n    model_params\
    \ (dict[str, Any] | None): Optional configuration parameters for the language\
    \ model.\n    context_builder_params (dict[str, Any] | None): Optional configuration\
    \ parameters for the context builder.\n\nReturns:\n    None: This constructor\
    \ initializes internal state and does not return a value."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 70
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
  file: graphrag/query/structured_search/basic_search/basic_context.py
  name: __init__
  signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
    \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
    \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
    \ str = \"id\",\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a BasicSearchContext with the provided embedding model and\
    \ text unit embeddings and prepare internal mappings.\n\nArgs:\n    text_embedder:\
    \ EmbeddingModel The embedding model used to embed text for similarity search.\n\
    \    text_unit_embeddings: BaseVectorStore The vector store containing embeddings\
    \ for text units.\n    text_units: list[TextUnit] | None Optional list of text\
    \ units to consider.\n    tokenizer: Tokenizer | None Optional tokenizer to use;\
    \ if not provided, get_tokenizer() is used.\n    embedding_vectorstore_key: str\
    \ Key in the vector store for identifying text units (default: \"id\").\n\nReturns:\n\
    \    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 28
  line_end: 41
  dependencies:
  - graphrag/query/structured_search/basic_search/basic_context.py::_map_ids
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
    \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
    \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n      \
    \  reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
    \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
    \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
    \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None = None,\n\
    \        local_system_prompt: str | None = None,\n        local_mixed_context:\
    \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str |\
    \ None = None,\n        response_type: str | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the DRIFT search context builder with necessary components.\n\
    \nThis constructor wires together the core components required for DRIFT-style\n\
    structured search, including the language model interface, embedding model,\n\
    entity context, prompts, and optional metadata. If some optional inputs are not\n\
    provided, sensible defaults are created.\n\nArgs:\n    model (ChatModel): The\
    \ chat-based language model to drive queries.\n    text_embedder (EmbeddingModel):\
    \ Embedding model used to encode text.\n    entities (list[Entity]): Entities\
    \ present in the current context.\n    entity_text_embeddings (BaseVectorStore):\
    \ Vector store containing entity text embeddings.\n    text_units (list[TextUnit]\
    \ | None): Optional list of TextUnit objects for the context.\n    reports (list[CommunityReport]\
    \ | None): Optional list of CommunityReport objects.\n    relationships (list[Relationship]\
    \ | None): Optional relationships among entities.\n    covariates (dict[str, list[Covariate]]\
    \ | None): Optional covariates keyed by name.\n    tokenizer (Tokenizer | None):\
    \ Optional Tokenizer to use; if None, a tokenizer will be created via get_tokenizer().\n\
    \    embedding_vectorstore_key (str): Key for the embedding vector store; defaults\
    \ to EntityVectorStoreKey.ID.\n    config (DRIFTSearchConfig | None): Optional\
    \ configuration for DRIFT search behavior; if None, a default config is created.\n\
    \    local_system_prompt (str | None): Optional override for the local system\
    \ prompt; defaults to DRIFT_LOCAL_SYSTEM_PROMPT.\n    local_mixed_context (LocalSearchMixedContext\
    \ | None): Optional prebuilt local mixed context; if None, a new one is initialized.\n\
    \    reduce_system_prompt (str | None): Optional prompt reduction instruction;\
    \ defaults to DRIFT_REDUCE_PROMPT.\n    response_type (str | None): Optional specifier\
    \ for the desired response type.\n\nReturns:\n    None\n\nSide effects:\n    -\
    \ Creates a default DRIFTSearchConfig if none is provided.\n    - Creates or retrieves\
    \ a tokenizer if one is not supplied.\n    - Sets default local system and reduction\
    \ prompts if not provided.\n    - May initialize a new LocalSearchMixedContext\
    \ via init_local_context_builder() when\n      local_mixed_context is not supplied.\n\
    \nRaises:\n    - Propagates any exceptions raised by get_tokenizer() or the LocalSearchMixedContext\
    \ constructor."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 78
  dependencies:
  - graphrag/config/models/drift_search_config.py::DRIFTSearchConfig
  - graphrag/query/structured_search/drift_search/drift_context.py::init_local_context_builder
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
  file: graphrag/query/structured_search/drift_search/primer.py
  name: __init__
  signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n     \
    \   text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
    \        tokenizer: Tokenizer | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the PrimerQueryProcessor.\n\nArgs:\n    chat_model (ChatModel):\
    \ The language model used to process the query.\n    text_embedder (EmbeddingModel):\
    \ The text embedding model.\n    reports (list[CommunityReport]): List of community\
    \ reports.\n    tokenizer (Tokenizer | None, optional): Token encoder for token\
    \ counting.\n\nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 31
  line_end: 50
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
  file: graphrag/query/structured_search/drift_search/primer.py
  name: __init__
  signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n \
    \       chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
    \    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the DRIFTPrimer.\n\nArgs:\n    config (DRIFTSearchConfig):\
    \ Configuration settings for DRIFT search.\n    chat_model (ChatModel): The language\
    \ model used for searching.\n    tokenizer (Tokenizer, optional): Tokenizer for\
    \ managing tokens. If not provided, a default tokenizer is obtained.\n\nReturns:\n\
    \    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 104
  line_end: 120
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
  file: graphrag/query/structured_search/drift_search/search.py
  name: __init__
  signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
    \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n  \
    \      query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
    \ | None = None,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the DRIFTSearch class.\n\nThis constructor wires core components\
    \ for DRIFT-style search, including the\nlanguage model interface, context, token\
    \ handling, and query lifecycle.\n\nArgs:\n    model (ChatModel): The chat-based\
    \ language model used for searching.\n    context_builder (DRIFTSearchContextBuilder):\
    \ Builder that holds DRIFT configuration and context.\n    tokenizer (Tokenizer,\
    \ optional): Token encoder used to tokenize input and manage tokens.\n       \
    \ If not provided, a default tokenizer is obtained via get_tokenizer().\n    query_state\
    \ (QueryState, optional): State tracked for the current search query.\n      \
    \  If not provided, a new QueryState is created.\n    callbacks (list[QueryCallbacks],\
    \ optional): Callback handlers for query events.\n        If not provided, an\
    \ empty list is used.\n\nReturns:\n    None\n\nSide effects:\n    - Assigns instance\
    \ attributes for model, context_builder, tokenizer, and query_state.\n    - Creates\
    \ a DRIFTPrimer instance configured with the current context and tokenizer.\n\
    \    - Initializes the local search component by calling init_local_search(),\n\
    \      preparing a LocalSearch with parameters derived from the DRIFT configuration."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 66
  dependencies:
  - graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer
  - graphrag/query/structured_search/drift_search/search.py::init_local_search
  - graphrag/query/structured_search/drift_search/state.py::QueryState
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
  file: graphrag/query/structured_search/global_search/community_context.py
  name: __init__
  signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
    \        communities: list[Community],\n        entities: list[Entity] | None\
    \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
    \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any] |\
    \ None = None,\n        random_state: int = 86,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a GlobalCommunityContext instance with the provided data\
    \ and optional configuration.\n\nArgs:\n    community_reports: Reports for communities\
    \ to consider.\n    communities: Community objects used to build the hierarchy\
    \ and starting points.\n    entities: Optional list of Entity objects to include\
    \ in the context.\n    tokenizer: Tokenizer to use; if None, a default tokenizer\
    \ is obtained via get_tokenizer().\n    dynamic_community_selection: Enable dynamic\
    \ selection of communities during processing.\n    dynamic_community_selection_kwargs:\
    \ Optional keyword arguments for configuring DynamicCommunitySelection.\n    random_state:\
    \ Seed for random number generation (default 86).\n\nReturns:\n    None\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 29
  line_end: 53
  dependencies:
  - graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: __init__
  signature: "def __init__(\n        self,\n        entities: list[Entity],\n    \
    \    entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
    \        text_units: list[TextUnit] | None = None,\n        community_reports:\
    \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
    \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
    \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
    \ str = EntityVectorStoreKey.ID,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize a LocalSearchMixedContext with the provided data and optional\
    \ configuration.\n\nArgs:\n    entities: list[Entity] The list of entities to\
    \ include.\n    entity_text_embeddings: BaseVectorStore The vector store containing\
    \ embeddings for entity text.\n    text_embedder: EmbeddingModel The embedding\
    \ model used to embed text for similarity search.\n    text_units: list[TextUnit]\
    \ | None Optional list of TextUnit objects to include.\n    community_reports:\
    \ list[CommunityReport] | None Optional list of CommunityReport objects to include.\n\
    \    relationships: list[Relationship] | None Optional list of Relationship objects\
    \ to include.\n    covariates: dict[str, list[Covariate]] | None Optional mapping\
    \ of covariates by key.\n    tokenizer: Tokenizer | None Optional tokenizer to\
    \ use; if None, get_tokenizer() will be used.\n    embedding_vectorstore_key:\
    \ str The key for the embedding vector store; defaults to EntityVectorStoreKey.ID.\n\
    \nReturns:\n    None\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 53
  line_end: 85
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: tests/unit/utils/test_encoding.py::test_encode_basic
  file: tests/unit/utils/test_encoding.py
  name: test_encode_basic
  signature: def test_encode_basic()
  decorators: []
  raises: []
  visibility: public
  docstring: "Get a tokenizer configured for the provided model configuration or encoding\
    \ model.\n\nThis function returns a Tokenizer suitable for the given model configuration\
    \ or for the specified encoding model. If a model_config is not provided, or if\
    \ model_config.encoding_model is explicitly set, a tiktoken-based tokenizer is\
    \ returned using the encoding_model. If a model_config is provided and encoding_model\
    \ is not set, a LitellmTokenizer is instantiated based on the model name found\
    \ in the configuration.\n\nParameters:\n- model_config (LanguageModelConfig |\
    \ None): The model configuration to determine which tokenizer to instantiate.\
    \ If None or if model_config.encoding_model is set, a tiktoken-based tokenizer\
    \ is returned.\n- encoding_model (str): The tiktoken encoding to use when falling\
    \ back to a tiktoken-based tokenizer.\n\nReturns:\n- Tokenizer: The tokenizer\
    \ instance configured for the provided model configuration or encoding model.\n\
    \nRaises:\n- ValueError, TypeError: If the inputs are invalid or incompatible\
    \ with the available tokenizers.\n\nExamples:\n    # Use tiktoken tokenizer with\
    \ a specified encoding\n    tokenizer = get_tokenizer(encoding_model=\"cl100k_base\"\
    )\n    tokens = tokenizer.encode(\"abc def\")\n\n    # Use LitellmTokenizer based\
    \ on a model configuration\n    cfg = LanguageModelConfig(name=\"my-model\", encoding_model=None)\n\
    \    tokenizer2 = get_tokenizer(model_config=cfg)\n    tokens2 = tokenizer2.encode(\"\
    hello world\")"
  code_example: null
  example_source: null
  line_start: 7
  line_end: 11
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: tests/unit/utils/test_encoding.py::test_num_tokens_empty_input
  file: tests/unit/utils/test_encoding.py
  name: test_num_tokens_empty_input
  signature: def test_num_tokens_empty_input()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the tokenizer returns zero tokens for an empty string.\n\n\
    Returns:\n    None\n\nRaises:\n    AssertionError: If the token count for empty\
    \ input is not zero."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 18
  dependencies:
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/mock.py::run
  file: graphrag/index/operations/embed_text/strategies/mock.py
  name: run
  signature: "def run(  # noqa RUF029 async is required for interface\n    input:\
    \ list[str],\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n \
    \   _args: dict[str, Any],\n) -> TextEmbeddingResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the embedding generation for the given texts using a mock strategy.\
    \ This asynchronous function processes an input collection of texts and returns\
    \ a TextEmbeddingResult containing embeddings for each input text. It reports\
    \ progress via a progress ticker and uses the _embed_text helper to generate a\
    \ 3-dimensional embedding per text.\n\nParameters\n    input: Iterable[str]\n\
    \        The input texts to embed. Note: strings are Iterable; if a single string\
    \ is passed, it will be treated as an iterable of characters unless wrapped in\
    \ a collection of strings.\n    callbacks: WorkflowCallbacks\n        Callback\
    \ hooks invoked for progress updates.\n    cache: PipelineCache\n        Cache\
    \ used for embedding operations.\n    _args: dict[str, Any]\n        Additional\
    \ optional arguments.\n\nReturns\n    TextEmbeddingResult: Result containing embeddings\
    \ for the input texts. Each embedding is a list[float] of length 3.\n\nRaises\n\
    \    This function may raise exceptions propagated from the progress ticker or\
    \ the embedding operation. No exceptions are guaranteed."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 29
  dependencies:
  - graphrag/index/operations/embed_text/strategies/mock.py::_embed_text
  - graphrag/index/operations/embed_text/strategies/typing.py::TextEmbeddingResult
  - graphrag/logger/progress.py::progress_ticker
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
  file: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  name: get_summarized
  signature: "def get_summarized(\n        nodes: pd.DataFrame, edges: pd.DataFrame,\
    \ semaphore: asyncio.Semaphore\n    )"
  decorators: []
  raises: []
  visibility: public
  docstring: "Summarize descriptions for nodes and edges and return summary dataframes.\n\
    \nArgs:\n    nodes: pd.DataFrame\n        DataFrame containing node information\
    \ with at least a title and a description per node.\n    edges: pd.DataFrame\n\
    \        DataFrame containing edge information with at least source, target, and\
    \ a description per edge.\n    semaphore: asyncio.Semaphore\n        Semaphore\
    \ used to limit concurrent summarization operations.\n\nReturns:\n    tuple[pd.DataFrame,\
    \ pd.DataFrame]\n        A tuple containing:\n        - entity_descriptions: DataFrame\
    \ with columns 'title' and 'description' summarizing each node.\n        - relationship_descriptions:\
    \ DataFrame with columns 'source', 'target', and 'description' summarizing each\
    \ edge.\n\nRaises:\n    Exceptions propagated from the underlying asynchronous\
    \ summarization processes and tasks."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 93
  dependencies:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::do_summarize_descriptions
  - graphrag/logger/progress.py::progress_ticker
  called_by:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_rpm
  signature: def test_rpm()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the rate limiter enforces RPM limits.\n\nArgs:\n    None\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails during\
    \ the test."
  code_example: null
  example_source: null
  line_start: 93
  line_end: 118
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::assert_stagger
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_tpm
  signature: def test_tpm()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the rate limiter enforces TPM limits.\n\nThis test creates\
    \ a rate limiter with a static strategy configured with a TPM and a period,\n\
    then issues _num_requests token acquisitions with token_count set to _tokens_per_request\
    \ and\nrecords the time elapsed for each. The recorded times are binned into intervals\
    \ of _period_in_seconds.\nThe test expects the number of bins to be equal to ceil((_num_requests\
    \ * _tokens_per_request) / _tpm)\nand that the maximum number of requests per\
    \ bin is _tpm // _tokens_per_request.\n\nReturns:\n    None\n\nRaises:\n    AssertionError:\
    \ If TPM constraints are violated."
  code_example: null
  example_source: null
  line_start: 121
  line_end: 146
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_token_in_request_exceeds_tpm
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_token_in_request_exceeds_tpm
  signature: def test_token_in_request_exceeds_tpm()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that the rate limiter allows for requests that use more tokens
    than the TPM.


    A rate limiter could be configured with a tpm of 1000 but a request may use 2000
    tokens,

    greater than the tpm limit but still below the context window limit of the underlying
    model.

    In this case, the request should still be allowed to proceed but may take up its
    own rate limit bin.'
  code_example: null
  example_source: null
  line_start: 149
  line_end: 175
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_rpm_as_limiting_factor
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_rpm_and_tpm_with_rpm_as_limiting_factor
  signature: def test_rpm_and_tpm_with_rpm_as_limiting_factor()
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Test that the rate limiter enforces RPM and TPM limits.\n\nArgs:\n\
    \    None\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion\
    \ fails during the test.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 178
  line_end: 204
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::assert_stagger
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_and_tpm_with_tpm_as_limiting_factor
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_rpm_and_tpm_with_tpm_as_limiting_factor
  signature: def test_rpm_and_tpm_with_tpm_as_limiting_factor()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the rate limiter enforces TPM limits when TPM is the limiting\
    \ factor.\n\nThis test configures a static rate limiter with rpm, tpm and a period,\
    \ and issues _num_requests acquisitions\nwith token_count set to _tokens_per_request\
    \ (non-zero). It records the elapsed time for each acquisition and bins\nthe results\
    \ into intervals of _period_in_seconds. The TPM value drives the binning and per\
    \ bin capacity.\n\nExpected behavior:\n- Number of bins equals ceil((_num_requests\
    \ * _tokens_per_request) / _tpm).\n- Maximum number of requests per bin equals\
    \ _tpm // _tokens_per_request.\n\nArgs:\n    None\n\nReturns:\n    None\n\nRaises:\n\
    \    AssertionError: If any assertion fails during the test."
  code_example: null
  example_source: null
  line_start: 207
  line_end: 233
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::assert_stagger
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_rpm_threaded
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_rpm_threaded
  signature: def test_rpm_threaded()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the rate limiter enforces RPM limits in a threaded environment.\n\
    \nArgs:\n  None\n\nReturns:\n  None\n\nRaises:\n  AssertionError: If any assertion\
    \ fails during the test."
  code_example: null
  example_source: null
  line_start: 251
  line_end: 308
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::assert_stagger
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/litellm_services/test_rate_limiter.py::test_tpm_threaded
  file: tests/unit/litellm_services/test_rate_limiter.py
  name: test_tpm_threaded
  signature: def test_tpm_threaded()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the rate limiter enforces TPM limits in a threaded environment.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails during\
    \ the test."
  code_example: null
  example_source: null
  line_start: 311
  line_end: 368
  dependencies:
  - tests/unit/litellm_services/utils.py::assert_max_num_values_per_period
  - tests/unit/litellm_services/utils.py::assert_stagger
  - tests/unit/litellm_services/utils.py::bin_time_intervals
  called_by: []
- node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::test_split_multiple_texts_on_tokens
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: test_split_multiple_texts_on_tokens
  signature: def test_split_multiple_texts_on_tokens()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test that split_multiple_texts_on_tokens calls the tick callback when
    processing multiple texts.


    This test creates a tokenizer configured with a mock tokenizer, passes two texts
    to split_multiple_texts_on_tokens, and asserts that the tick callback is invoked.'
  code_example: null
  example_source: null
  line_start: 113
  line_end: 129
  dependencies:
  - graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
  - graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
  called_by: []
- node_id: graphrag/index/run/run_pipeline.py::_run_pipeline
  file: graphrag/index/run/run_pipeline.py
  name: _run_pipeline
  signature: "def _run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
    \    context: PipelineRunContext,\n) -> AsyncIterable[PipelineRunResult]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Execute the provided pipeline asynchronously and yield results for each\
    \ workflow as it completes.\n\nArgs:\n    pipeline: Pipeline - The pipeline to\
    \ run\n    config: GraphRagConfig - Configuration for the run\n    context: PipelineRunContext\
    \ - Runtime context, including storage, callbacks, and state\n\nReturns:\n   \
    \ AsyncIterable[PipelineRunResult] - An async iterable that yields a PipelineRunResult\
    \ for each workflow as it runs, and yields a final result with errors if an exception\
    \ occurs.\n\nRaises:\n    None - This function handles exceptions internally and\
    \ does not propagate them to the caller."
  code_example: null
  example_source: null
  line_start: 104
  line_end: 139
  dependencies:
  - graphrag/index/run/run_pipeline.py::_dump_json
  - graphrag/index/typing/pipeline_run_result.py::PipelineRunResult
  called_by:
  - graphrag/index/run/run_pipeline.py::run_pipeline
- node_id: graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
  file: graphrag/query/input/retrieval/relationships.py
  name: get_in_network_relationships
  signature: "def get_in_network_relationships(\n    selected_entities: list[Entity],\n\
    \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
    ,\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get all directed relationships between the selected entities, sorted\
    \ by ranking_attribute.\n\nArgs:\n  selected_entities: The selected entities to\
    \ consider.\n  relationships: The pool of relationships to search within.\n  ranking_attribute:\
    \ The attribute name used for sorting; defaults to \"rank\".\n\nReturns:\n  list[Relationship]:\
    \ The relationships where both the source and target are in the selected entities;\
    \ if more than one such relationship exists, they are returned sorted by ranking_attribute;\
    \ otherwise the original list is returned."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 31
  dependencies:
  - graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
  called_by:
  - graphrag/query/context_builder/local_context.py::_filter_relationships
- node_id: graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
  file: graphrag/query/input/retrieval/relationships.py
  name: get_out_network_relationships
  signature: "def get_out_network_relationships(\n    selected_entities: list[Entity],\n\
    \    relationships: list[Relationship],\n    ranking_attribute: str = \"rank\"\
    ,\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get relationships that connect the selected entities to other entities,\
    \ considering both directions (outgoing from and incoming to the selected set).\
    \ The other endpoint may lie outside the selected set. The resulting relationships\
    \ are sorted by the specified ranking_attribute.\n\nArgs:\n  selected_entities\
    \ (list[Entity]): The selected entities; used as one end of the relationships\
    \ to consider.\n  relationships (list[Relationship]): The pool of relationships\
    \ to search within.\n  ranking_attribute (str): The attribute name used for sorting;\
    \ defaults to \"rank\".\n\nReturns:\n  list[Relationship]: The relationships where\
    \ either the source is in the selected_entities and the target is outside the\
    \ selected set, or the target is in the selected_entities and the source is outside\
    \ the selected set; i.e., relationships connected to the selected entities but\
    \ with the other endpoint possibly outside the set. The list is sorted by ranking_attribute.\n\
    \nNotes:\n  If the input is empty, an empty list is returned."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 54
  dependencies:
  - graphrag/query/input/retrieval/relationships.py::sort_relationships_by_rank
  called_by:
  - graphrag/query/context_builder/local_context.py::_filter_relationships
- node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  file: graphrag/index/operations/summarize_communities/graph_context/sort_context.py
  name: sort_context
  signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer: Tokenizer,\n\
    \    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
    \ int | None = None,\n    node_name_column: str = schemas.TITLE,\n    node_details_column:\
    \ str = schemas.NODE_DETAILS,\n    edge_id_column: str = schemas.SHORT_ID,\n \
    \   edge_details_column: str = schemas.EDGE_DETAILS,\n    edge_degree_column:\
    \ str = schemas.EDGE_DEGREE,\n    edge_source_column: str = schemas.EDGE_SOURCE,\n\
    \    edge_target_column: str = schemas.EDGE_TARGET,\n    claim_details_column:\
    \ str = schemas.CLAIM_DETAILS,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Sorts context by degree in descending order, optimizing for performance.\n\
    \nArgs:\n    local_context: list[dict]. Local context data; each entry may contain\
    \ edge details under edge_details_column and associated node and claim information\
    \ as defined by the surrounding schema.\n    tokenizer: Tokenizer. Tokenizer used\
    \ to count tokens for max_context_tokens to enforce length constraints.\n    sub_community_reports:\
    \ list[dict] | None. Optional list of sub-community reports to include at the\
    \ top of the context.\n    max_context_tokens: int | None. Optional maximum number\
    \ of tokens for the produced context; if exceeded, the context is truncated accordingly.\n\
    \    node_name_column: str. Column name used to identify a node's display name.\n\
    \    node_details_column: str. Column name for the node's details payload.\n \
    \   edge_id_column: str. Column name for the edge identifier.\n    edge_details_column:\
    \ str. Column name for the edge details payload.\n    edge_degree_column: str.\
    \ Column name for the edge degree measure.\n    edge_source_column: str. Column\
    \ name for the edge source node.\n    edge_target_column: str. Column name for\
    \ the edge target node.\n    claim_details_column: str. Column name for the claim\
    \ details associated with nodes.\n\nReturns:\n    str. The consolidated context\
    \ string built from Entities, Claims, and Relationships, optionally prefixed with\
    \ sub-community reports, truncated to max_context_tokens if specified.\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 11
  line_end: 126
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::_get_context_string
  called_by:
  - graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
  - tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
  - tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
- node_id: graphrag/index/utils/is_null.py::is_null
  file: graphrag/index/utils/is_null.py
  name: is_null
  signature: 'def is_null(value: Any) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Check if value is None or NaN.\n\nArgs:\n    value (Any): The\
    \ value to check.\n\nReturns:\n    bool: True if value is None or NaN (NaN is\
    \ recognized only for floating-point values); otherwise False.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 10
  line_end: 19
  dependencies:
  - graphrag/index/utils/is_null.py::is_nan
  - graphrag/index/utils/is_null.py::is_none
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_str
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_run_strategy_str
  signature: def test_run_strategy_str()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Run the given chunking strategy on the input data and return the produced
    text chunks as strings.


    Args:

    - strategy_exec (Callable): The strategy function to execute to generate TextChunk
    objects. It should accept (input, config, tick) and return a list of TextChunk-like
    objects that expose a text_chunk attribute.

    - input: str | list[str] | list[tuple[str, str]]: The input data to chunk. May
    be a single string, a list of strings, or a list of (document_id, text) tuples
    depending on the strategy.

    - config: Any: Configuration for chunking, including size, overlap, and encoding
    model. This can be a real configuration object or a mock used in tests.

    - tick: Any: Progress ticker used to report progress.


    Returns:

    - list[str]: The produced text chunks, extracted from each TextChunk as text_chunk,
    in the same order as produced by the strategy.


    Raises:

    - Propagates any exception raised by strategy_exec. If a produced chunk lacks
    a text_chunk attribute, an AttributeError may be raised.


    Examples:

    - run_strategy(my_strategy, "text", cfg, tick) -> ["text"]'
  code_example: null
  example_source: null
  line_start: 62
  line_end: 76
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_str
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_run_strategy_arr_str
  signature: def test_run_strategy_arr_str()
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the given chunking strategy on the input data and return the produced\
    \ chunks.\n\nArgs:\n    strategy_exec: ChunkStrategy\n        The strategy function\
    \ to execute to generate TextChunk objects. It should accept\n        (input,\
    \ config, tick) and return a list of TextChunk-like objects that expose a\n  \
    \      text_chunk attribute.\n    input: ChunkInput\n        The input data to\
    \ chunk. May be a string or a list of strings, or a list of tuples\n        of\
    \ (document_id, text content).\n    config: ChunkingConfig\n        Configuration\
    \ for chunking, including size, overlap, and encoding model.\n    tick: ProgressTicker\n\
    \        Progress ticker used during execution.\n\nReturns:\n    list[str | tuple[list[str]\
    \ | None, str, int]]\n    The produced chunks. Each element is either a string\
    \ (for string-based input) or a tuple\n    of the form (list[str] | None, str,\
    \ int) representing the chunked content, a representative\n    text, and the token\
    \ count for that chunk."
  code_example: null
  example_source: null
  line_start: 79
  line_end: 98
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_run_strategy_arr_tuple
  signature: def test_run_strategy_arr_tuple()
  decorators: []
  raises: []
  visibility: public
  docstring: 'Test run_strategy with input as a list of (text, token) tuples.


    Verifies that when input is [("text test for run strategy", "3"), ("use for strategy",
    "5")] and the strategy returns two TextChunk objects with text_chunk values corresponding
    to the input texts and n_tokens values 5 and 3, run_strategy returns a list of
    tuples where each tuple is: ( [corresponding source texts], text_chunk, n_tokens
    as int ). For the given setup, the expected result is:


    - ( ["text test for run strategy"], "text test for run strategy", 5 )

    - ( ["use for strategy"], "use for strategy", 3 )


    The test uses mocks for config and tick and asserts the produced value matches
    the expected list.'
  code_example: null
  example_source: null
  line_start: 101
  line_end: 128
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_run_strategy_arr_tuple_same_doc
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_run_strategy_arr_tuple_same_doc
  signature: def test_run_strategy_arr_tuple_same_doc()
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the given chunking strategy on the input data and return the produced\
    \ chunks.\n\nArgs:\n    strategy_exec (ChunkStrategy): The strategy function to\
    \ execute to generate TextChunk-like objects. It should accept (input, config,\
    \ tick) and return a list of objects exposing a text_chunk attribute.\n    input\
    \ (ChunkInput): The input data to chunk. May be a string or a list of strings,\
    \ or a list of tuples (text, token) as used in tests.\n    config (ChunkingConfig):\
    \ Configuration for chunking, including size, overlap, and encoding model.\n \
    \   tick (ProgressTicker): Progress ticker used to report progress.\n\nReturns:\n\
    \    list[str | tuple[list[str] | None, str, int]]: A list of produced chunks.\
    \ Each item is either a string or a tuple where\n        the first element is\
    \ a list of source texts (or None), the second element is the text chunk content,\
    \ and the\n        third element is the number of tokens.\n\nRaises:\n    Propagates\
    \ exceptions raised by strategy_exec or invalid input processing."
  code_example: null
  example_source: null
  line_start: 131
  line_end: 158
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  - graphrag/index/operations/chunk_text/typing.py::TextChunk
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _antijoin_reports
  signature: 'def _antijoin_reports(df: pd.DataFrame, reports: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return records in df that are not in reports.\n\nArgs:\n    df: The\
    \ DataFrame to apply the exclusion to.\n    reports: The DataFrame containing\
    \ rows to remove from df.\n\nReturns:\n    pd.DataFrame: The rows from df whose\
    \ COMMUNITY_ID value is not present in reports."
  code_example: null
  example_source: null
  line_start: 274
  line_end: 276
  dependencies:
  - graphrag/index/utils/dataframes.py::antijoin
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::_execute
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: _execute
  signature: "def _execute(\n    model: EmbeddingModel,\n    chunks: list[list[str]],\n\
    \    tick: ProgressTicker,\n    semaphore: asyncio.Semaphore,\n) -> list[list[float]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronously embed batches of text chunks using the provided EmbeddingModel,\
    \ honoring the concurrency limit with the supplied semaphore and reporting progress\
    \ through the tick callback after processing each batch. The embeddings from all\
    \ input chunks are flattened into a single 1D list of floats in batch order and\
    \ returned.\n\nArgs:\n  model: EmbeddingModel - The embedding model to use.\n\
    \  chunks: list[list[str]] - A list of text chunks grouped into batches to embed.\n\
    \  tick: ProgressTicker - Callback to report progress after each batch is processed.\n\
    \  semaphore: asyncio.Semaphore - Semaphore controlling concurrent embedding calls.\n\
    \nReturns:\n  list[float] - A flat list of embedding floats for all inputs, concatenated\
    \ in batch order.\n\nRaises:\n  Exceptions raised by the embedding model (via\
    \ model.aembed_batch) may be propagated to the caller."
  code_example: null
  example_source: null
  line_start: 88
  line_end: 104
  dependencies:
  - graphrag/index/operations/embed_text/strategies/openai.py::embed
  called_by:
  - graphrag/index/operations/embed_text/strategies/openai.py::run
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _at_level
  signature: 'def _at_level(level: int, df: pd.DataFrame) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Return records at the given level.\n\nArgs:\n    level: The level to\
    \ filter by (int).\n    df: DataFrame containing community records, expected to\
    \ have a COMMUNITY_LEVEL column.\n\nReturns:\n    pd.DataFrame: A DataFrame containing\
    \ only records where COMMUNITY_LEVEL equals level.\n\nRaises:\n    KeyError: If\
    \ the COMMUNITY_LEVEL column is not present in df."
  code_example: null
  example_source: null
  line_start: 269
  line_end: 271
  dependencies:
  - graphrag/index/utils/dataframes.py::where_column_equals
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
- node_id: graphrag/query/context_builder/local_context.py::get_candidate_context
  file: graphrag/query/context_builder/local_context.py
  name: get_candidate_context
  signature: "def get_candidate_context(\n    selected_entities: list[Entity],\n \
    \   entities: list[Entity],\n    relationships: list[Relationship],\n    covariates:\
    \ dict[str, list[Covariate]],\n    include_entity_rank: bool = True,\n    entity_rank_description:\
    \ str = \"number of relationships\",\n    include_relationship_weight: bool =\
    \ False,\n) -> dict[str, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare candidate context data tables (entities, relationships, and\
    \ covariates) to be used in a system prompt.\n\nArgs:\n  selected_entities (list[Entity]):\
    \ The selected entities for which to build candidate context.\n  entities (list[Entity]):\
    \ The pool of entities to consider when constructing the candidate entity DataFrame.\n\
    \  relationships (list[Relationship]): The pool of relationships to filter to\
    \ candidate relationships.\n  covariates (dict[str, list[Covariate]]): Mapping\
    \ from covariate group names to Covariate objects to include in the context.\n\
    \  include_entity_rank (bool): Whether to include a rank column for entities.\
    \ Default is True.\n  entity_rank_description (str): Header name for the rank\
    \ column when include_entity_rank is True. Default is \"number of relationships\"\
    .\n  include_relationship_weight (bool): Whether to include a weight column in\
    \ the relationships DataFrame. Default is False.\n\nReturns:\n  dict[str, pd.DataFrame]:\
    \ A dictionary containing the following DataFrames:\n    - \"relationships\":\
    \ DataFrame of candidate relationships (optionally including weight).\n    - \"\
    entities\": DataFrame of candidate entities (with optional rank column).\n   \
    \ - For each key in covariates, a lowercase-keyed DataFrame of the corresponding\
    \ covariates."
  code_example: null
  example_source: null
  line_start: 320
  line_end: 357
  dependencies:
  - graphrag/query/input/retrieval/covariates.py::get_candidate_covariates
  - graphrag/query/input/retrieval/covariates.py::to_covariate_dataframe
  - graphrag/query/input/retrieval/entities.py::to_entity_dataframe
  - graphrag/query/input/retrieval/relationships.py::get_candidate_relationships
  - graphrag/query/input/retrieval/relationships.py::get_entities_from_relationships
  - graphrag/query/input/retrieval/relationships.py::to_relationship_dataframe
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
- node_id: graphrag/cli/query.py::_resolve_output_files
  file: graphrag/cli/query.py
  name: _resolve_output_files
  signature: "def _resolve_output_files(\n    config: GraphRagConfig,\n    output_list:\
    \ list[str],\n    optional_list: list[str] | None = None,\n) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "\"\"\"Read indexing output files to a dataframe dict.\n\nArgs:\n   \
    \ config: GraphRagConfig The configuration for GraphRag, including outputs for\
    \ multi-index search.\n    output_list: list[str] Names of the output dataframe\
    \ keys to load from storage.\n    optional_list: list[str] | None Optional list\
    \ of additional output dataframe keys to load if present.\n\nReturns:\n    dict[str,\
    \ Any]: A dictionary containing the loaded dataframes and metadata describing\
    \ the indexing layout.\n        If config.outputs is truthy (multi-index search):\n\
    \          - \"multi-index\": True\n          - \"num_indexes\": int number of\
    \ indexes (len(config.outputs))\n          - \"index_names\": config.outputs.keys()\n\
    \          - For each name in output_list: a list of DataFrames loaded from storage\
    \ (one per index)\n          - For each optional_file in optional_list, a key\
    \ optional_file with a list of DataFrames if present, otherwise an empty list\n\
    \        If config.outputs is falsy (single-index search):\n          - \"multi-index\"\
    : False\n          - For each name in output_list: the loaded DataFrame\n    \
    \      - For each optional_file in optional_list: the loaded DataFrame if present,\
    \ otherwise None\n\"\"\""
  code_example: null
  example_source: null
  line_start: 477
  line_end: 534
  dependencies:
  - graphrag/utils/api.py::create_storage_from_config
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::storage_has_table
  called_by:
  - graphrag/cli/query.py::run_global_search
  - graphrag/cli/query.py::run_local_search
  - graphrag/cli/query.py::run_drift_search
  - graphrag/cli/query.py::run_basic_search
- node_id: graphrag/config/load_config.py::_get_config_path
  file: graphrag/config/load_config.py
  name: _get_config_path
  signature: 'def _get_config_path(root_dir: Path, config_filepath: Path | None) ->
    Path'
  decorators: []
  raises:
  - FileNotFoundError
  visibility: protected
  docstring: "Resolve and return the configuration file path.\n\nArgs:\n    root_dir\
    \ (Path): The root directory of the project to search when config_filepath is\
    \ not provided.\n    config_filepath (Path | None): The explicit path to the config\
    \ file. If None, the config file will be searched for in root_dir.\n\nReturns:\n\
    \    Path: The resolved configuration file path.\n\nRaises:\n    FileNotFoundError:\
    \ If the specified config file does not exist or if no configuration file can\
    \ be found in the root directory."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 112
  dependencies:
  - graphrag/config/load_config.py::_search_for_config_in_root_dir
  called_by:
  - graphrag/config/load_config.py::load_config
- node_id: graphrag/index/operations/embed_text/embed_text.py::_get_index_name
  file: graphrag/index/operations/embed_text/embed_text.py
  name: _get_index_name
  signature: 'def _get_index_name(vector_store_config: dict, embedding_name: str)
    -> str'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get the index name for the embedding in the vector store.\n\nArgs:\n\
    \    vector_store_config (dict): Configuration for the vector store; may include\
    \ container_name (defaults to \"default\") and type.\n    embedding_name (str):\
    \ The embedding name used to construct the index name.\n\nReturns:\n    str: The\
    \ computed index name.\n\nRaises:\n    Exception: Propagates exceptions raised\
    \ by create_index_name or other internal calls."
  code_example: null
  example_source: null
  line_start: 220
  line_end: 226
  dependencies:
  - graphrag/config/embeddings.py::create_index_name
  called_by:
  - graphrag/index/operations/embed_text/embed_text.py::embed_text
- node_id: graphrag/utils/api.py::get_embedding_store
  file: graphrag/utils/api.py
  name: get_embedding_store
  signature: "def get_embedding_store(\n    config_args: dict[str, dict],\n    embedding_name:\
    \ str,\n) -> BaseVectorStore"
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the embedding description store.\n\nGiven a mapping of index\
    \ configurations in config_args and a target embedding name embedding_name, construct\
    \ and connect the appropriate vector store(s). If there is only a single configured\
    \ index, this returns that single vector store; otherwise it returns a MultiVectorStore\
    \ that aggregates multiple vector stores across indexes.\n\nArgs:\n    config_args:\
    \ dict[str, dict]\n        Configuration for one or more embedding indexes. Each\
    \ key is an index identifier and each value is a dictionary of store configuration\
    \ options used to instantiate a vector store.\n    embedding_name: str\n     \
    \   Name of the embedding to configure and fetch the store for.\n\nReturns:\n\
    \    BaseVectorStore\n        A vector store capable of handling the requested\
    \ embedding. If multiple indexes are configured, a MultiVectorStore is returned.\n\
    \nRaises:\n    Exception\n        If underlying vector store creation or connection\
    \ fails.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 97
  line_end: 141
  dependencies:
  - graphrag/config/embeddings.py::create_index_name
  - graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  - graphrag/vector_stores/factory.py::VectorStoreFactory
  called_by:
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/api/query.py::basic_search_streaming
- node_id: tests/unit/utils/test_embeddings.py::test_create_index_name
  file: tests/unit/utils/test_embeddings.py
  name: test_create_index_name
  signature: def test_create_index_name()
  decorators: []
  raises: []
  visibility: public
  docstring: "Create an index name for the embedding store.\n\nThis function creates\
    \ a string by prefixing the embedding's index with the container_name and replacing\
    \ dots in the embedding_name with dashes to accommodate vector stores that do\
    \ not support dots.\n\nArgs:\n    container_name (str): The partition/prefix for\
    \ the index name.\n    embedding_name (str): The embedding name; must be one of\
    \ the supported embeddings defined in graphrag.index.config.embeddings.\n    validate\
    \ (bool): Whether to validate the embedding_name against the supported list. Defaults\
    \ to True.\n\nReturns:\n    str: The generated index name in the format \"<container_name>-<embedding_name_with_dashes>\"\
    .\n\nRaises:\n    KeyError: If validate is True and the embedding_name is not\
    \ a supported embedding."
  code_example: null
  example_source: null
  line_start: 9
  line_end: 11
  dependencies:
  - graphrag/config/embeddings.py::create_index_name
  called_by: []
- node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_throws
  file: tests/unit/utils/test_embeddings.py
  name: test_create_index_name_invalid_embedding_throws
  signature: def test_create_index_name_invalid_embedding_throws()
  decorators: []
  raises: []
  visibility: public
  docstring: "Create an index name for the embedding store.\n\nArgs:\n    container_name\
    \ (str): Partition identifier used for differentiating multiple embedding sets\
    \ within a vector store; it is added as a prefix to the index name.\n    embedding_name\
    \ (str): The fixed embedding name; the available list is defined in graphrag.index.config.embeddings.\n\
    \    validate (bool): If True, validate embedding_name and raise KeyError for\
    \ invalid names; if False, skip validation.\n\nReturns:\n    str: The constructed\
    \ index name with dots in embedding_name replaced by dashes and concatenated with\
    \ container_name using a dash (e.g., 'default-entity-title').\n\nRaises:\n   \
    \ KeyError: If validate is True and embedding_name is not a valid embedding."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 16
  dependencies:
  - graphrag/config/embeddings.py::create_index_name
  called_by: []
- node_id: tests/unit/utils/test_embeddings.py::test_create_index_name_invalid_embedding_does_not_throw
  file: tests/unit/utils/test_embeddings.py
  name: test_create_index_name_invalid_embedding_does_not_throw
  signature: def test_create_index_name_invalid_embedding_does_not_throw()
  decorators: []
  raises: []
  visibility: public
  docstring: "Create an index name for the embedding store by prefixing the container\
    \ name and normalizing the embedding name.\n\nThe container_name parameter is\
    \ used for partitioning across multiple embedding sets within a vector store and\
    \ is added as a prefix to the index name.\nThe embedding_name is fixed, with the\
    \ available list defined in graphrag.index.config.embeddings. Dots in the embedding_name\
    \ are replaced with dashes to accommodate vector stores that do not support dots.\n\
    \nArgs:\n    container_name (str): Partition identifier used for differentiating\
    \ multiple embedding sets within a vector store; it is added as a prefix to the\
    \ index name.\n    embedding_name (str): The embedding name; the available list\
    \ is defined in graphrag.index.config.embeddings.\n    validate (bool): If True,\
    \ validate embedding_name and raise KeyError if invalid; if False, skip validation.\n\
    \nReturns:\n    str: The constructed index name.\n\nRaises:\n    KeyError: If\
    \ validate is True and embedding_name is not a valid embedding name."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 21
  dependencies:
  - graphrag/config/embeddings.py::create_index_name
  called_by: []
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: _create_embeddings
  signature: "def _create_embeddings(\n    model_config: \"LanguageModelConfig\",\n\
    \    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelEmbedding,\
    \ AFixedModelEmbedding]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wrap the base litellm embedding function with the model configuration\
    \ and additional features.\n\nWrap the base litellm embedding function with instance\
    \ variables based on the model configuration. Then wrap additional features such\
    \ as rate limiting, retries, and caching, if enabled.\n\nFinal function composition\
    \ order:\n- Logging(Cache(Retries(RateLimiter(ModelEmbedding()))))\n\nArgs:\n\
    \  model_config: LanguageModelConfig. The configuration for the language model.\n\
    \  cache: PipelineCache | None. Optional cache for storing responses.\n  cache_key_prefix:\
    \ str. Prefix for cache keys.\n\nReturns:\n  tuple[FixedModelEmbedding, AFixedModelEmbedding].\
    \ A tuple containing the synchronous and asynchronous embedding functions.\n\n\
    Raises:\n  ValueError: Azure Managed Identity authentication is only supported\
    \ for Azure models."
  code_example: null
  example_source: null
  line_start: 100
  line_end: 172
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_base_embeddings
  - graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
  - graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
  - graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
  - graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
  called_by:
  - graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
- node_id: graphrag/index/workflows/update_community_reports.py::_update_community_reports
  file: graphrag/index/workflows/update_community_reports.py
  name: _update_community_reports
  signature: "def _update_community_reports(\n    previous_storage: PipelineStorage,\n\
    \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n \
    \   community_id_mapping: dict,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update the community reports output by merging old and delta reports\
    \ and writing the result to storage.\n\nArgs:\n    previous_storage: PipelineStorage\n\
    \        Storage containing the existing/previous community reports.\n    delta_storage:\
    \ PipelineStorage\n        Storage containing the delta (updated) community reports.\n\
    \    output_storage: PipelineStorage\n        Storage to write the merged community\
    \ reports to.\n    community_id_mapping: dict\n        Mapping from original delta\
    \ community IDs to final IDs.\n\nReturns:\n    pd.DataFrame\n        The updated\
    \ community reports aligned to COMMUNITY_REPORTS_FINAL_COLUMNS.\n\nRaises:\n \
    \   ValueError\n        Could not find community_reports.parquet in storage.\n\
    \    Exception\n        Exceptions raised by the storage backend or parquet reader\
    \ during the load operation.\n    Exception\n        Exceptions raised by the\
    \ storage backend during the write operation may propagate.\n    KeyError\n  \
    \      If required columns such as 'community' or 'parent' are missing from the\
    \ input data when merging."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 66
  dependencies:
  - graphrag/index/update/communities.py::_update_and_merge_community_reports
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_community_reports.py::run_workflow
- node_id: graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: _create_completions
  signature: "def _create_completions(\n    model_config: \"LanguageModelConfig\"\
    ,\n    cache: \"PipelineCache | None\",\n    cache_key_prefix: str,\n) -> tuple[FixedModelCompletion,\
    \ AFixedModelCompletion]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Wrap the base litellm completion function with the model configuration\
    \ and additional features.\n\nWrap the base litellm completion function with instance\
    \ variables based on the model configuration.\nThen wrap additional features such\
    \ as rate limiting, retries, and caching, if enabled.\n\nFinal function composition\
    \ order:\n- Logging(Cache(Retries(RateLimiter(ModelCompletion()))))\n\nArgs:\n\
    \    model_config: The configuration for the language model.\n    cache: Optional\
    \ cache for storing responses.\n    cache_key_prefix: Prefix for cache keys.\n\
    \nReturns:\n    A tuple containing the synchronous and asynchronous completion\
    \ functions."
  code_example: null
  example_source: null
  line_start: 114
  line_end: 186
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_base_completions
  - graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::with_cache
  - graphrag/language_model/providers/litellm/request_wrappers/with_logging.py::with_logging
  - graphrag/language_model/providers/litellm/request_wrappers/with_rate_limiter.py::with_rate_limiter
  - graphrag/language_model/providers/litellm/request_wrappers/with_retries.py::with_retries
  called_by:
  - graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
- node_id: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
  file: graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py
  name: sort_context
  signature: "def sort_context(\n    local_context: list[dict],\n    tokenizer: Tokenizer,\n\
    \    sub_community_reports: list[dict] | None = None,\n    max_context_tokens:\
    \ int | None = None,\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Sort local context (list of text units) by total degree of associated\
    \ nodes in descending order.\n\nArgs:\n    local_context: list[dict] - Local context\
    \ data; a list of dictionaries representing text units.\n    tokenizer: Tokenizer\
    \ - Tokenizer used to count tokens for max_context_tokens to enforce length constraints.\n\
    \    sub_community_reports: list[dict] | None - Optional list of dictionaries\
    \ for sub-community reports to include at the top of the resulting context string.\n\
    \    max_context_tokens: int | None - Maximum number of tokens allowed in the\
    \ resulting context string; if provided, text units are added until the limit\
    \ would be exceeded.\n\nReturns:\n    str: The context string built from the selected\
    \ text units; if max_context_tokens is provided, the string includes as many units\
    \ as fit within the token limit; otherwise, the full sorted context is returned,\
    \ including sub_community_reports if present."
  code_example: null
  example_source: null
  line_start: 58
  line_end: 85
  dependencies:
  - graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::get_context_string
  called_by:
  - graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
  - graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
- node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::run_generate
  file: graphrag/index/operations/summarize_communities/summarize_communities.py
  name: run_generate
  signature: def run_generate(record)
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate a community summary for a single record.\n\nArgs:\n  record:\
    \ dict-like containing the keys defined by schemas.COMMUNITY_ID, schemas.COMMUNITY_LEVEL,\
    \ and schemas.CONTEXT_STRING. The function uses record[schemas.COMMUNITY_ID],\
    \ record[schemas.COMMUNITY_LEVEL], and record[schemas.CONTEXT_STRING] to generate\
    \ the report.\n\nReturns:\n  CommunityReport | None: The generated report for\
    \ the given community, or None if no report could be produced.\n\nRaises:\n  Exception:\
    \ May raise exceptions propagated from _generate_report and the asynchronous operations\
    \ involved in generating the report."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 82
  dependencies:
  - graphrag/index/operations/summarize_communities/summarize_communities.py::_generate_report
  called_by: []
- node_id: unified-search-app/app/ui/search.py::display_citations
  file: unified-search-app/app/ui/search.py
  name: display_citations
  signature: "def display_citations(\n    container: DeltaGenerator | None = None,\
    \ result: SearchResult | None = None\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Display citations into the UI.\n\nArgs:\n  container: DeltaGenerator\
    \ | None = None \u2014 The UI container to render citations into. If None, citations\
    \ will not be rendered.\n  result: SearchResult | None = None \u2014 The SearchResult\
    \ containing the context data to display as citations. If provided, the context\
    \ data will be processed and displayed.\n\nReturns:\n  None."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 97
  dependencies:
  - unified-search-app/app/ui/search.py::render_html_table
  called_by: []
- node_id: unified-search-app/app/ui/search.py::display_graph_citations
  file: unified-search-app/app/ui/search.py
  name: display_graph_citations
  signature: "def display_graph_citations(\n    entities: pd.DataFrame, relationships:\
    \ pd.DataFrame, citation_type: str\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Display graph citations into the UI.\n\nArgs:\n  entities: pd.DataFrame\
    \ \u2014 AI-extracted entities to render in the UI.\n  relationships: pd.DataFrame\
    \ \u2014 AI-extracted relationships to render in the UI.\n  citation_type: str\
    \ \u2014 The type used when rendering the HTML tables (passed to render_html_table).\n\
    \nReturns:\n  None \u2014 This function does not return a value."
  code_example: null
  example_source: null
  line_start: 267
  line_end: 284
  dependencies:
  - unified-search-app/app/ui/search.py::render_html_table
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
  file: graphrag/storage/file_pipeline_storage.py
  name: get
  signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
    \ str | None = None\n    ) -> Any"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the contents of a file identified by key from the storage root or\
    \ directly from the provided path if present.\n\nArgs:\n    key: str - The file\
    \ key or path relative to the root storage.\n    as_bytes: bool | None - If True,\
    \ read the file as bytes; otherwise read as text.\n    encoding: str | None -\
    \ Encoding to use when reading text. Ignored when reading bytes.\n\nReturns:\n\
    \    Any - The file contents, read as bytes when as_bytes is True, otherwise as\
    \ text, or None if no file is found.\n\nRaises:\n    Propagates exceptions raised\
    \ by has() or _read_file() during I/O operations."
  code_example: null
  example_source: null
  line_start: 87
  line_end: 100
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::_read_file
  - graphrag/storage/file_pipeline_storage.py::has
  - graphrag/storage/file_pipeline_storage.py::join_path
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
  file: graphrag/storage/file_pipeline_storage.py
  name: set
  signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
    None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Set a value for the given key in the file-based storage.\n\nArgs:\n\
    \    key: str\n        The key, used as the relative path under the storage root.\n\
    \    value: Any\n        The value to store. If value is bytes, write in binary\
    \ mode; otherwise write as text.\n    encoding: str | None\n        Encoding to\
    \ use when writing text. If None, uses the default encoding.\n\nReturns:\n   \
    \ None\n        This coroutine completes after the value has been written."
  code_example: null
  example_source: null
  line_start: 119
  line_end: 129
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::join_path
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
  file: graphrag/storage/file_pipeline_storage.py
  name: has
  signature: 'def has(self, key: str) -> bool'
  decorators: []
  raises: []
  visibility: public
  docstring: "Check whether a file for the given key exists in the storage.\n\nThis\
    \ coroutine checks the existence of the file located at the path formed by joining\
    \ the storage root directory with the provided key.\n\nArgs:\n    key (str): The\
    \ key (relative path) to check within the storage root directory.\n\nReturns:\n\
    \    bool: True if the file exists, False otherwise."
  code_example: null
  example_source: null
  line_start: 131
  line_end: 133
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::join_path
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
  file: graphrag/storage/file_pipeline_storage.py
  name: delete
  signature: 'def delete(self, key: str) -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously delete the item associated with the given key from storage.\n\
    \nIf the key exists, delete the corresponding file; if the key does not exist,\
    \ this operation is a no-op. This method is asynchronous.\n\nArgs:\n    key (str):\
    \ The key of the item to delete.\n\nReturns:\n    None: This method does not return\
    \ a value.\n\nRaises:\n    OSError: If a filesystem operation fails during removal\
    \ of the file."
  code_example: null
  example_source: null
  line_start: 135
  line_end: 138
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::has
  - graphrag/storage/file_pipeline_storage.py::join_path
  called_by: []
- node_id: graphrag/index/workflows/create_final_text_units.py::create_final_text_units
  file: graphrag/index/workflows/create_final_text_units.py
  name: create_final_text_units
  signature: "def create_final_text_units(\n    text_units: pd.DataFrame,\n    final_entities:\
    \ pd.DataFrame,\n    final_relationships: pd.DataFrame,\n    final_covariates:\
    \ pd.DataFrame | None,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transfroms input text units and their associated entities, relationships,\
    \ and optional covariates into the final text units DataFrame.\n\nArgs:\n    text_units\
    \ (pd.DataFrame): Input text units. Expected to contain at least the columns:\n\
    \        - id\n        - text\n        - document_ids\n        - n_tokens\n  \
    \  final_entities (pd.DataFrame): Mapping of entities to text units. Must contain:\n\
    \        - id (entity_id)\n        - text_unit_ids (list-like of text_unit_ids)\n\
    \    final_relationships (pd.DataFrame): Mapping of relationships to text units.\
    \ Must contain:\n        - id (relationship_id)\n        - text_unit_ids (list-like\
    \ of text_unit_ids)\n    final_covariates (pd.DataFrame | None): Optional covariates\
    \ mapping. If provided, must contain:\n        - id (covariate_id)\n        -\
    \ text_unit_id (text_unit_id to which the covariate applies)\n\nReturns:\n   \
    \ pd.DataFrame: Final text units data frame with columns defined by TEXT_UNITS_FINAL_COLUMNS.\n\
    \nRaises:\n    KeyError: If required columns are missing from any input DataFrame:\n\
    \        - text_units must include id, text, document_ids, and n_tokens\n    \
    \    - final_entities must include id and text_unit_ids\n        - final_relationships\
    \ must include id and text_unit_ids\n        - final_covariates (if not None)\
    \ must include id and text_unit_id\n\nProcessing details:\n    1) Select core\
    \ fields from text_units (id, text, document_ids, n_tokens) and add a human_readable_id\
    \ derived from the index.\n    2) Build text-unit-to-entity and text-unit-to-relationship\
    \ mappings via _entities and _relationships.\n    3) Join the selected text units\
    \ with the entity mapping, then with the relationship mapping to propagate IDs.\n\
    \    4) If final_covariates is provided, build the covariate mapping via _covariates\
    \ and join; otherwise initialize covariate_ids with empty lists.\n    5) Group\
    \ by text unit id and take the first row per id to collapse duplicates.\n    6)\
    \ Return only the columns defined by TEXT_UNITS_FINAL_COLUMNS.\n\nNotes:\n   \
    \ - When final_covariates is None, covariate_ids are set to empty lists for every\
    \ row.\n    - The exact output columns depend on TEXT_UNITS_FINAL_COLUMNS and\
    \ may vary with configuration."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 83
  dependencies:
  - graphrag/index/workflows/create_final_text_units.py::_covariates
  - graphrag/index/workflows/create_final_text_units.py::_entities
  - graphrag/index/workflows/create_final_text_units.py::_join
  - graphrag/index/workflows/create_final_text_units.py::_relationships
  called_by:
  - graphrag/index/workflows/create_final_text_units.py::run_workflow
- node_id: graphrag/index/operations/summarize_communities/strategies.py::run_graph_intelligence
  file: graphrag/index/operations/summarize_communities/strategies.py
  name: run_graph_intelligence
  signature: "def run_graph_intelligence(\n    community: str | int,\n    input: str,\n\
    \    level: int,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
    \    args: StrategyConfig,\n) -> CommunityReport | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the graph intelligence entity extraction strategy.\n\nArgs:\n  \
    \  community: Identifier for the community being processed.\n    input: The input\
    \ text to extract information from.\n    level: The reporting level to assign\
    \ to the resulting CommunityReport.\n    callbacks: WorkflowCallbacks instance\
    \ providing callback hooks during processing.\n    cache: PipelineCache instance\
    \ used for caching language model results and computations.\n    args: StrategyConfig\
    \ containing strategy settings, including llm configuration and extraction prompts.\n\
    \nReturns:\n    CommunityReport | None: The produced CommunityReport if extraction\
    \ succeeds, otherwise None."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 43
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/summarize_communities/strategies.py::_run_extractor
  - graphrag/language_model/manager.py::ModelManager
  called_by: []
- node_id: tests/integration/logging/test_standard_logging.py::test_logger_hierarchy
  file: tests/integration/logging/test_standard_logging.py
  name: test_logger_hierarchy
  signature: def test_logger_hierarchy()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that logger hierarchy works correctly.\n\nArgs:\n    None: This\
    \ test does not accept any parameters.\n\nReturns:\n    None: The test does not\
    \ return a value.\n\nRaises:\n    AssertionError: If the logger hierarchy does\
    \ not propagate the root level to child loggers as expected."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 34
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_config
  file: tests/integration/logging/test_standard_logging.py
  name: test_init_loggers_file_config
  signature: def test_init_loggers_file_config()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that init_loggers works with file configuration.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 37
  line_end: 69
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_file_verbose
  file: tests/integration/logging/test_standard_logging.py
  name: test_init_loggers_file_verbose
  signature: def test_init_loggers_file_verbose()
  decorators: []
  raises: []
  visibility: public
  docstring: "Initialize logging for graphrag using the provided GraphRagConfig.\n\
    \nA logger named \"graphrag\" is configured with a handler derived from the given\
    \ configuration. The log level is set to DEBUG when verbose is True, otherwise\
    \ INFO. Before attaching the new handler, all existing handlers on the logger\
    \ are removed; any FileHandler instances are closed to avoid resource leaks and\
    \ duplicate logs.\n\nArgs:\n    config (GraphRagConfig): The GraphRagConfig instance\
    \ providing logging settings.\n    verbose (bool): If True, set the log level\
    \ to DEBUG; otherwise INFO.\n    filename (str): The log filename to use for the\
    \ log output. Defaults to DEFAULT_LOG_FILENAME.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 72
  line_end: 97
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: tests/integration/logging/test_standard_logging.py::test_init_loggers_custom_filename
  file: tests/integration/logging/test_standard_logging.py
  name: test_init_loggers_custom_filename
  signature: def test_init_loggers_custom_filename()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that init_loggers writes logs to a custom filename.\n\nThis test\
    \ creates a temporary Graphrag configuration, initializes the loggers with a custom\n\
    filename (\"custom-log.log\"), and asserts that a file named logs/custom-log.log\
    \ is created inside\nthe temporary root directory. It then cleans up by closing\
    \ and removing any FileHandler instances\nattached to the graphrag logger.\n\n\
    Returns:\n    None: The test function does not return a value."
  code_example: null
  example_source: null
  line_start: 100
  line_end: 118
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
  file: graphrag/index/operations/layout_graph/layout_graph.py
  name: layout_graph
  signature: "def layout_graph(\n    graph: nx.Graph,\n    enabled: bool,\n    embeddings:\
    \ NodeEmbeddings | None,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Apply a layout algorithm to a nx.Graph. The method returns a dataframe\
    \ containing the node positions.\n\nArgs:\n    graph: The nx.Graph to layout.\n\
    \    enabled: If True, use the UMAP-based layout; otherwise fall back to the Zero\
    \ layout.\n    embeddings: NodeEmbeddings | None. Embeddings for each node in\
    \ the graph. If None, embeddings are treated as empty.\n\nReturns:\n    pandas.DataFrame:\
    \ A DataFrame containing the layout with columns 'label', 'x', 'y', 'size'.\n\n\
    Raises:\n    Exceptions raised by the underlying layout implementations (UMAP\
    \ or Zero) during layout computation."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 55
  dependencies:
  - graphrag/index/operations/layout_graph/layout_graph.py::_run_layout
  called_by:
  - graphrag/index/operations/finalize_entities.py::finalize_entities
- node_id: graphrag/index/operations/chunk_text/strategies.py::run_tokens
  file: graphrag/index/operations/chunk_text/strategies.py
  name: run_tokens
  signature: "def run_tokens(\n    input: list[str],\n    config: ChunkingConfig,\n\
    \    tick: ProgressTicker,\n) -> Iterable[TextChunk]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Chunks text into chunks based on encoding tokens.\n\nArgs:\n    input:\
    \ list[str] - The input texts to be chunked.\n    config: ChunkingConfig - Chunking\
    \ configuration. Uses:\n        size: number of tokens per chunk,\n        overlap:\
    \ number of overlapping tokens between consecutive chunks,\n        encoding_model:\
    \ name of the encoding model used to tokenize.\n    tick: ProgressTicker - Progress\
    \ reporter; invoked to indicate progress.\n\nReturns:\n    Iterable[TextChunk]\
    \ - An iterable of TextChunk objects representing the resulting chunks.\n\nRaises:\n\
    \    Exception - Propagates exceptions raised by underlying encoding retrieval\
    \ or tokenization."
  code_example: null
  example_source: null
  line_start: 35
  line_end: 55
  dependencies:
  - graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
  - graphrag/index/text_splitting/text_splitting.py::TokenChunkerOptions
  - graphrag/index/text_splitting/text_splitting.py::split_multiple_texts_on_tokens
  called_by:
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
  - tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_encode
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_get_encoding_fn_encode
  signature: def test_get_encoding_fn_encode(mock_get_encoding)
  decorators:
  - '@patch("tiktoken.get_encoding")'
  raises: []
  visibility: public
  docstring: "Get encoding functions for a given encoding model.\n\nArgs:\n- encoding_name:\
    \ str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
    \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
    \ list[int]] - Encodes input text into token ids using the selected encoding;\
    \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
    \ str] - Decodes a list of token ids back into a string using the selected encoding."
  code_example: null
  example_source: null
  line_start: 94
  line_end: 109
  dependencies:
  - graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::test_get_encoding_fn_decode
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_get_encoding_fn_decode
  signature: def test_get_encoding_fn_decode(mock_get_encoding)
  decorators:
  - '@patch("tiktoken.get_encoding")'
  raises: []
  visibility: public
  docstring: "Get encoding functions for a given encoding model.\n\nParameters:\n\
    - encoding_name: str - The name of the encoding model to retrieve via tiktoken.get_encoding.\n\
    \nReturns:\n- encode, decode: tuple of callables\n  - encode: Callable[[str],\
    \ list[int]] - Encodes input text into token ids using the selected encoding;\
    \ if input is not a string, it is converted to string.\n  - decode: Callable[[list[int]],\
    \ str] - Decodes a list of token ids back into a string using the selected encoding.\n\
    \nRaises:\n- Exception: Propagates exceptions raised by the underlying tiktoken.get_encoding\
    \ or encoding object."
  code_example: null
  example_source: null
  line_start: 113
  line_end: 127
  dependencies:
  - graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
  called_by: []
- node_id: graphrag/cli/main.py::completer
  file: graphrag/cli/main.py
  name: completer
  signature: 'def completer(incomplete: str) -> list[str]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return a list of possible completions for the given incomplete input\
    \ from the current directory.\n\nArgs:\n    incomplete: str\n        The partial\
    \ string to match against directory item names in the current directory.\n\nReturns:\n\
    \    list[str]\n        A list of completion strings that start with the provided\
    \ incomplete string, after applying\n        filtering based on external configuration\
    \ (file_okay, dir_okay, readable, writable) and\n        optional wildcard matching.\n\
    \nRaises:\n    TypeError\n        If the underlying wildcard matching function\
    \ is invoked with non-string arguments."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 74
  dependencies:
  - graphrag/cli/main.py::wildcard_match
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/model.py::load_model
  file: unified-search-app/app/knowledge_loader/model.py
  name: load_model
  signature: "def load_model(\n    dataset: str,\n    datasource: Datasource,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load all relevant graph-indexed data into collections of knowledge model\
    \ objects and store the model collections in the session variables.\n\nThis is\
    \ a one-time data retrieval and preparation per session.\n\nArgs:\n    dataset\
    \ (str): The dataset identifier to load from.\n    datasource (Datasource): The\
    \ Datasource descriptor used to access the data.\n\nReturns:\n    KnowledgeModel:\
    \ A KnowledgeModel containing the loaded DataFrames for entities, relationships,\
    \ community_reports, communities, text_units, and covariates (covariates will\
    \ be None if empty).\n\nRaises:\n    Exception: Propagates any exceptions raised\
    \ by the underlying data-loading helpers (e.g., get_entity_data, get_relationship_data,\
    \ get_covariate_data, get_community_report_data, get_communities_data, get_text_unit_data)."
  code_example: null
  example_source: null
  line_start: 87
  line_end: 110
  dependencies:
  - unified-search-app/app/knowledge_loader/model.py::load_communities
  - unified-search-app/app/knowledge_loader/model.py::load_community_reports
  - unified-search-app/app/knowledge_loader/model.py::load_covariates
  - unified-search-app/app/knowledge_loader/model.py::load_entities
  - unified-search-app/app/knowledge_loader/model.py::load_entity_relationships
  - unified-search-app/app/knowledge_loader/model.py::load_text_units
  called_by: []
- node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
  file: graphrag/storage/blob_pipeline_storage.py
  name: get_creation_date
  signature: 'def get_creation_date(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Get the creation date for the given key from Azure Blob storage\
    \ and format it with the local time zone.\n\nArgs:\n    key (str): The key for\
    \ which to retrieve the creation date from blob storage.\n\nReturns:\n    str:\
    \ The creation date as a string formatted with the local time zone. Returns an\
    \ empty string if an error occurs.\n\nRaises:\n    None: This method does not\
    \ raise exceptions; it returns an empty string if an error occurs.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 300
  line_end: 312
  dependencies:
  - graphrag/storage/blob_pipeline_storage.py::_keyname
  - graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
  called_by: []
- node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: get_creation_date
  signature: 'def get_creation_date(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the creation date for the given key from Cosmos DB storage and format\
    \ it with the local time zone.\n\nArgs:\n    key (str): The key for which to retrieve\
    \ the creation date.\n\nReturns:\n    str: The creation date as a string formatted\
    \ with the local time zone. Returns an empty string if the key is not found or\
    \ an error occurs.\n\nRaises:\n    None: This method does not raise exceptions;\
    \ it returns an empty string on error."
  code_example: null
  example_source: null
  line_start: 341
  line_end: 353
  dependencies:
  - graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
  called_by: []
- node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
  file: graphrag/storage/file_pipeline_storage.py
  name: get_creation_date
  signature: 'def get_creation_date(self, key: str) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the creation date of a file.\n\nArgs:\n    key (str): The key of\
    \ the file for which to retrieve the creation date.\n\nReturns:\n    str: The\
    \ creation date as a string formatted with the local time zone.\n\nRaises:\n \
    \   FileNotFoundError: If the file does not exist at the constructed path.\n \
    \   OSError: If an OS error occurs while accessing file metadata."
  code_example: null
  example_source: null
  line_start: 159
  line_end: 166
  dependencies:
  - graphrag/storage/file_pipeline_storage.py::join_path
  - graphrag/storage/pipeline_storage.py::get_timestamp_formatted_with_local_tz
  called_by: []
- node_id: unified-search-app/app/app_logic.py::run_all_searches
  file: unified-search-app/app/app_logic.py
  name: run_all_searches
  signature: 'def run_all_searches(query: str, sv: SessionVariables) -> list[SearchResult]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Run all enabled search engines and return the results.\n\nArgs:\n  \
    \  query: str\n        The search query string used by the enabled searches.\n\
    \    sv: SessionVariables\n        The SessionVariables instance containing configuration\
    \ and state used to determine which searches to run.\n\nReturns:\n    list[SearchResult]\n\
    \        The results from the enabled searches.\n\nRaises:\n    Exception\n  \
    \      If an error occurs during the execution of any of the searches."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 103
  dependencies:
  - unified-search-app/app/app_logic.py::run_basic_search
  - unified-search-app/app/app_logic.py::run_drift_search
  - unified-search-app/app/app_logic.py::run_global_search
  - unified-search-app/app/app_logic.py::run_local_search
  called_by: []
- node_id: graphrag/cli/main.py::_initialize_cli
  file: graphrag/cli/main.py
  name: _initialize_cli
  signature: "def _initialize_cli(\n    root: Path = typer.Option(\n        Path(),\n\
    \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
    ,\n        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
    \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    force: bool = typer.Option(\n\
    \        False,\n        \"--force\",\n        \"-f\",\n        help=\"Force initialization\
    \ even if the project already exists.\",\n    ),\n) -> None"
  decorators:
  - '@app.command("init")'
  raises: []
  visibility: protected
  docstring: "\"\"\"Generate a default configuration file.\n\nArgs:\n    root (Path):\
    \ The project root directory.\n    force (bool): Force initialization even if\
    \ the project already exists.\n\nReturns:\n    None: This function does not return\
    \ a value.\n\nRaises:\n    ValueError: If the project already exists and force\
    \ is False.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 95
  line_end: 116
  dependencies:
  - graphrag/cli/initialize.py::initialize_project_at
  called_by: []
- node_id: graphrag/index/operations/layout_graph/umap.py::run
  file: graphrag/index/operations/layout_graph/umap.py
  name: run
  signature: "def run(\n    graph: nx.Graph,\n    embeddings: NodeEmbeddings,\n  \
    \  on_error: ErrorHandlerFn,\n) -> GraphLayout"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute a UMAP-based layout for the given graph using node embeddings\
    \ and optional per-node attributes, with a fallback layout if the UMAP computation\
    \ fails.\n\nArgs:\n  graph: nx.Graph\n      The input graph. Nodes may have attributes\
    \ such as cluster (or community) and degree (or size) that are used to influence\
    \ the layout.\n  embeddings: NodeEmbeddings\n      Mapping of node identifiers\
    \ to embedding vectors; entries with None are ignored.\n  on_error: ErrorHandlerFn\n\
    \      Callback invoked when an error occurs during layout computation.\n\nReturns:\n\
    \  GraphLayout\n      The resulting layout as a list of NodePosition objects representing\
    \ node positions. If UMAP succeeds, positions reflect the embeddings; otherwise\
    \ a fallback layout with all nodes at (0,0) is returned.\n\nRaises:\n  None\n\
    \      This function does not raise exceptions; errors are reported via on_error\
    \ and a fallback layout is returned."
  code_example: null
  example_source: null
  line_start: 26
  line_end: 69
  dependencies:
  - graphrag/index/operations/layout_graph/typing.py::NodePosition
  - graphrag/index/operations/layout_graph/umap.py::_filter_raw_embeddings
  - graphrag/index/operations/layout_graph/umap.py::compute_umap_positions
  called_by: []
- node_id: graphrag/index/workflows/load_input_documents.py::load_input_documents
  file: graphrag/index/workflows/load_input_documents.py
  name: load_input_documents
  signature: "def load_input_documents(\n    config: InputConfig, storage: PipelineStorage\n\
    ) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load and parse input documents into a standard format.\n\nArgs:\n  \
    \  config: InputConfig containing input configuration (such as file_type and metadata)\
    \ and storage base_dir information.\n    storage: PipelineStorage used to access\
    \ the input data.\n\nReturns:\n    pandas.DataFrame: The loaded input data as\
    \ a DataFrame."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 43
  dependencies:
  - graphrag/index/input/factory.py::create_input
  called_by:
  - graphrag/index/workflows/load_input_documents.py::run_workflow
- node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file
  file: tests/unit/indexing/input/test_csv_loader.py
  name: test_csv_loader_one_file
  signature: def test_csv_loader_one_file()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single CSV file using the input loader and verify the\
    \ resulting DataFrame.\n\nThis test constructs an InputConfig configured for CSV\
    \ files in a single directory, creates storage via create_storage_from_config,\
    \ loads documents with create_input, and asserts:\n- the resulting DataFrame has\
    \ shape (2, 4)\n- the first row in the title column is \"input.csv\"\n\nReturns:\n\
    \  None. This test does not return a value.\n\nRaises:\n  ValueError: If the storage\
    \ type is not registered in create_storage_from_config."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 22
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_title
  file: tests/unit/indexing/input/test_csv_loader.py
  name: test_csv_loader_one_file_with_title
  signature: def test_csv_loader_one_file_with_title()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test that loads a single CSV file with a title column using\
    \ the input loader.\n\nConfigures InputConfig to read CSV files from tests/unit/indexing/input/data/one-csv\
    \ with a file_pattern that matches CSVs and sets title_column to \"title\". It\
    \ loads documents via create_input and asserts that the resulting DataFrame has\
    \ shape (2, 4) and that the first row in the title column is \"Hello\".\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 25
  line_end: 37
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_one_file_with_metadata
  file: tests/unit/indexing/input/test_csv_loader.py
  name: test_csv_loader_one_file_with_metadata
  signature: def test_csv_loader_one_file_with_metadata()
  decorators: []
  raises: []
  visibility: public
  docstring: "Async test that loads a single CSV file with metadata and validates\
    \ the loaded DataFrame and metadata content.\n\nArgs:\n    None: This test does\
    \ not take any parameters.\n\nReturns:\n    None: The test does not return a value;\
    \ it performs assertions to verify behavior.\n\nRaises:\n    AssertionError: If\
    \ the loaded DataFrame shape or metadata content does not match the expected values."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 53
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_csv_loader.py::test_csv_loader_multiple_files
  file: tests/unit/indexing/input/test_csv_loader.py
  name: test_csv_loader_multiple_files
  signature: def test_csv_loader_multiple_files()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading multiple CSV files and verify the resulting DataFrame shape.\n\
    \nConfigures an InputConfig to load CSV files from tests/unit/indexing/input/data/multiple-csvs,\
    \ using file_type csv and a pattern that matches CSV files, creates storage from\
    \ the config, loads the documents, and asserts the DataFrame shape is (4, 4).\n\
    \nReturns:\n  None"
  code_example: null
  example_source: null
  line_start: 56
  line_end: 66
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_one_object
  file: tests/unit/indexing/input/test_json_loader.py
  name: test_json_loader_one_file_one_object
  signature: def test_json_loader_one_file_one_object()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single JSON file containing one object and validating\
    \ the loaded data.\n\nThis test builds an InputConfig using json input, loads\
    \ documents via create_input, and asserts that the resulting DataFrame has shape\
    \ (1, 4) and that the title field equals \"input.json\".\n\nReturns:\n    None\n\
    \nRaises:\n    AssertionError: If the loaded data does not match the expected\
    \ shape or values."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 22
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_multiple_objects
  file: tests/unit/indexing/input/test_json_loader.py
  name: test_json_loader_one_file_multiple_objects
  signature: def test_json_loader_one_file_multiple_objects()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a JSON file containing multiple objects and verify the\
    \ loaded DataFrame.\n\nArgs:\n    None: This test function does not accept any\
    \ parameters.\n\nReturns:\n    None: The function does not return a value.\n\n\
    Raises:\n    AssertionError: If the loaded data does not match the expected shape\
    \ or values."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 37
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_title
  file: tests/unit/indexing/input/test_json_loader.py
  name: test_json_loader_one_file_with_title
  signature: def test_json_loader_one_file_with_title()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single JSON file with an explicit title column ('title')\
    \ and verify that the resulting DataFrame has shape (1, 4) and the title value\
    \ equals 'Hello'.\n\nReturns:\n    None\n\nRaises:\n    AssertionError: If the\
    \ loaded data does not match the expected shape or values."
  code_example: null
  example_source: null
  line_start: 40
  line_end: 52
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_one_file_with_metadata
  file: tests/unit/indexing/input/test_json_loader.py
  name: test_json_loader_one_file_with_metadata
  signature: def test_json_loader_one_file_with_metadata()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single JSON file with metadata and validating that the\
    \ metadata is included in the resulting DataFrame.\n\nArgs:\n    None\n\nReturns:\n\
    \    None\n\nRaises:\n    AssertionError: If the loaded data does not match the\
    \ expected shape or metadata values."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 68
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_json_loader.py::test_json_loader_multiple_files
  file: tests/unit/indexing/input/test_json_loader.py
  name: test_json_loader_multiple_files
  signature: def test_json_loader_multiple_files()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading multiple JSON files from tests/unit/indexing/input/data/multiple-jsons\
    \ and verifying the loaded DataFrame shape (4, 4).\n\nThe test builds an InputConfig\
    \ for JSON input using a file pattern that matches JSON files, creates a storage\
    \ object from the config, loads documents via create_input, and asserts that the\
    \ resulting DataFrame has shape (4, 4). This is a test function and does not return\
    \ any data.\n\nArgs:\n    None: This test function does not accept any parameters.\n\
    \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
    \ If the loaded data does not have the expected shape (4, 4).\n    ValueError:\
    \ If storage creation from the config fails."
  code_example: null
  example_source: null
  line_start: 71
  line_end: 81
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file
  file: tests/unit/indexing/input/test_txt_loader.py
  name: test_txt_loader_one_file
  signature: def test_txt_loader_one_file()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single TXT file using the input loader and verify the\
    \ resulting DataFrame shape and title.\n\nArgs:\n    None: This test function\
    \ does not accept any parameters.\n\nReturns:\n    None: This test does not return\
    \ a value.\n\nRaises:\n    None: This test does not raise exceptions under normal\
    \ operation."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 22
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_one_file_with_metadata
  file: tests/unit/indexing/input/test_txt_loader.py
  name: test_txt_loader_one_file_with_metadata
  signature: def test_txt_loader_one_file_with_metadata()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading a single TXT file with metadata and verify the metadata\
    \ content.\n\nArgs:\n    None: This test does not accept any parameters.\n\nReturns:\n\
    \    None: The test does not return a value; it performs assertions to verify\
    \ behavior.\n\nRaises:\n    AssertionError: If the loaded DataFrame shape or metadata\
    \ content does not match the expected values."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 38
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: tests/unit/indexing/input/test_txt_loader.py::test_txt_loader_multiple_files
  file: tests/unit/indexing/input/test_txt_loader.py
  name: test_txt_loader_multiple_files
  signature: def test_txt_loader_multiple_files()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test loading multiple TXT files using the input loader and verify the\
    \ resulting DataFrame shape (2, 4).\n\nArgs:\n    None: This test function does\
    \ not accept any parameters.\n\nReturns:\n    None: This test does not return\
    \ a value.\n\nRaises:\n    None: This test does not raise exceptions under normal\
    \ operation."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 51
  dependencies:
  - graphrag/config/models/input_config.py::InputConfig
  - graphrag/config/models/storage_config.py::StorageConfig
  - graphrag/index/input/factory.py::create_input
  - graphrag/utils/api.py::create_storage_from_config
  called_by: []
- node_id: graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
  file: graphrag/index/operations/compute_edge_combined_degree.py
  name: compute_edge_combined_degree
  signature: "def compute_edge_combined_degree(\n    edge_df: pd.DataFrame,\n    node_degree_df:\
    \ pd.DataFrame,\n    node_name_column: str,\n    node_degree_column: str,\n  \
    \  edge_source_column: str,\n    edge_target_column: str,\n) -> pd.Series"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute the combined degree for each edge in a graph.\n\nArgs:\n   \
    \ edge_df: pd.DataFrame\n        The DataFrame containing edges with columns for\
    \ source and target nodes.\n    node_degree_df: pd.DataFrame\n        DataFrame\
    \ containing node degree information, keyed by node name.\n    node_name_column:\
    \ str\n        The column in node_degree_df that contains node names to join on.\n\
    \    node_degree_column: str\n        The column in node_degree_df that contains\
    \ the degree values to join.\n    edge_source_column: str\n        The column\
    \ name in edge_df that identifies the source node for each edge.\n    edge_target_column:\
    \ str\n        The column name in edge_df that identifies the target node for\
    \ each edge.\n\nReturns:\n    pd.Series\n        A Series of the per-edge combined\
    \ degree values (sum of the source and target node degrees; missing degrees are\
    \ treated as 0).\n\nRaises:\n    Propagates exceptions raised by pandas operations\
    \ or invalid inputs."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 39
  dependencies:
  - graphrag/index/operations/compute_edge_combined_degree.py::_degree_colname
  - graphrag/index/operations/compute_edge_combined_degree.py::join_to_degree
  called_by:
  - graphrag/index/operations/finalize_relationships.py::finalize_relationships
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: _tag_noun_phrases
  signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str], entities:\
    \ set[str] | None = None\n    ) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\nArgs:\n\
    \  noun_chunk: tuple[str, str] - The noun chunk as (text, pos) where text is the\
    \ chunk text and pos is its part-of-speech tag.\n  entities: set[str] | None -\
    \ Optional set of entity strings to consider when validating the noun chunk. If\
    \ provided and the noun_chunk[0] is in entities, is_valid_entity is computed.\n\
    \nReturns:\n  dict[str, Any] - A dictionary containing the following keys:\n \
    \     cleaned_tokens: List[str] - Tokens after removing excluded nouns.\n    \
    \  cleaned_text: str - The cleaned tokens joined by self.word_delimiter, with\
    \ newline characters removed, and converted to upper-case.\n      is_valid_entity:\
    \ bool - True if the noun chunk corresponds to a valid entity given the tokens\
    \ and optional entities.\n      has_proper_nouns: bool - True if noun_chunk[1]\
    \ == \"PROPN\".\n      has_compound_words: bool - True if cleaned_tokens form\
    \ a hyphenated compound.\n      has_valid_tokens: bool - True if all cleaned_tokens\
    \ pass the maximum word length constraint.\n\nRaises:\n  None"
  code_example: null
  example_source: null
  line_start: 153
  line_end: 177
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: _tag_noun_phrases
  signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities: list[Span]\n\
    \    ) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\nArgs:\n\
    \  noun_chunk: Span - The noun chunk to analyze.\n  entities: list[Span] - Entities\
    \ to consider when validating the noun chunk; used to determine if the chunk matches\
    \ an entity and, if so, whether it's a valid entity.\n\nReturns:\n  dict[str,\
    \ Any] - A dictionary containing the following keys:\n    cleaned_tokens: List[Token]\
    \ - The tokens from the noun_chunk after filtering out tokens based on excluded\
    \ POS tags, excluded nouns, spaces, and punctuation.\n    cleaned_text: str -\
    \ The tokens joined using the configured delimiter (word_delimiter), with newline\
    \ characters removed and converted to uppercase.\n    is_valid_entity: bool -\
    \ True if the noun_chunk corresponds to a valid entity among the provided entities;\
    \ otherwise False.\n    has_proper_nouns: bool - True if any of the cleaned tokens\
    \ is a proper noun.\n    has_compound_words: bool - True if the cleaned token\
    \ texts form a hyphenated compound (as determined by is_compound).\n    has_valid_tokens:\
    \ bool - True if all cleaned token texts satisfy the maximum token length constraint\
    \ (has_valid_token_length with self.max_word_length).\n\nRaises:\n  None."
  code_example: null
  example_source: null
  line_start: 123
  line_end: 158
  dependencies:
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::has_valid_token_length
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_compound
  - graphrag/index/operations/build_noun_graph/np_extractors/np_validator.py::is_valid_entity
  called_by: []
- node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::run_strategy
  file: graphrag/index/operations/extract_covariates/extract_covariates.py
  name: run_strategy
  signature: def run_strategy(row)
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the strategy on a single input row to asynchronously extract covariates\
    \ from text.\n\nArgs:\n  row: The input row to process. The text to analyze is\
    \ read from row[column].\n\nReturns:\n  List[dict[str, Any]]: A list of rows augmented\
    \ with covariate data. Each item is produced by merging the original row with\
    \ the corresponding covariate data (converted to a dict) and including a covariate_type\
    \ field set to covariate_type.\n\nRaises:\n  Exception: Propagates exceptions\
    \ raised during claim extraction or row construction."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 66
  dependencies:
  - graphrag/index/operations/extract_covariates/extract_covariates.py::create_row_from_claim_data
  - graphrag/index/operations/extract_covariates/extract_covariates.py::run_extract_claims
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  file: graphrag/language_model/providers/fnllm/utils.py
  name: get_openai_model_parameters_from_config
  signature: "def get_openai_model_parameters_from_config(\n    config: LanguageModelConfig,\n\
    ) -> dict[str, Any]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Get the OpenAI API parameters for a given language model config.\n\n\
    Args:\n    config (LanguageModelConfig): The language model configuration. This\
    \ is converted to a dictionary via model_dump() and then used to derive OpenAI\
    \ API parameters, with adjustments for reasoning models handled by get_openai_model_parameters_from_dict.\n\
    \nReturns:\n    dict[str, Any]: A dictionary of OpenAI API parameters derived\
    \ from the input config, including 'n' and model-specific fields such as 'max_tokens',\
    \ 'temperature', 'max_completion_tokens', etc., depending on whether the model\
    \ is a reasoning model."
  code_example: null
  example_source: null
  line_start: 140
  line_end: 144
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
  called_by:
  - graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  - graphrag/query/factory.py::get_local_search_engine
  - graphrag/query/factory.py::get_global_search_engine
  - graphrag/query/factory.py::get_basic_search_engine
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
  file: graphrag/query/structured_search/drift_search/search.py
  name: init_local_search
  signature: def init_local_search(self) -> LocalSearch
  decorators: []
  raises: []
  visibility: public
  docstring: "Initialize a LocalSearch instance configured from the DRIFT search configuration.\n\
    \nThis method reads values from the DRIFTSearch instance's configuration (via\
    \ self.context_builder.config) to build the LocalSearch parameters, including\
    \ how context is built, how many tokens to consider, and how the OpenAI model\
    \ is prompted.\n\nParameters\n    None: This method does not accept explicit parameters\
    \ and relies on the DRIFTSearch instance state\n    (model, context_builder, tokenizer,\
    \ and config) to construct and configure the LocalSearch.\n\nReturns\n    LocalSearch:\
    \ A LocalSearch instance configured with:\n        model: the current ChatModel\n\
    \        system_prompt: the DRIFT context's system prompt\n        context_builder:\
    \ the DRIFT local mixed context\n        tokenizer: the tokenizer in use\n   \
    \     model_params: OpenAI API parameters derived from the local_search fields\n\
    \        context_builder_params: local context parameters derived from the local_search\
    \ fields\n        response_type: multiple paragraphs\n        callbacks: any provided\
    \ callbacks\n\nConfiguration details used from DRIFT config\n    local_search_text_unit_prop:\
    \ text unit property used by the local search\n    local_search_community_prop:\
    \ property for community weighting\n    local_search_top_k_mapped_entities: maximum\
    \ number of mapped entities\n    local_search_top_k_relationships: maximum number\
    \ of relationships\n    local_search_max_data_tokens: maximum tokens for local\
    \ context\n    local_search_llm_max_gen_tokens: maximum tokens for LLM generation\n\
    \    local_search_temperature: sampling temperature\n    local_search_n: number\
    \ of candidate generations\n    local_search_top_p: nucleus sampling parameter\n\
    \    local_search_llm_max_gen_completion_tokens: maximum tokens for LLM completion\n\
    \nNotes\n    This method relies on the presence and validity of the above config\
    \ fields.\n    Missing or invalid values may raise exceptions when constructing\
    \ LocalSearch or its parameters.\n\nRaises\n    ValueError: if required configuration\
    \ values are missing or invalid\n    TypeError: if internal components are not\
    \ properly initialized\n    Exception: propagates any exception raised by LocalSearch\
    \ construction or parameter derivation"
  code_example: null
  example_source: null
  line_start: 68
  line_end: 108
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
  - graphrag/query/structured_search/local_search/search.py::LocalSearch
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
  file: graphrag/query/structured_search/drift_search/search.py
  name: search
  signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
    \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Perform an asynchronous DRIFT search.\n\nArgs:\n    query (str): The\
    \ query to search for.\n    conversation_history (Any, optional): The conversation\
    \ history, if any.\n    reduce (bool, optional): Whether to reduce the response\
    \ to a single comprehensive response.\n    **kwargs: Additional keyword arguments\
    \ that may be used by the search implementation.\n\nReturns:\n    SearchResult:\
    \ The search result containing the response and context data.\n\nRaises:\n   \
    \ ValueError: If the query is empty."
  code_example: null
  example_source: null
  line_start: 179
  line_end: 301
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
  - graphrag/query/structured_search/base.py::SearchResult
  - graphrag/query/structured_search/drift_search/search.py::_process_primer_results
  - graphrag/query/structured_search/drift_search/search.py::_reduce_response
  - graphrag/query/structured_search/drift_search/search.py::_search_step
  called_by: []
- node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
  file: graphrag/query/structured_search/drift_search/search.py
  name: stream_search
  signature: "def stream_search(\n        self, query: str, conversation_history:\
    \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a streaming DRIFT search asynchronously.\n\nArgs:\n    query\
    \ (str): The query to search for.\n    conversation_history (ConversationHistory\
    \ | None, optional): The conversation history.\n\nReturns:\n    AsyncGenerator[str,\
    \ None]: An asynchronous generator yielding pieces of the reduced response as\
    \ they are produced.\n\nRaises:\n    Exception: If the underlying search or streaming\
    \ operations fail."
  code_example: null
  example_source: null
  line_start: 303
  line_end: 340
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_dict
  - graphrag/query/structured_search/drift_search/search.py::_reduce_response_streaming
  - graphrag/query/structured_search/drift_search/search.py::search
  called_by: []
- node_id: graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  file: graphrag/index/operations/chunk_text/chunk_text.py
  name: chunk_text
  signature: "def chunk_text(\n    input: pd.DataFrame,\n    column: str,\n    size:\
    \ int,\n    overlap: int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n\
    \    callbacks: WorkflowCallbacks,\n) -> pd.Series"
  decorators: []
  raises: []
  visibility: public
  docstring: "Chunk a piece of text into smaller pieces.\n\nArgs:\n    input: DataFrame\
    \ containing the data to chunk.\n    column: The name of the column containing\
    \ the text to chunk, this can either be a column with text, or a column with a\
    \ list[tuple[doc_id, str]].\n    size: The chunk size to use.\n    overlap: The\
    \ chunk overlap to use.\n    encoding_model: The encoding model to use for chunking.\n\
    \    strategy: The strategy to use to chunk the text, see below for more details.\n\
    \    callbacks: WorkflowCallbacks for progress reporting.\n\nReturns:\n    A pandas\
    \ Series where each element corresponds to the chunked result for the input row.\
    \ Each element is a list of chunks; for string inputs, each item is a string text\
    \ chunk. For inputs with document IDs, each item is a tuple of (doc_ids, text_chunk,\
    \ n_tokens).\n\nRaises:\n    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 79
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/chunk_text.py::_get_num_total
  - graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  - graphrag/index/operations/chunk_text/chunk_text.py::run_strategy
  - graphrag/logger/progress.py::progress_ticker
  called_by:
  - graphrag/index/workflows/create_base_text_units.py::chunker
  - tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_chunk_text
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_tokens
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_load_strategy_tokens
  signature: def test_load_strategy_tokens()
  decorators: []
  raises: []
  visibility: public
  docstring: "Load the strategy callable for the given ChunkStrategyType.\n\nArgs:\n\
    \    strategy (ChunkStrategyType): The type of chunk strategy to load. If ChunkStrategyType.tokens,\
    \ the tokens strategy is returned. If ChunkStrategyType.sentence, NLP resources\
    \ are bootstrapped and the sentences strategy is returned.\n\nReturns:\n    ChunkStrategy:\
    \ The loaded strategy callable corresponding to the provided strategy type.\n\n\
    Raises:\n    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 42
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_sentence
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_load_strategy_sentence
  signature: def test_load_strategy_sentence()
  decorators: []
  raises: []
  visibility: public
  docstring: "Load and return the strategy callable for the given chunking strategy.\n\
    \nArgs:\n    strategy (ChunkStrategyType): The type of chunk strategy to load.\
    \ If ChunkStrategyType.tokens, the tokens strategy is returned. If ChunkStrategyType.sentence,\
    \ NLP resources are bootstrapped and the sentences strategy is returned.\n\nReturns:\n\
    \    ChunkStrategy: The loaded strategy callable corresponding to the provided\
    \ strategy type.\n\nRaises:\n    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 50
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_load_strategy_none
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_load_strategy_none
  signature: def test_load_strategy_none()
  decorators: []
  raises: []
  visibility: public
  docstring: "Load and return the strategy callable for a given chunking strategy\
    \ type.\n\nArgs:\n    strategy (ChunkStrategyType): The type of chunk strategy\
    \ to load. If ChunkStrategyType.tokens, the tokens strategy is returned. If ChunkStrategyType.sentence,\
    \ NLP resources are bootstrapped and the sentences strategy is returned.\n\nReturns:\n\
    \    ChunkStrategy: The loaded strategy callable corresponding to the provided\
    \ strategy type.\n\nRaises:\n    ValueError: If an unknown strategy is provided."
  code_example: null
  example_source: null
  line_start: 53
  line_end: 59
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::load_strategy
  called_by: []
- node_id: tests/unit/config/utils.py::assert_graphrag_configs
  file: tests/unit/config/utils.py
  name: assert_graphrag_configs
  signature: 'def assert_graphrag_configs(actual: GraphRagConfig, expected: GraphRagConfig)
    -> None'
  decorators: []
  raises: []
  visibility: public
  docstring: "Assert that two GraphRagConfig objects are equivalent by validating\
    \ all nested configurations match.\n\nArgs:\n    actual: GraphRagConfig - The\
    \ actual GraphRagConfig instance to validate.\n    expected: GraphRagConfig -\
    \ The expected GraphRagConfig instance to compare against.\n\nReturns:\n    None\n\
    \nRaises:\n    AssertionError: If any of the fields differ between actual and\
    \ expected."
  code_example: null
  example_source: null
  line_start: 386
  line_end: 435
  dependencies:
  - tests/unit/config/utils.py::assert_basic_search_configs
  - tests/unit/config/utils.py::assert_cache_configs
  - tests/unit/config/utils.py::assert_chunking_configs
  - tests/unit/config/utils.py::assert_cluster_graph_configs
  - tests/unit/config/utils.py::assert_community_reports_configs
  - tests/unit/config/utils.py::assert_drift_search_configs
  - tests/unit/config/utils.py::assert_embed_graph_configs
  - tests/unit/config/utils.py::assert_extract_claims_configs
  - tests/unit/config/utils.py::assert_extract_graph_configs
  - tests/unit/config/utils.py::assert_extract_graph_nlp_configs
  - tests/unit/config/utils.py::assert_global_search_configs
  - tests/unit/config/utils.py::assert_input_configs
  - tests/unit/config/utils.py::assert_language_model_configs
  - tests/unit/config/utils.py::assert_local_search_configs
  - tests/unit/config/utils.py::assert_output_configs
  - tests/unit/config/utils.py::assert_prune_graph_configs
  - tests/unit/config/utils.py::assert_reporting_configs
  - tests/unit/config/utils.py::assert_snapshots_configs
  - tests/unit/config/utils.py::assert_summarize_descriptions_configs
  - tests/unit/config/utils.py::assert_text_embedding_configs
  - tests/unit/config/utils.py::assert_umap_configs
  - tests/unit/config/utils.py::assert_update_output_configs
  - tests/unit/config/utils.py::assert_vector_store_configs
  called_by:
  - tests/unit/config/test_config.py::test_default_config
  - tests/unit/config/test_config.py::test_load_minimal_config
  - tests/unit/config/test_config.py::test_load_config_with_cli_overrides
- node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_id
  file: tests/unit/query/input/retrieval/test_entities.py
  name: test_get_entity_by_id
  signature: def test_get_entity_by_id()
  decorators: []
  raises: []
  visibility: public
  docstring: "Get entity by id.\n\nArgs:\n    entities (dict[str, Entity]): Mapping\
    \ of entity IDs to Entity objects.\n    value (str): The id value to look up.\
    \ If value is a valid UUID, also try the same value with dashes removed.\n\nReturns:\n\
    \    Entity | None: The matching Entity if found, otherwise None.\n\nRaises:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 11
  line_end: 89
  dependencies:
  - graphrag/data_model/entity.py::Entity
  - graphrag/query/input/retrieval/entities.py::get_entity_by_id
  called_by: []
- node_id: graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
  file: graphrag/query/context_builder/entity_extraction.py
  name: map_query_to_entities
  signature: "def map_query_to_entities(\n    query: str,\n    text_embedding_vectorstore:\
    \ BaseVectorStore,\n    text_embedder: EmbeddingModel,\n    all_entities_dict:\
    \ dict[str, Entity],\n    embedding_vectorstore_key: str = EntityVectorStoreKey.ID,\n\
    \    include_entity_names: list[str] | None = None,\n    exclude_entity_names:\
    \ list[str] | None = None,\n    k: int = 10,\n    oversample_scaler: int = 2,\n\
    ) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Extracts entities that match a given query using semantic similarity\
    \ between the query and entity descriptions, with optional explicit inclusion\
    \ or exclusion of entities.\n\nArgs:\n    query: str. The query string to search\
    \ for relevant entities. If empty, top-ranked entities by rank are returned.\n\
    \    text_embedding_vectorstore: BaseVectorStore. The vector store used to perform\
    \ semantic similarity search over entity descriptions.\n    text_embedder: EmbeddingModel.\
    \ The embedding model used to encode text for similarity search.\n    all_entities_dict:\
    \ dict[str, Entity]. Mapping of entity IDs to Entity objects.\n    embedding_vectorstore_key:\
    \ str. Key used to extract the corresponding field from embedding results (e.g.,\
    \ \"id\" or \"title\"). Default EntityVectorStoreKey.ID.\n    include_entity_names:\
    \ list[str] | None. Names of entities to explicitly include. If provided, those\
    \ entities are retrieved and prepended to the final results.\n    exclude_entity_names:\
    \ list[str] | None. Names of entities to exclude from the matched results portion.\n\
    \    k: int. Number of top results to consider when a non-empty query is provided\
    \ (before exclusions/inclusions). When the query is empty, this caps the number\
    \ of top-ranked entities.\n    oversample_scaler: int. Multiplier to oversample\
    \ results to account for potential exclusions.\n\nReturns:\n    list[Entity].\
    \ The selected entities, with explicitly included entities first, followed by\
    \ the matched or top-ranked entities. Note that included entities are not filtered\
    \ by exclude_entity_names, and duplicates between the two groups may occur.\n\n\
    Raises:\n    AttributeError: May be raised by underlying retrieval helpers if\
    \ an expected attribute is missing on an Entity or in a retrieved result. This\
    \ behavior is not guaranteed by the function's signature."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 92
  dependencies:
  - graphrag/query/input/retrieval/entities.py::get_entity_by_id
  - graphrag/query/input/retrieval/entities.py::get_entity_by_key
  - graphrag/query/input/retrieval/entities.py::get_entity_by_name
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
  - tests/unit/query/context_builder/test_entity_extraction.py::test_map_query_to_entities
- node_id: tests/unit/query/input/retrieval/test_entities.py::test_get_entity_by_key
  file: tests/unit/query/input/retrieval/test_entities.py
  name: test_get_entity_by_key
  signature: def test_get_entity_by_key()
  decorators: []
  raises: []
  visibility: public
  docstring: "Get an Entity by key from a collection.\n\nThis helper searches through\
    \ an iterable of Entity objects and returns the first Entity whose attribute named\
    \ by key equals the provided value. If value is a string that represents a UUID\
    \ (with or without dashes), the comparison also considers the same UUID with dashes\
    \ removed to accommodate both representations.\n\nParameters:\n- entities: Iterable[Entity].\
    \ The collection of Entity objects to search.\n- key: str. The attribute name\
    \ on Entity to compare.\n- value: str | int. The value to match against the attribute.\
    \ If value is a UUID string, also compare the undashed form of the UUID.\n\nReturns:\n\
    - Entity | None: The first matching Entity if found, otherwise None.\n\nRaises:\n\
    - AttributeError: If any item in entities does not have the attribute named by\
    \ key.\n\nExamples:\n- No match returns None:\n  get_entity_by_key([Entity(id=\"\
    id1\", short_id=\"sid1\", title=\"title1\")], \"id\", \"00000000-0000-0000-0000-000000000000\"\
    ) -> None\n- Match by dashed UUID:\n  get_entity_by_key([\n    Entity(id=\"2da37c7a-50a8-44d4-aa2c-fd401e19976c\"\
    , short_id=\"sid1\", title=\"title1\"),\n    Entity(id=\"c4f93564-4507-4ee4-b102-98add401a965\"\
    , short_id=\"sid2\", title=\"title2\"),\n    Entity(id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
    , short_id=\"sid3\", title=\"title3\"),\n  ],\n  \"id\",\n  \"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\"\
    ,\n) -> Entity(id=\"7c6f2bc9-47c9-4453-93a3-d2e174a02cd9\", short_id=\"sid3\"\
    , title=\"title3\")\n- Match by undashed UUID:\n  get_entity_by_key([\n    Entity(id=\"\
    2da37c7a50a844d4aa2cfd401e19976c\", short_id=\"sid1\", title=\"title1\"),\n  \
    \  Entity(id=\"c4f9356445074ee4b10298add401a965\", short_id=\"sid2\", title=\"\
    title2\"),\n    Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\"\
    , title=\"title3\"),\n  ],\n  \"id\",\n  \"7c6f2bc947c9445393a3d2e174a02cd9\"\
    ,\n) -> Entity(id=\"7c6f2bc947c9445393a3d2e174a02cd9\", short_id=\"sid3\", title=\"\
    title3\")\n- Match by numeric rank:\n  get_entity_by_key([\n    Entity(id=\"id1\"\
    , short_id=\"sid1\", title=\"title1\", rank=1),\n    Entity(id=\"id2\", short_id=\"\
    sid2\", title=\"title2a\", rank=2),\n    Entity(id=\"id3\", short_id=\"sid3\"\
    , title=\"title3\", rank=3),\n  ],\n  \"rank\",\n  2,\n) -> Entity(id=\"id2\"\
    , short_id=\"sid2\", title=\"title2a\", rank=2)"
  code_example: null
  example_source: null
  line_start: 92
  line_end: 167
  dependencies:
  - graphrag/data_model/entity.py::Entity
  - graphrag/query/input/retrieval/entities.py::get_entity_by_key
  called_by: []
- node_id: graphrag/index/utils/graphs.py::calculate_modularity
  file: graphrag/index/utils/graphs.py
  name: calculate_modularity
  signature: "def calculate_modularity(\n    graph: nx.Graph,\n    max_cluster_size:\
    \ int = 10,\n    random_seed: int = 0xDEADBEEF,\n    use_root_modularity: bool\
    \ = True,\n    modularity_metric: ModularityMetric = ModularityMetric.WeightedComponents,\n\
    ) -> float"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Calculate modularity of the graph based on the modularity metric type.\n\
    \nArgs:\n    graph (nx.Graph): The input graph.\n    max_cluster_size (int): Maximum\
    \ cluster size for the root-level clustering produced by Hierarchical Leiden.\n\
    \    random_seed (int): Seed for random number generation.\n    use_root_modularity\
    \ (bool): If True, compute modularity using root-level clustering; otherwise compute\
    \ using leaf-level clustering.\n    modularity_metric (ModularityMetric): The\
    \ modularity metric to use (Graph, LCC, or WeightedComponents).\n\nReturns:\n\
    \    float: The modularity value for the graph with respect to the chosen clustering.\n\
    \nRaises:\n    ValueError: If an unknown modularity metric type is provided."
  code_example: null
  example_source: null
  line_start: 117
  line_end: 152
  dependencies:
  - graphrag/index/utils/graphs.py::calculate_graph_modularity
  - graphrag/index/utils/graphs.py::calculate_lcc_modularity
  - graphrag/index/utils/graphs.py::calculate_weighted_modularity
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::_cut_batch
  file: graphrag/query/context_builder/community_context.py
  name: _cut_batch
  signature: def _cut_batch() -> None
  decorators: []
  raises: []
  visibility: protected
  docstring: 'Convert the current batch of context records to a DataFrame, convert
    it to CSV, and append it to the aggregated context. This function calls _convert_report_context_to_df
    with context_records=batch_records and header=header, passing weight_column as
    community_weight_name if entities and include_community_weight are truthy, otherwise
    None, and rank_column as community_rank_name if include_community_rank is truthy,
    otherwise None. If the resulting DataFrame is empty, the function returns without
    modification. Otherwise, it converts the DataFrame to CSV with index=None and
    sep=column_delimiter. If not all_context_text and single_batch, it prefixes the
    current context header to the CSV text. Finally, it appends the CSV text to all_context_text
    and the DataFrame to all_context_records. Returns: None'
  code_example: null
  example_source: null
  line_start: 132
  line_end: 149
  dependencies:
  - graphrag/query/context_builder/community_context.py::_convert_report_context_to_df
  called_by:
  - graphrag/query/context_builder/community_context.py::build_community_context
- node_id: graphrag/index/workflows/load_update_documents.py::load_update_documents
  file: graphrag/index/workflows/load_update_documents.py
  name: load_update_documents
  signature: "def load_update_documents(\n    config: InputConfig,\n    input_storage:\
    \ PipelineStorage,\n    previous_storage: PipelineStorage,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load and parse update-only input documents into a standard format.\n\
    \nThis asynchronous function loads input documents using create_input(config,\
    \ input_storage), computes the delta against previously stored documents using\
    \ get_delta_docs(input_documents, previous_storage), and returns the new inputs\
    \ as a DataFrame.\n\nArgs:\n    config: InputConfig containing input configuration\
    \ (such as file_type and metadata) and storage base_dir information.\n    input_storage:\
    \ PipelineStorage used to access the input data.\n    previous_storage: PipelineStorage\
    \ containing the previously stored final documents to diff against.\n\nReturns:\n\
    \    pandas.DataFrame: The new/update input documents as determined by the delta\
    \ computation (delta_documents.new_inputs).\n\nRaises:\n    Exceptions raised\
    \ by create_input and get_delta_docs as encountered."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 55
  dependencies:
  - graphrag/index/input/factory.py::create_input
  - graphrag/index/update/incremental_index.py::get_delta_docs
  called_by:
  - graphrag/index/workflows/load_update_documents.py::run_workflow
- node_id: graphrag/index/workflows/update_covariates.py::run_workflow
  file: graphrag/index/workflows/update_covariates.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the covariates from a incremental index run.\n\nArgs:\n    config:\
    \ GraphRagConfig configuration for the workflow.\n    context: PipelineRunContext\
    \ containing state for the run, including update_timestamp.\n\nReturns:\n    WorkflowFunctionOutput:\
    \ The output of the workflow function. The result is None.\n\nRaises:\n    Exception:\
    \ Propagates exceptions raised by storage backends or related IO operations."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 42
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/update_covariates.py::_update_covariates
  - graphrag/utils/storage.py::storage_has_table
  called_by: []
- node_id: graphrag/index/workflows/update_final_documents.py::run_workflow
  file: graphrag/index/workflows/update_final_documents.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the final documents from an incremental index run.\n\nArgs:\n\
    \    config: GraphRagConfig\n        GraphRagConfig containing configuration for\
    \ the workflow.\n\n    context: PipelineRunContext\n        PipelineRunContext\
    \ carrying the state for the run.\n\nReturns:\n    WorkflowFunctionOutput\n  \
    \      A WorkflowFunctionOutput with result=None.\n\nRaises:\n    KeyError\n \
    \       If 'update_timestamp' is not present in context.state.\n\nSide effects:\n\
    \    Updates context.state['incremental_update_final_documents'] with the final\n\
    \    documents dataframe produced by concatenating previous and delta\n    documents\
    \ into the output storage."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 34
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/update/incremental_index.py::concat_dataframes
  called_by: []
- node_id: graphrag/index/workflows/update_text_units.py::run_workflow
  file: graphrag/index/workflows/update_text_units.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the text units from a incremental index run.\n\nArgs:\n    config:\
    \ GraphRagConfig containing configuration for the workflow.\n    context: PipelineRunContext\
    \ carrying the state for the run.\n\nReturns:\n    WorkflowFunctionOutput: A WorkflowFunctionOutput\
    \ with result=None.\n\nRaises:\n    KeyError: If 'update_timestamp' is not present\
    \ in context.state."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 39
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/update_text_units.py::_update_text_units
  called_by: []
- node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
  file: graphrag/query/context_builder/dynamic_community_selection.py
  name: select
  signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
    Any]]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously select relevant communities with respect to the query.\n\
    \nArgs:\n    query (str): The query to rate against.\n\nReturns:\n    tuple[list[CommunityReport],\
    \ dict[str, Any]]: A tuple containing a list of CommunityReport objects representing\
    \ the relevant communities and a dictionary with additional information including\
    \ llm usage metrics (llm_calls, prompt_tokens, output_tokens) and the ratings\
    \ mapping under the key \"ratings\".\n\nRaises:\n    Exceptions raised by rate_relevancy\
    \ or asyncio.gather may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 70
  line_end: 171
  dependencies:
  - graphrag/query/context_builder/rate_relevancy.py::rate_relevancy
  called_by: []
- node_id: unified-search-app/app/ui/search.py::display_search_result
  file: unified-search-app/app/ui/search.py
  name: display_search_result
  signature: "def display_search_result(\n    container: DeltaGenerator, result: SearchResult,\
    \ stats: SearchStats | None = None\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Display search results in the UI and update the corresponding placeholder.\n\
    \nThis function formats the search result response with hyperlinks via format_response_hyperlinks,\
    \ renders it as HTML in the provided Streamlit container, and stores the rendered\
    \ content in a session_state placeholder derived from the result's search_type\
    \ (for example, a placeholder named \"<search_type>_response_placeholder\"). If\
    \ stats are provided and completion_time is available, it also renders a summary\
    \ line showing tokens used, LLM calls, and elapsed time.\n\nArgs:\n    container\
    \ (DeltaGenerator): The Streamlit container to render the search result into.\n\
    \    result (SearchResult): The search result data to display, including response\
    \ and the\n        search_type used to derive UI keys and the HTML element id.\n\
    \    stats (SearchStats | None): Optional statistics about the search operation.\
    \ When provided\n        and completion_time is not None, a formatted stats line\
    \ is shown.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 39
  line_end: 60
  dependencies:
  - unified-search-app/app/ui/search.py::format_response_hyperlinks
  called_by: []
- node_id: graphrag/index/utils/derive_from_rows.py::gather
  file: graphrag/index/utils/derive_from_rows.py
  name: gather
  signature: 'def gather(execute: ExecuteFn[ItemType]) -> list[ItemType | None]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Gather results by applying the given execute function to each row of\
    \ the input DataFrame and returning the results as a list.\n\nArgs:\n    execute:\
    \ ExecuteFn[ItemType] - A function that accepts a tuple[Hashable, pd.Series] representing\
    \ a DataFrame row and returns an ItemType or None. This may be an awaitable.\n\
    \nReturns:\n    list[ItemType | None]: A list of results corresponding to each\
    \ input row. Each element is either an ItemType or None.\n\nRaises:\n    Exception:\
    \ If the underlying execute raises an exception, it will propagate to the caller."
  code_example: null
  example_source: null
  line_start: 108
  line_end: 118
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::execute_row_protected
  called_by:
  - graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache
  file: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py
  name: _wrapped_with_cache
  signature: 'def _wrapped_with_cache(**kwargs: Any) -> Any'
  decorators: []
  raises: []
  visibility: protected
  docstring: "Synchronous cache wrapper for Litellm requests.\n\nArgs\n----\nkwargs:\
    \ Any\n    The keyword arguments forwarded to the wrapped synchronous function.\
    \ This\n    includes the streaming flag ('stream') and other inputs used to build\
    \ the\n    cache key. When streaming is requested, caching is bypassed and the\n\
    \    underlying function is called directly.\n\nReturns\n-------\nAny\n    The\
    \ response produced by the wrapped function. If a valid cached response is\n \
    \   found, a corresponding ModelResponse or EmbeddingResponse is returned\n  \
    \  instead of calling the wrapped function.\n\nRaises\n------\nPropagates exceptions\
    \ raised by the wrapped function or by the cache\noperations."
  code_example: null
  example_source: null
  line_start: 48
  line_end: 76
  dependencies:
  - graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
  called_by: []
- node_id: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py::_wrapped_with_cache_async
  file: graphrag/language_model/providers/litellm/request_wrappers/with_cache.py
  name: _wrapped_with_cache_async
  signature: "def _wrapped_with_cache_async(\n        **kwargs: Any,\n    ) -> Any"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Asynchronous cache wrapper for Litellm requests.\n\nArgs:\n    kwargs:\
    \ Any\n        The keyword arguments forwarded to the wrapped asynchronous function.\
    \ This includes the streaming flag ('stream') and other inputs used to build the\
    \ cache key. When streaming is requested, caching is bypassed and the underlying\
    \ function is called directly.\n\nReturns:\n    Any\n        The response produced\
    \ by the wrapper. This may be a ModelResponse or EmbeddingResponse constructed\
    \ from a cached entry, or the result of the wrapped asynchronous function. When\
    \ streaming is requested, the underlying function is called directly and no caching\
    \ is performed.\n\nRaises:\n    Exception\n        Exceptions raised by the underlying\
    \ asynchronous function or by cache operations."
  code_example: null
  example_source: null
  line_start: 78
  line_end: 105
  dependencies:
  - graphrag/language_model/providers/litellm/get_cache_key.py::get_cache_key
  called_by: []
- node_id: graphrag/query/input/loaders/dfs.py::read_text_units
  file: graphrag/query/input/loaders/dfs.py
  name: read_text_units
  signature: "def read_text_units(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
    ,\n    text_col: str = \"text\",\n    entities_col: str | None = \"entity_ids\"\
    ,\n    relationships_col: str | None = \"relationship_ids\",\n    covariates_col:\
    \ str | None = \"covariate_ids\",\n    tokens_col: str | None = \"n_tokens\",\n\
    \    document_ids_col: str | None = \"document_ids\",\n    attributes_cols: list[str]\
    \ | None = None,\n) -> list[TextUnit]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read text units from a dataframe using pre-converted records.\n\nArgs:\n\
    \  df (pd.DataFrame): The DataFrame to process.\n  id_col (str): Column name for\
    \ the text unit identifier. Default 'id'.\n  text_col (str): Column name containing\
    \ the text of the unit. Default 'text'.\n  entities_col (str | None): Column name\
    \ for entity_ids, or None to omit. Default 'entity_ids'.\n  relationships_col\
    \ (str | None): Column name for relationship_ids, or None. Default 'relationship_ids'.\n\
    \  covariates_col (str | None): Column name for covariate_ids, or None. Default\
    \ 'covariate_ids'.\n  tokens_col (str | None): Column name for the token count.\
    \ Default 'n_tokens'.\n  document_ids_col (str | None): Column name for document_ids,\
    \ or None. Default 'document_ids'.\n  attributes_cols (list[str] | None): Additional\
    \ per-row attributes to include in the TextUnit. If None, no extra attributes\
    \ are captured. Default None.\n\nReturns:\n  list[TextUnit]: A list of TextUnit\
    \ objects constructed from the dataframe rows.\n\nRaises:\n  ValueError: If column_name\
    \ is None or the column is missing from data during value extraction (as enforced\
    \ by to_str).\n  TypeError: If a value in a column cannot be converted to the\
    \ expected type by the helper utilities (as raised by to_optional_list, to_optional_dict,\
    \ or to_optional_int)."
  code_example: null
  example_source: null
  line_start: 229
  line_end: 261
  dependencies:
  - graphrag/data_model/text_unit.py::TextUnit
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_optional_dict
  - graphrag/query/input/loaders/utils.py::to_optional_int
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_text_units
- node_id: graphrag/query/input/loaders/dfs.py::read_entities
  file: graphrag/query/input/loaders/dfs.py
  name: read_entities
  signature: "def read_entities(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
    ,\n    short_id_col: str | None = \"human_readable_id\",\n    title_col: str =\
    \ \"title\",\n    type_col: str | None = \"type\",\n    description_col: str |\
    \ None = \"description\",\n    name_embedding_col: str | None = \"name_embedding\"\
    ,\n    description_embedding_col: str | None = \"description_embedding\",\n  \
    \  community_col: str | None = \"community_ids\",\n    text_unit_ids_col: str\
    \ | None = \"text_unit_ids\",\n    rank_col: str | None = \"degree\",\n    attributes_cols:\
    \ list[str] | None = None,\n) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read entities from a dataframe using pre-converted records.\n\nThis\
    \ function prepares the input DataFrame by resetting the index and converting\
    \ it to a list of dictionaries, then constructs Entity objects from each record.\n\
    \nArgs:\n  df (pd.DataFrame): The DataFrame to process.\n  id_col (str): Column\
    \ name for the entity identifier. Default id.\n  short_id_col (str | None): Column\
    \ name for a short/human readable id. If None, uses the index value. Default human_readable_id.\n\
    \  title_col (str): Column name for the entity title. Default title.\n  type_col\
    \ (str | None): Column name for the entity type. If None, the type is omitted.\
    \ Default type.\n  description_col (str | None): Column name for the entity description.\
    \ If None, the description is omitted. Default description.\n  name_embedding_col\
    \ (str | None): Column name for the name embedding vector. If None, omitted. Default\
    \ name_embedding.\n  description_embedding_col (str | None): Column name for the\
    \ description embedding vector. If None, omitted. Default description_embedding.\n\
    \  community_col (str | None): Column name for the list of community IDs. If None,\
    \ omitted. Default community_ids.\n  text_unit_ids_col (str | None): Column name\
    \ for the list of text unit IDs. If None, omitted. Default text_unit_ids.\n  rank_col\
    \ (str | None): Column name for the rank/degree. If None, omitted. Default degree.\n\
    \  attributes_cols (list[str] | None): Optional list of additional attribute column\
    \ names to include in the resulting Entity.attributes. If None, no extra attributes\
    \ are included.\n\nReturns:\n  list[Entity]: A list of Entity objects created\
    \ from the dataframe records.\n\nRaises:\n  ValueError, TypeError: If required\
    \ data is missing or has invalid type for the expected columns, as raised by the\
    \ underlying conversion helpers (e.g., to_str, to_optional_str, to_optional_list,\
    \ to_optional_int)."
  code_example: null
  example_source: null
  line_start: 35
  line_end: 74
  dependencies:
  - graphrag/data_model/entity.py::Entity
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_optional_int
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_entities
- node_id: graphrag/query/input/loaders/dfs.py::read_relationships
  file: graphrag/query/input/loaders/dfs.py
  name: read_relationships
  signature: "def read_relationships(\n    df: pd.DataFrame,\n    id_col: str = \"\
    id\",\n    short_id_col: str | None = \"human_readable_id\",\n    source_col:\
    \ str = \"source\",\n    target_col: str = \"target\",\n    description_col: str\
    \ | None = \"description\",\n    rank_col: str | None = \"combined_degree\",\n\
    \    description_embedding_col: str | None = \"description_embedding\",\n    weight_col:\
    \ str | None = \"weight\",\n    text_unit_ids_col: str | None = \"text_unit_ids\"\
    ,\n    attributes_cols: list[str] | None = None,\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read relationships from a dataframe using pre-converted records.\n\n\
    Args:\n    df: The DataFrame to process.\n    id_col: Column name for the relationship\
    \ identifier.\n    short_id_col: Column name for a short/human-readable id. If\
    \ None, uses the index-derived id.\n    source_col: Column name for the relationship\
    \ source.\n    target_col: Column name for the relationship target.\n    description_col:\
    \ Optional column with a textual description of the relationship.\n    rank_col:\
    \ Optional column with the rank (degree) of the relationship.\n    description_embedding_col:\
    \ Optional column containing a description embedding as a list of floats.\n  \
    \  weight_col: Optional column with the weight of the relationship.\n    text_unit_ids_col:\
    \ Optional column with text unit identifiers as a list of strings.\n    attributes_cols:\
    \ Optional list of additional attribute column names to include in the attributes\
    \ dict.\n\nReturns:\n    list[Relationship]: A list of Relationship objects constructed\
    \ from the dataframe records.\n\nRaises:\n    ValueError: If a required column\
    \ name is None or missing from the input data when converting values.\n    TypeError:\
    \ If a value cannot be converted to the expected type (e.g., incorrect list element\
    \ types or mismatched types in conversions)."
  code_example: null
  example_source: null
  line_start: 77
  line_end: 114
  dependencies:
  - graphrag/data_model/relationship.py::Relationship
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_optional_float
  - graphrag/query/input/loaders/utils.py::to_optional_int
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_relationships
- node_id: graphrag/query/input/loaders/dfs.py::read_covariates
  file: graphrag/query/input/loaders/dfs.py
  name: read_covariates
  signature: "def read_covariates(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
    ,\n    short_id_col: str | None = \"human_readable_id\",\n    subject_col: str\
    \ = \"subject_id\",\n    covariate_type_col: str | None = \"type\",\n    text_unit_ids_col:\
    \ str | None = \"text_unit_ids\",\n    attributes_cols: list[str] | None = None,\n\
    ) -> list[Covariate]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read covariates from a dataframe using pre-converted records.\n\nArgs:\n\
    \    df (pd.DataFrame): The DataFrame to process.\n    id_col (str): Column name\
    \ for the covariate identifier. Default 'id'.\n    short_id_col (str | None):\
    \ Column name for a short/human-readable id. If None, uses the index-derived id.\n\
    \    subject_col (str): Column name for the subject identifier. Default 'subject_id'.\n\
    \    covariate_type_col (str | None): Column name for covariate type. If None,\
    \ defaults to 'claim'.\n    text_unit_ids_col (str | None): Column name for text_unit_ids.\
    \ Default 'text_unit_ids'.\n    attributes_cols (list[str] | None): Optional list\
    \ of additional attribute column names to include in Covariate.attributes.\n\n\
    Returns:\n    list[Covariate]: A list of Covariate objects constructed from the\
    \ dataframe.\n\nRaises:\n    ValueError: If a required column name is None or\
    \ missing in a row.\n    TypeError: If the value for text_unit_ids_col cannot\
    \ be interpreted as a list (as required by to_optional_list)."
  code_example: null
  example_source: null
  line_start: 117
  line_end: 146
  dependencies:
  - graphrag/data_model/covariate.py::Covariate
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_covariates
- node_id: graphrag/query/input/loaders/dfs.py::read_community_reports
  file: graphrag/query/input/loaders/dfs.py
  name: read_community_reports
  signature: "def read_community_reports(\n    df: pd.DataFrame,\n    id_col: str\
    \ = \"id\",\n    short_id_col: str | None = \"community\",\n    title_col: str\
    \ = \"title\",\n    community_col: str = \"community\",\n    summary_col: str\
    \ = \"summary\",\n    content_col: str = \"full_content\",\n    rank_col: str\
    \ | None = \"rank\",\n    content_embedding_col: str | None = \"full_content_embedding\"\
    ,\n    attributes_cols: list[str] | None = None,\n) -> list[CommunityReport]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read community reports from a dataframe using pre-converted records.\n\
    \nArgs:\n    df: The DataFrame to process.\n    id_col: The column name to use\
    \ for the report id.\n    short_id_col: The column name for the short identifier;\
    \ if None, the code falls back to the index.\n    title_col: The column name for\
    \ the report title.\n    community_col: The column name containing the community\
    \ identifier.\n    summary_col: The column name containing the summary.\n    content_col:\
    \ The column name containing the full content.\n    rank_col: The column name\
    \ containing the rank value, if present.\n    content_embedding_col: The column\
    \ name containing the full content embeddings, if present.\n    attributes_cols:\
    \ Optional list of additional attribute columns to include; if None, no extra\
    \ attributes are added.\n\nReturns:\n    list[CommunityReport]: A list of CommunityReport\
    \ objects constructed from the dataframe rows.\n\nRaises:\n    ValueError: If\
    \ a required column is None or missing from the input data, or if value conversion\
    \ fails in helper utilities.\n    TypeError: If attribute column values are not\
    \ of expected types."
  code_example: null
  example_source: null
  line_start: 191
  line_end: 226
  dependencies:
  - graphrag/data_model/community_report.py::CommunityReport
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_optional_float
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_reports
- node_id: graphrag/query/input/loaders/dfs.py::read_communities
  file: graphrag/query/input/loaders/dfs.py
  name: read_communities
  signature: "def read_communities(\n    df: pd.DataFrame,\n    id_col: str = \"id\"\
    ,\n    short_id_col: str | None = \"community\",\n    title_col: str = \"title\"\
    ,\n    level_col: str = \"level\",\n    entities_col: str | None = \"entity_ids\"\
    ,\n    relationships_col: str | None = \"relationship_ids\",\n    text_units_col:\
    \ str | None = \"text_unit_ids\",\n    covariates_col: str | None = \"covariate_ids\"\
    ,\n    parent_col: str | None = \"parent\",\n    children_col: str | None = \"\
    children\",\n    attributes_cols: list[str] | None = None,\n) -> list[Community]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read communities from a dataframe using pre-converted records.\n\nArgs:\n\
    \  df: The DataFrame to process.\n  id_col: Column name for the community identifier.\n\
    \  short_id_col: Column name for the short identifier; if None, uses the index\
    \ as a fallback.\n  title_col: Title column name.\n  level_col: Level column name.\n\
    \  entities_col: Column name for the list of entity_ids associated with the community.\n\
    \  relationships_col: Column name for the list of relationship_ids associated\
    \ with the community.\n  text_units_col: Column name for the list of text_unit_ids.\n\
    \  covariates_col: Column name for the dictionary of covariate_ids.\n  parent_col:\
    \ Column name for the parent identifier.\n  children_col: Column name for the\
    \ children list.\n  attributes_cols: Optional list of additional attribute columns\
    \ to include in the Community attributes; None to skip.\n\nReturns:\n  list[Community]:\
    \ A list of Community objects constructed from the dataframe.\n\nRaises:\n  ValueError:\
    \ If a required column is None or missing from a row during conversion.\n  TypeError:\
    \ If a value is not of an expected type."
  code_example: null
  example_source: null
  line_start: 149
  line_end: 188
  dependencies:
  - graphrag/data_model/community.py::Community
  - graphrag/query/input/loaders/dfs.py::_prepare_records
  - graphrag/query/input/loaders/utils.py::to_list
  - graphrag/query/input/loaders/utils.py::to_optional_dict
  - graphrag/query/input/loaders/utils.py::to_optional_list
  - graphrag/query/input/loaders/utils.py::to_optional_str
  - graphrag/query/input/loaders/utils.py::to_str
  called_by:
  - graphrag/query/indexer_adapters.py::read_indexer_communities
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: read
  signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
    \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
  decorators: []
  raises:
  - FileNotFoundError
  visibility: public
  docstring: "\"\"\"Read parquet file for a given table from blob storage.\n\nArgs:\n\
    \    table: The table name to read (without the .parquet extension).\n    throw_on_missing:\
    \ If True, raise FileNotFoundError when the table file does not exist.\n    columns:\
    \ Optional list of column names to read from the parquet file. If None, all columns\
    \ are read.\n\nReturns:\n    pd.DataFrame: A DataFrame containing the data from\
    \ the parquet file. If the table file does not exist and throw_on_missing is False,\
    \ an empty DataFrame is returned. If columns are provided, the empty DataFrame\
    \ will have those columns.\n\nRaises:\n    FileNotFoundError: If the table does\
    \ not exist and throw_on_missing is True.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 88
  line_end: 104
  dependencies:
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: read_settings
  signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
    \ bool = False,\n    ) -> GraphRagConfig | None"
  decorators: []
  raises:
  - FileNotFoundError
  visibility: public
  docstring: "Read settings from container.\n\nArgs:\n    self: The BlobDatasource\
    \ instance.\n    file: The blob file name containing the settings.\n    throw_on_missing:\
    \ If True, raise FileNotFoundError when the file does not exist.\n\nReturns:\n\
    \    GraphRagConfig | None: The graphrag configuration loaded from the settings\
    \ file, or None if not found.\n\nRaises:\n    FileNotFoundError: If the file does\
    \ not exist and throw_on_missing is True."
  code_example: null
  example_source: null
  line_start: 106
  line_end: 127
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - unified-search-app/app/knowledge_loader/data_sources/blob_source.py::load_blob_file
  called_by: []
- node_id: graphrag/index/workflows/prune_graph.py::run_workflow
  file: graphrag/index/workflows/prune_graph.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Executes the prune-graph workflow in three steps: load entities and\
    \ relationships from storage, prune using the provided pruning configuration,\
    \ and write the pruned entities and relationships back to storage. Returns the\
    \ pruned data as part of the WorkflowFunctionOutput.\n\nArgs:\n  config (GraphRagConfig):\
    \ Configuration for pruning, including parameters exposed under prune_graph to\
    \ control pruning behavior.\n  context (PipelineRunContext): Execution context\
    \ containing the storage backend and runtime information used for reading and\
    \ writing tables.\n\nReturns:\n  WorkflowFunctionOutput: Output with a result\
    \ dictionary containing:\n  - \"entities\": pruned entities DataFrame\n  - \"\
    relationships\": pruned relationships DataFrame\n\nRaises:\n  ValueError: If required\
    \ input tables (entities or relationships) are not found in storage.\n  Exception:\
    \ Exceptions raised by the storage backend during load or write operations may\
    \ propagate."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 50
  dependencies:
  - graphrag/index/operations/prune_graph.py::prune_graph
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_prune_graph.py::test_prune_graph
- node_id: graphrag/index/workflows/update_communities.py::run_workflow
  file: graphrag/index/workflows/update_communities.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the communities from an incremental index run.\n\nArgs:\n   \
    \ config (GraphRagConfig): GraphRagConfig configuration for the workflow.\n  \
    \  context (PipelineRunContext): PipelineRunContext carrying the state for the\
    \ run, including update_timestamp.\n\nReturns:\n    WorkflowFunctionOutput: The\
    \ output of the workflow function. The result is None.\n\nNotes:\n    During execution,\
    \ context.state[\"incremental_update_community_id_mapping\"] is set to the\n \
    \   mapping produced by updating and merging the communities.\n\nRaises:\n   \
    \ Exception: Propagates exceptions raised by storage backends or related IO operations\
    \ (e.g.,\n        storage IO errors, network issues, or data serialization problems)."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 36
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/update_communities.py::_update_communities
  called_by: []
- node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents
  file: tests/verbs/test_create_final_documents.py
  name: test_create_final_documents
  signature: def test_create_final_documents()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the final documents creation workflow.\n\nThis asynchronous test\
    \ loads the expected documents data, initializes a test context with text_units\
    \ storage, constructs a Graphrag config, executes the final documents workflow,\
    \ loads the produced documents from storage, and compares the actual output to\
    \ the expected data. It also verifies that all columns listed in DOCUMENTS_FINAL_COLUMNS\
    \ are present in the produced table.\n\nReturns:\n    None\n\nRaises:\n    Exception:\
    \ If any step in setup, execution, or validation fails."
  code_example: null
  example_source: null
  line_start: 20
  line_end: 36
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_final_documents.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: tests/verbs/test_create_final_documents.py::test_create_final_documents_with_metadata_column
  file: tests/verbs/test_create_final_documents.py
  name: test_create_final_documents_with_metadata_column
  signature: def test_create_final_documents_with_metadata_column()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the final documents workflow when a metadata column is provided.\n\
    \nThe test builds a test context with storage for text_units, creates a Graphrag\
    \ config, enables metadata extraction by setting config.input.metadata = [\"title\"\
    ], simulates the metadata construction during initial input loading by calling\
    \ update_document_metadata, captures the expected documents table from storage,\
    \ runs the final documents workflow, then loads the actual documents table from\
    \ storage, and compares the two results. It also asserts that every column listed\
    \ in DOCUMENTS_FINAL_COLUMNS is present in the produced final documents table.\n\
    \nReturns:\n    None\nRaises:\n    Exception: Exceptions raised by the utilities\
    \ used in the test (e.g., load_table_from_storage, update_document_metadata, run_workflow)\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 59
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_final_documents.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::update_document_metadata
  called_by: []
- node_id: tests/verbs/test_finalize_graph.py::_prep_tables
  file: tests/verbs/test_finalize_graph.py
  name: _prep_tables
  signature: def _prep_tables()
  decorators: []
  raises: []
  visibility: protected
  docstring: "Prepare test tables for the finalize_graph workflow by loading test\
    \ data into a test context, dropping final columns that wouldn't be present in\
    \ inputs (x, y, degree from entities and combined_degree from relationships),\
    \ and returning the initialized context.\n\nReturns:\n    PipelineRunContext:\
    \ The initialized pipeline run context with the test data loaded into its output\
    \ storage.\n\nRaises:\n    Exception: Exceptions raised by create_test_context,\
    \ load_test_table, or write_table_to_storage may propagate."
  code_example: null
  example_source: null
  line_start: 68
  line_end: 80
  dependencies:
  - graphrag/utils/storage.py::write_table_to_storage
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by:
  - tests/verbs/test_finalize_graph.py::test_finalize_graph
  - tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
- node_id: graphrag/index/input/csv.py::load_file
  file: graphrag/index/input/csv.py
  name: load_file
  signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously load a CSV file from storage and return it as a DataFrame.\n\
    \nThis function reads the CSV using the configured encoding (config.encoding).\n\
    If grouping data is provided via 'group', the corresponding keys are added as\
    \ new columns to every row (one column per key), rather than merging or stacking\
    \ rows.\n\nThe DataFrame is then augmented by process_data_columns to include\
    \ additional configured metadata columns (for example, id, text, and title) as\
    \ defined by the configuration.\n\nA creation_date column is added to all rows,\
    \ with the same value derived from storage.get_creation_date(path) for the given\
    \ path.\n\nArgs:\n  path (str): Path to the CSV file in storage.\n  group (dict\
    \ | None): Optional mapping of grouping metadata. If None, treated as {}.\n\n\
    Returns:\n  pd.DataFrame: Loaded CSV data with grouping columns (if provided),\
    \ processed data columns, and a creation_date column added to every row.\n\nRaises:\n\
    \  Exception: If storage access, CSV parsing, or data processing fails."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 41
  dependencies:
  - graphrag/index/input/util.py::process_data_columns
  called_by: []
- node_id: graphrag/index/input/json.py::load_file
  file: graphrag/index/input/json.py
  name: load_file
  signature: 'def load_file(path: str, group: dict | None) -> pd.DataFrame'
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously load a JSON input file from storage and return it as\
    \ a DataFrame.\n\nArgs:\n  path (str): Path to the JSON file.\n  group (dict |\
    \ None): Optional grouping metadata to merge with the item. If None, an empty\
    \ dict is used.\n\nReturns:\n  pd.DataFrame: DataFrame loaded from the JSON content,\
    \ augmented with grouping keys (if any), processed by process_data_columns, and\
    \ annotated with a creation_date column for each row.\n\nRaises:\n  json.JSONDecodeError:\
    \ If the loaded content cannot be decoded as JSON."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 45
  dependencies:
  - graphrag/index/input/util.py::process_data_columns
  called_by: []
- node_id: graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  file: graphrag/index/utils/stable_lcc.py
  name: stable_largest_connected_component
  signature: 'def stable_largest_connected_component(graph: nx.Graph) -> nx.Graph'
  decorators: []
  raises: []
  visibility: public
  docstring: "Return the largest connected component of the graph, with nodes and\
    \ edges sorted in a stable way.\n\nArgs:\n    graph (nx.Graph): Input graph from\
    \ which to compute the stable largest connected component.\n\nReturns:\n    nx.Graph:\
    \ The stabilized largest connected component graph with deterministic ordering\
    \ of nodes and edges."
  code_example: null
  example_source: null
  line_start: 12
  line_end: 20
  dependencies:
  - graphrag/index/utils/stable_lcc.py::_stabilize_graph
  - graphrag/index/utils/stable_lcc.py::normalize_node_names
  called_by:
  - graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
  - graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: _build_text_unit_context
  signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
    \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
    \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
    \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Rank matching text units and add them to the context window until it\
    \ hits the max_context_tokens limit.\n\nArgs:\n    selected_entities (list[Entity]):\
    \ Entities for which to collect and rank associated text units.\n    max_context_tokens\
    \ (int): Maximum number of tokens to include in the context.\n    return_candidate_context\
    \ (bool): If True, also compute and include candidate text units context data.\n\
    \    column_delimiter (str): Delimiter used to separate fields in the context\
    \ rows.\n    context_name (str): Name of the context section to populate (default\
    \ \"Sources\").\n\nReturns:\n    tuple[str, dict[str, pd.DataFrame]]: The textual\
    \ context and a mapping from context names to DataFrames containing the context\
    \ data."
  code_example: null
  example_source: null
  line_start: 306
  line_end: 375
  dependencies:
  - graphrag/query/context_builder/source_context.py::build_text_unit_context
  - graphrag/query/context_builder/source_context.py::count_relationships
  - graphrag/query/input/retrieval/text_units.py::get_candidate_text_units
  called_by: []
- node_id: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
  file: graphrag/index/operations/summarize_descriptions/summarize_descriptions.py
  name: summarize_descriptions
  signature: "def summarize_descriptions(\n    entities_df: pd.DataFrame,\n    relationships_df:\
    \ pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n\
    \    strategy: dict[str, Any] | None = None,\n    num_threads: int = 4,\n) ->\
    \ tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Summarize entity and relationship descriptions from an entity graph,\
    \ using a language model.\n\nArgs:\n    entities_df: DataFrame containing entity\
    \ nodes with at least a title and a description per node.\n    relationships_df:\
    \ DataFrame containing edge information with at least source, target, and a description\
    \ per edge.\n    callbacks: WorkflowCallbacks providing progress reporting hooks\
    \ for long-running operations.\n    cache: PipelineCache used to cache results\
    \ from strategy execution.\n    strategy: dict[str, Any] | None: Strategy configuration\
    \ for the summarization strategy. If None, defaults are applied.\n    num_threads:\
    \ int: Number of concurrent workers to use for summarization.\n\nReturns:\n  \
    \  tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the entity descriptions\
    \ DataFrame and the relationship descriptions DataFrame.\n\nRaises:\n    ValueError:\
    \ If an unknown strategy type is provided."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 108
  dependencies:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::get_summarized
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::load_strategy
  called_by:
  - graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
- node_id: graphrag/index/run/run_pipeline.py::run_pipeline
  file: graphrag/index/run/run_pipeline.py
  name: run_pipeline
  signature: "def run_pipeline(\n    pipeline: Pipeline,\n    config: GraphRagConfig,\n\
    \    callbacks: WorkflowCallbacks,\n    is_update_run: bool = False,\n    additional_context:\
    \ dict[str, Any] | None = None,\n    input_documents: pd.DataFrame | None = None,\n\
    ) -> AsyncIterable[PipelineRunResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run all workflows using a simplified pipeline.\n\nArgs:\n    pipeline:\
    \ Pipeline\n        The pipeline to run.\n    config: GraphRagConfig\n       \
    \ The GraphRag configuration to use for the run.\n    callbacks: WorkflowCallbacks\n\
    \        The callbacks to invoke during workflow execution.\n    is_update_run:\
    \ bool\n        Whether this run should perform an incremental update (default:\
    \ False).\n    additional_context: dict[str, Any] | None\n        Optional additional\
    \ context to merge into the run state.\n    input_documents: pd.DataFrame | None\n\
    \        Optional input documents. If provided, they will be written directly\
    \ to storage\n        (skipping the usual load/parse steps) before running the\
    \ pipeline.\n\nReturns:\n    AsyncIterable[PipelineRunResult]\n        An asynchronous\
    \ iterable that yields a PipelineRunResult for each workflow as it runs.\n\nRaises:\n\
    \    ValueError: If the storage type is not registered.\n    Exception: May raise\
    \ any exception raised by storage backends during read/write operations or by\
    \ the pipeline execution."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 101
  dependencies:
  - graphrag/index/run/run_pipeline.py::_copy_previous_output
  - graphrag/index/run/run_pipeline.py::_run_pipeline
  - graphrag/index/run/utils.py::create_run_context
  - graphrag/utils/api.py::create_cache_from_config
  - graphrag/utils/api.py::create_storage_from_config
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/api/index.py::build_index
- node_id: graphrag/query/context_builder/local_context.py::_filter_relationships
  file: graphrag/query/context_builder/local_context.py
  name: _filter_relationships
  signature: "def _filter_relationships(\n    selected_entities: list[Entity],\n \
    \   relationships: list[Relationship],\n    top_k_relationships: int = 10,\n \
    \   relationship_ranking_attribute: str = \"rank\",\n) -> list[Relationship]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Filter and sort relationships based on a set of selected entities and\
    \ a ranking attribute.\n\nFirst priority: in-network relationships (i.e. relationships\
    \ between selected entities). Second priority: out-of-network relationships (i.e.\
    \ relationships between selected entities and other entities not in the selected\
    \ set). Within out-of-network relationships, mutual relationships (shared with\
    \ multiple selected entities) are prioritized by counting links per out-network\
    \ entity.\n\nArgs:\n    selected_entities (list[Entity]): The selected entities\
    \ to consider.\n    relationships (list[Relationship]): The pool of relationships\
    \ to search within.\n    top_k_relationships (int): The maximum number of out-of-network\
    \ relationships to include per selected entity (default 10).\n    relationship_ranking_attribute\
    \ (str): The attribute name used for ranking; defaults to \"rank\".\n\nReturns:\n\
    \    list[Relationship]: The filtered and sorted relationships. The result is\
    \ the concatenation of in-network relationships and the top-ranked out-of-network\
    \ relationships, truncated to a budget equal to top_k_relationships * len(selected_entities)."
  code_example: null
  example_source: null
  line_start: 232
  line_end: 317
  dependencies:
  - graphrag/query/input/retrieval/relationships.py::get_in_network_relationships
  - graphrag/query/input/retrieval/relationships.py::get_out_network_relationships
  called_by:
  - graphrag/query/context_builder/local_context.py::build_relationship_context
- node_id: graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
  file: graphrag/index/operations/summarize_communities/build_mixed_context.py
  name: build_mixed_context
  signature: "def build_mixed_context(\n    context: list[dict], tokenizer: Tokenizer,\
    \ max_context_tokens: int\n) -> str"
  decorators: []
  raises: []
  visibility: public
  docstring: "Builds the parent context by concatenating all sub-communities' contexts,\
    \ with a fallback to sub-community reports if the combined context would exceed\
    \ the token limit.\n\nArgs:\n    context: list[dict]\n        List of sub-community\
    \ contexts to process.\n    tokenizer: Tokenizer\n        Tokenizer used to count\
    \ tokens to enforce max_context_tokens.\n    max_context_tokens: int\n       \
    \ Maximum number of tokens allowed for the resulting context.\n\nReturns:\n  \
    \  str\n        The resulting context as a string; may be a concatenation of local\
    \ contexts, or a CSV of substitute reports if limits are reached."
  code_example: null
  example_source: null
  line_start: 14
  line_end: 73
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
  - graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _sort_and_trim_context
  signature: "def _sort_and_trim_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
    \ max_context_tokens: int\n) -> pd.Series"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Sort and trim the context to fit the token limit.\n\nArgs:\n    df:\
    \ DataFrame containing the contexts, with a column named by schemas.ALL_CONTEXT\
    \ that holds the per-row context data to be processed.\n    tokenizer: Tokenizer.\
    \ Tokenizer used to count tokens when trimming.\n    max_context_tokens: int.\
    \ Maximum number of tokens allowed for each context after trimming.\n\nReturns:\n\
    \    pd.Series. A Series containing the processed contexts after sorting and trimming\
    \ per entry.\n\nRaises:\n    Exception: Propagates any exception raised by the\
    \ transformation (e.g., via transform_series or sort_context) when processing\
    \ the contexts."
  code_example: null
  example_source: null
  line_start: 279
  line_end: 289
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  - graphrag/index/utils/dataframes.py::transform_series
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
- node_id: graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
  file: graphrag/index/operations/summarize_communities/graph_context/sort_context.py
  name: parallel_sort_context_batch
  signature: "def parallel_sort_context_batch(\n    community_df, tokenizer: Tokenizer,\
    \ max_context_tokens, parallel=False\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Calculate context strings for each community entry, optionally using\
    \ parallel execution, and populate related context columns.\n\nArgs:\n  community_df:\
    \ DataFrame containing community data to be enriched with context strings.\n \
    \ tokenizer: Tokenizer used to count tokens for context strings.\n  max_context_tokens:\
    \ Maximum number of tokens allowed for a context string.\n  parallel: Whether\
    \ to enable parallel computation of context strings using ThreadPoolExecutor.\n\
    \nReturns:\n  The input DataFrame with additional context-related columns populated:\n\
    \  - CONTEXT_STRING: the computed context string for each row.\n  - CONTEXT_SIZE:\
    \ token length of CONTEXT_STRING.\n  - CONTEXT_EXCEED_FLAG: whether CONTEXT_SIZE\
    \ exceeds max_context_tokens.\n\nRaises:\n  Exceptions raised by sort_context\
    \ or by the parallel execution machinery if parallel is enabled."
  code_example: null
  example_source: null
  line_start: 129
  line_end: 164
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
- node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context
  file: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py
  name: test_sort_context
  signature: def test_sort_context()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that sort_context returns a non-null context and that the token\
    \ count matches platform-dependent expectations.\n\nReturns:\n    None. This is\
    \ a unit test function and does not return a value.\n\nRaises:\n    AssertionError:\
    \ If any assertion fails."
  code_example: null
  example_source: null
  line_start: 206
  line_end: 213
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py::test_sort_context_max_tokens
  file: tests/unit/indexing/graph/extractors/community_reports/test_sort_context.py
  name: test_sort_context_max_tokens
  signature: def test_sort_context_max_tokens()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that sort_context respects the max_context_tokens constraint by\
    \ returning a non-null context whose token count is less than or equal to the\
    \ specified maximum.\n\nParameters:\n    None: This test has no input parameters.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails."
  code_example: null
  example_source: null
  line_start: 216
  line_end: 221
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::sort_context
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/index/operations/embed_text/strategies/openai.py::run
  file: graphrag/index/operations/embed_text/strategies/openai.py
  name: run
  signature: "def run(\n    input: list[str],\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    args: dict[str, Any],\n) -> TextEmbeddingResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the Claim extraction chain.\n\nArgs:\n    input: list[str] The input\
    \ texts to process.\n    callbacks: WorkflowCallbacks The callbacks interface\
    \ for progress and other hooks.\n    cache: PipelineCache The cache to use for\
    \ model embeddings and related data.\n    args: dict[str, Any] Additional arguments\
    \ for configuring the run (e.g., batch_size, batch_max_tokens, llm).\n\nReturns:\n\
    \    TextEmbeddingResult The embedding results for the input texts, or embeddings=None\
    \ if the input is null.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 26
  line_end: 76
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/embed_text/strategies/openai.py::_create_text_batches
  - graphrag/index/operations/embed_text/strategies/openai.py::_execute
  - graphrag/index/operations/embed_text/strategies/openai.py::_get_splitter
  - graphrag/index/operations/embed_text/strategies/openai.py::_prepare_embed_texts
  - graphrag/index/operations/embed_text/strategies/openai.py::_reconstitute_embeddings
  - graphrag/index/operations/embed_text/strategies/typing.py::TextEmbeddingResult
  - graphrag/index/utils/is_null.py::is_null
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/logger/progress.py::progress_ticker
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _get_subcontext_df
  signature: "def _get_subcontext_df(\n    level: int, report_df: pd.DataFrame, local_context_df:\
    \ pd.DataFrame\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get sub-community context for each community.\n\nArgs:\n    level: int\n\
    \        The level to extract sub-context for.\n    report_df: pd.DataFrame\n\
    \        DataFrame containing the reports for communities at the given level.\n\
    \    local_context_df: pd.DataFrame\n        DataFrame containing local context\
    \ information for communities.\n\nReturns:\n    pd.DataFrame\n        DataFrame\
    \ containing sub-context for each community, with COMMUNITY_ID renamed to SUB_COMMUNITY.\n\
    \nRaises:\n    KeyError: If the COMMUNITY_LEVEL column is not present in report_df\
    \ or local_context_df."
  code_example: null
  example_source: null
  line_start: 305
  line_end: 315
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
  - graphrag/index/utils/dataframes.py::join
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
- node_id: graphrag/config/load_config.py::load_config
  file: graphrag/config/load_config.py
  name: load_config
  signature: "def load_config(\n    root_dir: Path,\n    config_filepath: Path | None\
    \ = None,\n    cli_overrides: dict[str, Any] | None = None,\n) -> GraphRagConfig"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load configuration from a file.\n\nArgs:\n    root_dir: The root directory\
    \ of the project. Will search for the config file in this directory.\n    config_filepath:\
    \ The path to the config file. If None, searches for config file in root.\n  \
    \  cli_overrides: A flat dictionary of cli overrides. Example: {'output.base_dir':\
    \ 'override_value'}\n\nReturns:\n    GraphRagConfig\n        The loaded configuration.\n\
    \nRaises:\n    FileNotFoundError\n        If the config file is not found.\n \
    \   ValueError\n        If the config file extension is not supported.\n    TypeError\n\
    \        If applying cli overrides to the config fails.\n    KeyError\n      \
    \  If config file references a non-existent environment variable.\n    ValidationError\n\
    \        If there are pydantic validation errors when instantiating the config."
  code_example: null
  example_source: null
  line_start: 146
  line_end: 191
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/config/load_config.py::_apply_overrides
  - graphrag/config/load_config.py::_get_config_path
  - graphrag/config/load_config.py::_load_dotenv
  - graphrag/config/load_config.py::_parse
  - graphrag/config/load_config.py::_parse_env_variables
  called_by:
  - graphrag/cli/index.py::index_cli
  - graphrag/cli/index.py::update_cli
  - graphrag/cli/prompt_tune.py::prompt_tune
  - graphrag/cli/query.py::run_global_search
  - graphrag/cli/query.py::run_local_search
  - graphrag/cli/query.py::run_drift_search
  - graphrag/cli/query.py::run_basic_search
  - tests/unit/config/test_config.py::test_load_minimal_config
  - tests/unit/config/test_config.py::test_load_config_with_cli_overrides
  - tests/unit/config/test_config.py::test_load_config_missing_env_vars
  - unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
- node_id: graphrag/index/operations/embed_text/embed_text.py::embed_text
  file: graphrag/index/operations/embed_text/embed_text.py
  name: embed_text
  signature: "def embed_text(\n    input: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    embed_column: str,\n    strategy: dict,\n    embedding_name:\
    \ str,\n    id_column: str = \"id\",\n    title_column: str | None = None,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Embed text from a DataFrame into a vector space and return per-row embeddings.\
    \ If a vector store is configured in the provided strategy, embeddings are generated\
    \ and stored in that vector store using the given id and optional title information;\
    \ otherwise embeddings are computed in memory. The function returns a sequence\
    \ of embedding vectors, one per input row corresponding to embed_column.\n\nArgs:\n\
    \    input (pd.DataFrame): Input data; must include embed_column and id_column,\
    \ and may include title_column.\n    callbacks (WorkflowCallbacks): Callbacks\
    \ used during embedding.\n    cache (PipelineCache): Cache object used by the\
    \ embedding strategy.\n    embed_column (str): Name of the DataFrame column to\
    \ embed.\n    strategy (dict): Embedding strategy configuration; must include\
    \ a \"type\" key and may include vector_store settings.\n    embedding_name (str):\
    \ The embedding configuration name used to determine indexing in the vector store.\n\
    \    id_column (str): The ID column name. Defaults to \"id\".\n    title_column\
    \ (str | None): Optional column containing the title for each document; may be\
    \ None.\n\nReturns:\n    embeddings (List[List[float]]): The embeddings produced\
    \ by the embedding process, one vector per input row. The shape is (N, D) where\
    \ N is the number of rows and D is the embedding dimension.\n\nRaises:\n    Exception:\
    \ Propagates exceptions raised by internal operations such as vector store creation\
    \ or embedding strategy execution."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 78
  dependencies:
  - graphrag/index/operations/embed_text/embed_text.py::_create_vector_store
  - graphrag/index/operations/embed_text/embed_text.py::_get_index_name
  - graphrag/index/operations/embed_text/embed_text.py::_text_embed_in_memory
  - graphrag/index/operations/embed_text/embed_text.py::_text_embed_with_vector_store
  called_by:
  - graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
- node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: __init__
  signature: "def __init__(\n        self,\n        name: str,\n        config: \"\
    LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n    \
    \    **kwargs: Any,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the Litellm embedding model with the given name and configuration.\n\
    \nArgs:\n    name: str\n        The name of the model instance.\n    config: LanguageModelConfig\n\
    \        The configuration for the language model.\n    cache: PipelineCache |\
    \ None\n        Optional cache to use for embeddings. If provided, a child cache\
    \ scoped to this model's name is created.\n    **kwargs: Any\n        Additional\
    \ keyword arguments accepted for compatibility. They are not used to configure\
    \ the embedding model at initialization and will be ignored here; some kwargs\
    \ may be processed by the embedding methods via _get_kwargs.\n\nReturns:\n   \
    \ None\n\nRaises:\n    Exception\n        If the underlying embedding initialization\
    \ fails (e.g., invalid configuration or cache-related errors)."
  code_example: null
  example_source: null
  line_start: 178
  line_end: 190
  dependencies:
  - graphrag/language_model/providers/litellm/embedding_model.py::_create_embeddings
  called_by: []
- node_id: graphrag/index/workflows/update_community_reports.py::run_workflow
  file: graphrag/index/workflows/update_community_reports.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the community reports from a incremental index run.\n\nArgs:\n\
    \    config: GraphRagConfig\n        GraphRagConfig configuration for the workflow.\n\
    \    context: PipelineRunContext\n        PipelineRunContext carrying the state\
    \ for the run, including update_timestamp and incremental_update_community_id_mapping.\n\
    \nReturns:\n    WorkflowFunctionOutput\n        The output of the workflow function.\
    \ The result is None.\n\nRaises:\n    KeyError\n        If 'update_timestamp'\
    \ is not present in context.state."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 42
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/update_community_reports.py::_update_community_reports
  called_by: []
- node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: __init__
  signature: "def __init__(\n        self,\n        name: str,\n        config: \"\
    LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n    \
    \    **kwargs: Any,\n    )"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize the Litellm chat model with the provided name and configuration.\n\
    \nArgs:\n    name: str\n        The name of the model instance.\n    config: LanguageModelConfig\n\
    \        The configuration for the language model.\n    cache: PipelineCache |\
    \ None\n        Optional cache to use for responses. If provided, a child cache\
    \ scoped to this model's name is created.\n    **kwargs: Any\n        Additional\
    \ keyword arguments.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 214
  line_end: 226
  dependencies:
  - graphrag/language_model/providers/litellm/chat_model.py::_create_completions
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
  file: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py
  name: build_local_context
  signature: "def build_local_context(\n    community_membership_df: pd.DataFrame,\n\
    \    text_units_df: pd.DataFrame,\n    node_df: pd.DataFrame,\n    tokenizer:\
    \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare local context data for community report generation using text\
    \ unit data.\n\nComputes per-community local context by enriching text units with\
    \ degree information, merging with community membership, and producing a per-community\
    \ context string sorted by relevance. The function relies on prep_text_units to\
    \ obtain text unit details (including short_id, text, and entity_degree) and merges\
    \ these details with membership information to build a per-community ALL_CONTEXT\
    \ list. The resulting DataFrame includes a sorted CONTEXT_STRING, its token size\
    \ CONTEXT_SIZE, and a flag CONTEXT_EXCEED_FLAG indicating whether the context\
    \ exceeds max_context_tokens.\n\nArgs:\n  community_membership_df: DataFrame containing\
    \ community membership data with columns including COMMUNITY_ID, COMMUNITY_LEVEL,\
    \ and TEXT_UNIT_IDS.\n  text_units_df: DataFrame of text units used to enrich\
    \ with degree information via prep_text_units.\n  node_df: DataFrame of nodes\
    \ used to compute entity degrees for text units.\n  tokenizer: Tokenizer used\
    \ to compute token counts and to sort contexts.\n  max_context_tokens: Maximum\
    \ number of tokens allowed for a community's local context.\n\nReturns:\n  A pandas\
    \ DataFrame containing per-community local context data, keyed by COMMUNITY_ID\
    \ and COMMUNITY_LEVEL, including ALL_CONTEXT (list of dictionaries with id, text,\
    \ entity_degree), CONTEXT_STRING (sorted context), CONTEXT_SIZE (token count),\
    \ and CONTEXT_EXCEED_FLAG (whether CONTEXT_SIZE exceeds max_context_tokens)."
  code_example: null
  example_source: null
  line_start: 26
  line_end: 82
  dependencies:
  - graphrag/index/operations/summarize_communities/text_unit_context/prep_text_units.py::prep_text_units
  - graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
  called_by:
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
- node_id: graphrag/index/workflows/create_final_text_units.py::run_workflow
  file: graphrag/index/workflows/create_final_text_units.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the final text units transformation workflow by loading input tables\
    \ from storage, constructing final text units using entities, relationships, and\
    \ optional covariates, and writing the output back to storage.\n\nArgs:\n  config\
    \ (GraphRagConfig): GraphRag configuration for the workflow\n  context (PipelineRunContext):\
    \ Pipeline run context containing the output storage used for load/write operations\n\
    \nReturns:\n  WorkflowFunctionOutput: The output object containing the final text\
    \ units DataFrame in its result attribute\n\nRaises:\n  Exception: Exceptions\
    \ raised by storage backends during load or write operations may propagate."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 52
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_final_text_units.py::create_final_text_units
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::storage_has_table
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_final_text_units.py::test_create_final_text_units
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_basic_functionality
  signature: def test_basic_functionality(self, mock_get_encoding)
  decorators:
  - '@patch("tiktoken.get_encoding")'
  raises: []
  visibility: public
  docstring: "Chunks text into chunks based on encoding tokens.\n\nArgs:\n    input:\
    \ list[str] - The input texts to be chunked.\n    config: ChunkingConfig - Chunking\
    \ configuration. Uses: size (number of tokens per chunk), overlap (number of overlapping\
    \ tokens between consecutive chunks), encoding_model (name of the encoding model\
    \ used to tokenize).\n    tick: ProgressTicker - Progress reporter; invoked to\
    \ indicate progress.\n\nReturns:\n    Iterable[TextChunk] - An iterable of TextChunk\
    \ objects.\n\nRaises:\n    None"
  code_example: null
  example_source: null
  line_start: 53
  line_end: 72
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/strategies.py::run_tokens
  called_by: []
- node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: test_non_string_input
  signature: def test_non_string_input(self, mock_get_encoding)
  decorators:
  - '@patch("tiktoken.get_encoding")'
  raises: []
  visibility: public
  docstring: "Test handling of non-string input (e.g., numbers) when tokenizing text.\n\
    \nArgs:\n  - self: The test case instance.\n  - mock_get_encoding: The patched\
    \ tiktoken.get_encoding function; a Mock that returns a mock encoder used to encode/decode\
    \ tokens.\n\nReturns:\n  - None. This test does not return a value.\n\nRaises:\n\
    \  - None. No exceptions are expected during the test execution."
  code_example: null
  example_source: null
  line_start: 75
  line_end: 90
  dependencies:
  - graphrag/config/models/chunking_config.py::ChunkingConfig
  - graphrag/index/operations/chunk_text/strategies.py::run_tokens
  called_by: []
- node_id: graphrag/index/workflows/load_input_documents.py::run_workflow
  file: graphrag/index/workflows/load_input_documents.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Load input documents, write them to storage, and return the result as\
    \ a WorkflowFunctionOutput.\n\nArgs:\n    config: GraphRagConfig containing input\
    \ configuration and related settings.\n    context: PipelineRunContext providing\
    \ access to input_storage, output_storage, and runtime statistics.\n\nReturns:\n\
    \    WorkflowFunctionOutput: The output containing the loaded input documents\
    \ as a pandas DataFrame in the result field.\n\nRaises:\n    Exception: Exceptions\
    \ raised by the storage backend during the write operation may propagate."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 36
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/load_input_documents.py::load_input_documents
  - graphrag/utils/storage.py::write_table_to_storage
  called_by: []
- node_id: graphrag/index/operations/finalize_relationships.py::finalize_relationships
  file: graphrag/index/operations/finalize_relationships.py
  name: finalize_relationships
  signature: "def finalize_relationships(\n    relationships: pd.DataFrame,\n) ->\
    \ pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transform input relationships into finalized relationship records.\n\
    \nThis function builds a graph from the input relationships, computes node degrees,\n\
    deduplicates edges, computes a combined degree for each edge, assigns stable\n\
    human-readable and UUID-based identifiers, and returns a DataFrame containing\n\
    the expected final columns.\n\nArgs:\n    relationships: DataFrame containing\
    \ edge information for relationships to finalize.\n\nReturns:\n    A DataFrame\
    \ containing the finalized relationships, including the columns\n    defined in\
    \ RELATIONSHIPS_FINAL_COLUMNS, as well as additional columns:\n    - human_readable_id:\
    \ the index value assigned prior to UUID generation\n    - id: a string UUID for\
    \ each row\n    - combined_degree: the computed combined degree per edge\n\nRaises:\n\
    \    Propagates exceptions from underlying operations such as graph creation,\n\
    \    degree computation, edge degree computation, and DataFrame manipulation."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 44
  dependencies:
  - graphrag/index/operations/compute_degree.py::compute_degree
  - graphrag/index/operations/compute_edge_combined_degree.py::compute_edge_combined_degree
  - graphrag/index/operations/create_graph.py::create_graph
  called_by:
  - graphrag/index/workflows/finalize_graph.py::finalize_graph
- node_id: graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  file: graphrag/language_model/providers/fnllm/utils.py
  name: _create_openai_config
  signature: 'def _create_openai_config(config: LanguageModelConfig, azure: bool)
    -> OpenAIConfig'
  decorators: []
  raises:
  - ValueError
  visibility: protected
  docstring: "Create an OpenAIConfig from a LanguageModelConfig.\n\nArgs:\n    config:\
    \ LanguageModelConfig. The configuration used to derive the OpenAI parameters,\
    \ including encoding_model, model_supports_json, and other fields; a chat_parameters\
    \ object is built from get_openai_model_parameters_from_config(config).\n    azure:\
    \ bool. If True, construct an AzureOpenAIConfig; otherwise construct a PublicOpenAIConfig.\n\
    \nReturns:\n    OpenAIConfig. The constructed OpenAI configuration (AzureOpenAIConfig\
    \ when azure is True, PublicOpenAIConfig otherwise).\n\nRaises:\n    ValueError:\
    \ Azure OpenAI Chat LLM requires an API base when azure is True."
  code_example: null
  example_source: null
  line_start: 57
  line_end: 107
  dependencies:
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  called_by:
  - graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
  - graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
- node_id: graphrag/query/factory.py::get_local_search_engine
  file: graphrag/query/factory.py
  name: get_local_search_engine
  signature: "def get_local_search_engine(\n    config: GraphRagConfig,\n    reports:\
    \ list[CommunityReport],\n    text_units: list[TextUnit],\n    entities: list[Entity],\n\
    \    relationships: list[Relationship],\n    covariates: dict[str, list[Covariate]],\n\
    \    response_type: str,\n    description_embedding_store: BaseVectorStore,\n\
    \    system_prompt: str | None = None,\n    callbacks: list[QueryCallbacks] |\
    \ None = None,\n) -> LocalSearch"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a local search engine based on data + configuration.\n\nArgs:\n\
    \    config: GraphRagConfig\n        GraphRag configuration object containing\
    \ local search settings used to configure the search engine and models.\n    reports:\
    \ list[CommunityReport]\n        Community reports to be used by the local search\
    \ engine context.\n    text_units: list[TextUnit]\n        Text units to be included\
    \ in the search context.\n    entities: list[Entity]\n        Entities to be considered\
    \ in the search context.\n    relationships: list[Relationship]\n        Relationships\
    \ to be considered in the search context.\n    covariates: dict[str, list[Covariate]]\n\
    \        Covariates to augment context for the local search.\n    response_type:\
    \ str\n        Type of response to return from the search engine.\n    description_embedding_store:\
    \ BaseVectorStore\n        Vector store containing description embeddings for\
    \ entities.\n    system_prompt: str | None\n        Optional system prompt to\
    \ guide the local search model.\n    callbacks: list[QueryCallbacks] | None\n\
    \        Optional list of query callbacks to execute during search.\n\nReturns:\n\
    \    LocalSearch\n        A configured LocalSearch instance ready to execute queries.\n\
    \nRaises:\n    Exception\n        If an error occurs during model initialization\
    \ or tokenizer setup."
  code_example: null
  example_source: null
  line_start: 39
  line_end: 108
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
  - graphrag/query/structured_search/local_search/search.py::LocalSearch
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/api/query.py::local_search_streaming
- node_id: graphrag/query/factory.py::get_global_search_engine
  file: graphrag/query/factory.py
  name: get_global_search_engine
  signature: "def get_global_search_engine(\n    config: GraphRagConfig,\n    reports:\
    \ list[CommunityReport],\n    entities: list[Entity],\n    communities: list[Community],\n\
    \    response_type: str,\n    dynamic_community_selection: bool = False,\n   \
    \ map_system_prompt: str | None = None,\n    reduce_system_prompt: str | None\
    \ = None,\n    general_knowledge_inclusion_prompt: str | None = None,\n    callbacks:\
    \ list[QueryCallbacks] | None = None,\n) -> GlobalSearch"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a global search engine based on data + configuration.\n\nArgs:\n\
    \    config (GraphRagConfig): GraphRag configuration object containing global\
    \ search settings used to configure the global search engine and models.\n   \
    \ reports (list[CommunityReport]): Community reports to be used by the global\
    \ search context.\n    entities (list[Entity]): Entities to be included in the\
    \ global search context.\n    communities (list[Community]): Communities to be\
    \ included in the global search context.\n    response_type (str): Response type\
    \ to be used by the global search engine.\n    dynamic_community_selection (bool):\
    \ Whether to enable dynamic community selection for the global search.\n    map_system_prompt\
    \ (str | None): Optional system prompt used for mapping in the global search.\n\
    \    reduce_system_prompt (str | None): Optional system prompt used to reduce\
    \ content for the global search.\n    general_knowledge_inclusion_prompt (str\
    \ | None): Optional prompt to include general knowledge in the global search.\n\
    \    callbacks (list[QueryCallbacks] | None): Optional callbacks to handle query\
    \ events during global search.\n\nReturns:\n    GlobalSearch: A GlobalSearch instance\
    \ configured with the provided data and settings."
  code_example: null
  example_source: null
  line_start: 111
  line_end: 192
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  - graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext
  - graphrag/query/structured_search/global_search/search.py::GlobalSearch
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/api/query.py::global_search_streaming
- node_id: graphrag/query/factory.py::get_basic_search_engine
  file: graphrag/query/factory.py
  name: get_basic_search_engine
  signature: "def get_basic_search_engine(\n    text_units: list[TextUnit],\n    text_unit_embeddings:\
    \ BaseVectorStore,\n    config: GraphRagConfig,\n    system_prompt: str | None\
    \ = None,\n    response_type: str = \"multiple paragraphs\",\n    callbacks: list[QueryCallbacks]\
    \ | None = None,\n) -> BasicSearch"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create a basic search engine based on data + configuration.\n\nArgs:\n\
    \    text_units (list[TextUnit]): Text units to be included in the search context.\n\
    \    text_unit_embeddings (BaseVectorStore): Vector store for text unit embeddings.\n\
    \    config (GraphRagConfig): GraphRag configuration containing basic_search settings.\n\
    \    system_prompt (str | None): Optional system prompt to override the default\
    \ prompt.\n    response_type (str): Type of response to generate. Default: \"\
    multiple paragraphs\".\n    callbacks (list[QueryCallbacks] | None): Optional\
    \ callbacks for query handling.\n\nReturns:\n    BasicSearch: A configured BasicSearch\
    \ instance."
  code_example: null
  example_source: null
  line_start: 250
  line_end: 303
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/language_model/providers/fnllm/utils.py::get_openai_model_parameters_from_config
  - graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext
  - graphrag/query/structured_search/basic_search/search.py::BasicSearch
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/api/query.py::basic_search_streaming
- node_id: graphrag/index/workflows/create_base_text_units.py::chunker
  file: graphrag/index/workflows/create_base_text_units.py
  name: chunker
  signature: 'def chunker(row: pd.Series) -> Any'
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Chunk a row into text chunks, optionally prepending metadata to each\
    \ chunk.\n\nArgs:\n    row (pd.Series): The input row containing the data to be\
    \ chunked. It is expected to have the 'texts' column, and may include 'metadata'.\
    \ This function also relies on outer-scope configuration such as prepend_metadata,\
    \ size, overlap, encoding_model, strategy, and callbacks.\n\nReturns:\n    pd.Series:\
    \ The input row augmented with a 'chunks' field containing the list of text chunks\
    \ (with metadata prepended if configured).\n\nRaises:\n    ValueError: Metadata\
    \ tokens exceed the maximum tokens per chunk. Please increase the tokens per chunk."
  code_example: null
  example_source: null
  line_start: 86
  line_end: 128
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  - graphrag/index/operations/chunk_text/strategies.py::get_encoding_fn
  called_by:
  - graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
- node_id: tests/unit/indexing/operations/chunk_text/test_chunk_text.py::test_chunk_text
  file: tests/unit/indexing/operations/chunk_text/test_chunk_text.py
  name: test_chunk_text
  signature: def test_chunk_text(mock_progress_ticker, mock_run_strategy, mock_load_strategy)
  decorators:
  - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.load_strategy")'
  - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.run_strategy")'
  - '@mock.patch("graphrag.index.operations.chunk_text.chunk_text.progress_ticker")'
  raises: []
  visibility: public
  docstring: "Chunk a piece of text into smaller pieces.\n\nThis function chunks the\
    \ text contained in the specified DataFrame column into smaller pieces according\
    \ to the given chunking strategy and encoding model. It loads the configured chunking\
    \ strategy, processes the input texts, and reports progress via the provided callbacks.\
    \ The function returns a pandas Series containing the resulting chunks.\n\nArgs:\n\
    \    input (pd.DataFrame): DataFrame containing the data to chunk.\n    column\
    \ (str): The name of the column containing the text to chunk. This can be a column\
    \ with plain text, or a column with a list/tuple of (doc_id, text).\n    size\
    \ (int): The chunk size in tokens.\n    overlap (int): The number of tokens to\
    \ overlap between adjacent chunks.\n    encoding_model (str): The encoding model\
    \ to use for chunking.\n    strategy (ChunkStrategyType): The strategy to use\
    \ for chunking (e.g., sentence, word). See graphrag.config.enums.ChunkStrategyType\
    \ for supported values.\n    callbacks (WorkflowCallbacks): Object exposing progress\
    \ reporting callbacks (e.g., a progress attribute or method).\n\nReturns:\n  \
    \  pd.Series: A Series containing the generated chunks. The exact shape depends\
    \ on the input and strategy.\n\nRaises:\n    ValueError: If an unknown or unsupported\
    \ strategy is provided.\n\nExamples:\n    chunk_text(df, \"text\", size=10, overlap=2,\
    \ encoding_model=\"model\", strategy=ChunkStrategyType.sentence, callbacks=my_callbacks)"
  code_example: null
  example_source: null
  line_start: 164
  line_end: 181
  dependencies:
  - graphrag/index/operations/chunk_text/chunk_text.py::chunk_text
  called_by: []
- node_id: tests/unit/config/test_config.py::test_default_config
  file: tests/unit/config/test_config.py
  name: test_default_config
  signature: def test_default_config() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Test that the default Graphrag configuration is created as expected.\n\
    \nReturns:\n    None\n\nRaises:\n    AssertionError: If the actual Graphrag configuration\
    \ does not match the expected configuration.\n    ValidationError: If the input\
    \ configuration dictionary cannot be validated by pydantic when creating the config.\n\
    \"\"\""
  code_example: null
  example_source: null
  line_start: 136
  line_end: 139
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - tests/unit/config/utils.py::assert_graphrag_configs
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
    \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n   \
    \     conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
    \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
    \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
    \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
    \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
    \ str = \"number of relationships\",\n        include_relationship_weight: bool\
    \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n        return_candidate_context:\
    \ bool = False,\n        use_community_summary: bool = False,\n        min_community_rank:\
    \ int = 0,\n        community_context_name: str = \"Reports\",\n        column_delimiter:\
    \ str = \"|\",\n        **kwargs: dict[str, Any],\n    ) -> ContextBuilderResult"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Build data context for local search prompt.\n\nBuild a context by combining\
    \ community reports and entity/relationship/covariate tables, and text units using\
    \ a predefined ratio set by text_unit_prop and community_prop.\n\nArgs:\n  query\
    \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
    \ | None): Optional conversation history to consider while constructing the context.\n\
    \  include_entity_names (list[str] | None): Entity names to explicitly include\
    \ in the mapping.\n  exclude_entity_names (list[str] | None): Entity names to\
    \ exclude from consideration.\n  conversation_history_max_turns (int | None):\
    \ Maximum number of user turns from conversation history to include.\n  conversation_history_user_turns_only\
    \ (bool): If True, only user turns from the conversation history are used.\n \
    \ max_context_tokens (int): Maximum token budget for the constructed context.\n\
    \  text_unit_prop (float): Proportion of the context allocated to text units.\n\
    \  community_prop (float): Proportion of the context allocated to community context.\n\
    \  top_k_mapped_entities (int): Number of top entities to map from the query.\n\
    \  top_k_relationships (int): Number of top relationships to include for local\
    \ context.\n  include_community_rank (bool): Whether to include community ranking\
    \ information in the community context.\n  include_entity_rank (bool): Whether\
    \ to include entity ranking information in the local context.\n  rank_description\
    \ (str): Description used when presenting rankings.\n  include_relationship_weight\
    \ (bool): Whether to include relationship weights in the local context.\n  relationship_ranking_attribute\
    \ (str): Attribute name used for ranking relationships.\n  return_candidate_context\
    \ (bool): If True, return candidate context alongside the main context data.\n\
    \  use_community_summary (bool): If True, use a summarized representation of community\
    \ context when available.\n  min_community_rank (int): Minimum rank threshold\
    \ for including a community.\n  community_context_name (str): Label/name for the\
    \ community context section.\n  column_delimiter (str): Delimiter used to separate\
    \ fields in generated context.\n  **kwargs (dict[str, Any]): Additional keyword\
    \ arguments that may influence how the context is built.\n\nReturns:\n  ContextBuilderResult:\
    \ The result containing the built context and associated data.\n\nRaises:\n  ValueError:\
    \ If the sum of community_prop and text_unit_prop exceeds 1."
  code_example: null
  example_source: null
  line_start: 91
  line_end: 222
  dependencies:
  - graphrag/query/context_builder/builders.py::ContextBuilderResult
  - graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
  - graphrag/query/structured_search/local_search/mixed_context.py::_build_community_context
  - graphrag/query/structured_search/local_search/mixed_context.py::_build_local_context
  - graphrag/query/structured_search/local_search/mixed_context.py::_build_text_unit_context
  called_by: []
- node_id: tests/unit/query/context_builder/test_entity_extraction.py::test_map_query_to_entities
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: test_map_query_to_entities
  signature: def test_map_query_to_entities()
  decorators: []
  raises: []
  visibility: public
  docstring: "Maps a user query to the corresponding Entity objects by performing\
    \ a semantic similarity search over a vector store of entity descriptions and\
    \ mapping the retrieved documents back to Entity records.\n\nArgs:\n- query: str.\
    \ The query string to search for relevant entities. If empty, the function returns\
    \ the top-k entities ordered by rank (see oversample_scaler for behavior when\
    \ non-empty).\n- text_embedding_vectorstore: BaseVectorStore. The vector store\
    \ used to perform semantic similarity search over entity descriptions.\n- text_embedder:\
    \ EmbeddingModel. The model used to encode the query into an embedding for similarity\
    \ search.\n- all_entities_dict: dict[str, Entity]. Mapping from the vector-store\
    \ document identifiers (as determined by embedding_vectorstore_key) to the corresponding\
    \ Entity objects.\n- embedding_vectorstore_key: str. Key indicating how documents\
    \ in text_embedding_vectorstore identify an entity (for example, EntityVectorStoreKey.ID\
    \ or EntityVectorStoreKey.TITLE).\n- include_entity_names: list[str] | None. Optional\
    \ list of entity titles/names to explicitly include in the results.\n- exclude_entity_names:\
    \ list[str] | None. Optional list of entity titles/names to exclude from the results.\n\
    - k: int. Maximum number of entities to return. If query is empty, up to k entities\
    \ are returned, ordered by rank with the highest rank first.\n- oversample_scaler:\
    \ int. Multiplier controlling how many candidate documents to fetch beyond k to\
    \ improve robustness of the top-k selection. A value of 1 disables oversampling;\
    \ higher values fetch more candidates. Default in the implementation is 2, but\
    \ tests may override this.\n\nReturns:\n- list[Entity]. The matched Entity objects,\
    \ in the order determined by the search and ranking, up to length k.\n\nRaises:\n\
    - ValueError: If k <= 0 or oversample_scaler < 1.\n- KeyError: If a retrieved\
    \ document\u2019s identifier cannot be found in all_entities_dict.\n- TypeError:\
    \ If argument types are invalid.\n\nNotes:\n- Vector-store to-entity mapping depends\
    \ on embedding_vectorstore_key. When embedding_vectorstore_key is ID, documents\
    \ store the Entity.id; when TITLE, documents store the Entity.title. This mapping\
    \ is how the function resolves a document back to an Entity.\n- Edge cases:\n\
    \  - Empty query returns the top-k entities by rank (highest numeric rank first).\n\
    \  - If no candidates are produced or all candidates are filtered out, an empty\
    \ list is returned.\n\nExample notes:\n- If embedding_vectorstore_key is EntityVectorStoreKey.ID\
    \ and a document has id equal to an Entity.id, map_query_to_entities will return\
    \ the corresponding Entity from all_entities_dict.\n- If embedding_vectorstore_key\
    \ is EntityVectorStoreKey.TITLE and a document has id equal to an Entity.title,\
    \ the function will resolve to that Entity via all_entities_dict."
  code_example: null
  example_source: null
  line_start: 66
  line_end: 190
  dependencies:
  - graphrag/data_model/entity.py::Entity
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/query/context_builder/entity_extraction.py::map_query_to_entities
  - graphrag/vector_stores/base.py::VectorStoreDocument
  called_by: []
- node_id: graphrag/query/context_builder/community_context.py::build_community_context
  file: graphrag/query/context_builder/community_context.py
  name: build_community_context
  signature: "def build_community_context(\n    community_reports: list[CommunityReport],\n\
    \    entities: list[Entity] | None = None,\n    tokenizer: Tokenizer | None =\
    \ None,\n    use_community_summary: bool = True,\n    column_delimiter: str =\
    \ \"|\",\n    shuffle_data: bool = True,\n    include_community_rank: bool = False,\n\
    \    min_community_rank: int = 0,\n    community_rank_name: str = \"rank\",\n\
    \    include_community_weight: bool = True,\n    community_weight_name: str =\
    \ \"occurrence weight\",\n    normalize_community_weight: bool = True,\n    max_context_tokens:\
    \ int = 8000,\n    single_batch: bool = True,\n    context_name: str = \"Reports\"\
    ,\n    random_state: int = 86,\n) -> tuple[str | list[str], dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: public
  docstring: 'Build context data from community reports for use in a system prompt.


    If entities are provided, compute per-community weights from the number of text
    units associated with entities within each community. The computed weight is added
    to each CommunityReport''s attributes and included in the context data table.


    Args:

    - community_reports (list[CommunityReport]): Reports representing each community
    to be included in the context.

    - entities (list[Entity] | None): Optional entities used to derive weights by
    counting text units linked to each community.

    - tokenizer (Tokenizer | None): Tokenizer to use for estimating token counts.
    If None, a default tokenizer is obtained.

    - use_community_summary (bool): If True, include report.summary in the context
    line; otherwise include report.full_content.

    - column_delimiter (str): Delimiter used to join fields when constructing per-report
    context lines.

    - shuffle_data (bool): If True, shuffle the selected reports before batching.

    - include_community_rank (bool): If True, append a rank value to each line and
    include a rank column in the header.

    - min_community_rank (int): Minimum rank for a report to be included.

    - community_rank_name (str): Header name for the rank column when included.

    - include_community_weight (bool): If True, include a weight column (occurrence
    weight) for each report.

    - community_weight_name (str): Attribute name used to store the weight on each
    report.

    - normalize_community_weight (bool): If True, apply normalization to computed
    weights.

    - max_context_tokens (int): Maximum token budget per batch; when exceeded, a new
    batch is started.

    - single_batch (bool): If True, produce a single batch; otherwise accumulate multiple
    batches up to the token limit.

    - context_name (str): Name used as the batch header for the context section.

    - random_state (int): Seed for deterministic shuffling when shuffle_data is True.


    Returns:

    - tuple[str | list[str], dict[str, pd.DataFrame]]: A pair containing the generated
    context text(s) and a mapping from the lower-cased context name to a pandas DataFrame
    with the compiled context records. The first element is either a single string
    or a list of strings representing the context segments produced; the second element
    is a dictionary like {"reports": <DataFrame>} containing the concatenated context
    data.


    Raises:

    - May raise exceptions from the underlying tokenizer, pandas operations, or user-provided
    data models if inputs are invalid or internal processing fails.


    Notes:

    - When no reports pass the inclusion filter, an empty context is returned as ([],
    {}).

    - The function is designed to support batching for large contexts and to prefer
    deterministic outputs when a random_state is provided.'
  code_example: null
  example_source: null
  line_start: 24
  line_end: 186
  dependencies:
  - graphrag/query/context_builder/community_context.py::_compute_community_weights
  - graphrag/query/context_builder/community_context.py::_cut_batch
  - graphrag/query/context_builder/community_context.py::_get_header
  - graphrag/query/context_builder/community_context.py::_init_batch
  - graphrag/query/context_builder/community_context.py::_is_included
  - graphrag/query/context_builder/community_context.py::_report_context_text
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
- node_id: graphrag/index/workflows/load_update_documents.py::run_workflow
  file: graphrag/index/workflows/load_update_documents.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the update-document loading workflow: load update-only input documents,\
    \ write them to storage, and return the result.\n\nArgs:\n  config: GraphRagConfig\
    \ containing input configuration and related settings.\n  context: PipelineRunContext\
    \ providing access to input_storage, output_storage, and runtime statistics.\n\
    \nReturns:\n  WorkflowFunctionOutput: The output containing the loaded update\
    \ documents as a DataFrame, or a stop signal if no update documents were found.\n\
    \nRaises:\n  Exception: Exceptions raised by the input loading or storage backends\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 42
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/load_update_documents.py::load_update_documents
  - graphrag/utils/storage.py::write_table_to_storage
  called_by: []
- node_id: graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
  file: graphrag/index/utils/derive_from_rows.py
  name: _derive_from_rows_base
  signature: "def _derive_from_rows_base(\n    input: pd.DataFrame,\n    transform:\
    \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
    \    gather: GatherFn[ItemType],\n    progress_msg: str = \"\",\n) -> list[ItemType\
    \ | None]"
  decorators: []
  raises:
  - ParallelizationError
  visibility: protected
  docstring: "Derive from rows asynchronously.\n\nThis is useful for IO bound operations.\n\
    \nArgs:\n    input: pd.DataFrame\n        The input data to process, where each\
    \ row will be transformed.\n    transform: Callable[[pd.Series], Awaitable[ItemType]]\n\
    \        Async function applied to a row's Series to produce an ItemType.\n  \
    \  callbacks: WorkflowCallbacks\n        Callbacks used to report progress and\
    \ handle workflow events.\n    gather: GatherFn[ItemType]\n        Function that\
    \ gathers results by applying the given execute function to each row.\n    progress_msg:\
    \ str\n        Optional description to accompany progress updates.\n\nReturns:\n\
    \    list[ItemType | None]\n        A list of results corresponding to each input\
    \ row; elements may be ItemType or None if an error occurred for that row.\n\n\
    Raises:\n    ParallelizationError\n        If any errors were encountered during\
    \ processing."
  code_example: null
  example_source: null
  line_start: 131
  line_end: 173
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::gather
  - graphrag/logger/progress.py::progress_ticker
  called_by:
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
- node_id: graphrag/query/indexer_adapters.py::read_indexer_text_units
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_text_units
  signature: 'def read_indexer_text_units(final_text_units: pd.DataFrame) -> list[TextUnit]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Text Units from the raw indexing outputs.\n\nArgs:\n  final_text_units\
    \ (pd.DataFrame): The DataFrame containing the final text units produced by indexing\
    \ outputs.\n\nReturns:\n  list[TextUnit]: A list of TextUnit objects parsed from\
    \ the input DataFrame."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 42
  dependencies:
  - graphrag/query/input/loaders/dfs.py::read_text_units
  called_by:
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/api/query.py::basic_search_streaming
- node_id: graphrag/query/indexer_adapters.py::read_indexer_entities
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_entities
  signature: "def read_indexer_entities(\n    entities: pd.DataFrame,\n    communities:\
    \ pd.DataFrame,\n    community_level: int | None,\n) -> list[Entity]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Entities from the raw indexing outputs.\n\nThis function\
    \ joins the entity records with their associated communities, optionally filters\
    \ by a\nspecified community_level, aggregates community memberships per entity,\
    \ and converts the resulting\nrecords into Entity objects using read_entities.\n\
    \nArgs:\n    entities (pd.DataFrame): DataFrame containing entity information\
    \ with at least an \"id\" column.\n    communities (pd.DataFrame): DataFrame containing\
    \ community information including \"entity_ids\",\n        \"community\", and\
    \ \"level\".\n    community_level (int | None): If not None, keep entities with\
    \ associated communities at or below this level.\n\nReturns:\n    list[Entity]:\
    \ The list of Entity objects constructed from the input data.\n\nRaises:\n   \
    \ AttributeError: If community_level is not None and the intermediate DataFrame\
    \ does not contain a\n        \"level\" column (as expected by _filter_under_community_level)."
  code_example: null
  example_source: null
  line_start: 139
  line_end: 178
  dependencies:
  - graphrag/query/indexer_adapters.py::_filter_under_community_level
  - graphrag/query/input/loaders/dfs.py::read_entities
  called_by:
  - graphrag/api/query.py::global_search_streaming
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
- node_id: graphrag/query/indexer_adapters.py::read_indexer_relationships
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_relationships
  signature: 'def read_indexer_relationships(final_relationships: pd.DataFrame) ->
    list[Relationship]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Relationships from the raw indexing outputs.\n\nThis function\
    \ delegates to read_relationships with the following mappings: df = final_relationships,\
    \ short_id_col = \"human_readable_id\", rank_col = \"combined_degree\", description_embedding_col\
    \ = None, attributes_cols = None.\n\nArgs:\n    final_relationships (pd.DataFrame):\
    \ The DataFrame containing raw indexing outputs for relationships.\n\nReturns:\n\
    \    list[Relationship]: A list of Relationship objects constructed from the input\
    \ data."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 71
  dependencies:
  - graphrag/query/input/loaders/dfs.py::read_relationships
  called_by:
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
- node_id: graphrag/query/indexer_adapters.py::read_indexer_covariates
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_covariates
  signature: 'def read_indexer_covariates(final_covariates: pd.DataFrame) -> list[Covariate]'
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Covariates from the raw indexing outputs.\n\nThis function\
    \ converts the id column to a string and delegates to read_covariates to construct\
    \ Covariate objects using the input DataFrame with id cast to string, short_id_col\
    \ set to human_readable_id, attributes_cols set to object_id, status, start_date,\
    \ end_date, and description, and text_unit_ids_col set to None.\n\nArgs:\n  final_covariates\
    \ (pd.DataFrame): DataFrame containing covariate records produced by the indexing\
    \ process. Must include an id column (which will be cast to string) and a human_readable_id\
    \ column used as the short identifier. The covariate attributes to export are\
    \ object_id, status, start_date, end_date, and description.\n\nReturns:\n  list[Covariate]:\
    \ A list of Covariate objects parsed from the covariates DataFrame.\n\nRaises:\n\
    \  KeyError: If required columns are missing from final_covariates.\n  ValueError:\
    \ If data in columns cannot be coerced to the expected types.\n  Exception: Propagates\
    \ exceptions raised by read_covariates."
  code_example: null
  example_source: null
  line_start: 45
  line_end: 60
  dependencies:
  - graphrag/query/input/loaders/dfs.py::read_covariates
  called_by:
  - graphrag/api/query.py::local_search_streaming
- node_id: graphrag/query/indexer_adapters.py::read_indexer_reports
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_reports
  signature: "def read_indexer_reports(\n    final_community_reports: pd.DataFrame,\n\
    \    final_communities: pd.DataFrame,\n    community_level: int | None,\n    dynamic_community_selection:\
    \ bool = False,\n    content_embedding_col: str = \"full_content_embedding\",\n\
    \    config: GraphRagConfig | None = None,\n) -> list[CommunityReport]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Community Reports from the raw indexing outputs.\n\nIf not\
    \ dynamic_community_selection, then select reports with the max community level\
    \ that an entity belongs to.\n\nArgs:\n    final_community_reports: pd.DataFrame\n\
    \        The DataFrame containing raw community reports produced by the indexer.\n\
    \    final_communities: pd.DataFrame\n        The DataFrame containing final communities;\
    \ entities are exploded from the entity_ids column.\n    community_level: int\
    \ | None\n        Threshold on community level to filter results. If None, no\
    \ filtering is applied.\n    dynamic_community_selection: bool\n        If False,\
    \ perform a community level roll up to the maximum level per title.\n    content_embedding_col:\
    \ str\n        Column name for content embeddings. Defaults to \"full_content_embedding\"\
    .\n    config: GraphRagConfig | None\n        Optional configuration object for\
    \ embedding model lookup and generation.\n\nReturns:\n    list[CommunityReport]\n\
    \        A list of CommunityReport objects read from the processed reports dataframe.\n\
    \nRaises:\n    AttributeError: if the input DataFrame does not have a level column...."
  code_example: null
  example_source: null
  line_start: 74
  line_end: 127
  dependencies:
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/query/indexer_adapters.py::_filter_under_community_level
  - graphrag/query/indexer_adapters.py::embed_community_reports
  - graphrag/query/input/loaders/dfs.py::read_community_reports
  called_by:
  - graphrag/api/query.py::global_search_streaming
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
- node_id: graphrag/query/indexer_adapters.py::read_indexer_communities
  file: graphrag/query/indexer_adapters.py
  name: read_indexer_communities
  signature: "def read_indexer_communities(\n    final_communities: pd.DataFrame,\n\
    \    final_community_reports: pd.DataFrame,\n) -> list[Community]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read in the Communities from the raw indexing outputs and reconstruct\
    \ the community hierarchy information.\n\nThe function explodes the entity_ids\
    \ per community to propagate entity membership, validates that the communities\
    \ present in final_communities are covered by final_community_reports, logs a\
    \ warning and filters if any are missing, and finally builds Community objects\
    \ via read_communities with a mapping of columns.\n\nArgs:\n  final_communities\
    \ (pd.DataFrame): The final communities DataFrame produced by the indexer. Expected\
    \ to contain at minimum id, community, title, level, and entity_ids (a list of\
    \ related entity IDs) for proper expansion.\n  final_community_reports (pd.DataFrame):\
    \ The DataFrame containing community reports used to validate and align the community\
    \ structure. Must include a community column used for matching.\n\nReturns:\n\
    \  list[Community]: A list of reconstructed Community objects, including hierarchy\
    \ information and sub-community relations, as produced by read_communities. The\
    \ input communities are filtered to align with reported communities when necessary.\n\
    \nRaises:\n  ValueError: If required columns are missing from inputs (e.g., id,\
    \ title, level, community, or entity_ids) or if downstream read_communities raises\
    \ due to malformed data.\n  Exceptions propagated from read_communities (e.g.,\
    \ KeyError, TypeError) as a result of input validation or data shape issues.\n\
    \nNotes:\n  - If there are communities in final_communities that do not appear\
    \ in final_community_reports,\n    a warning is logged and those communities (and\
    \ their exploded entity mappings) are dropped to\n    yield results consistent\
    \ with the available reports.\n  - Internal steps include exploding entity_ids\
    \ to map entities to communities, filtering by\n    reported communities, and\
    \ delegating to read_communities with specific column mappings:\n      id_col=\"\
    id\", short_id_col=\"community\", title_col=\"title\", level_col=\"level\",\n\
    \      parent_col=\"parent\", children_col=\"children\"."
  code_example: null
  example_source: null
  line_start: 181
  line_end: 216
  dependencies:
  - graphrag/query/input/loaders/dfs.py::read_communities
  called_by:
  - graphrag/api/query.py::global_search_streaming
- node_id: tests/verbs/test_prune_graph.py::test_prune_graph
  file: tests/verbs/test_prune_graph.py
  name: test_prune_graph
  signature: def test_prune_graph()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that pruning the graph results in 20 entities.\n\nReturns:\n  \
    \  None\n        No return value.\n\nRaises:\n    Exception\n        Exceptions\
    \ may propagate from the underlying operations used in the test (e.g., test utilities,\
    \ storage I/O, or workflow execution)."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 31
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/config/models/prune_graph_config.py::PruneGraphConfig
  - graphrag/index/workflows/prune_graph.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::create_test_context
  called_by: []
- node_id: graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
  file: graphrag/index/operations/cluster_graph.py
  name: _compute_leiden_communities
  signature: "def _compute_leiden_communities(\n    graph: nx.Graph | nx.DiGraph,\n\
    \    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int | None = None,\n\
    ) -> tuple[dict[int, dict[str, int]], dict[int, int]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Compute Leiden root communities for a graph and return level wise node\
    \ to community mappings and a cluster hierarchy.\n\nArgs:\n    graph: nx.Graph\
    \ | nx.DiGraph\n        Input graph from which to compute Leiden communities.\
    \ If use_lcc is True, the graph's largest connected component is used.\n    max_cluster_size:\
    \ int\n        Maximum size of a cluster allowed by Leiden algorithm.\n    use_lcc:\
    \ bool\n        If True, compute on the largest connected component of the input\
    \ graph.\n    seed: int | None\n        Random seed for reproducibility in the\
    \ Leiden clustering.\n\nReturns:\n    tuple[dict[int, dict[str, int]], dict[int,\
    \ int]]\n        A tuple containing:\n        - node_id_to_community_map: dict[int,\
    \ dict[str, int]]\n            Mapping from level to a dictionary that maps node_id\
    \ to its community id.\n        - parent_mapping: dict[int, int]\n           \
    \ Mapping from cluster_id to its parent cluster id, or -1 if there is no parent.\n\
    \nRaises:\n    Propagates exceptions from the underlying libraries or graph processing\
    \ steps; there are no explicit raises documented for this function."
  code_example: null
  example_source: null
  line_start: 57
  line_end: 80
  dependencies:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  called_by:
  - graphrag/index/operations/cluster_graph.py::cluster_graph
- node_id: graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
  file: graphrag/index/operations/embed_graph/embed_graph.py
  name: embed_graph
  signature: "def embed_graph(\n    graph: nx.Graph,\n    config: EmbedGraphConfig,\n\
    ) -> NodeEmbeddings"
  decorators: []
  raises: []
  visibility: public
  docstring: "Embed a graph into a vector space using node2vec.\n\nThe graph is expected\
    \ to be in nx.Graph format. The operation outputs a mapping between node name\
    \ and vector.\n\nArgs:\n    graph: Input graph to embed.\n    config: Embedding\
    \ configuration, including dimensions, num_walks, walk_length, window_size, iterations,\
    \ random_seed, and use_lcc.\n\nReturns:\n    NodeEmbeddings: Mapping from node\
    \ name to embedding vector.\n\nRaises:\n    Exception: If underlying operations\
    \ fail."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 50
  dependencies:
  - graphrag/index/operations/embed_graph/embed_node2vec.py::embed_node2vec
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  called_by:
  - graphrag/index/operations/finalize_entities.py::finalize_entities
- node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: test_undirected_graph_run_twice_produces_same_graph
  signature: def test_undirected_graph_run_twice_produces_same_graph(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Verify that running stable_largest_connected_component on an undirected\
    \ graph twice yields the same graph, even if the input edges are flipped.\n\n\
    Args:\n    self (TestStableLCC): Instance of the test class.\n\nReturns:\n   \
    \ None: This test does not return a value.\n\nRaises:\n    AssertionError: If\
    \ the graphs produced by stable_largest_connected_component differ."
  code_example: null
  example_source: null
  line_start: 11
  line_end: 21
  dependencies:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
  called_by: []
- node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: test_directed_graph_keeps_source_target_intact
  signature: def test_directed_graph_keeps_source_target_intact(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that a directed graph keeps source and target intact when computing\
    \ the stable largest connected component.\n\nArgs:\n    self (TestStableLCC):\
    \ Instance of the test class.\n\nReturns:\n    None: This test does not return\
    \ a value.\n\nRaises:\n    AssertionError: If the directed edges in the input\
    \ and output graphs differ, indicating the edge directions were not preserved."
  code_example: null
  example_source: null
  line_start: 23
  line_end: 34
  dependencies:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
  called_by: []
- node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: test_directed_graph_run_twice_produces_same_graph
  signature: def test_directed_graph_run_twice_produces_same_graph(self)
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that running stable_largest_connected_component on a directed graph\
    \ twice yields the same graph.\n\nArgs:\n    self (TestStableLCC): Instance of\
    \ the test class.\n\nReturns:\n    None: This test does not return a value.\n\n\
    Raises:\n    AssertionError: If the graphs produced by stable_largest_connected_component\
    \ differ between runs."
  code_example: null
  example_source: null
  line_start: 36
  line_end: 47
  dependencies:
  - graphrag/index/utils/stable_lcc.py::stable_largest_connected_component
  - tests/unit/indexing/graph/utils/test_stable_lcc.py::_create_strongly_connected_graph_with_edges_flipped
  called_by: []
- node_id: graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
  file: graphrag/index/workflows/extract_graph.py
  name: get_summarized_entities_relationships
  signature: "def get_summarized_entities_relationships(\n    extracted_entities:\
    \ pd.DataFrame,\n    extracted_relationships: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    summarization_strategy: dict[str, Any] | None\
    \ = None,\n    summarization_num_threads: int = 4,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Summarize the entities and relationships using the provided summarization\
    \ strategy.\n\nArgs:\n    extracted_entities: DataFrame containing extracted entity\
    \ nodes to be summarized.\n    extracted_relationships: DataFrame containing extracted\
    \ relationships to be summarized.\n    callbacks: WorkflowCallbacks providing\
    \ progress reporting hooks for long-running operations.\n    cache: PipelineCache\
    \ used to cache results from the summarization strategy.\n    summarization_strategy:\
    \ dictionary configuring the summarization approach; may be None to use defaults.\n\
    \    summarization_num_threads: number of threads to use for summarization.\n\n\
    Returns:\n    tuple[pd.DataFrame, pd.DataFrame]: A tuple containing:\n       \
    \ - entities: DataFrame with entity summaries merged on \"title\" (after dropping\
    \ the original \"description\" column).\n        - relationships: DataFrame with\
    \ relationship summaries merged on [\"source\", \"target\"] (after dropping the\
    \ original \"description\" column).\n\nRaises:\n    Exception: If summarization\
    \ or subsequent DataFrame operations fail, propagating exceptions from summarize_descriptions\
    \ or pandas.\"}"
  code_example: null
  example_source: null
  line_start: 135
  line_end: 159
  dependencies:
  - graphrag/index/operations/summarize_descriptions/summarize_descriptions.py::summarize_descriptions
  called_by:
  - graphrag/index/workflows/extract_graph.py::extract_graph
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
- node_id: graphrag/api/index.py::build_index
  file: graphrag/api/index.py
  name: build_index
  signature: "def build_index(\n    config: GraphRagConfig,\n    method: IndexingMethod\
    \ | str = IndexingMethod.Standard,\n    is_update_run: bool = False,\n    memory_profile:\
    \ bool = False,\n    callbacks: list[WorkflowCallbacks] | None = None,\n    additional_context:\
    \ dict[str, Any] | None = None,\n    verbose: bool = False,\n    input_documents:\
    \ pd.DataFrame | None = None,\n) -> list[PipelineRunResult]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the indexing pipeline with the given configuration.\n\nThe asynchronous\
    \ function orchestrates the indexing workflow: it initializes logging, prepares\
    \ a callback chain, determines the final indexing method (including update semantics),\
    \ builds the pipeline, executes the run, collects outputs, and returns them as\
    \ a list of PipelineRunResult.\n\nParameters\n----------\nconfig : GraphRagConfig\n\
    \    The configuration for GraphRAG indexing.\nmethod : IndexingMethod | str default=IndexingMethod.Standard\n\
    \    Styling of indexing to perform (full LLM, NLP + LLM, etc.). If a string is\
    \ provided, it will be used directly.\nis_update_run : bool\n    Indicates whether\
    \ this is an incremental update run. When True, the final method name will be\
    \ suffixed with \"-update\".\nmemory_profile : bool\n    Whether to enable memory\
    \ profiling. Note: Memory profiling is not yet supported and a warning is logged.\n\
    callbacks : list[WorkflowCallbacks] | None default=None\n    A list of callbacks\
    \ to register for the pipeline lifecycle. If None, a NoopWorkflowCallbacks is\
    \ used.\nadditional_context : dict[str, Any] | None default=None\n    Additional\
    \ context to pass to the pipeline run. This can be accessed in the pipeline state\
    \ under the 'additional_context' key.\nverbose : bool\n    Enables verbose logging\
    \ during initialization. Affects the logging configuration.\ninput_documents :\
    \ pd.DataFrame | None default=None\n    Override document loading and parsing\
    \ and supply your own dataframe of documents to index.\n\nReturns\n-------\nlist[PipelineRunResult]\n\
    \    The list of pipeline run results\n\nRaises\n------\nException\n    Exceptions\
    \ raised by underlying components (e.g., logging initialization, pipeline execution)\
    \ may propagate to the caller.\n\nNotes\n-----\n- This function is asynchronous\
    \ and must be awaited.\n- The final method name is determined by _get_method(method,\
    \ is_update_run)."
  code_example: null
  example_source: null
  line_start: 29
  line_end: 96
  dependencies:
  - graphrag/api/index.py::_get_method
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/index/run/run_pipeline.py::run_pipeline
  - graphrag/index/run/utils.py::create_callback_chain
  - graphrag/logger/standard_logging.py::init_loggers
  called_by: []
- node_id: graphrag/query/context_builder/local_context.py::build_relationship_context
  file: graphrag/query/context_builder/local_context.py
  name: build_relationship_context
  signature: "def build_relationship_context(\n    selected_entities: list[Entity],\n\
    \    relationships: list[Relationship],\n    tokenizer: Tokenizer | None = None,\n\
    \    include_relationship_weight: bool = False,\n    max_context_tokens: int =\
    \ 8000,\n    top_k_relationships: int = 10,\n    relationship_ranking_attribute:\
    \ str = \"rank\",\n    column_delimiter: str = \"|\",\n    context_name: str =\
    \ \"Relationships\",\n) -> tuple[str, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare relationship context data tables as context data for system\
    \ prompt.\n\nArgs:\n  selected_entities (list[Entity]): The selected entities\
    \ for which to build the relationship context.\n  relationships (list[Relationship]):\
    \ The pool of relationships to filter to generate context from.\n  tokenizer (Tokenizer\
    \ | None): Tokenizer to count tokens; if None, get_tokenizer() is used.\n  include_relationship_weight\
    \ (bool): Whether to include the relationship weight in the generated context.\n\
    \  max_context_tokens (int): Maximum allowed tokens for the generated context\
    \ text. Parsing stops when adding a new relationship would exceed this limit.\n\
    \  top_k_relationships (int): Number of top relationships to consider when building\
    \ the context.\n  relationship_ranking_attribute (str): Attribute name used for\
    \ ranking relationships.\n  column_delimiter (str): Delimiter used to separate\
    \ columns in the generated text.\n  context_name (str): Name used in the header\
    \ block of the context.\n\nReturns:\n  tuple[str, pd.DataFrame]: The generated\
    \ context text and a DataFrame containing the records (excluding the header).\
    \ If no relevant entities or relationships exist, returns an empty string and\
    \ an empty DataFrame."
  code_example: null
  example_source: null
  line_start: 158
  line_end: 229
  dependencies:
  - graphrag/query/context_builder/local_context.py::_filter_relationships
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _build_mixed_context
  signature: "def _build_mixed_context(\n    df: pd.DataFrame, tokenizer: Tokenizer,\
    \ max_context_tokens: int\n) -> pd.Series"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Build mixed context for each row by applying build_mixed_context to\
    \ the ALL_CONTEXT data and trimming to the token limit.\n\nArgs:\n    df: DataFrame\
    \ containing ALL_CONTEXT column to be processed.\n    tokenizer: Tokenizer used\
    \ to count tokens during trimming.\n    max_context_tokens: int Maximum number\
    \ of tokens allowed for the resulting context.\n\nReturns:\n    pd.Series: A Series\
    \ of processed mixed contexts per row.\n\nRaises:\n    Exception: Any exception\
    \ raised by the transformation function will be propagated to the caller."
  code_example: null
  example_source: null
  line_start: 292
  line_end: 302
  dependencies:
  - graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
  - graphrag/index/utils/dataframes.py::transform_series
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
- node_id: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_level_context
  file: graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py
  name: build_level_context
  signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n    community_hierarchy_df:\
    \ pd.DataFrame,\n    local_context_df: pd.DataFrame,\n    level: int,\n    tokenizer:\
    \ Tokenizer,\n    max_context_tokens: int = 16000,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prep context for each community in a given level.\n\nFor each community:\n\
    - Check if local context fits within the limit, if yes use local context\n- If\
    \ local context exceeds the limit, iteratively replace local context with sub-community\
    \ reports, starting from the biggest sub-community\n\nArgs:\n    report_df: pd.DataFrame\
    \ | None\n        DataFrame with reports for communities. May be None if no reports\
    \ are available.\n    community_hierarchy_df: pd.DataFrame\n        DataFrame\
    \ describing the community hierarchy.\n    local_context_df: pd.DataFrame\n  \
    \      DataFrame containing the local context per community.\n    level: int\n\
    \        The level in the community hierarchy to prepare context for.\n    tokenizer:\
    \ Tokenizer\n        Tokenizer used to count tokens and enforce the max_context_tokens\
    \ limit.\n    max_context_tokens: int\n        Maximum number of tokens allowed\
    \ for the resulting context. Defaults to 16000.\n\nReturns:\n    pd.DataFrame\n\
    \        DataFrame containing prepared context for communities at the specified\
    \ level, with context strings, sizes and exceed flags as computed."
  code_example: null
  example_source: null
  line_start: 85
  line_end: 235
  dependencies:
  - graphrag/index/operations/summarize_communities/build_mixed_context.py::build_mixed_context
  - graphrag/index/operations/summarize_communities/text_unit_context/sort_context.py::sort_context
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _prepare_reports_at_level
  signature: "def _prepare_reports_at_level(\n    node_df: pd.DataFrame,\n    edge_df:\
    \ pd.DataFrame,\n    claim_df: pd.DataFrame | None,\n    tokenizer: Tokenizer,\n\
    \    level: int,\n    max_context_tokens: int = 16_000,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Prepare reports at a given level.\n\nArgs:\n    node_df: pd.DataFrame\n\
    \        DataFrame containing node details; filtered to the specified level using\
    \ schemas.COMMUNITY_LEVEL.\n    edge_df: pd.DataFrame\n        DataFrame containing\
    \ edge details between nodes.\n    claim_df: pd.DataFrame | None\n        Optional\
    \ DataFrame containing claims related to nodes; may be None.\n    tokenizer: Tokenizer\n\
    \        Tokenizer used to compute context token counts for context generation.\n\
    \    level: int\n        Target community level for which to prepare reports.\n\
    \    max_context_tokens: int\n        Maximum number of tokens allowed for the\
    \ generated context strings (default 16_000).\n\nReturns:\n    pd.DataFrame\n\
    \        DataFrame containing prepared reports at the given level, with node and\
    \ edge details\n        aggregated, optional claim details merged, ALL_CONTEXT\
    \ populated, and context strings\n        generated for each community by parallel_sort_context_batch.\n\
    \nRaises:\n    Exception\n        If an error occurs during processing (for example,\
    \ due to missing expected columns or invalid data)."
  code_example: null
  example_source: null
  line_start: 63
  line_end: 188
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/sort_context.py::parallel_sort_context_batch
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
- node_id: graphrag/cli/query.py::run_global_search
  file: graphrag/cli/query.py
  name: run_global_search
  signature: "def run_global_search(\n    config_filepath: Path | None,\n    data_dir:\
    \ Path | None,\n    root_dir: Path,\n    community_level: int | None,\n    dynamic_community_selection:\
    \ bool,\n    response_type: str,\n    streaming: bool,\n    query: str,\n    verbose:\
    \ bool,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a global search with a given query.\n\nLoads index files required\
    \ for global search and calls the Query API.\n\nArgs:\n    config_filepath: Path\
    \ to a config file, or None.\n    data_dir: Optional data directory to override\
    \ output.base_dir, or None.\n    root_dir: Root directory of the project.\n  \
    \  community_level: Optional integer indicating the target community level.\n\
    \    dynamic_community_selection: Whether to dynamically select communities.\n\
    \    response_type: Type of response to return.\n    streaming: True to use streaming\
    \ mode; False for a standard response.\n    query: The search query.\n    verbose:\
    \ If True, enable verbose output.\n\nReturns:\n    tuple[str, dict[str, Any]]:\
    \ The response as a string and a context data dictionary.\n\nRaises:\n    FileNotFoundError:\
    \ If the config file cannot be found.\n    ValueError: If the loaded configuration\
    \ is invalid.\n    Exception: Exceptions raised by the underlying API calls or\
    \ asyncio operations may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 133
  dependencies:
  - graphrag.api::global_search
  - graphrag.api::multi_index_global_search
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/cli/query.py::run_streaming_search
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_query_cli
- node_id: graphrag/cli/query.py::run_local_search
  file: graphrag/cli/query.py
  name: run_local_search
  signature: "def run_local_search(\n    config_filepath: Path | None,\n    data_dir:\
    \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
    \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a local search with a given query.\n\nLoads index files required\
    \ for local search and calls the Query API.\n\nArgs:\n    config_filepath: Path\
    \ | None\n        Path to the configuration file to use, or None to use the default\
    \ location.\n    data_dir: Path | None\n        Optional directory containing\
    \ precomputed data to override the base output directory.\n    root_dir: Path\n\
    \        Root directory of the project.\n    community_level: int\n        Target\
    \ community level for the search.\n    response_type: str\n        The type of\
    \ response to return.\n    streaming: bool\n        If True, perform a streaming\
    \ search; otherwise perform a non-streaming search.\n    query: str\n        The\
    \ search query string used to perform the local search.\n    verbose: bool\n \
    \       If True, enable verbose logging output.\n\nReturns:\n    tuple[str, dict[str,\
    \ Any]]\n        The textual response and the context data dictionary produced\
    \ by the search operation.\n\nRaises:\n    FileNotFoundError\n        If the configuration\
    \ file cannot be found.\n    ValueError\n        If the configuration is invalid\
    \ or required components are missing.\n    Exception\n        Exceptions raised\
    \ by the underlying API calls or asyncio operations may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 136
  line_end: 265
  dependencies:
  - graphrag.api::local_search
  - graphrag.api::multi_index_local_search
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/cli/query.py::run_streaming_search
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_query_cli
- node_id: graphrag/cli/query.py::run_drift_search
  file: graphrag/cli/query.py
  name: run_drift_search
  signature: "def run_drift_search(\n    config_filepath: Path | None,\n    data_dir:\
    \ Path | None,\n    root_dir: Path,\n    community_level: int,\n    response_type:\
    \ str,\n    streaming: bool,\n    query: str,\n    verbose: bool,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a local drift search for a given query across either a multi-index\
    \ or single-index dataset, with optional streaming of results.\n\nLoads index\
    \ files required for local search and calls the appropriate Query API depending\
    \ on the loaded index layout.\n\nArgs:\n    config_filepath (Path | None): Path\
    \ to the configuration file to use, or None to use the default configuration.\n\
    \    data_dir (Path | None): Optional directory containing precomputed data to\
    \ override the base output directory.\n    root_dir (Path): Root directory of\
    \ the project.\n    community_level (int): Target community level for the drift\
    \ search.\n    response_type (str): The type of response to request from the API.\n\
    \    streaming (bool): If True, stream results as they are produced. Streaming\
    \ prints each received chunk to stdout (without buffering) and may update the\
    \ context data during streaming.\n    query (str): The drift search query string\
    \ used to perform the local search.\n    verbose (bool): If True, print verbose\
    \ progress and diagnostic information.\n\nReturns:\n    tuple[str, dict[str, Any]]:\
    \ A pair consisting of the final response as a string and a dictionary containing\
    \ context data captured during the operation.\n\nRaises:\n    FileNotFoundError:\
    \ If the configuration file cannot be found.\n    ValueError: If the configuration\
    \ is invalid or required configuration data are missing.\n    Exceptions propagated\
    \ from the underlying API calls or asyncio operations may occur (e.g., during\
    \ streaming or remote calls).\n\nNotes on behavior:\n    - Internal flow depends\
    \ on the index layout loaded by _resolve_output_files:\n      \u2022 If dataframe_dict[\"\
    multi-index\"] is True, this function uses the Multi-Index Drift Search API\n\
    \        and returns the API response along with context data. The response is\
    \ printed to stdout.\n      \u2022 Otherwise, a Single-Index Drift Search workflow\
    \ is used:\n        - If streaming is enabled, an asynchronous streaming search\
    \ is performed. Chunks are printed as they arrive,\n          and a final full\
    \ response string along with context data is returned.\n        - If streaming\
    \ is disabled, a single non-streaming search is performed; the final response\
    \ is printed and\n          returned with context data.\n\nNotes on side effects:\n\
    \    - Printing: In both multi-index and non-streaming single-index paths, the\
    \ final response is printed to stdout.\n    - Streaming path: Each streamed chunk\
    \ is printed immediately as it is received, followed by a newline at the end.\n\
    \    - Context data: The context data is captured via an on_context callback during\
    \ streaming and returned with the response."
  code_example: null
  example_source: null
  line_start: 268
  line_end: 386
  dependencies:
  - graphrag.api::drift_search
  - graphrag.api::multi_index_drift_search
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/cli/query.py::run_streaming_search
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_query_cli
- node_id: graphrag/cli/query.py::run_basic_search
  file: graphrag/cli/query.py
  name: run_basic_search
  signature: "def run_basic_search(\n    config_filepath: Path | None,\n    data_dir:\
    \ Path | None,\n    root_dir: Path,\n    streaming: bool,\n    query: str,\n \
    \   verbose: bool,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Perform a basics search with a given query.\n\nLoads index files required\
    \ for basic search and calls the Query API.\n\nArgs:\n    config_filepath: Path\
    \ | None\n        Path to a config file to use, or None to locate one in root_dir.\n\
    \    data_dir: Path | None\n        Optional directory containing precomputed\
    \ data to override the base output directory.\n    root_dir: Path\n        Root\
    \ directory of the project.\n    streaming: bool\n        If True, perform a streaming\
    \ search and print chunks as they are produced.\n    query: str\n        The search\
    \ query string used to perform the basic search.\n    verbose: bool\n        If\
    \ True, enable verbose output from the API calls.\n\nReturns:\n    tuple[str,\
    \ dict[str, Any]]\n        The full response string and the context data collected\
    \ during the search.\n\nRaises:\n    FileNotFoundError\n        If the config\
    \ file cannot be found.\n    ValueError\n        If the config is invalid.\n \
    \   Exception\n        Exceptions raised by the underlying streaming or non-streaming\
    \ API calls may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 389
  line_end: 474
  dependencies:
  - graphrag.api::basic_search
  - graphrag.api::multi_index_basic_search
  - graphrag/cli/query.py::_resolve_output_files
  - graphrag/cli/query.py::run_streaming_search
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_query_cli
- node_id: tests/unit/config/test_config.py::test_load_minimal_config
  file: tests/unit/config/test_config.py
  name: test_load_minimal_config
  signature: def test_load_minimal_config() -> None
  decorators:
  - '@mock.patch.dict(os.environ, {"CUSTOM_API_KEY": FAKE_API_KEY}, clear=True)'
  raises: []
  visibility: public
  docstring: "Test loading a minimal Graphrag configuration and verify it matches\
    \ the expected default Graphrag configuration for the given root directory.\n\n\
    Returns:\n    None"
  code_example: null
  example_source: null
  line_start: 143
  line_end: 148
  dependencies:
  - graphrag/config/load_config.py::load_config
  - tests/unit/config/utils.py::assert_graphrag_configs
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_load_config_with_cli_overrides
  file: tests/unit/config/test_config.py
  name: test_load_config_with_cli_overrides
  signature: def test_load_config_with_cli_overrides() -> None
  decorators:
  - '@mock.patch.dict(os.environ, {"CUSTOM_API_KEY": FAKE_API_KEY}, clear=True)'
  raises: []
  visibility: public
  docstring: 'Test that load_config applies CLI overrides for the output base directory
    when an environment variable is provided.


    The test patches the environment to include CUSTOM_API_KEY, derives the root_dir
    from the minimal_config fixture, overrides the output base directory via cli_overrides,
    loads the configuration, and asserts that the resulting GraphRagConfig matches
    the expected configuration with the overridden base directory.'
  code_example: null
  example_source: null
  line_start: 152
  line_end: 163
  dependencies:
  - graphrag/config/load_config.py::load_config
  - tests/unit/config/utils.py::assert_graphrag_configs
  - tests/unit/config/utils.py::get_default_graphrag_config
  called_by: []
- node_id: tests/unit/config/test_config.py::test_load_config_missing_env_vars
  file: tests/unit/config/test_config.py
  name: test_load_config_missing_env_vars
  signature: def test_load_config_missing_env_vars() -> None
  decorators: []
  raises: []
  visibility: public
  docstring: "Load configuration from a file and environment variables.\n\nNote: Loading\
    \ may depend on environment variables (for example API keys) in addition to the\
    \ configuration file.\n\nArgs:\n    root_dir (Path): The root directory to search\
    \ for the config file.\n    config_filepath (Path | None): The path to the config\
    \ file. If None, searches for the config file in root_dir.\n    cli_overrides\
    \ (dict[str, Any] | None): Flat dictionary of CLI overrides. Example: {'output.base_dir':\
    \ 'override_value'}. Overrides are applied after loading the base configuration.\n\
    \nReturns:\n    GraphRagConfig: The loaded configuration.\n\nRaises:\n    FileNotFoundError:\
    \ If the config file cannot be found.\n    KeyError: If a required environment\
    \ variable is missing.\n    ValidationError: If the loaded configuration fails\
    \ validation (e.g., invalid structure or types)."
  code_example: null
  example_source: null
  line_start: 166
  line_end: 170
  dependencies:
  - graphrag/config/load_config.py::load_config
  called_by: []
- node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: read_settings
  signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
    \ bool = False,\n    ) -> GraphRagConfig | None"
  decorators: []
  raises: []
  visibility: public
  docstring: "Read settings file from local source.\n\nNote: The 'file' parameter\
    \ is unused. Settings are loaded by invoking load_config with root_dir derived\
    \ from the datasource's base_path.\n\nArgs:\n    self: The LocalDatasource instance.\n\
    \    file: str. Unused. Path to the settings file; present for API compatibility.\n\
    \    throw_on_missing: bool. Ignored by this implementation.\n\nReturns:\n   \
    \ GraphRagConfig | None: The GraphRagConfig produced by load_config, or None if\
    \ no configuration could be loaded.\n\nRaises:\n    None: This method does not\
    \ raise exceptions; any loading errors originate from load_config."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 72
  dependencies:
  - graphrag/config/load_config.py::load_config
  called_by: []
- node_id: graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
  file: graphrag/index/workflows/generate_text_embeddings.py
  name: _run_embeddings
  signature: "def _run_embeddings(\n    name: str,\n    data: pd.DataFrame,\n    embed_column:\
    \ str,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    text_embed_config:\
    \ dict,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "All steps to generate a single embedding.\n\nArgs:\n  name: The name\
    \ of the embedding, used as the embedding_name when calling embed_text.\n  data:\
    \ DataFrame containing input data; the function adds an embedding column and returns\
    \ a DataFrame with only id and embedding.\n  embed_column: The column in data\
    \ to embed; passed to embed_text as the embed_column.\n  callbacks: WorkflowCallbacks\
    \ used to report progress and handle lifecycle events during embedding.\n  cache:\
    \ PipelineCache used by embed_text for caching and resource management.\n  text_embed_config:\
    \ Dictionary with embedding configuration; should include a strategy key.\n\n\
    Returns:\n  pd.DataFrame: A DataFrame containing the id and embedding columns.\n\
    \nRaises:\n  May raise exceptions from embed_text or DataFrame operations."
  code_example: null
  example_source: null
  line_start: 174
  line_end: 192
  dependencies:
  - graphrag/index/operations/embed_text/embed_text.py::embed_text
  called_by:
  - graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
- node_id: tests/verbs/test_create_final_text_units.py::test_create_final_text_units
  file: tests/verbs/test_create_final_text_units.py
  name: test_create_final_text_units
  signature: def test_create_final_text_units()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the asynchronous creation of final text units and validate the\
    \ produced output against the expected test table.\n\nReturns:\n    None. This\
    \ test does not return a value; it asserts correctness by comparing actual output\
    \ to expected data."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 41
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_final_text_units.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
  file: graphrag/language_model/providers/fnllm/models.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        name: str,\n     \
    \   config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
    \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize an OpenAI Chat FNLLM provider using the given configuration\
    \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
    \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
    \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
    \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided, an\
    \ error handler will be created and used.\n    cache: PipelineCache | None\n \
    \       Optional PipelineCache to back the model cache provider.\n\nReturns:\n\
    \    None"
  code_example: null
  example_source: null
  line_start: 47
  line_end: 65
  dependencies:
  - graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  - graphrag/language_model/providers/fnllm/utils.py::_create_cache
  - graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
  - graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
  file: graphrag/language_model/providers/fnllm/models.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        name: str,\n     \
    \   config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
    \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize an OpenAI Embedding FNLLM provider using the given configuration\
    \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
    \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
    \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
    \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided, an\
    \ error handler will be created to log errors.\n    cache: PipelineCache | None\n\
    \        The pipeline cache to wrap. If None, no cache is used.\n\nReturns:\n\
    \    None\n        This initializer does not return a value.\n\nRaises:\n    Exception\n\
    \        Propagates exceptions raised by the underlying helper functions and OpenAI\
    \ client initialization."
  code_example: null
  example_source: null
  line_start: 156
  line_end: 174
  dependencies:
  - graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  - graphrag/language_model/providers/fnllm/utils.py::_create_cache
  - graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
  - graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
  file: graphrag/language_model/providers/fnllm/models.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        name: str,\n     \
    \   config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
    \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize an Azure OpenAI Chat LLM provider instance.\n\nArgs:\n  \
    \  name: str\n        The name to assign to the internal cache provider and model\
    \ instance.\n    config: LanguageModelConfig\n        The configuration used to\
    \ derive the OpenAI configuration.\n    callbacks: WorkflowCallbacks | None\n\
    \        Optional WorkflowCallbacks; if provided, an error handler will be created\
    \ to log issues.\n    cache: PipelineCache | None\n        Optional cache to wrap\
    \ for the underlying FNLLM cache provider; if None, no caching is used.\n\nReturns:\n\
    \    None\n\nRaises:\n    Exception: If initialization of the OpenAI config, client,\
    \ or FNLLM components fails due to underlying library errors."
  code_example: null
  example_source: null
  line_start: 248
  line_end: 266
  dependencies:
  - graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  - graphrag/language_model/providers/fnllm/utils.py::_create_cache
  - graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
  - graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  called_by: []
- node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
  file: graphrag/language_model/providers/fnllm/models.py
  name: __init__
  signature: "def __init__(\n        self,\n        *,\n        name: str,\n     \
    \   config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
    \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Initialize an Azure OpenAI Embedding FNLLM provider using the given\
    \ configuration and optional components.\n\nArgs:\n    name (str): The name to\
    \ assign to the internal cache provider and model instance.\n    config (LanguageModelConfig):\
    \ The configuration used to derive the OpenAI configuration.\n    callbacks (WorkflowCallbacks\
    \ | None): Optional WorkflowCallbacks; if provided, an error handler will be created\
    \ to log issues.\n    cache (PipelineCache | None): Optional PipelineCache to\
    \ back the embedding cache.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 359
  line_end: 377
  dependencies:
  - graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  - graphrag/language_model/providers/fnllm/utils.py::_create_cache
  - graphrag/language_model/providers/fnllm/utils.py::_create_error_handler
  - graphrag/language_model/providers/fnllm/utils.py::_create_openai_config
  called_by: []
- node_id: graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
  file: graphrag/index/workflows/create_base_text_units.py
  name: chunker_with_logging
  signature: 'def chunker_with_logging(row: pd.Series, row_index: int) -> Any'
  decorators: []
  raises: []
  visibility: public
  docstring: "Log chunker progress for a row during chunking.\n\nExecutes the chunker\
    \ on the given row and logs progress using total_rows from the surrounding scope.\n\
    \nArgs:\n    row (pd.Series): The input row to be chunked.\n    row_index (int):\
    \ The index of the row being processed (0-based).\n\nReturns:\n    Any: The result\
    \ of the chunker applied to the row.\n\nRaises:\n    Exception: Propagates any\
    \ exception raised by chunker(row)."
  code_example: null
  example_source: null
  line_start: 134
  line_end: 138
  dependencies:
  - graphrag/index/workflows/create_base_text_units.py::chunker
  called_by:
  - graphrag/index/workflows/create_base_text_units.py::create_base_text_units
- node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
  file: graphrag/query/structured_search/global_search/community_context.py
  name: build_context
  signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
    \ ConversationHistory | None = None,\n        use_community_summary: bool = True,\n\
    \        column_delimiter: str = \"|\",\n        shuffle_data: bool = True,\n\
    \        include_community_rank: bool = False,\n        min_community_rank: int\
    \ = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
    \ bool = True,\n        community_weight_name: str = \"occurrence\",\n       \
    \ normalize_community_weight: bool = True,\n        max_context_tokens: int =\
    \ 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
    \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n    \
    \    **kwargs: Any,\n    ) -> ContextBuilderResult"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare batches of community report data table as context data for global\
    \ search.\n\nArgs:\n  query: The user query to build context for.\n  conversation_history:\
    \ Optional conversation history to consider while constructing the context.\n\
    \  use_community_summary: Whether to use the community summary in the context\
    \ data.\n  column_delimiter: Delimiter used to separate columns in the context\
    \ representation.\n  shuffle_data: Whether to shuffle the data before context\
    \ assembly.\n  include_community_rank: Whether to include per-community rank in\
    \ the context.\n  min_community_rank: Minimum rank value to include in the context.\n\
    \  community_rank_name: Name to assign to the rank field in the context data.\n\
    \  include_community_weight: Whether to include per-community weight in the context.\n\
    \  community_weight_name: Name to assign to the weight field in the context data.\n\
    \  normalize_community_weight: Whether to normalize the community weights.\n \
    \ max_context_tokens: Maximum allowed tokens for the generated context.\n  context_name:\
    \ Descriptive name for the context data (e.g., \"Reports\").\n  conversation_history_user_turns_only:\
    \ Whether to include only user turns from the conversation history.\n  conversation_history_max_turns:\
    \ Maximum number of turns to include from the conversation history.\n  kwargs:\
    \ Additional keyword arguments.\n\nReturns:\n  ContextBuilderResult: The result\
    \ containing context chunks, context data, and related usage statistics.\n\nRaises:\n\
    \  Exceptions raised by underlying components (e.g., conversation_history, dynamic_community_selection,\
    \ and build_community_context) during context construction."
  code_example: null
  example_source: null
  line_start: 55
  line_end: 144
  dependencies:
  - graphrag/query/context_builder/builders.py::ContextBuilderResult
  - graphrag/query/context_builder/community_context.py::build_community_context
  called_by: []
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: _build_community_context
  signature: "def _build_community_context(\n        self,\n        selected_entities:\
    \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
    \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
    \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
    \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
    \ dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Add community data to the context window until it hits the max_context_tokens\
    \ limit.\n\nArgs:\n  selected_entities (list[Entity]): Entities selected for which\
    \ related communities should be added to the context.\n  max_context_tokens (int):\
    \ Maximum number of tokens to include for the community context.\n  use_community_summary\
    \ (bool): Whether to utilize a summarized representation of communities.\n  column_delimiter\
    \ (str): Delimiter used between columns in the generated context.\n  include_community_rank\
    \ (bool): Whether to include the community rank in the context.\n  min_community_rank\
    \ (int): Minimum community rank to include in the context.\n  return_candidate_context\
    \ (bool): If True, also compute and return candidate context data.\n  context_name\
    \ (str): Name of the context section (default \"Reports\").\n\nReturns:\n  tuple[str,\
    \ dict[str, pd.DataFrame]]: A tuple consisting of\n    - context_text: String\
    \ containing the assembled community context; may be empty.\n    - context_data:\
    \ Mapping from a lowercase context name to a DataFrame with additional context\
    \ information.\n\nRaises:\n  Exceptions raised by underlying calls (e.g., build_community_context,\
    \ get_candidate_communities)\n  may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 224
  line_end: 304
  dependencies:
  - graphrag/query/context_builder/community_context.py::build_community_context
  - graphrag/query/input/retrieval/community_reports.py::get_candidate_communities
  called_by: []
- node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
  file: graphrag/index/utils/derive_from_rows.py
  name: derive_from_rows_asyncio_threads
  signature: "def derive_from_rows_asyncio_threads(\n    input: pd.DataFrame,\n  \
    \  transform: Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
    \    num_threads: int | None = 4,\n    progress_msg: str = \"\",\n) -> list[ItemType\
    \ | None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Threaded (IO-bound) variant: derive results from DataFrame rows by dispatching\
    \ per-row work to a thread pool via asyncio.to_thread. The transform is expected\
    \ to be an asynchronous function that operates on the row's pandas Series and\
    \ returns an ItemType (or None). Internally, rows are passed to the executor as\
    \ a tuple (index, row); the transform should use only the Series, not the full\
    \ tuple.\n\nThis function constrains concurrency with a semaphore using num_threads\
    \ (default 4).\n\nArgs:\n  input: pandas.DataFrame\n      The input data to process,\
    \ where each row will be transformed.\n  transform: Callable[[pd.Series], Awaitable[ItemType]]\n\
    \      Async function applied to the row's Series to produce an ItemType. The\
    \ function should\n      operate on the Series alone and not expect the full (index,\
    \ Series) tuple.\n  callbacks: WorkflowCallbacks\n      Callbacks used to report\
    \ progress and handle workflow lifecycle events.\n  num_threads: int | None\n\
    \      Maximum number of concurrent threads to use; if None, a default of 4 is\
    \ used.\n  progress_msg: str\n      Optional message to display for progress tracking.\n\
    \nReturns:\n  list[ItemType | None]\n      A list containing the result for each\
    \ input row. Each element is either the ItemType\n      produced by transform\
    \ or None if the row could not be transformed.\n\nRaises:\n  Propagates exceptions\
    \ raised by the per-row transform or by callbacks. If multiple errors\n  occur\
    \ during parallel execution, a summary exception may be raised by the internal\
    \ error\n  aggregator.\n\nNotes:\n  This function is the threaded variant of derive_from_rows\
    \ and relies on asyncio.to_thread to\n  execute transforms in a thread pool while\
    \ an asyncio event loop drives orchestration."
  code_example: null
  example_source: null
  line_start: 61
  line_end: 88
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
  called_by:
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
- node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
  file: graphrag/index/utils/derive_from_rows.py
  name: derive_from_rows_asyncio
  signature: "def derive_from_rows_asyncio(\n    input: pd.DataFrame,\n    transform:\
    \ Callable[[pd.Series], Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks,\n\
    \    num_threads: int = 4,\n    progress_msg: str = \"\",\n) -> list[ItemType\
    \ | None]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Derive from rows asynchronously.\n\nThis is useful for IO bound operations.\n\
    \nArgs:\n    input: pd.DataFrame\n        The input data to process, where each\
    \ row will be transformed.\n    transform: Callable[[pd.Series], Awaitable[ItemType]]\n\
    \        Async function applied to a row's Series to produce an ItemType.\n  \
    \  callbacks: WorkflowCallbacks\n        Callbacks used to report progress and\
    \ handle workflow events.\n    num_threads: int\n        Number of concurrent\
    \ workers to use. This limits the degree of parallelism; default is 4.\n    progress_msg:\
    \ str\n        Description shown in the progress ticker during processing.\n\n\
    Returns:\n    list[ItemType | None]\n        A list of results corresponding to\
    \ each input row; each element is either an ItemType or None if a row could not\
    \ be processed."
  code_example: null
  example_source: null
  line_start: 94
  line_end: 122
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::_derive_from_rows_base
  called_by:
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
- node_id: graphrag/api/query.py::global_search_streaming
  file: graphrag/api/query.py
  name: global_search_streaming
  signature: "def global_search_streaming(\n    config: GraphRagConfig,\n    entities:\
    \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
    \    community_level: int | None,\n    dynamic_community_selection: bool,\n  \
    \  response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks] |\
    \ None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a global search and stream the response in chunks via an async\
    \ generator.\n\nArgs:\n  config: GraphRagConfig: A graphrag configuration (from\
    \ settings.yaml).\n  entities: pd.DataFrame: A DataFrame containing the final\
    \ entities (from entities.parquet).\n  communities: pd.DataFrame: A DataFrame\
    \ containing the final communities (from communities.parquet).\n  community_reports:\
    \ pd.DataFrame: A DataFrame containing the final community reports (from community_reports.parquet).\n\
    \  community_level: int | None: The community level to search at.\n  dynamic_community_selection:\
    \ bool: Enable dynamic community selection instead of using all community reports\
    \ at a fixed level. Note that you can still provide community_level cap the maximum\
    \ level to search.\n  response_type: str: The type of response to return.\n  query:\
    \ str: The user query to search for.\n  callbacks: list[QueryCallbacks] | None:\
    \ Optional callbacks to receive streaming events.\n  verbose: bool: If True, enable\
    \ verbose logging.\nReturns:\n  AsyncGenerator: An asynchronous generator yielding\
    \ strings; each yielded chunk is part of the streaming global search response.\n\
    Raises:\n  pydantic.ValidationError: If input arguments fail validation via the\
    \ validate_call decorator."
  code_example: null
  example_source: null
  line_start: 128
  line_end: 192
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/query/factory.py::get_global_search_engine
  - graphrag/query/indexer_adapters.py::read_indexer_communities
  - graphrag/query/indexer_adapters.py::read_indexer_entities
  - graphrag/query/indexer_adapters.py::read_indexer_reports
  - graphrag/utils/api.py::load_search_prompt
  called_by:
  - graphrag/api/query.py::global_search
- node_id: graphrag/index/operations/cluster_graph.py::cluster_graph
  file: graphrag/index/operations/cluster_graph.py
  name: cluster_graph
  signature: "def cluster_graph(\n    graph: nx.Graph,\n    max_cluster_size: int,\n\
    \    use_lcc: bool,\n    seed: int | None = None,\n) -> Communities"
  decorators: []
  raises: []
  visibility: public
  docstring: "Compute hierarchical Leiden-based clusters for the input graph and return\
    \ them as a list of (level, cluster_id, parent_cluster_id, nodes).\n\nArgs:\n\
    \    graph: nx.Graph\n        Input graph on which to perform clustering.\n  \
    \  max_cluster_size: int\n        Maximum size of a cluster allowed by Leiden\
    \ algorithm.\n    use_lcc: bool\n        If True, operate on the largest connected\
    \ component of the input graph.\n    seed: int | None\n        Random seed for\
    \ reproducibility of the clustering process.\n\nReturns:\n    Communities\n  \
    \      A list of tuples (level, cluster_id, parent_cluster_id, nodes) describing\
    \ the\n        clusters found at different hierarchical levels, the cluster's\
    \ parent, and the\n        member node identifiers."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 53
  dependencies:
  - graphrag/index/operations/cluster_graph.py::_compute_leiden_communities
  called_by:
  - graphrag/index/workflows/create_communities.py::create_communities
- node_id: graphrag/index/operations/finalize_entities.py::finalize_entities
  file: graphrag/index/operations/finalize_entities.py
  name: finalize_entities
  signature: "def finalize_entities(\n    entities: pd.DataFrame,\n    relationships:\
    \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
    \ bool = False,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transforms input entities into final entity records by building a graph\
    \ from relationships, optionally embedding, applying a layout, and attaching identifiers.\n\
    \nArgs:\n    entities (pd.DataFrame): Input entities to be transformed. The function\
    \ merges on the 'title' column and augments with layout and degree information.\n\
    \    relationships (pd.DataFrame): DataFrame containing edge information used\
    \ to construct the graph; edge attributes include 'weight'.\n    embed_config\
    \ (EmbedGraphConfig | None): Embedding configuration. If provided and embed_config.enabled\
    \ is True, graph embeddings are computed.\n    layout_enabled (bool): If True,\
    \ a layout is applied to the graph to compute node positions.\n\nReturns:\n  \
    \  pd.DataFrame: The final entities DataFrame containing the columns defined by\
    \ ENTITIES_FINAL_COLUMNS.\n\nRaises:\n    Exception: Propagates exceptions raised\
    \ by underlying operations such as create_graph, embed_graph, layout_graph, and\
    \ compute_degree, as well as DataFrame operations."
  code_example: null
  example_source: null
  line_start: 18
  line_end: 54
  dependencies:
  - graphrag/index/operations/compute_degree.py::compute_degree
  - graphrag/index/operations/create_graph.py::create_graph
  - graphrag/index/operations/embed_graph/embed_graph.py::embed_graph
  - graphrag/index/operations/layout_graph/layout_graph.py::layout_graph
  called_by:
  - graphrag/index/workflows/finalize_graph.py::finalize_graph
- node_id: graphrag/index/workflows/extract_graph.py::extract_graph
  file: graphrag/index/workflows/extract_graph.py
  name: extract_graph
  signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    extraction_strategy: dict[str, Any] | None = None,\n\
    \    extraction_num_threads: int = 4,\n    extraction_async_mode: AsyncType =\
    \ AsyncType.AsyncIO,\n    entity_types: list[str] | None = None,\n    summarization_strategy:\
    \ dict[str, Any] | None = None,\n    summarization_num_threads: int = 4,\n) ->\
    \ tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "All steps to create the base entity graph.\n\nThis asynchronous function\
    \ processes the input text units to extract entities and relationships, validates\n\
    the extraction results, preserves raw extraction outputs, and summarizes the data\
    \ to produce the final\nentities and relationships DataFrames. It returns a tuple\
    \ of (entities, relationships, raw_entities, raw_relationships).\n\nArgs:\n  \
    \  text_units: DataFrame containing the text units to process.\n    callbacks:\
    \ Callbacks to report progress during the workflow.\n    cache: Cache to store/retrieve\
    \ intermediate results.\n    extraction_strategy: Strategy configuration for entity\
    \ extraction.\n    extraction_num_threads: Number of threads to use for extraction.\n\
    \    extraction_async_mode: Async mode for extraction (e.g., AsyncIO).\n    entity_types:\
    \ Optional list of entity types to constrain extraction.\n    summarization_strategy:\
    \ Strategy configuration for summarization.\n    summarization_num_threads: Number\
    \ of threads to use for summarization.\n\nReturns:\n    tuple[pd.DataFrame, pd.DataFrame,\
    \ pd.DataFrame, pd.DataFrame]:\n        A tuple of four DataFrames:\n        -\
    \ entities: summarized entities\n        - relationships: summarized relationships\n\
    \        - raw_entities: entities extracted before summarization\n        - raw_relationships:\
    \ relationships extracted before summarization\n\nRaises:\n    ValueError: If\
    \ no entities detected during extraction or no relationships detected during extraction."
  code_example: null
  example_source: null
  line_start: 82
  line_end: 132
  dependencies:
  - graphrag/index/workflows/extract_graph.py::_validate_data
  - graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
  called_by: []
- node_id: graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
  file: graphrag/index/workflows/update_entities_relationships.py
  name: _update_entities_and_relationships
  signature: "def _update_entities_and_relationships(\n    previous_storage: PipelineStorage,\n\
    \    delta_storage: PipelineStorage,\n    output_storage: PipelineStorage,\n \
    \   config: GraphRagConfig,\n    cache: PipelineCache,\n    callbacks: WorkflowCallbacks,\n\
    ) -> tuple[pd.DataFrame, pd.DataFrame, dict]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Update Final Entities and Relationships output.\n\nThis function merges\
    \ the existing (previous) entities with the delta of new/updated entities, updates\
    \ and merges relationships, applies summarization to the merged entities and relationships,\
    \ and writes the results to the provided output storage.\n\nParameters:\n  previous_storage\
    \ (PipelineStorage): The storage containing the previous state data.\n  delta_storage\
    \ (PipelineStorage): The storage containing delta (new/updated) data.\n  output_storage\
    \ (PipelineStorage): The storage to write updated entities and relationships to.\n\
    \  config (GraphRagConfig): GraphRag configuration used for summarization and\
    \ merging.\n  cache (PipelineCache): Cache used by the summarization routine.\n\
    \  callbacks (WorkflowCallbacks): Callbacks for progress reporting during the\
    \ workflow.\n\nReturns:\n  tuple[pd.DataFrame, pd.DataFrame, dict]: The updated\
    \ entities DataFrame, the updated relationships DataFrame, and the entity_id_mapping\
    \ dictionary.\n\nRaises:\n  ValueError: If required data is missing from storage\
    \ or a storage read error occurs.\n  KeyError: If required columns are missing\
    \ from the input DataFrames used by _update_and_merge_relationships.\n  TypeError:\
    \ If inputs to _update_and_merge_relationships are not pandas DataFrames.\n  Exception:\
    \ General exceptions raised by the storage backends during read/write operations\
    \ or by the summarization step."
  code_example: null
  example_source: null
  line_start: 56
  line_end: 106
  dependencies:
  - graphrag/index/update/entities.py::_group_and_resolve_entities
  - graphrag/index/update/relationships.py::_update_and_merge_relationships
  - graphrag/index/workflows/extract_graph.py::get_summarized_entities_relationships
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - graphrag/index/workflows/update_entities_relationships.py::run_workflow
- node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: _build_local_context
  signature: "def _build_local_context(\n        self,\n        selected_entities:\
    \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
    \ bool = False,\n        rank_description: str = \"relationship count\",\n   \
    \     include_relationship_weight: bool = False,\n        top_k_relationships:\
    \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n       \
    \ return_candidate_context: bool = False,\n        column_delimiter: str = \"\
    |\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Build data context for local search prompt by combining entity/relationship/covariate\
    \ tables.\n\nArgs:\n  selected_entities (list[Entity]): Entities to include in\
    \ the context.\n  max_context_tokens (int): Maximum allowed tokens for the generated\
    \ context text. Parsing stops when adding a new entity would exceed this limit.\n\
    \  include_entity_rank (bool): Whether to include the entity's rank in the context\
    \ rows.\n  rank_description (str): Description of the ranking used in the entity\
    \ context (default: 'relationship count').\n  include_relationship_weight (bool):\
    \ Whether to include the relationship weight in the generated context.\n  top_k_relationships\
    \ (int): Number of top relationships to include for each entity.\n  relationship_ranking_attribute\
    \ (str): Attribute name used to rank relationships (default: 'rank').\n  return_candidate_context\
    \ (bool): If True, return all candidate entities/relationships/covariates (not\
    \ only those fitted into the context window) and tag them accordingly.\n  column_delimiter\
    \ (str): Delimiter used to separate fields in the generated context.\n\nReturns:\n\
    \  tuple[str, dict[str, pd.DataFrame]]: The final context text and a mapping from\
    \ context section names to their corresponding DataFrames."
  code_example: null
  example_source: null
  line_start: 377
  line_end: 493
  dependencies:
  - graphrag/query/context_builder/local_context.py::build_covariates_context
  - graphrag/query/context_builder/local_context.py::build_entity_context
  - graphrag/query/context_builder/local_context.py::build_relationship_context
  - graphrag/query/context_builder/local_context.py::get_candidate_context
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: _get_community_df
  signature: "def _get_community_df(\n    level: int,\n    invalid_context_df: pd.DataFrame,\n\
    \    sub_context_df: pd.DataFrame,\n    community_hierarchy_df: pd.DataFrame,\n\
    \    tokenizer: Tokenizer,\n    max_context_tokens: int,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Get community context for each community.\n\nArgs:\n  level: The level\
    \ to process.\n  invalid_context_df: DataFrame containing IDs of communities considered\
    \ invalid at this level.\n  sub_context_df: DataFrame containing sub-community\
    \ context data for each community.\n  community_hierarchy_df: DataFrame representing\
    \ the community hierarchy from which communities at the given level are selected.\n\
    \  tokenizer: Tokenizer used to build and trim mixed context strings.\n  max_context_tokens:\
    \ Maximum number of tokens allowed for the resulting context.\n\nReturns:\n  pd.DataFrame:\
    \ DataFrame containing one row per community at the specified level, including:\n\
    \    - COMMUNITY_ID\n    - ALL_CONTEXT: a list of dictionaries with keys SUB_COMMUNITY,\
    \ ALL_CONTEXT, FULL_CONTENT, CONTEXT_SIZE\n    - CONTEXT_STRING: mixed context\
    \ string built from ALL_CONTEXT\n    - COMMUNITY_LEVEL: the level value\n\nRaises:\n\
    \  KeyError: If required columns are missing from the input DataFrames."
  code_example: null
  example_source: null
  line_start: 318
  line_end: 362
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_at_level
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_build_mixed_context
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_drop_community_level
  - graphrag/index/utils/dataframes.py::join
  - graphrag/index/utils/dataframes.py::select
  called_by:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: build_local_context
  signature: "def build_local_context(\n    nodes,\n    edges,\n    claims,\n    tokenizer:\
    \ Tokenizer,\n    callbacks: WorkflowCallbacks,\n    max_context_tokens: int =\
    \ 16_000,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare initial local context for all communities, processing level-by-level.\n\
    \nThis function computes per-level local context data and concatenates the results.\
    \ It determines the processing levels with get_levels, iterates through them with\
    \ progress_iterable using callbacks.progress, and for each level builds a per-level\
    \ DataFrame by calling _prepare_reports_at_level with the provided inputs and\
    \ the max_context_tokens budget. The resulting per-level DataFrame has its COMMUNITY_LEVEL\
    \ column set to the corresponding level, and all per-level DataFrames are concatenated\
    \ into a single DataFrame that represents the initial local context for all communities.\n\
    \nArgs:\n  nodes: DataFrame containing community node details, including a COMMUNITY_LEVEL\
    \ column.\n  edges: DataFrame containing edge details between nodes.\n  claims:\
    \ DataFrame or None containing claims related to communities.\n  tokenizer: Tokenizer\
    \ used to compute context token counts during per-level processing.\n  callbacks:\
    \ WorkflowCallbacks used for progress reporting.\n  max_context_tokens: int, maximum\
    \ number of tokens allocated for per-level processing; defaults to 16,000.\n\n\
    Returns:\n  pd.DataFrame: A concatenated DataFrame with prepared local context\
    \ for all levels; each row includes COMMUNITY_LEVEL indicating its level.\n\n\
    Raises:\n  None."
  code_example: null
  example_source: null
  line_start: 38
  line_end: 60
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_prepare_reports_at_level
  - graphrag/index/operations/summarize_communities/utils.py::get_levels
  - graphrag/logger/progress.py::progress_iterable
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
- node_id: graphrag/cli/main.py::_query_cli
  file: graphrag/cli/main.py
  name: _query_cli
  signature: "def _query_cli(\n    method: SearchMethod = typer.Option(\n        ...,\n\
    \        \"--method\",\n        \"-m\",\n        help=\"The query algorithm to\
    \ use.\",\n    ),\n    query: str = typer.Option(\n        ...,\n        \"--query\"\
    ,\n        \"-q\",\n        help=\"The query to execute.\",\n    ),\n    config:\
    \ Path | None = typer.Option(\n        None,\n        \"--config\",\n        \"\
    -c\",\n        help=\"The configuration to use.\",\n        exists=True,\n   \
    \     file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
    \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
    ,\n        \"-v\",\n        help=\"Run the query with verbose logging.\",\n  \
    \  ),\n    data: Path | None = typer.Option(\n        None,\n        \"--data\"\
    ,\n        \"-d\",\n        help=\"Index output directory (contains the parquet\
    \ files).\",\n        exists=True,\n        dir_okay=True,\n        readable=True,\n\
    \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
    \    root: Path = typer.Option(\n        Path(),\n        \"--root\",\n      \
    \  \"-r\",\n        help=\"The project root directory.\",\n        exists=True,\n\
    \        dir_okay=True,\n        writable=True,\n        resolve_path=True,\n\
    \        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n    community_level: int =\
    \ typer.Option(\n        2,\n        \"--community-level\",\n        help=(\n\
    \            \"Leiden hierarchy level from which to load community reports. \"\
    \n            \"Higher values represent smaller communities.\"\n        ),\n \
    \   ),\n    dynamic_community_selection: bool = typer.Option(\n        False,\n\
    \        \"--dynamic-community-selection/--no-dynamic-selection\",\n        help=\"\
    Use global search with dynamic community selection.\",\n    ),\n    response_type:\
    \ str = typer.Option(\n        \"Multiple Paragraphs\",\n        \"--response-type\"\
    ,\n        help=(\n            \"Free-form description of the desired response\
    \ format \"\n            \"(e.g. 'Single Sentence', 'List of 3-7 Points', etc.).\"\
    \n        ),\n    ),\n    streaming: bool = typer.Option(\n        False,\n  \
    \      \"--streaming/--no-streaming\",\n        help=\"Print the response in a\
    \ streaming manner.\",\n    ),\n) -> None"
  decorators:
  - '@app.command("query")'
  raises:
  - ValueError
  visibility: protected
  docstring: "Query a knowledge graph index.\n\nArgs:\n    method: The query algorithm\
    \ to use.\n    query: The query to execute.\n    config: The configuration to\
    \ use.\n    verbose: Run the query with verbose logging.\n    data: Index output\
    \ directory (contains the parquet files).\n    root: The project root directory.\n\
    \    community_level: Leiden hierarchy level from which to load community reports.\
    \ Higher values represent smaller communities.\n    dynamic_community_selection:\
    \ Use global search with dynamic community selection.\n    response_type: Free-form\
    \ description of the desired response format (e.g. 'Single Sentence', 'List of\
    \ 3-7 Points', etc.).\n    streaming: Print the response in a streaming manner.\n\
    \nReturns:\n    None\n\nRaises:\n    ValueError"
  code_example: null
  example_source: null
  line_start: 414
  line_end: 545
  dependencies:
  - graphrag/cli/query.py::run_basic_search
  - graphrag/cli/query.py::run_drift_search
  - graphrag/cli/query.py::run_global_search
  - graphrag/cli/query.py::run_local_search
  called_by: []
- node_id: graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
  file: graphrag/index/workflows/generate_text_embeddings.py
  name: generate_text_embeddings
  signature: "def generate_text_embeddings(\n    documents: pd.DataFrame | None,\n\
    \    relationships: pd.DataFrame | None,\n    text_units: pd.DataFrame | None,\n\
    \    entities: pd.DataFrame | None,\n    community_reports: pd.DataFrame | None,\n\
    \    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    text_embed_config:\
    \ dict,\n    embedded_fields: list[str],\n) -> dict[str, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "All the steps to generate all embeddings.\n\nArgs:\n  documents: DataFrame\
    \ or None. Data for document text embeddings; when provided, expected to include\
    \ id and text columns.\n  relationships: DataFrame or None. Data for relationship\
    \ descriptions; when provided, expected to include id and description columns.\n\
    \  text_units: DataFrame or None. Data for text units; when provided, expected\
    \ to include id and text columns.\n  entities: DataFrame or None. Data for entities;\
    \ may include id, title, and description used for embeddings.\n  community_reports:\
    \ DataFrame or None. Data for community reports; used for title, summary, and\
    \ full content embeddings if provided.\n  callbacks: WorkflowCallbacks. Callbacks\
    \ used during embedding processing.\n  cache: PipelineCache. Cache used by the\
    \ embedding routine.\n  text_embed_config: dict. Embedding configuration (e.g.,\
    \ strategy) passed to the embedding function.\n  embedded_fields: list[str]. The\
    \ list of embedding fields to generate; each corresponds to a key in the embedding_param_map.\n\
    \nReturns:\n  dict[str, pd.DataFrame]. Mapping from embedding name to a DataFrame\
    \ containing columns [\"id\", \"embedding\"] for that embedding.\n\nRaises:\n\
    \  Exceptions raised by the underlying embedding operations (e.g., _run_embeddings\
    \ or embed_text) may propagate to the caller if embedding fails."
  code_example: null
  example_source: null
  line_start: 96
  line_end: 171
  dependencies:
  - graphrag/index/workflows/generate_text_embeddings.py::_run_embeddings
  called_by:
  - graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  - graphrag/index/workflows/update_text_embeddings.py::run_workflow
- node_id: graphrag/index/workflows/create_base_text_units.py::create_base_text_units
  file: graphrag/index/workflows/create_base_text_units.py
  name: create_base_text_units
  signature: "def create_base_text_units(\n    documents: pd.DataFrame,\n    callbacks:\
    \ WorkflowCallbacks,\n    group_by_columns: list[str],\n    size: int,\n    overlap:\
    \ int,\n    encoding_model: str,\n    strategy: ChunkStrategyType,\n    prepend_metadata:\
    \ bool = False,\n    chunk_size_includes_metadata: bool = False,\n) -> pd.DataFrame"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "\"\"\"Converts input documents into base text units by grouping, chunking,\
    \ and optional metadata preprocessing.\n\nArgs:\n    documents (pd.DataFrame):\
    \ Input table containing documents. Expected to contain at least the columns \"\
    id\" and \"text\". May also include optional \"metadata\".\n    callbacks (WorkflowCallbacks):\
    \ Callbacks used during the chunking process.\n    group_by_columns (list[str]):\
    \ Columns to group documents by before text chunking. If empty, all documents\
    \ are treated as a single group.\n    size (int): Maximum number of tokens per\
    \ chunk (excluding any metadata unless chunk_size_includes_metadata is True).\n\
    \    overlap (int): Number of tokens to overlap between consecutive chunks.\n\
    \    encoding_model (str): Encoding model name used to compute token lengths for\
    \ chunking.\n    strategy (ChunkStrategyType): Strategy used by the underlying\
    \ chunk_text operation.\n    prepend_metadata (bool): If True, prepend the document\
    \ metadata to each generated chunk.\n    chunk_size_includes_metadata (bool):\
    \ If True, metadata is counted towards the per-chunk size. When True, metadata\
    \ tokens are subtracted from size and may raise ValueError if they exceed the\
    \ per-chunk limit.\n\nReturns:\n    pd.DataFrame: A dataframe containing one row\
    \ per chunk. Columns include the grouping keys from group_by_columns, \"id\" (SHA-512\
    \ hash of the chunk), \"text\" (the chunk text), \"document_ids\" (list of document\
    \ ids contributing to the chunk), and \"n_tokens\" (token length of the chunk).\
    \ The exact set of columns may also include the original grouping keys.\n\nRaises:\n\
    \    ValueError: If prepend_metadata is enabled and chunk_size_includes_metadata\
    \ is True and the computed metadata token length exceeds the per-chunk size.\n\
    \nNotes:\n    The function logs progress during processing. It sorts documents\
    \ by id, aggregates text with ids, chunks large text units into smaller chunks,\
    \ optionally prepends metadata to chunks, and computes a stable hash id for each\
    \ chunk.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 53
  line_end: 163
  dependencies:
  - graphrag/index/utils/hashing.py::gen_sha512_hash
  - graphrag/index/workflows/create_base_text_units.py::chunker_with_logging
  called_by:
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
- node_id: graphrag/index/utils/derive_from_rows.py::derive_from_rows
  file: graphrag/index/utils/derive_from_rows.py
  name: derive_from_rows
  signature: "def derive_from_rows(\n    input: pd.DataFrame,\n    transform: Callable[[pd.Series],\
    \ Awaitable[ItemType]],\n    callbacks: WorkflowCallbacks | None = None,\n   \
    \ num_threads: int = 4,\n    async_type: AsyncType = AsyncType.AsyncIO,\n    progress_msg:\
    \ str = \"\",\n) -> list[ItemType | None]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Apply a generic transform function to each row in the input DataFrame.\n\
    \n Any errors raised by the transform will be reported and thrown.\n\n Args:\n\
    \   input: The input DataFrame to process; each row will be transformed.\n   transform:\
    \ Async function applied to a row's Series to produce an ItemType.\n   callbacks:\
    \ Workflow callbacks; used to report progress and handle events. If None, NoopWorkflowCallbacks\
    \ is used.\n   num_threads: Number of concurrent workers to use.\n   async_type:\
    \ Scheduling type to use for execution (AsyncIO or Threaded). Defaults to AsyncType.AsyncIO.\n\
    \   progress_msg: Optional progress message to display during processing.\n\n\
    \ Returns:\n   list[ItemType | None]: A list containing the transformed results,\
    \ one per input row; entries may be None.\n\n Raises:\n   ValueError: If an unsupported\
    \ scheduling type is provided."
  code_example: null
  example_source: null
  line_start: 34
  line_end: 55
  dependencies:
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows_asyncio_threads
  called_by:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
  - graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
  - graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
- node_id: graphrag/api/query.py::global_search
  file: graphrag/api/query.py
  name: global_search
  signature: "def global_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
    \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    community_level:\
    \ int | None,\n    dynamic_community_selection: bool,\n    response_type: str,\n\
    \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
    \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
    \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a global search and return the full response and context data.\n\
    \nArgs:\n    config: GraphRagConfig. A graphrag configuration (from settings.yaml).\n\
    \    entities: pd.DataFrame. A DataFrame containing the final entities (from entities.parquet).\n\
    \    communities: pd.DataFrame. A DataFrame containing the final communities (from\
    \ communities.parquet).\n    community_reports: pd.DataFrame. A DataFrame containing\
    \ the final community reports (from community_reports.parquet).\n    community_level:\
    \ int | None. The community level to search at.\n    dynamic_community_selection:\
    \ bool. Enable dynamic community selection instead of using all community reports\
    \ at a fixed level. Note that you can still provide a community_level to cap the\
    \ maximum level to search.\n    response_type: str. The type of response to return.\n\
    \    query: str. The user query to search for.\n    callbacks: list[QueryCallbacks]\
    \ | None. Optional list of QueryCallbacks to be invoked during the search.\n \
    \   verbose: bool. Enable verbose logging during the search.\n\nReturns:\n   \
    \ tuple[str | dict[str, Any] | list[dict[str, Any]], str | list[pd.DataFrame]\
    \ | dict[str, pd.DataFrame]]: \n        The first element is the aggregated response\
    \ produced by the search. This can be a string or a structured object (e.g., dict\
    \ or list) depending on response_type.\n        The second element is the context\
    \ data captured during the search. This may be a string, a list of DataFrames,\
    \ or a dictionary of DataFrames depending on how context was populated.\n\nRaises:\n\
    \    pydantic.ValidationError: If input arguments fail type validation.\n    Exception:\
    \ If an unexpected error occurs during streaming or processing."
  code_example: null
  example_source: null
  line_start: 64
  line_end: 124
  dependencies:
  - graphrag/api/query.py::global_search_streaming
  - graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  called_by:
  - graphrag/api/query.py::multi_index_global_search
- node_id: graphrag/index/workflows/create_communities.py::create_communities
  file: graphrag/index/workflows/create_communities.py
  name: create_communities
  signature: "def create_communities(\n    entities: pd.DataFrame,\n    relationships:\
    \ pd.DataFrame,\n    max_cluster_size: int,\n    use_lcc: bool,\n    seed: int\
    \ | None = None,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Create final communities from entities and relationships using graph-based\
    \ clustering and metadata enrichment.\n\nThis function builds a graph from the\
    \ provided relationships, performs Leiden-based clustering to group entities into\
    \ hierarchical communities, and then aggregates related entities and relationships\
    \ into a final, metadata-rich DataFrame. The result is aligned to the column schema\
    \ defined by COMMUNITIES_FINAL_COLUMNS and is suitable for storage and downstream\
    \ processing.\n\nArgs:\n    entities (pd.DataFrame): DataFrame containing entities.\
    \ Must include at least:\n        - title (str): The display title used to link\
    \ to relationships\n        - id (str): The unique identifier for the entity\n\
    \    relationships (pd.DataFrame): DataFrame containing relationships. Must include:\n\
    \        - source (str): Title of the source entity\n        - target (str): Title\
    \ of the target entity\n        - id (str): Unique identifier for the relationship\n\
    \        - text_unit_ids (list): Identifiers of the text units associated with\
    \ the relationship\n        - weight (float, optional): Edge weight used when\
    \ constructing the graph\n    max_cluster_size (int): Maximum allowed size for\
    \ a cluster produced by the clustering step.\n    use_lcc (bool): If True, operate\
    \ on the largest connected component of the input graph.\n    seed (int | None,\
    \ optional): Random seed for reproducibility of the clustering process.\n\nReturns:\n\
    \    pd.DataFrame: Final communities DataFrame containing metadata and ready for\
    \ storage. The exact\n    columns are defined by COMMUNITIES_FINAL_COLUMNS and\
    \ include fields such as identifiers (id,\n    human_readable_id, title), hierarchical\
    \ and grouping fields (community, level, parent, children),\n    membership aggregations\
    \ (entity_ids, relationship_ids, text_unit_ids), and update-tracking fields\n\
    \    (period, size).\n\nRaises:\n    ValueError: If required input columns are\
    \ missing or inputs have invalid types.\n    KeyError: If expected keys are missing\
    \ during processing (indicative of unexpected input shape)."
  code_example: null
  example_source: null
  line_start: 54
  line_end: 156
  dependencies:
  - graphrag/index/operations/cluster_graph.py::cluster_graph
  - graphrag/index/operations/create_graph.py::create_graph
  called_by:
  - graphrag/index/workflows/create_communities.py::run_workflow
- node_id: graphrag/index/workflows/finalize_graph.py::finalize_graph
  file: graphrag/index/workflows/finalize_graph.py
  name: finalize_graph
  signature: "def finalize_graph(\n    entities: pd.DataFrame,\n    relationships:\
    \ pd.DataFrame,\n    embed_config: EmbedGraphConfig | None = None,\n    layout_enabled:\
    \ bool = False,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Finalize the entity and relationship formats by applying the finalization\
    \ steps.\n\nArgs:\n    entities (pd.DataFrame): Input entities to be transformed\
    \ into final records.\n    relationships (pd.DataFrame): DataFrame containing\
    \ edge information used to finalize relationships.\n    embed_config (EmbedGraphConfig\
    \ | None): Optional configuration for embedding graphs; passed to finalization.\n\
    \    layout_enabled (bool): If True, enables applying a layout during finalization.\n\
    \nReturns:\n    tuple[pd.DataFrame, pd.DataFrame]: A tuple containing the final_entities\
    \ DataFrame and final_relationships DataFrame.\n\nRaises:\n    Propagates exceptions\
    \ raised by finalize_entities or finalize_relationships."
  code_example: null
  example_source: null
  line_start: 65
  line_end: 76
  dependencies:
  - graphrag/index/operations/finalize_entities.py::finalize_entities
  - graphrag/index/operations/finalize_relationships.py::finalize_relationships
  called_by:
  - graphrag/index/workflows/finalize_graph.py::run_workflow
- node_id: graphrag/index/workflows/update_entities_relationships.py::run_workflow
  file: graphrag/index/workflows/update_entities_relationships.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the entities and relationships from a incremental index run.\n\
    \nArgs:\n    config: GraphRagConfig\n        GraphRagConfig containing configuration\
    \ for the workflow.\n    context: PipelineRunContext\n        PipelineRunContext\
    \ carrying the state for the run, including update_timestamp.\n\nReturns:\n  \
    \  WorkflowFunctionOutput\n        The output of the workflow function.\n\nRaises:\n\
    \    KeyError\n        If 'update_timestamp' is not present in context.state."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 53
  dependencies:
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/update_entities_relationships.py::_update_entities_and_relationships
  called_by: []
- node_id: graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_level_context
  file: graphrag/index/operations/summarize_communities/graph_context/context_builder.py
  name: build_level_context
  signature: "def build_level_context(\n    report_df: pd.DataFrame | None,\n    community_hierarchy_df:\
    \ pd.DataFrame,\n    local_context_df: pd.DataFrame,\n    tokenizer: Tokenizer,\n\
    \    level: int,\n    max_context_tokens: int,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prepare context for each community at a given level.\n\nThis function\
    \ selects the communities at the specified level from local_context_df, then classifies\
    \ their local contexts into valid (CONTEXT_EXCEED_FLAG is False) and invalid (CONTEXT_EXCEED_FLAG\
    \ is True) records. It returns a DataFrame containing the prepared context for\
    \ that level, including the contextual strings and metadata required for downstream\
    \ processing. Note that this function may create or update columns such as CONTEXT_STRING,\
    \ CONTEXT_SIZE, and CONTEXT_EXCEED_FLAG in the resulting DataFrame. The level\
    \ scoping ensures only communities at the given level are processed.\n\nProcessing\
    \ flow by branch:\n- Early return when there are no invalid contexts: if invalid_context_df\
    \ is empty, returns valid_context_df unchanged.\n- No available reports (report_df\
    \ is None or empty): for all invalid contexts, the context string is trimmed to\
    \ fit max_context_tokens via _sort_and_trim_context, CONTEXT_SIZE is computed,\
    \ CONTEXT_EXCEED_FLAG is set to False, and the function returns the union of valid_context_df\
    \ and the trimmed invalid contexts.\n- Reports are available: remove the observed\
    \ reports from level_context_df (level_context_df = _antijoin_reports(level_context_df,\
    \ report_df)); for each remaining invalid context, attempt substitution with sub-community\
    \ reports by computing sub_context_df and building community_df; any remaining\
    \ invalids not covered by sub-communities are collected in remaining_df, trimmed\
    \ via _sort_and_trim_context, and finally all parts are united via union(valid_context_df,\
    \ community_df, remaining_df). The resulting CONTEXT_SIZE is computed from CONTEXT_STRING,\
    \ and CONTEXT_EXCEED_FLAG is set to False in the result.\n\nReturns:\n    pd.DataFrame:\
    \ A DataFrame containing prepared contexts for communities at the specified level.\
    \ The rows include CONTEXT_STRING and metadata columns such as CONTEXT_SIZE and\
    \ CONTEXT_EXCEED_FLAG; the output is filtered to the given level.\n\nRaises:\n\
    \    Propagated exceptions from helper utilities may be raised (e.g., KeyError\
    \ if required columns are missing); the function does not raise its own explicit\
    \ errors."
  code_example: null
  example_source: null
  line_start: 191
  line_end: 261
  dependencies:
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_antijoin_reports
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_community_df
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_get_subcontext_df
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::_sort_and_trim_context
  - graphrag/index/utils/dataframes.py::union
  called_by: []
- node_id: graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  file: graphrag/index/workflows/generate_text_embeddings.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generates text embeddings for the configured fields and returns the\
    \ results. This workflow loads input data conditionally based on the configured\
    \ embed_text fields, invokes the embedding generation, and optionally persists\
    \ the resulting embeddings as snapshots to storage.\n\nArgs:\n  config: GraphRagConfig\n\
    \      GraphRagConfig containing embed_text and snapshot settings used by the\
    \ workflow.\n  context: PipelineRunContext\n      PipelineRunContext providing\
    \ output_storage, callbacks, and cache.\n\nReturns:\n  WorkflowFunctionOutput\n\
    \      The workflow result containing generated embeddings as a mapping from embedding\
    \ name to DataFrame.\n\nRaises:\n  ValueError\n      Could not find {name}.parquet\
    \ in storage!\n  Exception\n      Exceptions raised by the storage backend or\
    \ parquet reader during the load operation, or by the storage backend during the\
    \ write operation when snapshots are enabled."
  code_example: null
  example_source: null
  line_start: 35
  line_end: 93
  dependencies:
  - graphrag/config/get_embedding_settings.py::get_embedding_settings
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
- node_id: graphrag/index/workflows/update_text_embeddings.py::run_workflow
  file: graphrag/index/workflows/update_text_embeddings.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Update the text embeddings from an incremental index run.\n\nArgs:\n\
    \  config: GraphRagConfig containing configuration for embedding and storage behavior.\n\
    \  context: PipelineRunContext carrying the state for the run, including update_timestamp\
    \ and incremental update data. The function reads the following keys from context.state:\
    \ 'update_timestamp', 'incremental_update_final_documents', 'incremental_update_merged_relationships',\
    \ 'incremental_update_merged_text_units', 'incremental_update_merged_entities',\
    \ and 'incremental_update_merged_community_reports'. It also uses context.callbacks\
    \ and context.cache.\n\nReturns:\n  WorkflowFunctionOutput: A WorkflowFunctionOutput\
    \ with result=None.\n\nRaises:\n  KeyError: If required keys are missing from\
    \ context.state (for example, update_timestamp or any incremental_update_* keys).\n\
    \nNotes:\n  The function calls get_update_storages(config, context.state['update_timestamp'])\
    \ to obtain storage backends, and then generate_text_embeddings with the incremental\
    \ update data to produce embeddings.\n  If config.snapshots.embeddings is True,\
    \ the resulting embedding tables are written to storage using write_table_to_storage\
    \ with names embeddings.<name>.\n\nState requirements:\n  context.state must include:\n\
    \    update_timestamp (str)\n    incremental_update_final_documents\n    incremental_update_merged_relationships\n\
    \    incremental_update_merged_text_units\n    incremental_update_merged_entities\n\
    \    incremental_update_merged_community_reports"
  code_example: null
  example_source: null
  line_start: 19
  line_end: 59
  dependencies:
  - graphrag/config/get_embedding_settings.py::get_embedding_settings
  - graphrag/index/run/utils.py::get_update_storages
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/generate_text_embeddings.py::generate_text_embeddings
  - graphrag/utils/storage.py::write_table_to_storage
  called_by: []
- node_id: graphrag/index/workflows/create_base_text_units.py::run_workflow
  file: graphrag/index/workflows/create_base_text_units.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the base text units workflow to transform documents into base text\
    \ units by loading documents from storage, chunking them, and writing the resulting\
    \ text units back to storage.\n\nArgs:\n  config (GraphRagConfig): GraphRag configuration\
    \ for the workflow, including chunking settings such as group_by_columns, size,\
    \ overlap, encoding_model, strategy, prepend_metadata, and chunk_size_includes_metadata.\n\
    \  context (PipelineRunContext): Pipeline run context containing the output storage\
    \ and callbacks used for loading input and writing output.\n\nReturns:\n  WorkflowFunctionOutput:\
    \ The workflow output containing the produced text_units DataFrame.\n\nRaises:\n\
    \  ValueError: Could not find documents.parquet in storage!\n  Exception: Exceptions\
    \ raised by the storage backend or parquet reader during the load or write operations\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 50
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_base_text_units.py::create_base_text_units
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  - tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
- node_id: graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
  file: graphrag/prompt_tune/loader/input.py
  name: load_docs_in_chunks
  signature: "def load_docs_in_chunks(\n    config: GraphRagConfig,\n    select_method:\
    \ DocSelectionType,\n    limit: int,\n    logger: logging.Logger,\n    chunk_size:\
    \ int,\n    overlap: int,\n    n_subset_max: int = N_SUBSET_MAX,\n    k: int =\
    \ K,\n) -> list[str]"
  decorators: []
  raises:
  - ValueError
  visibility: public
  docstring: "Load documents into chunks for generating prompts.\n\nLoad documents\
    \ from the configured input, convert them into base text units according\nto the\
    \ chunking configuration, and optionally sample or embed chunks to meet the\n\
    requested selection method. The function returns a list of chunk texts, with braces\n\
    escaped to avoid issues with Python's str.format when parsing LaTeX in markdown.\n\
    \nArgs:\n    config: GraphRagConfig The overall configuration for the graph-based\
    \ RAG pipeline, including input sources and chunking options.\n    select_method:\
    \ DocSelectionType The strategy to select chunks: TOP, RANDOM, or AUTO.\n    limit:\
    \ int Maximum number of chunks to return. If out of range, a default is used.\n\
    \    logger: logging.Logger Logger used to emit warnings and information during\
    \ processing.\n    chunk_size: int The size of each chunk.\n    overlap: int The\
    \ amount of overlap between consecutive chunks.\n    n_subset_max: int Maximum\
    \ number of chunks to sample when using AUTO selection (default N_SUBSET_MAX).\n\
    \    k: int Number of chunks to select when using AUTO (must be > 0 when AUTO\
    \ is chosen).\n\nReturns:\n    list[str] A list containing the chunk texts. Each\
    \ text has braces escaped by doubling\n    braces to prevent the str.format parser\
    \ from interpreting LaTeX or other content.\n\nRaises:\n    ValueError: If select_method\
    \ is DocSelectionType.AUTO and k is not a positive integer."
  code_example: null
  example_source: null
  line_start: 41
  line_end: 108
  dependencies:
  - graphrag.cache.noop_pipeline_cache::NoopPipelineCache
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/index/input/factory.py::create_input
  - graphrag/index/workflows/create_base_text_units.py::create_base_text_units
  - graphrag/prompt_tune/loader/input.py::_sample_chunks_from_embeddings
  - graphrag/utils/api.py::create_storage_from_config
  called_by:
  - graphrag/api/prompt_tune.py::generate_indexing_prompts
- node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
  file: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  name: _extract_nodes
  signature: "def _extract_nodes(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
    \ BaseNounPhraseExtractor,\n    num_threads: int = 4,\n    async_mode: AsyncType\
    \ = AsyncType.Threaded,\n    cache: PipelineCache | None = None,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Extract noun-phrase nodes from text units.\n\nThis internal helper asynchronously\
    \ extracts noun phrases from each text unit and aggregates them into a node DataFrame.\
    \ It does not create edges; edge creation is handled by _extract_edges elsewhere.\n\
    \nArgs:\n  text_unit_df (pd.DataFrame): Input text units with schema [id, text].\n\
    \  text_analyzer (BaseNounPhraseExtractor): Noun-phrase extractor used to determine\
    \ noun phrases from text.\n  num_threads (int): Number of worker threads to use\
    \ for parallel processing.\n  async_mode (AsyncType): Scheduling type to use for\
    \ asynchronous operations.\n  cache (PipelineCache | None): Optional cache for\
    \ intermediate results; if None, NoopPipelineCache is used.\n\nReturns:\n  pd.DataFrame:\
    \ DataFrame with columns [title, frequency, text_unit_ids].\n\nRaises:\n  Exceptions\
    \ raised by derive_from_rows and text_analyzer.extract propagate to the caller.\n\
    \nNotes:\n  This function is focused on noun-phrase extraction only; edge construction\
    \ is performed by _extract_edges elsewhere."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 89
  dependencies:
  - graphrag.cache.noop_pipeline_cache::NoopPipelineCache
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
  called_by:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
- node_id: graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
  file: graphrag/index/operations/extract_covariates/extract_covariates.py
  name: extract_covariates
  signature: "def extract_covariates(\n    input: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    column: str,\n    covariate_type: str,\n    strategy:\
    \ dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n   \
    \ entity_types: list[str] | None = None,\n    num_threads: int = 4,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Process a DataFrame by applying covariate extraction to the text in\
    \ a specified column for each row, returning covariate rows as a DataFrame.\n\n\
    Args:\n  input: The input DataFrame to process.\n  callbacks: Workflow callbacks\
    \ for progress reporting and events.\n  cache: Cache to use for model and data\
    \ storage.\n  column: The name of the column containing text to extract covariates\
    \ from.\n  covariate_type: The type/tag to assign to produced covariate rows.\n\
    \  strategy: Strategy configuration for covariate extraction; if None, an empty\
    \ configuration is used.\n  async_mode: Asynchronous mode to use when applying\
    \ the strategy (default: AsyncType.AsyncIO).\n  entity_types: Entity types to\
    \ consider when extracting covariates; defaults to DEFAULT_ENTITY_TYPES if None.\n\
    \  num_threads: Number of concurrent workers to use.\n\nReturns:\n  A DataFrame\
    \ containing the extracted covariate rows, where each input row may contribute\
    \ zero or more covariate rows. Each covariate row includes the original row fields,\
    \ covariate data fields, and a covariate_type column.\n\nRaises:\n  Propagates\
    \ exceptions raised by underlying operations (e.g., derive_from_rows, run_extract_claims)."
  code_example: null
  example_source: null
  line_start: 32
  line_end: 76
  dependencies:
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
  called_by:
  - graphrag/index/workflows/extract_covariates.py::run_workflow
- node_id: graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
  file: graphrag/index/operations/extract_graph/extract_graph.py
  name: extract_graph
  signature: "def extract_graph(\n    text_units: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    text_column: str,\n    id_column: str,\n    strategy:\
    \ dict[str, Any] | None,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n   \
    \ entity_types=DEFAULT_ENTITY_TYPES,\n    num_threads: int = 4,\n) -> tuple[pd.DataFrame,\
    \ pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Extract a graph from a piece of text using a language model.\n\nArgs:\n\
    \    text_units: The input DataFrame containing the text data to process.\n  \
    \  callbacks: WorkflowCallbacks used for progress reporting and event handling.\n\
    \    cache: PipelineCache instance used for caching results.\n    text_column:\
    \ Name of the column in text_units that contains the text to analyze.\n    id_column:\
    \ Name of the column in text_units that contains a unique identifier for each\
    \ row.\n    strategy: Strategy configuration dictionary; may be None to use defaults.\n\
    \    async_mode: AsyncType controlling how tasks are scheduled.\n    entity_types:\
    \ List of entity types to extract; if None, defaults to DEFAULT_ENTITY_TYPES.\n\
    \    num_threads: Number of worker threads to use for processing.\n\nReturns:\n\
    \    tuple[pd.DataFrame, pd.DataFrame]: A pair of DataFrames: the first contains\
    \ aggregated entities and the second contains aggregated relationships.\n\nRaises:\n\
    \    ValueError: If an unknown strategy type is provided."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 82
  dependencies:
  - graphrag/index/operations/extract_graph/extract_graph.py::_load_strategy
  - graphrag/index/operations/extract_graph/extract_graph.py::_merge_entities
  - graphrag/index/operations/extract_graph/extract_graph.py::_merge_relationships
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
  called_by:
  - graphrag/index/workflows/extract_graph.py::run_workflow
- node_id: graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
  file: graphrag/index/operations/summarize_communities/summarize_communities.py
  name: summarize_communities
  signature: "def summarize_communities(\n    nodes: pd.DataFrame,\n    communities:\
    \ pd.DataFrame,\n    local_contexts,\n    level_context_builder: Callable,\n \
    \   callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    strategy: dict,\n\
    \    tokenizer: Tokenizer,\n    max_input_length: int,\n    async_mode: AsyncType\
    \ = AsyncType.AsyncIO,\n    num_threads: int = 4,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Generate community summaries across all levels and return a DataFrame\
    \ of CommunityReport records.\n\nArgs:\n    nodes: pd.DataFrame\n        DataFrame\
    \ containing node data used to determine levels and contexts.\n    communities:\
    \ pd.DataFrame\n        DataFrame containing community definitions and hierarchical\
    \ relationships.\n    local_contexts:\n        Local context data used to build\
    \ level contexts; passed to level_context_builder.\n    level_context_builder:\
    \ Callable\n        Function used to construct context objects for each level.\n\
    \    callbacks: WorkflowCallbacks\n        Callbacks for progress reporting and\
    \ other workflow events.\n    cache: PipelineCache\n        Cache to store intermediate\
    \ results during report generation.\n    strategy: dict\n        Strategy configuration\
    \ for report generation; expects a 'type' key to select the strategy.\n    tokenizer:\
    \ Tokenizer\n        Tokenizer used to process text during context construction.\n\
    \    max_input_length: int\n        Maximum number of tokens allowed in the combined\
    \ input for generation.\n    async_mode: AsyncType\n        Async scheduling mode\
    \ to use (default AsyncType.AsyncIO).\n    num_threads: int\n        Number of\
    \ concurrent worker threads to use.\n\nReturns:\n    pd.DataFrame\n        DataFrame\
    \ of generated CommunityReport records. Each row corresponds to a\n        report\
    \ for a specific community at a specific level; columns match the fields\n   \
    \     defined by the CommunityReport type.\n\nNotes:\n    - Strategy loading is\
    \ performed dynamically via load_strategy based on strategy['type'];\n      an\
    \ unsupported type will be resolved at runtime by the strategy loader."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 94
  dependencies:
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/index/operations/summarize_communities/summarize_communities.py::load_strategy
  - graphrag/index/operations/summarize_communities/utils.py::get_levels
  - graphrag/index/utils/derive_from_rows.py::derive_from_rows
  - graphrag/logger/progress.py::progress_ticker
  called_by:
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
- node_id: graphrag/api/query.py::multi_index_global_search
  file: graphrag/api/query.py
  name: multi_index_global_search
  signature: "def multi_index_global_search(\n    config: GraphRagConfig,\n    entities_list:\
    \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
    \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int |\
    \ None,\n    dynamic_community_selection: bool,\n    response_type: str,\n   \
    \ streaming: bool,\n    query: str,\n    callbacks: list[QueryCallbacks] | None\
    \ = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str, Any] |\
    \ list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
    ]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Perform a global search across multiple indexes and return the results\
    \ and associated context data.\n\nDeprecated: Multi-index global search is deprecated\
    \ and will be removed in GraphRAG v3.\n\nParameters\n    config: GraphRagConfig.\
    \ A graphrag configuration (from settings.yaml).\n    entities_list: list[pd.DataFrame].\
    \ A list of DataFrames containing the final entities (from entities.parquet).\n\
    \    communities_list: list[pd.DataFrame]. A list of DataFrames containing the\
    \ final communities (from communities.parquet).\n    community_reports_list: list[pd.DataFrame].\
    \ A list of DataFrames containing the final community reports (from community_reports.parquet).\n\
    \    index_names: list[str]. A list of index names.\n    community_level: int\
    \ | None. The community level to search at.\n    dynamic_community_selection:\
    \ bool. Enable dynamic community selection instead of using all community reports\
    \ at a fixed level. Note that you can still provide a community_level cap to the\
    \ maximum level to search.\n    response_type: str. The type of response to return.\n\
    \    streaming: bool. Whether to stream the results or not. Streaming is currently\
    \ not supported and will raise NotImplementedError.\n    query: str. The user\
    \ query to search for.\n    callbacks: list[QueryCallbacks] | None. Optional callbacks\
    \ to handle streaming results or events.\n    verbose: bool. Verbose logging.\n\
    \nReturns\n    tuple[\n        str | dict[str, Any] | list[dict[str, Any]],\n\
    \        str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n    ]\n    The first\
    \ element is the search response payload, which may be a plain string, a structured\
    \ payload (dict), or a list of dicts depending on the response_type. The second\
    \ element is the updated context payload, which can be a string, a list of DataFrames,\
    \ or a mapping of keys to DataFrames representing the contextual data for the\
    \ response.\n\nRaises\n    NotImplementedError: If streaming is requested (streaming\
    \ == True).\n\nExample\n    Typical usage:\n    result, context = await multi_index_global_search(\n\
    \        config=config,\n        entities_list=entities_list,\n        communities_list=communities_list,\n\
    \        community_reports_list=community_reports_list,\n        index_names=index_names,\n\
    \        community_level=community_level,\n        dynamic_community_selection=dynamic_community_selection,\n\
    \        response_type=\"default\",\n        streaming=False,\n        query=\"\
    example query\",\n        callbacks=None,\n        verbose=False,\n    )\n\n \
    \   Streaming usage (not implemented):\n        streaming=True currently raises\
    \ NotImplementedError."
  code_example: null
  example_source: null
  line_start: 196
  line_end: 338
  dependencies:
  - graphrag/api/query.py::global_search
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  - graphrag/utils/api.py::update_context_data
  called_by: []
- node_id: graphrag/index/workflows/create_communities.py::run_workflow
  file: graphrag/index/workflows/create_communities.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the create_communities workflow to generate final communities from\
    \ input entities and relationships.\n\nArgs:\n  config: GraphRagConfig containing\
    \ cluster_graph settings used by the workflow.\n  context: PipelineRunContext\
    \ providing storage context for input and output data.\n\nReturns:\n  WorkflowFunctionOutput:\
    \ The workflow output which includes the resulting communities.\n\nRaises:\n \
    \ ValueError: Could not find the required parquet files in storage when loading\
    \ inputs (entities or relationships).\n  Exception: Exceptions raised by the storage\
    \ backend or by downstream processing may propagate."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 51
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_communities.py::create_communities
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_communities.py::test_create_communities
- node_id: graphrag/index/workflows/finalize_graph.py::run_workflow
  file: graphrag/index/workflows/finalize_graph.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: Run the finalize_graph workflow to finalize the entity and relationship
    data, persist updates to storage, and optionally snapshot GraphML graphs.
  code_example: null
  example_source: null
  line_start: 23
  line_end: 62
  dependencies:
  - graphrag/index/operations/create_graph.py::create_graph
  - graphrag/index/operations/snapshot_graphml.py::snapshot_graphml
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/finalize_graph.py::finalize_graph
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_finalize_graph.py::test_finalize_graph
  - tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
- node_id: tests/verbs/test_generate_text_embeddings.py::test_generate_text_embeddings
  file: tests/verbs/test_generate_text_embeddings.py
  name: test_generate_text_embeddings
  signature: def test_generate_text_embeddings()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the generate_text_embeddings workflow using a mock embedding model\
    \ and validate produced embeddings.\n\nThis test creates a test context with several\
    \ storage tables, configures the graphrag embedding to use a mock embedding model,\n\
    runs the generate_text_embeddings workflow, and asserts that:\n- For every embedding\
    \ field, a parquet named embeddings.{field}.parquet exists in the output storage\n\
    - embeddings.entity.description has exactly two columns: id and embedding\n- embeddings.document.text\
    \ has exactly two columns: id and embedding\n\nThe test relies on utilities: create_test_context,\
    \ create_graphrag_config, run_workflow, and load_table_from_storage.\n\nArgs:\n\
    \    None\n\nReturns:\n    None\n\nRaises:\n    Exception: Exceptions raised by\
    \ underlying test utilities or storage operations may propagate."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 68
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/generate_text_embeddings.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::create_test_context
  called_by: []
- node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units
  file: tests/verbs/test_create_base_text_units.py
  name: test_create_base_text_units
  signature: def test_create_base_text_units()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the creation of base text units by running the workflow and validating\
    \ the produced text_units table against the expected test data.\n\nReturns:\n\
    \    None. This test does not return a value; it asserts correctness by comparing\
    \ actual output to expected data.\n\nRaises:\n    Exception: Exceptions raised\
    \ by the underlying test utilities may propagate to the caller."
  code_example: null
  example_source: null
  line_start: 17
  line_end: 28
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata
  file: tests/verbs/test_create_base_text_units.py
  name: test_create_base_text_units_metadata
  signature: def test_create_base_text_units_metadata()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test for creating base text units with metadata.\n\nArgs:\n\
    \    None: This test does not take any parameters.\n\nReturns:\n    None\n\nRaises:\n\
    \    Exception: Exceptions raised by the underlying test utilities may propagate\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 47
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  - tests/verbs/util.py::update_document_metadata
  called_by: []
- node_id: tests/verbs/test_create_base_text_units.py::test_create_base_text_units_metadata_included_in_chunk
  file: tests/verbs/test_create_base_text_units.py
  name: test_create_base_text_units_metadata_included_in_chunk
  signature: def test_create_base_text_units_metadata_included_in_chunk()
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronous test for creating base text units with metadata included\
    \ in a chunk when chunk_size_includes_metadata is True and metadata is prepended.\n\
    \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    Exception:\
    \ Exceptions raised by the underlying test utilities or storage backends may propagate\
    \ to the caller."
  code_example: null
  example_source: null
  line_start: 50
  line_end: 68
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_base_text_units.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  - tests/verbs/util.py::update_document_metadata
  called_by: []
- node_id: graphrag/api/prompt_tune.py::generate_indexing_prompts
  file: graphrag/api/prompt_tune.py
  name: generate_indexing_prompts
  signature: "def generate_indexing_prompts(\n    config: GraphRagConfig,\n    chunk_size:\
    \ PositiveInt = graphrag_config_defaults.chunks.size,\n    overlap: Annotated[\n\
    \        int, annotated_types.Gt(-1)\n    ] = graphrag_config_defaults.chunks.overlap,\n\
    \    limit: PositiveInt = 15,\n    selection_method: DocSelectionType = DocSelectionType.RANDOM,\n\
    \    domain: str | None = None,\n    language: str | None = None,\n    max_tokens:\
    \ int = MAX_TOKEN_COUNT,\n    discover_entity_types: bool = True,\n    min_examples_required:\
    \ PositiveInt = 2,\n    n_subset_max: PositiveInt = 300,\n    k: PositiveInt =\
    \ 15,\n    verbose: bool = False,\n) -> tuple[str, str, str]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Generate indexing prompts.\n\nParameters\n----------\nconfig: GraphRagConfig\n\
    \    The GraphRag configuration.\nchunk_size: PositiveInt\n    The chunk token\
    \ size to use for input text units.\noverlap: Annotated[int, annotated_types.Gt(-1)]\n\
    \    The number of tokens to overlap between consecutive chunks (must be greater\
    \ than -1).\nlimit: PositiveInt\n    The limit of chunks to load.\nselection_method:\
    \ DocSelectionType\n    The chunk selection method.\ndomain: str | None\n    The\
    \ domain to map the input documents to.\nlanguage: str | None\n    The language\
    \ to use for the prompts.\nmax_tokens: int\n    The maximum number of tokens to\
    \ use on entity extraction prompts.\ndiscover_entity_types: bool\n    Generate\
    \ entity types.\nmin_examples_required: PositiveInt\n    The minimum number of\
    \ examples required for entity extraction prompts.\nn_subset_max: PositiveInt\n\
    \    The number of text chunks to embed when using auto selection method.\nk:\
    \ PositiveInt\n    The number of documents to select when using auto selection\
    \ method.\nverbose: bool\n    Whether to enable verbose logging.\n\nReturns\n\
    -------\ntuple[str, str, str]\n    entity extraction prompt, entity summarization\
    \ prompt, community summarization prompt\n\nRaises\n------\nValidationError\n\
    \    If input arguments fail validation (thrown by pydantic\u2019s validate_call).\n\
    RuntimeError\n    If any underlying operation (loading docs, language model calls,\
    \ or prompt generation steps) fails."
  code_example: null
  example_source: null
  line_start: 56
  line_end: 202
  dependencies:
  - graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  - graphrag/language_model/manager.py::ModelManager
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/prompt_tune/generator/community_report_rating.py::generate_community_report_rating
  - graphrag/prompt_tune/generator/community_report_summarization.py::create_community_summarization_prompt
  - graphrag/prompt_tune/generator/community_reporter_role.py::generate_community_reporter_role
  - graphrag/prompt_tune/generator/domain.py::generate_domain
  - graphrag/prompt_tune/generator/entity_relationship.py::generate_entity_relationship_examples
  - graphrag/prompt_tune/generator/entity_summarization_prompt.py::create_entity_summarization_prompt
  - graphrag/prompt_tune/generator/entity_types.py::generate_entity_types
  - graphrag/prompt_tune/generator/extract_graph_prompt.py::create_extract_graph_prompt
  - graphrag/prompt_tune/generator/language.py::detect_language
  - graphrag/prompt_tune/generator/persona.py::generate_persona
  - graphrag/prompt_tune/loader/input.py::load_docs_in_chunks
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by: []
- node_id: graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
  file: graphrag/index/operations/build_noun_graph/build_noun_graph.py
  name: build_noun_graph
  signature: "def build_noun_graph(\n    text_unit_df: pd.DataFrame,\n    text_analyzer:\
    \ BaseNounPhraseExtractor,\n    normalize_edge_weights: bool,\n    num_threads:\
    \ int = 4,\n    async_mode: AsyncType = AsyncType.Threaded,\n    cache: PipelineCache\
    \ | None = None,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Build a noun graph from text units.\n\nArgs:\n    text_unit_df (pd.DataFrame):\
    \ Input text units with at least columns 'id' and 'text'; used to extract noun\
    \ phrases for graph construction.\n    text_analyzer (BaseNounPhraseExtractor):\
    \ Noun-phrase extractor used to determine noun phrases from text.\n    normalize_edge_weights\
    \ (bool): If True, compute PMI-based weights for edges; otherwise use raw co-occurrence\
    \ counts.\n    num_threads (int): Number of worker threads to use for parallel\
    \ processing.\n    async_mode (AsyncType): Async processing mode to use (e.g.,\
    \ Threaded).\n    cache (PipelineCache | None): Optional cache for results; if\
    \ None, a NoopPipelineCache is used.\n\nReturns:\n    tuple[pd.DataFrame, pd.DataFrame]:\
    \ A pair of DataFrames (nodes_df, edges_df) representing extracted noun nodes\
    \ and their connecting edges (with optional weights)."
  code_example: null
  example_source: null
  line_start: 22
  line_end: 40
  dependencies:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_edges
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::_extract_nodes
  called_by:
  - graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
- node_id: graphrag/index/workflows/extract_covariates.py::run_workflow
  file: graphrag/index/workflows/extract_covariates.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the covariates extraction workflow.\n\nThis asynchronous workflow\
    \ performs the following steps:\n- If extraction of claims is enabled in the provided\
    \ config:\n  - Load text_units from storage using the context's output storage.\n\
    \  - Get language model settings for the configured claims model and resolve the\
    \ extraction strategy.\n  - Execute extract_covariates with the prepared inputs\
    \ and write the resulting covariates to storage under the name covariates.\n-\
    \ If extraction is disabled:\n  - Skip loading, extraction, and writing; output\
    \ remains None.\n\nThe function returns a WorkflowFunctionOutput whose result\
    \ is the covariates DataFrame when extraction occurred, or None when extraction\
    \ was skipped.\n\nArgs:\n  config (GraphRagConfig): GraphRag configuration for\
    \ the covariates extraction workflow.\n  context (PipelineRunContext): Pipeline\
    \ run context containing the output storage, callbacks, and cache used by the\
    \ workflow.\n\nReturns:\n  WorkflowFunctionOutput: The workflow output wrapper\
    \ containing the covariates DataFrame, or None if extraction was skipped.\n\n\
    Raises:\n  ValueError: Could not find required parquet files in storage, or other\
    \ storage load errors.\n  Exception: Exceptions raised by the storage backend\
    \ or the covariate extraction process may propagate during the workflow.\n\nNote:\n\
    \  The function may propagate exceptions from storage or extraction; these are\
    \ not caught within the workflow."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 61
  dependencies:
  - graphrag/index/operations/extract_covariates/extract_covariates.py::extract_covariates
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_extract_covariates.py::test_extract_covariates
- node_id: graphrag/index/workflows/extract_graph.py::run_workflow
  file: graphrag/index/workflows/extract_graph.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: '"""Asynchronously run the extract_graph workflow to build the base entity
    graph and persist results to storage."""'
  code_example: null
  example_source: null
  line_start: 28
  line_end: 79
  dependencies:
  - graphrag/index/operations/extract_graph/extract_graph.py::extract_graph
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_extract_graph.py::test_extract_graph
- node_id: graphrag/index/workflows/create_community_reports.py::create_community_reports
  file: graphrag/index/workflows/create_community_reports.py
  name: create_community_reports
  signature: "def create_community_reports(\n    edges_input: pd.DataFrame,\n    entities:\
    \ pd.DataFrame,\n    communities: pd.DataFrame,\n    claims_input: pd.DataFrame\
    \ | None,\n    callbacks: WorkflowCallbacks,\n    cache: PipelineCache,\n    summarization_strategy:\
    \ dict,\n    async_mode: AsyncType = AsyncType.AsyncIO,\n    num_threads: int\
    \ = 4,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously transforms input data into finalized community reports\
    \ by preparing nodes and edges, constructing local contexts, performing level-based\
    \ summarization, and finalizing results. The function orchestrates multiple preprocessing\
    \ steps, builds prompts and tokenizer settings, and runs the summarization asynchronously\
    \ before returning the finalized reports.\n\nHigh-level steps:\n- Node and edge\
    \ preparation: explode_communities, _prep_nodes, and _prep_edges; if claims_input\
    \ is provided, _prep_claims is applied to incorporate claims into the context.\n\
    - Context setup: configure extraction_prompt from graph_prompt, initialize LanguageModelConfig\
    \ and tokenizer, determine max_input_length, and build local contexts via build_local_context.\n\
    - Summarization: asynchronously summarize across community levels with summarize_communities\
    \ using the provided callbacks, cache, and strategy, honoring async_mode and num_threads.\n\
    - Finalization: merge and enrich the summarized reports with community metadata\
    \ via finalize_community_reports.\n\nNotes:\n- claims_input is optional; when\
    \ provided, claims data are incorporated into local contexts.\n- The preparation\
    \ steps mutate their input DataFrames in place.\n- This function executes asynchronously\
    \ and may involve network or model API calls; latency and runtime errors may occur.\n\
    - Callbacks are used for progress reporting during processing.\n\nArgs:\n  edges_input:\
    \ pd.DataFrame - Edges data to process.\n  entities: pd.DataFrame - Entity data\
    \ used to explode communities.\n  communities: pd.DataFrame - Community definitions\
    \ and hierarchy.\n  claims_input: pd.DataFrame | None - Optional claims data;\
    \ if provided, used to augment context.\n  callbacks: WorkflowCallbacks - Callbacks\
    \ for progress reporting during processing.\n  cache: PipelineCache - Cache for\
    \ intermediate results during summarization.\n  summarization_strategy: dict -\
    \ Settings for summarization, including llm config and prompts.\n  async_mode:\
    \ AsyncType - Async execution mode for the summarization step.\n  num_threads:\
    \ int - Number of worker threads for parallel summarization.\n\nReturns:\n  pd.DataFrame:\
    \ The finalized community reports.\n\nRaises:\n  OSError: If an I/O error occurs\
    \ (e.g., storage or network interactions).\n  ValueError: If input data or strategy\
    \ configurations are invalid or missing required fields.\n  KeyError: If expected\
    \ keys are absent from dictionaries during setup.\n  Exception: Other exceptions\
    \ may be raised by underlying components (e.g., external services or the language\
    \ model) during processing."
  code_example: null
  example_source: null
  line_start: 84
  line_end: 137
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
  - graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
  - graphrag/index/operations/summarize_communities/graph_context/context_builder.py::build_local_context
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
  - graphrag/index/workflows/create_community_reports.py::_prep_claims
  - graphrag/index/workflows/create_community_reports.py::_prep_edges
  - graphrag/index/workflows/create_community_reports.py::_prep_nodes
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/index/workflows/create_community_reports.py::run_workflow
- node_id: graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
  file: graphrag/index/workflows/create_community_reports_text.py
  name: create_community_reports_text
  signature: "def create_community_reports_text(\n    entities: pd.DataFrame,\n  \
    \  communities: pd.DataFrame,\n    text_units: pd.DataFrame,\n    callbacks: WorkflowCallbacks,\n\
    \    cache: PipelineCache,\n    summarization_strategy: dict,\n    async_mode:\
    \ AsyncType = AsyncType.AsyncIO,\n    num_threads: int = 4,\n) -> pd.DataFrame"
  decorators: []
  raises: []
  visibility: public
  docstring: "Transforms input data into finalized community reports by building local\
    \ contexts and summarizing communities.\n\nArgs:\n    entities: DataFrame containing\
    \ entities data used to explode communities into nodes.\n    communities: DataFrame\
    \ containing community definitions and metadata.\n    text_units: DataFrame containing\
    \ text unit data.\n    callbacks: WorkflowCallbacks instance for workflow callbacks.\n\
    \    cache: PipelineCache instance used for caching results.\n    summarization_strategy:\
    \ dict configuring the summarization process (e.g., prompts and model settings).\n\
    \    async_mode: AsyncType indicating the asynchronous backend to use.\n    num_threads:\
    \ int number of worker threads to use.\n\nReturns:\n    pd.DataFrame: Finalized\
    \ community reports.\n\nRaises:\n    Propagates exceptions raised by underlying\
    \ helper functions and data processing steps."
  code_example: null
  example_source: null
  line_start: 74
  line_end: 114
  dependencies:
  - graphrag/config/models/language_model_config.py::LanguageModelConfig
  - graphrag/index/operations/finalize_community_reports.py::finalize_community_reports
  - graphrag/index/operations/summarize_communities/explode_communities.py::explode_communities
  - graphrag/index/operations/summarize_communities/summarize_communities.py::summarize_communities
  - graphrag/index/operations/summarize_communities/text_unit_context/context_builder.py::build_local_context
  - graphrag/tokenizer/get_tokenizer.py::get_tokenizer
  called_by:
  - graphrag/index/workflows/create_community_reports_text.py::run_workflow
- node_id: tests/verbs/test_create_communities.py::test_create_communities
  file: tests/verbs/test_create_communities.py
  name: test_create_communities
  signature: def test_create_communities()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the create_communities workflow by generating final communities\
    \ and validating the produced output against the test dataset.\n\nArgs:\n    None:\
    \ This test function does not accept any parameters.\n\nReturns:\n    None: The\
    \ test does not return a value; it performs assertions to verify correctness.\n\
    \nRaises:\n    Exception: Exceptions raised by the helper utilities used in this\
    \ test (load_test_table, create_test_context, create_graphrag_config, run_workflow,\
    \ load_table_from_storage, or compare_outputs) may propagate."
  code_example: null
  example_source: null
  line_start: 19
  line_end: 48
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_communities.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph
  file: tests/verbs/test_finalize_graph.py
  name: test_finalize_graph
  signature: def test_finalize_graph()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test that the finalize_graph workflow produces final entities and relationships\
    \ tables with default coordinates.\n\nThis test prepares a test context, creates\
    \ a GraphRag configuration using the default model setup, executes the finalize_graph\
    \ workflow, and then loads the resulting tables from storage to verify structure\
    \ and values. Specifically, it asserts that:\n- the x and y coordinates are zero\
    \ sums when embedding/UMAP are disabled by default,\n- all columns defined in\
    \ ENTITIES_FINAL_COLUMNS are present in the entities table,\n- all columns defined\
    \ in RELATIONSHIPS_FINAL_COLUMNS are present in the relationships table.\n\nReturns:\n\
    \    None\n        No return value.\n\nRaises:\n    Exception\n        Exceptions\
    \ raised by the underlying utilities used in this test (e.g., _prep_tables, create_test_context,\
    \ load_test_table, or storage I/O) may propagate."
  code_example: null
  example_source: null
  line_start: 21
  line_end: 40
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/finalize_graph.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/test_finalize_graph.py::_prep_tables
  called_by: []
- node_id: tests/verbs/test_finalize_graph.py::test_finalize_graph_umap
  file: tests/verbs/test_finalize_graph.py
  name: test_finalize_graph_umap
  signature: def test_finalize_graph_umap()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the finalize_graph workflow with UMAP enabled to verify that x\
    \ and y coordinates are produced and that final tables contain the expected columns.\n\
    \nArgs:\n    None: This function does not take any parameters.\n\nReturns:\n \
    \   None: This is an asynchronous test function and does not return a value.\n\
    \nRaises:\n    Exception: Exceptions raised by create_test_context, load_test_table,\
    \ or write_table_to_storage may propagate."
  code_example: null
  example_source: null
  line_start: 43
  line_end: 65
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/finalize_graph.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/test_finalize_graph.py::_prep_tables
  called_by: []
- node_id: graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
  file: graphrag/index/workflows/extract_graph_nlp.py
  name: extract_graph_nlp
  signature: "def extract_graph_nlp(\n    text_units: pd.DataFrame,\n    cache: PipelineCache,\n\
    \    extraction_config: ExtractGraphNLPConfig,\n) -> tuple[pd.DataFrame, pd.DataFrame]"
  decorators: []
  raises: []
  visibility: public
  docstring: "Asynchronously extract the base entity graph (nodes and edges) from\
    \ the given text units.\n\nArgs:\n    text_units: pd.DataFrame: Input text units\
    \ used to extract noun phrases for graph construction.\n    cache: PipelineCache:\
    \ Cache used during extraction and graph construction.\n    extraction_config:\
    \ ExtractGraphNLPConfig: Configuration for extraction settings, including text_analyzer,\
    \ normalize_edge_weights, concurrent_requests, and async_mode.\n\nReturns:\n \
    \   tuple[pd.DataFrame, pd.DataFrame]: A tuple containing extracted_nodes and\
    \ extracted_edges. extracted_nodes has an added 'type' column with value 'NOUN\
    \ PHRASE' and an added 'description' column (empty string); extracted_edges has\
    \ an added 'description' column (empty string).\n\nRaises:\n    Exception: Propagates\
    \ exceptions raised by the underlying noun-phrase extractor creation or by build_noun_graph."
  code_example: null
  example_source: null
  line_start: 51
  line_end: 73
  dependencies:
  - graphrag/index/operations/build_noun_graph/build_noun_graph.py::build_noun_graph
  - graphrag/index/operations/build_noun_graph/np_extractors/factory.py::create_noun_phrase_extractor
  called_by:
  - graphrag/index/workflows/extract_graph_nlp.py::run_workflow
- node_id: tests/verbs/test_extract_covariates.py::test_extract_covariates
  file: tests/verbs/test_extract_covariates.py
  name: test_extract_covariates
  signature: def test_extract_covariates()
  decorators: []
  raises: []
  visibility: public
  docstring: "\"\"\"Test the covariates extraction workflow using a mocked language\
    \ model response.\n\nReturns:\n    None: The test does not return a value.\n\n\
    Raises:\n    Exception: Exceptions raised by underlying utilities (e.g., load_test_table\
    \ or load_table_from_storage) may propagate during test execution.\n\"\"\""
  code_example: null
  example_source: null
  line_start: 27
  line_end: 79
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/extract_covariates.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: tests/verbs/test_extract_graph.py::test_extract_graph
  file: tests/verbs/test_extract_graph.py
  name: test_extract_graph
  signature: def test_extract_graph()
  decorators: []
  raises: []
  visibility: public
  docstring: Test that the extract_graph workflow runs with a test context and mocked
    LLM responses, persists the resulting entities and relationships to storage, and
    validates the stored graph for expected schema and content (including a mocked
    description).
  code_example: null
  example_source: null
  line_start: 37
  line_end: 78
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/extract_graph.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::create_test_context
  called_by: []
- node_id: graphrag/index/workflows/create_community_reports.py::run_workflow
  file: graphrag/index/workflows/create_community_reports.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Runs the create_community_reports workflow to transform community reports\
    \ and persist the results.\n\nArgs:\n  config: GraphRagConfig containing settings\
    \ used by the workflow, including language model and data extraction configurations.\n\
    \  context: PipelineRunContext providing access to output_storage, callbacks,\
    \ and cache used during the workflow.\n\nReturns:\n  WorkflowFunctionOutput: The\
    \ output of the workflow, which wraps the resulting community_reports DataFrame.\n\
    \nRaises:\n  ValueError: Could not find required parquet file in storage during\
    \ loading.\n  Exception: Exceptions raised by the storage backend or parquet reader\
    \ during load or write operations."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 81
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_community_reports.py::create_community_reports
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::storage_has_table
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_create_community_reports.py::test_create_community_reports
- node_id: graphrag/index/workflows/create_community_reports_text.py::run_workflow
  file: graphrag/index/workflows/create_community_reports_text.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Runs the workflow to transform community reports text and persists the\
    \ results to storage.\n\nThis workflow loads input data from storage (entities,\
    \ communities, and text_units), configures language-model and summarization settings,\
    \ builds contextual prompts, generates the community reports text, and writes\
    \ the resulting table to storage as \"community_reports\". The operation persists\
    \ the final output to storage and relies on the provided callbacks and cache through\
    \ the PipelineRunContext.\n\nArgs:\n  config: GraphRagConfig containing settings\
    \ used by the workflow, including language model, data extraction configurations,\
    \ and summarization strategy.\n  context: PipelineRunContext providing access\
    \ to output_storage, callbacks, and cache used during the workflow.\n\nReturns:\n\
    \  WorkflowFunctionOutput: The output of the workflow, containing the generated\
    \ community_reports DataFrame as the result.\n\nRaises:\n  FileNotFoundError:\
    \ If required parquet inputs (entities.parquet, communities.parquet, or text_units.parquet)\
    \ cannot be found in storage.\n  Exception: Exceptions raised by the storage backend\
    \ during load or write operations may propagate."
  code_example: null
  example_source: null
  line_start: 37
  line_end: 71
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/create_community_reports_text.py::create_community_reports_text
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by: []
- node_id: graphrag/index/workflows/extract_graph_nlp.py::run_workflow
  file: graphrag/index/workflows/extract_graph_nlp.py
  name: run_workflow
  signature: "def run_workflow(\n    config: GraphRagConfig,\n    context: PipelineRunContext,\n\
    ) -> WorkflowFunctionOutput"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the extract_graph_nlp workflow to build the base entity graph and\
    \ persist results to storage.\n\nThis coroutine orchestrates the extraction of\
    \ noun-phrase based graph components by loading text units from storage, invoking\
    \ extract_graph_nlp to produce entities and relationships, writing the resulting\
    \ tables back to storage, and returning a WorkflowFunctionOutput containing the\
    \ produced data.\n\nArgs:\n  config: GraphRagConfig\n      Configuration for the\
    \ extract_graph_nlp workflow, including extraction_config parameters.\n  context:\
    \ PipelineRunContext\n      The runtime context containing storage handles and\
    \ cache used by the workflow.\n\nReturns:\n  WorkflowFunctionOutput\n      The\
    \ output with a result dictionary containing:\n        entities: DataFrame of\
    \ extracted entities\n        relationships: DataFrame of extracted relationships\n\
    \nRaises:\n  ValueError\n      Could not find the required text_units.parquet\
    \ in storage (text_units).\n  Exception\n      Exceptions raised by the storage\
    \ backend or parquet reader/writer during load or write operations, or errors\
    \ raised by extract_graph_nlp."
  code_example: null
  example_source: null
  line_start: 24
  line_end: 48
  dependencies:
  - graphrag/index/typing/workflow.py::WorkflowFunctionOutput
  - graphrag/index/workflows/extract_graph_nlp.py::extract_graph_nlp
  - graphrag/utils/storage.py::load_table_from_storage
  - graphrag/utils/storage.py::write_table_to_storage
  called_by:
  - tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
- node_id: tests/verbs/test_create_community_reports.py::test_create_community_reports
  file: tests/verbs/test_create_community_reports.py
  name: test_create_community_reports
  signature: def test_create_community_reports()
  decorators: []
  raises: []
  visibility: public
  docstring: "Test the create_community_reports workflow by generating community reports\
    \ and validating the produced output against a test dataset.\n\nThe test loads\
    \ test data into a test context, configures a mock language model with predefined\
    \ responses, runs the workflow to generate community reports, and asserts that\
    \ the resulting community_reports table matches the expected test data, including\
    \ specific checks for mock-driven fields and the presence of all final columns.\n\
    \nReturns:\n  None: The test does not return a value; it performs assertions to\
    \ verify correctness.\n\nRaises:\n  Exception: Exceptions raised by the helper\
    \ utilities used in this test (load_test_table, create_test_context, load_table_from_storage)\
    \ may propagate."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 81
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/create_community_reports.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::compare_outputs
  - tests/verbs/util.py::create_test_context
  - tests/verbs/util.py::load_test_table
  called_by: []
- node_id: tests/verbs/test_extract_graph_nlp.py::test_extract_graph_nlp
  file: tests/verbs/test_extract_graph_nlp.py
  name: test_extract_graph_nlp
  signature: def test_extract_graph_nlp()
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the extract_graph_nlp workflow against a test context and verify\
    \ the produced entities and relationships in storage.\n\nThis async test creates\
    \ a test context with text units, builds a Graphrag config using DEFAULT_MODEL_CONFIG,\
    \ executes the workflow, and then loads the resulting tables to assert exact row\
    \ counts and schema.\n\nReturns:\n    None\n        This test does not return\
    \ a value; it performs assertions to validate correctness.\n\nRaises:\n    Exception\n\
    \        Exceptions may propagate from the underlying operations used in the test\
    \ (e.g., test utilities, storage I/O, or workflow execution)."
  code_example: null
  example_source: null
  line_start: 16
  line_end: 35
  dependencies:
  - graphrag/config/create_graphrag_config.py::create_graphrag_config
  - graphrag/index/workflows/extract_graph_nlp.py::run_workflow
  - graphrag/utils/storage.py::load_table_from_storage
  - tests/verbs/util.py::create_test_context
  called_by: []
- node_id: graphrag/api/query.py::basic_search
  file: graphrag/api/query.py
  name: basic_search
  signature: "def basic_search(\n    config: GraphRagConfig,\n    text_units: pd.DataFrame,\n\
    \    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
    \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
    \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a basic search and return the response and the context data.\n\
    \nParameters\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml).\n\
    \    text_units (pd.DataFrame): A DataFrame containing the final text units (from\
    \ text_units.parquet).\n    query (str): The user query to search for.\n    callbacks\
    \ (list[QueryCallbacks] | None): Optional callbacks for processing the search.\n\
    \    verbose (bool): If True, enable verbose logging.\n\nReturns\n    tuple[str,\
    \ dict[str, Any]]: The first element is the concatenated response string produced\
    \ by streaming, and the second element is the context data dict collected from\
    \ the streaming callbacks.\n\nNotes\n    - The function initializes logging via\
    \ init_loggers and writes to query.log.\n    - It attaches a local NoopQueryCallbacks\
    \ that updates context_data through its on_context callback.\n    - The returned\
    \ context_data reflects the most recent context produced by the streaming process.\n\
    \nRaises\n    pydantic.ValidationError: If input arguments fail validation by\
    \ the validate_call decorator."
  code_example: null
  example_source: null
  line_start: 1057
  line_end: 1102
  dependencies:
  - graphrag/api/query.py::basic_search_streaming
  - graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  called_by:
  - graphrag/api/query.py::multi_index_basic_search
- node_id: graphrag/api/query.py::basic_search_streaming
  file: graphrag/api/query.py
  name: basic_search_streaming
  signature: "def basic_search_streaming(\n    config: GraphRagConfig,\n    text_units:\
    \ pd.DataFrame,\n    query: str,\n    callbacks: list[QueryCallbacks] | None =\
    \ None,\n    verbose: bool = False,\n) -> AsyncGenerator"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Stream results from a local search as an asynchronous generator.\n\n\
    Args:\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
    \    text_units (pd.DataFrame): A DataFrame containing the final text units (from\
    \ text_units.parquet)\n    query (str): The user query to search for.\n    callbacks\
    \ (list[QueryCallbacks] | None): Optional list of QueryCallbacks to customize\
    \ streaming behavior.\n    verbose (bool): If True, enable verbose logging.\n\n\
    Returns:\n    AsyncGenerator: An asynchronous generator yielding chunks of the\
    \ search response as strings.\n\nRaises:\n    Exception: Propagates exceptions\
    \ raised by underlying components (e.g., during embedding retrieval, prompt loading,\
    \ or streaming)."
  code_example: null
  example_source: null
  line_start: 1106
  line_end: 1148
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/query/factory.py::get_basic_search_engine
  - graphrag/query/indexer_adapters.py::read_indexer_text_units
  - graphrag/utils/api.py::get_embedding_store
  - graphrag/utils/api.py::load_search_prompt
  - graphrag/utils/cli.py::redact
  called_by:
  - graphrag/api/query.py::basic_search
- node_id: graphrag/api/query.py::drift_search
  file: graphrag/api/query.py
  name: drift_search
  signature: "def drift_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
    \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
    \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level: int,\n\
    \    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
    \ | None = None,\n    verbose: bool = False,\n) -> tuple[\n    str | dict[str,\
    \ Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
    ]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: 'Perform a DRIFT search and return the context data and response.


    Parameters

    ----------

    - config (GraphRagConfig): A graphrag configuration (from settings.yaml).

    - entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet).

    - communities (pd.DataFrame): A DataFrame containing the final communities (from
    communities.parquet).

    - community_reports (pd.DataFrame): A DataFrame containing the final community
    reports (from community_reports.parquet).

    - text_units (pd.DataFrame): A DataFrame containing the final text units (from
    text_units.parquet).

    - relationships (pd.DataFrame): A DataFrame containing the final relationships
    (from relationships.parquet).

    - community_level (int): The community level to search at.

    - response_type (str): The type of response to generate (e.g., full, compact).

    - query (str): The user query to search for.

    - callbacks (list[QueryCallbacks] | None): Optional list of callback handlers
    to receive streaming and context information.

    - verbose (bool): Enable verbose logging for debugging.


    Returns

    ----------

    tuple[str | dict[str, Any] | list[dict[str, Any]], str | list[pd.DataFrame] |
    dict[str, pd.DataFrame]]: A tuple containing the search response and the context
    data. The first element is the generated response, which can be a string, a dictionary,
    or a list of dictionaries depending on the response_type. The second element is
    the context data, which may be a string, a list of DataFrames, or a mapping of
    strings to DataFrames.


    Raises

    ----------

    pydantic.ValidationError: If input arguments fail validation via the validate_call
    decorator.'
  code_example: null
  example_source: null
  line_start: 708
  line_end: 769
  dependencies:
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  called_by:
  - graphrag/api/query.py::multi_index_drift_search
- node_id: graphrag/api/query.py::drift_search_streaming
  file: graphrag/api/query.py
  name: drift_search_streaming
  signature: "def drift_search_streaming(\n    config: GraphRagConfig,\n    entities:\
    \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
    \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    community_level:\
    \ int,\n    response_type: str,\n    query: str,\n    callbacks: list[QueryCallbacks]\
    \ | None = None,\n    verbose: bool = False,\n) -> AsyncGenerator"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a DRIFT streaming search and yield streaming response chunks.\n\
    \nArgs:\n    config (GraphRagConfig): A graphrag configuration (from settings.yaml).\n\
    \    entities (pd.DataFrame): A DataFrame containing the final entities (from\
    \ entities.parquet).\n    communities (pd.DataFrame): A DataFrame containing the\
    \ final communities (from communities.parquet).\n    community_reports (pd.DataFrame):\
    \ A DataFrame containing the final community reports (from community_reports.parquet).\n\
    \    text_units (pd.DataFrame): A DataFrame containing the final text units (from\
    \ text_units.parquet).\n    relationships (pd.DataFrame): A DataFrame containing\
    \ the final relationships (from relationships.parquet).\n    community_level (int):\
    \ The community level to search at.\n    response_type (str): The response type\
    \ to shape the drift search output.\n    query (str): The user query to search\
    \ for.\n    callbacks (list[QueryCallbacks] | None): Optional list of callbacks\
    \ to customize the streaming search behavior. If None, a default empty list is\
    \ used.\n    verbose (bool): Enable verbose logging of the streaming search process.\n\
    \nReturns:\n    AsyncGenerator[str, None]: An asynchronous generator yielding\
    \ string chunks that together form the streaming search response. Callers should\
    \ consume chunks as they arrive and concatenate them to reconstruct the full response."
  code_example: null
  example_source: null
  line_start: 773
  line_end: 841
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/query/factory.py::get_drift_search_engine
  - graphrag/query/indexer_adapters.py::read_indexer_entities
  - graphrag/query/indexer_adapters.py::read_indexer_relationships
  - graphrag/query/indexer_adapters.py::read_indexer_report_embeddings
  - graphrag/query/indexer_adapters.py::read_indexer_reports
  - graphrag/query/indexer_adapters.py::read_indexer_text_units
  - graphrag/utils/api.py::get_embedding_store
  - graphrag/utils/api.py::load_search_prompt
  - graphrag/utils/cli.py::redact
  called_by:
  - graphrag/api/query.py::drift_search
- node_id: graphrag/api/query.py::local_search
  file: graphrag/api/query.py
  name: local_search
  signature: "def local_search(\n    config: GraphRagConfig,\n    entities: pd.DataFrame,\n\
    \    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n    text_units:\
    \ pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates: pd.DataFrame\
    \ | None,\n    community_level: int,\n    response_type: str,\n    query: str,\n\
    \    callbacks: list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n\
    ) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
    \ | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a local search and return the aggregated response and any context\
    \ data captured during streaming.\n\nThis function initializes logging, wires\
    \ a context-capturing callback, and streams results via local_search_streaming.\
    \ Chunks yielded by the streaming engine are concatenated into a single full_response\
    \ string. The latest context data emitted during streaming is returned as context_data.\n\
    \nArgs:\n  config: GraphRagConfig\n      A graphrag configuration (from settings.yaml).\n\
    \  entities: pandas.DataFrame\n      The final entities (from entities.parquet).\n\
    \  communities: pandas.DataFrame\n      The final communities (from communities.parquet).\n\
    \  community_reports: pandas.DataFrame\n      The final community reports (from\
    \ community_reports.parquet).\n  text_units: pandas.DataFrame\n      The final\
    \ text units (from text_units.parquet).\n  relationships: pandas.DataFrame\n \
    \     The final relationships (from relationships.parquet).\n  covariates: pandas.DataFrame\
    \ | None\n      The final covariates (from covariates.parquet), or None if not\
    \ available.\n  community_level: int\n      The community level to search at.\n\
    \  response_type: str\n      The response type to return.\n  query: str\n    \
    \  The user query to search for.\n  callbacks: list[QueryCallbacks] | None\n \
    \     Optional list of callback handlers. Defaults to None.\n  verbose: bool\n\
    \      Whether to enable verbose logging. Defaults to False.\n\nReturns:\n  tuple[str,\
    \ dict[str, Any] | list[dict[str, Any]] | dict[str, pd.DataFrame] | list[pd.DataFrame]]\n\
    \      A tuple containing:\n      - full_response: the aggregated response string\
    \ produced by streaming.\n      - context_data: the latest context data captured\
    \ from streaming (structure depends on the engine; commonly a dict).\n\nExamples:\n\
    \  result, ctx = local_search(\n      config=config,\n      entities=entities_df,\n\
    \      communities=communities_df,\n      community_reports=reports_df,\n    \
    \  text_units=text_units_df,\n      relationships=relationships_df,\n      covariates=covariates_df,\n\
    \      community_level=2,\n      response_type=\"full\",\n      query=\"quantum\
    \ computing\",\n      verbose=True,\n  )\n\nRaises:\n  pydantic.ValidationError:\
    \ If the input arguments do not satisfy the declared types.\n  Exception: Any\
    \ exception raised by the underlying streaming engine or callback processing."
  code_example: null
  example_source: null
  line_start: 342
  line_end: 406
  dependencies:
  - graphrag/api/query.py::local_search_streaming
  - graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  called_by:
  - graphrag/api/query.py::multi_index_local_search
- node_id: graphrag/api/query.py::local_search_streaming
  file: graphrag/api/query.py
  name: local_search_streaming
  signature: "def local_search_streaming(\n    config: GraphRagConfig,\n    entities:\
    \ pd.DataFrame,\n    communities: pd.DataFrame,\n    community_reports: pd.DataFrame,\n\
    \    text_units: pd.DataFrame,\n    relationships: pd.DataFrame,\n    covariates:\
    \ pd.DataFrame | None,\n    community_level: int,\n    response_type: str,\n \
    \   query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
    \ bool = False,\n) -> AsyncGenerator"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises: []
  visibility: public
  docstring: "Perform a local search and stream results via an asynchronous generator.\n\
    \nArgs:\n  config (GraphRagConfig): A graphrag configuration (from settings.yaml)\n\
    \  entities (pd.DataFrame): A DataFrame containing the final entities (from entities.parquet)\n\
    \  communities (pd.DataFrame): A DataFrame containing the final communities (from\
    \ communities.parquet)\n  community_reports (pd.DataFrame): A DataFrame containing\
    \ the final community reports (from community_reports.parquet)\n  text_units (pd.DataFrame):\
    \ A DataFrame containing the final text units (from text_units.parquet)\n  relationships\
    \ (pd.DataFrame): A DataFrame containing the final relationships (from relationships.parquet)\n\
    \  covariates (pd.DataFrame | None): A DataFrame containing the final covariates\
    \ (from covariates.parquet) or None\n  community_level (int): The community level\
    \ to search at.\n  response_type (str): The response type to return.\n  query\
    \ (str): The user query to search for.\n  callbacks (list[QueryCallbacks] | None):\
    \ Optional list of QueryCallbacks to customize streaming behavior.\n  verbose\
    \ (bool): Enable verbose logging when true.\n\nReturns:\n  AsyncGenerator[str,\
    \ None]: An asynchronous generator yielding chunks of the streaming local search\
    \ results as strings.\n\nEdge cases:\n  - If covariates is None, covariates are\
    \ treated as an empty list for the search."
  code_example: null
  example_source: null
  line_start: 410
  line_end: 472
  dependencies:
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/query/factory.py::get_local_search_engine
  - graphrag/query/indexer_adapters.py::read_indexer_covariates
  - graphrag/query/indexer_adapters.py::read_indexer_entities
  - graphrag/query/indexer_adapters.py::read_indexer_relationships
  - graphrag/query/indexer_adapters.py::read_indexer_reports
  - graphrag/query/indexer_adapters.py::read_indexer_text_units
  - graphrag/utils/api.py::get_embedding_store
  - graphrag/utils/api.py::load_search_prompt
  - graphrag/utils/cli.py::redact
  called_by:
  - graphrag/api/query.py::local_search
- node_id: graphrag/api/query.py::multi_index_basic_search
  file: graphrag/api/query.py
  name: multi_index_basic_search
  signature: "def multi_index_basic_search(\n    config: GraphRagConfig,\n    text_units_list:\
    \ list[pd.DataFrame],\n    index_names: list[str],\n    streaming: bool,\n   \
    \ query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n    verbose:\
    \ bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str, Any]],\n\
    \    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Perform a basic search across multiple indexes and return the search\
    \ results and associated context data.\n\nArgs:\n    config (GraphRagConfig):\
    \ A graphrag configuration (from settings.yaml).\n    text_units_list (list[pd.DataFrame]):\
    \ A list of DataFrames containing the final text units (from text_units.parquet).\n\
    \    index_names (list[str]): A list of index names.\n    streaming (bool): Whether\
    \ to stream the results or not. Streaming is not implemented for this function.\n\
    \    query (str): The user query to search for.\n    callbacks (list[QueryCallbacks]\
    \ | None): Optional callbacks for processing the search.\n    verbose (bool):\
    \ If True, enable verbose logging.\n\nReturns:\n    tuple[str | dict[str, Any]\
    \ | list[dict[str, Any]], str | list[pd.DataFrame] | dict[str, pd.DataFrame]]:\
    \ A tuple containing the search response and the context data. The first element\
    \ can be a string, a dict, or a list of dicts. The second element can be a string,\
    \ a list of DataFrames, or a dict mapping strings to DataFrames.\n\nRaises:\n\
    \    NotImplementedError: If streaming is True; streaming is not yet implemented\
    \ for multi_index_basic_search."
  code_example: null
  example_source: null
  line_start: 1152
  line_end: 1226
  dependencies:
  - graphrag/api/query.py::basic_search
  - graphrag/logger/standard_logging.py::init_loggers
  called_by: []
- node_id: graphrag/api/query.py::multi_index_drift_search
  file: graphrag/api/query.py
  name: multi_index_drift_search
  signature: "def multi_index_drift_search(\n    config: GraphRagConfig,\n    entities_list:\
    \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
    \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
    \ list[pd.DataFrame],\n    index_names: list[str],\n    community_level: int,\n\
    \    response_type: str,\n    streaming: bool,\n    query: str,\n    callbacks:\
    \ list[QueryCallbacks] | None = None,\n    verbose: bool = False,\n) -> tuple[\n\
    \    str | dict[str, Any] | list[dict[str, Any]],\n    str | list[pd.DataFrame]\
    \ | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Perform a DRIFT search across multiple indexes and return the search\
    \ result and updated context data.\n\nArgs:\n    config (GraphRagConfig): A graphrag\
    \ configuration (from settings.yaml).\n    entities_list (list[pd.DataFrame]):\
    \ A list of DataFrames containing the final entities (from entities.parquet).\n\
    \        Each DataFrame is expected to include columns such as human_readable_id,\
    \ id, title, and text_unit_ids.\n    communities_list (list[pd.DataFrame]): A\
    \ list of DataFrames containing the final communities (from communities.parquet).\n\
    \        Each DataFrame is expected to include columns such as community, entity_ids,\
    \ and human_readable_id.\n    community_reports_list (list[pd.DataFrame]): A list\
    \ of DataFrames containing the final community reports (from community_reports.parquet).\n\
    \        Each DataFrame is expected to include columns such as community, id,\
    \ and human_readable_id.\n    text_units_list (list[pd.DataFrame]): A list of\
    \ DataFrames containing the final text units (from text_units.parquet).\n    \
    \    Each DataFrame should include at least id and human_readable_id.\n    relationships_list\
    \ (list[pd.DataFrame]): A list of DataFrames containing the final relationships\
    \ (from relationships.parquet).\n        Each DataFrame is expected to include\
    \ human_readable_id, source, target, and text_unit_ids.\n    index_names (list[str]):\
    \ A list of index names.\n    community_level (int): The community level to search\
    \ at.\n    response_type (str): The type of response to return.\n    streaming\
    \ (bool): Whether to stream the results or not. Streaming is not yet implemented;\
    \ if True a NotImplementedError will be raised.\n    query (str): The user query\
    \ to search for.\n    callbacks (list[QueryCallbacks] | None): Optional callbacks\
    \ to apply to the search.\n    verbose (bool): Enable verbose logging.\n\nReturns:\n\
    \    tuple[\n        str | dict[str, Any] | list[dict[str, Any]],\n        str\
    \ | list[pd.DataFrame] | dict[str, pd.DataFrame],\n    ]: A tuple consisting of\
    \ the search result payload and the updated context data.\n        - The first\
    \ element (result) may be a string, a dictionary mapping string keys to result\
    \ data, or a list of result dictionaries.\n        - The second element (context)\
    \ is either a string, a list of DataFrames, or a dictionary mapping keys to DataFrames.\n\
    \nRaises:\n    NotImplementedError: If streaming is requested, as streaming is\
    \ not yet implemented for multi_index_drift_search.\n\nExamples:\n    result,\
    \ context = await multi_index_drift_search(\n        config=cfg,\n        entities_list=entities_per_index,\n\
    \        communities_list=communities_per_index,\n        community_reports_list=reports_per_index,\n\
    \        text_units_list=text_units_per_index,\n        relationships_list=relationships_per_index,\n\
    \        index_names=[\"idx_a\", \"idx_b\"],\n        community_level=1,\n   \
    \     response_type=\"full\",\n        streaming=False,\n        query=\"find\
    \ drift\",\n        callbacks=None,\n        verbose=True,\n    )"
  code_example: null
  example_source: null
  line_start: 845
  line_end: 1053
  dependencies:
  - graphrag/api/query.py::drift_search
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  - graphrag/utils/api.py::update_context_data
  called_by: []
- node_id: graphrag/api/query.py::multi_index_local_search
  file: graphrag/api/query.py
  name: multi_index_local_search
  signature: "def multi_index_local_search(\n    config: GraphRagConfig,\n    entities_list:\
    \ list[pd.DataFrame],\n    communities_list: list[pd.DataFrame],\n    community_reports_list:\
    \ list[pd.DataFrame],\n    text_units_list: list[pd.DataFrame],\n    relationships_list:\
    \ list[pd.DataFrame],\n    covariates_list: list[pd.DataFrame] | None,\n    index_names:\
    \ list[str],\n    community_level: int,\n    response_type: str,\n    streaming:\
    \ bool,\n    query: str,\n    callbacks: list[QueryCallbacks] | None = None,\n\
    \    verbose: bool = False,\n) -> tuple[\n    str | dict[str, Any] | list[dict[str,\
    \ Any]],\n    str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n]"
  decorators:
  - '@validate_call(config={"arbitrary_types_allowed": True})'
  raises:
  - NotImplementedError
  visibility: public
  docstring: "Perform a local search across multiple indexes and return the rendered\
    \ response and updated context data.\n\nArgs:\n  config: GraphRagConfig\n    \
    \  A graphrag configuration (from settings.yaml).\n  entities_list: list[pd.DataFrame]\n\
    \      A list of DataFrames containing the final entities (from entities.parquet).\n\
    \  communities_list: list[pd.DataFrame]\n      A list of DataFrames containing\
    \ the final communities (from communities.parquet).\n  community_reports_list:\
    \ list[pd.DataFrame]\n      A list of DataFrames containing the final community\
    \ reports (from community_reports.parquet).\n  text_units_list: list[pd.DataFrame]\n\
    \      A list of DataFrames containing the final text units (from text_units.parquet).\n\
    \  relationships_list: list[pd.DataFrame]\n      A list of DataFrames containing\
    \ the final relationships (from relationships.parquet).\n  covariates_list: list[pd.DataFrame]\
    \ | None\n      Optional; A list of DataFrames containing the final covariates\
    \ (from covariates.parquet).\n  index_names: list[str]\n      A list of index\
    \ names.\n  community_level: int\n      The community level to search at.\n  response_type:\
    \ str\n      The response type to return.\n  streaming: bool\n      Whether to\
    \ stream the results or not. Streaming is not yet implemented for this multi-index\
    \ search.\n  query: str\n      The user query to search for.\n  callbacks: list[QueryCallbacks]\
    \ | None\n      Optional; Callbacks to customize query handling.\n  verbose: bool\n\
    \      Enable verbose logging.\n\nReturns:\n  tuple[\n      str | dict[str, Any]\
    \ | list[dict[str, Any]],\n      str | list[pd.DataFrame] | dict[str, pd.DataFrame],\n\
    \  ]\n  A tuple containing the rendered response and the updated context data.\
    \ The first element is\n  the response as a string or a structured object (dict\
    \ or list of dicts) depending on the\n  requested format. The second element is\
    \ the context data, which may be a string, a list of DataFrames,\n  or a dict\
    \ mapping string keys to DataFrames.\n\nRaises:\n  NotImplementedError\n     \
    \ If streaming is requested (streaming == True), as streaming is not yet implemented\
    \ for this\n      multi-index local search."
  code_example: null
  example_source: null
  line_start: 476
  line_end: 704
  dependencies:
  - graphrag/api/query.py::local_search
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/api.py::truncate
  - graphrag/utils/api.py::update_context_data
  called_by: []
- node_id: graphrag/cli/index.py::_run_index
  file: graphrag/cli/index.py
  name: _run_index
  signature: "def _run_index(\n    config,\n    method,\n    is_update_run,\n    verbose,\n\
    \    memprofile,\n    cache,\n    dry_run,\n    skip_validation,\n)"
  decorators: []
  raises: []
  visibility: protected
  docstring: "Run the indexing pipeline using the provided configuration.\n\nParameters\
    \ (with types):\n  config: GraphRagConfig - The GraphRagConfig instance containing\
    \ run configuration.\n  method: IndexingMethod - The indexing method to use for\
    \ this run.\n  is_update_run: bool - True if this is an update run, False otherwise.\n\
    \  verbose: bool - Enable verbose logging/output.\n  memprofile: bool - Enable\
    \ memory profiling during execution.\n  cache: bool - Whether to enable caching;\
    \ if False, caching is disabled.\n  dry_run: bool - If True, perform a dry run\
    \ and exit before execution.\n  skip_validation: bool - If True, skip validation\
    \ of configuration names.\n\nReturns (None):\n  None\n\nRaises:\n  SystemExit:\n\
    \    - If dry_run is True, the function exits with status 0.\n    - At the end,\
    \ the function exits with status 0 if no errors were encountered, or 1 if errors\
    \ occurred.\n    - If validate_config_names raises SystemExit due to validation\
    \ failures (only when skip_validation is False)."
  code_example: null
  example_source: null
  line_start: 103
  line_end: 161
  dependencies:
  - graphrag.api::build_index
  - graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks
  - graphrag/cli/index.py::_register_signal_handlers
  - graphrag/index/validate_config.py::validate_config_names
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/cli.py::redact
  called_by:
  - graphrag/cli/index.py::index_cli
  - graphrag/cli/index.py::update_cli
- node_id: graphrag/cli/index.py::index_cli
  file: graphrag/cli/index.py
  name: index_cli
  signature: "def index_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n \
    \   verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
    \ Path | None,\n    dry_run: bool,\n    skip_validation: bool,\n    output_dir:\
    \ Path | None,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the indexing pipeline with the given configuration.\n\nParameters:\n\
    \    root_dir (Path): The root directory of the project. Will search for the configuration\
    \ file in this directory.\n    method (IndexingMethod): The indexing method to\
    \ use for this run.\n    verbose (bool): Enable verbose logging/output.\n    memprofile\
    \ (bool): Enable memory profiling during execution.\n    cache (bool): Whether\
    \ to enable caching; if False, caching is disabled.\n    config_filepath (Path\
    \ | None): Path to the configuration file. If None, searches for the config file\
    \ in root_dir.\n    dry_run (bool): If True, perform a dry run without making\
    \ changes.\n    skip_validation (bool): Skip validation of the loaded configuration.\n\
    \    output_dir (Path | None): Directory to write outputs. If provided, overrides\
    \ base output/reporting directories.\n\nReturns:\n    None\n\nRaises:\n    FileNotFoundError:\
    \ If the config file is not found.\n    ValueError: If the configuration is invalid."
  code_example: null
  example_source: null
  line_start: 42
  line_end: 69
  dependencies:
  - graphrag/cli/index.py::_run_index
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_index_cli
- node_id: graphrag/cli/index.py::update_cli
  file: graphrag/cli/index.py
  name: update_cli
  signature: "def update_cli(\n    root_dir: Path,\n    method: IndexingMethod,\n\
    \    verbose: bool,\n    memprofile: bool,\n    cache: bool,\n    config_filepath:\
    \ Path | None,\n    skip_validation: bool,\n    output_dir: Path | None,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Run the update pipeline with the given config.\n\nThis function applies\
    \ optional output directory overrides, loads the configuration, and executes the\
    \ update phase of the indexing pipeline.\n\nArgs:\n    root_dir: The root directory\
    \ of the project. Will search for the config file in this directory.\n    method:\
    \ The indexing method to use for this run.\n    verbose: Enable verbose logging/output.\n\
    \    memprofile: Enable memory profiling during execution.\n    cache: Whether\
    \ to enable caching; if False, caching is disabled.\n    config_filepath: The\
    \ path to the config file. If None, searches for config file in root.\n    skip_validation:\
    \ Whether to skip validation of the loaded configuration.\n    output_dir: Optional\
    \ output directory to override base directories used by the pipeline.\n\nReturns:\n\
    \    None\n\nRaises:\n    FileNotFoundError: If the config file is not found.\n\
    \    ValueError: If the configuration is invalid."
  code_example: null
  example_source: null
  line_start: 72
  line_end: 100
  dependencies:
  - graphrag/cli/index.py::_run_index
  - graphrag/config/load_config.py::load_config
  called_by:
  - graphrag/cli/main.py::_update_cli
- node_id: graphrag/cli/main.py::_index_cli
  file: graphrag/cli/main.py
  name: _index_cli
  signature: "def _index_cli(\n    config: Path | None = typer.Option(\n        None,\n\
    \        \"--config\",\n        \"-c\",\n        help=\"The configuration to use.\"\
    ,\n        exists=True,\n        file_okay=True,\n        readable=True,\n   \
    \     autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
    \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
    \ root directory.\",\n        exists=True,\n        dir_okay=True,\n        writable=True,\n\
    \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
    \    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
    \        \"--method\",\n        \"-m\",\n        help=\"The indexing method to\
    \ use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n       \
    \ \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline with\
    \ verbose logging\",\n    ),\n    memprofile: bool = typer.Option(\n        False,\n\
    \        \"--memprofile\",\n        help=\"Run the indexing pipeline with memory\
    \ profiling\",\n    ),\n    dry_run: bool = typer.Option(\n        False,\n  \
    \      \"--dry-run\",\n        help=(\n            \"Run the indexing pipeline\
    \ without executing any steps \"\n            \"to inspect and validate the configuration.\"\
    \n        ),\n    ),\n    cache: bool = typer.Option(\n        True,\n       \
    \ \"--cache/--no-cache\",\n        help=\"Use LLM cache.\",\n    ),\n    skip_validation:\
    \ bool = typer.Option(\n        False,\n        \"--skip-validation\",\n     \
    \   help=\"Skip any preflight validation. Useful when running no LLM steps.\"\
    ,\n    ),\n    output: Path | None = typer.Option(\n        None,\n        \"\
    --output\",\n        \"-o\",\n        help=(\n            \"Indexing pipeline\
    \ output directory. \"\n            \"Overrides output.base_dir in the configuration\
    \ file.\"\n        ),\n        dir_okay=True,\n        writable=True,\n      \
    \  resolve_path=True,\n    ),\n) -> None"
  decorators:
  - '@app.command("index")'
  raises: []
  visibility: protected
  docstring: "Build a knowledge graph index.\n\nArgs:\n    config: The configuration\
    \ to use.\n    root: The project root directory.\n    method: The indexing method\
    \ to use.\n    verbose: Run the indexing pipeline with verbose logging.\n    memprofile:\
    \ Run the indexing pipeline with memory profiling.\n    dry_run: Run the indexing\
    \ pipeline without executing any steps to inspect and validate the configuration.\n\
    \    cache: Use LLM cache.\n    skip_validation: Skip any preflight validation.\
    \ Useful when running no LLM steps.\n    output: Indexing pipeline output directory.\
    \ Overrides output.base_dir in the configuration file.\n\nReturns:\n    None\n\
    \nRaises:\n    Exceptions raised by graphrag.cli.index.index_cli may propagate."
  code_example: null
  example_source: null
  line_start: 120
  line_end: 203
  dependencies:
  - graphrag/cli/index.py::index_cli
  called_by: []
- node_id: graphrag/cli/main.py::_prompt_tune_cli
  file: graphrag/cli/main.py
  name: _prompt_tune_cli
  signature: "def _prompt_tune_cli(\n    root: Path = typer.Option(\n        Path(),\n\
    \        \"--root\",\n        \"-r\",\n        help=\"The project root directory.\"\
    ,\n        exists=True,\n        dir_okay=True,\n        writable=True,\n    \
    \    resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n \
    \   config: Path | None = typer.Option(\n        None,\n        \"--config\",\n\
    \        \"-c\",\n        help=\"The configuration to use.\",\n        exists=True,\n\
    \        file_okay=True,\n        readable=True,\n        autocompletion=CONFIG_AUTOCOMPLETE,\n\
    \    ),\n    verbose: bool = typer.Option(\n        False,\n        \"--verbose\"\
    ,\n        \"-v\",\n        help=\"Run the prompt tuning pipeline with verbose\
    \ logging.\",\n    ),\n    domain: str | None = typer.Option(\n        None,\n\
    \        \"--domain\",\n        help=(\n            \"The domain your input data\
    \ is related to. \"\n            \"For example 'space science', 'microbiology',\
    \ 'environmental news'. \"\n            \"If not defined, a domain will be inferred\
    \ from the input data.\"\n        ),\n    ),\n    selection_method: DocSelectionType\
    \ = typer.Option(\n        DocSelectionType.RANDOM.value,\n        \"--selection-method\"\
    ,\n        help=\"The text chunk selection method.\",\n    ),\n    n_subset_max:\
    \ int = typer.Option(\n        N_SUBSET_MAX,\n        \"--n-subset-max\",\n  \
    \      help=\"The number of text chunks to embed when --selection-method=auto.\"\
    ,\n    ),\n    k: int = typer.Option(\n        K,\n        \"--k\",\n        help=\"\
    The maximum number of documents to select from each centroid when --selection-method=auto.\"\
    ,\n    ),\n    limit: int = typer.Option(\n        LIMIT,\n        \"--limit\"\
    ,\n        help=\"The number of documents to load when --selection-method={random,top}.\"\
    ,\n    ),\n    max_tokens: int = typer.Option(\n        MAX_TOKEN_COUNT,\n   \
    \     \"--max-tokens\",\n        help=\"The max token count for prompt generation.\"\
    ,\n    ),\n    min_examples_required: int = typer.Option(\n        2,\n      \
    \  \"--min-examples-required\",\n        help=\"The minimum number of examples\
    \ to generate/include in the entity extraction prompt.\",\n    ),\n    chunk_size:\
    \ int = typer.Option(\n        graphrag_config_defaults.chunks.size,\n       \
    \ \"--chunk-size\",\n        help=\"The size of each example text chunk. Overrides\
    \ chunks.size in the configuration file.\",\n    ),\n    overlap: int = typer.Option(\n\
    \        graphrag_config_defaults.chunks.overlap,\n        \"--overlap\",\n  \
    \      help=\"The overlap size for chunking documents. Overrides chunks.overlap\
    \ in the configuration file.\",\n    ),\n    language: str | None = typer.Option(\n\
    \        None,\n        \"--language\",\n        help=\"The primary language used\
    \ for inputs and outputs in graphrag prompts.\",\n    ),\n    discover_entity_types:\
    \ bool = typer.Option(\n        True,\n        \"--discover-entity-types/--no-discover-entity-types\"\
    ,\n        help=\"Discover and extract unspecified entity types.\",\n    ),\n\
    \    output: Path = typer.Option(\n        Path(\"prompts\"),\n        \"--output\"\
    ,\n        \"-o\",\n        help=\"The directory to save prompts to, relative\
    \ to the project root directory.\",\n        dir_okay=True,\n        writable=True,\n\
    \        resolve_path=True,\n    ),\n) -> None"
  decorators:
  - '@app.command("prompt-tune")'
  raises: []
  visibility: protected
  docstring: "Generate custom graphrag prompts for a project using provided configuration\
    \ and options.\n\nArgs:\n    root: The project root directory.\n    config: The\
    \ configuration to use.\n    verbose: Run the prompt tuning pipeline with verbose\
    \ logging.\n    domain: The domain your input data is related to. If not defined,\
    \ a domain will be inferred from the input data.\n    selection_method: The text\
    \ chunk selection method.\n    n_subset_max: The number of text chunks to embed\
    \ when --selection-method=auto.\n    k: The maximum number of documents to select\
    \ from each centroid when --selection-method=auto.\n    limit: The number of documents\
    \ to load when --selection-method={random,top}.\n    max_tokens: The max token\
    \ count for prompt generation.\n    min_examples_required: The minimum number\
    \ of examples to generate/include in the entity extraction prompt.\n    chunk_size:\
    \ The size of each example text chunk. Overrides chunks.size in the configuration\
    \ file.\n    overlap: The overlap size for chunking documents. Overrides chunks.overlap\
    \ in the configuration file.\n    language: The primary language used for inputs\
    \ and outputs in graphrag prompts.\n    discover_entity_types: Discover and extract\
    \ unspecified entity types.\n    output: The directory to save prompts to, relative\
    \ to the project root directory.\n\nReturns:\n    None"
  code_example: null
  example_source: null
  line_start: 289
  line_end: 410
  dependencies:
  - graphrag/cli/prompt_tune.py::prompt_tune
  called_by: []
- node_id: graphrag/cli/main.py::_update_cli
  file: graphrag/cli/main.py
  name: _update_cli
  signature: "def _update_cli(\n    config: Path | None = typer.Option(\n        None,\n\
    \        \"--config\",\n        \"-c\",\n        help=\"The configuration to use.\"\
    ,\n        exists=True,\n        file_okay=True,\n        readable=True,\n   \
    \     autocompletion=CONFIG_AUTOCOMPLETE,\n    ),\n    root: Path = typer.Option(\n\
    \        Path(),\n        \"--root\",\n        \"-r\",\n        help=\"The project\
    \ root directory.\",\n        exists=True,\n        dir_okay=True,\n        writable=True,\n\
    \        resolve_path=True,\n        autocompletion=ROOT_AUTOCOMPLETE,\n    ),\n\
    \    method: IndexingMethod = typer.Option(\n        IndexingMethod.Standard.value,\n\
    \        \"--method\",\n        \"-m\",\n        help=\"The indexing method to\
    \ use.\",\n    ),\n    verbose: bool = typer.Option(\n        False,\n       \
    \ \"--verbose\",\n        \"-v\",\n        help=\"Run the indexing pipeline with\
    \ verbose logging.\",\n    ),\n    memprofile: bool = typer.Option(\n        False,\n\
    \        \"--memprofile\",\n        help=\"Run the indexing pipeline with memory\
    \ profiling.\",\n    ),\n    cache: bool = typer.Option(\n        True,\n    \
    \    \"--cache/--no-cache\",\n        help=\"Use LLM cache.\",\n    ),\n    skip_validation:\
    \ bool = typer.Option(\n        False,\n        \"--skip-validation\",\n     \
    \   help=\"Skip any preflight validation. Useful when running no LLM steps.\"\
    ,\n    ),\n    output: Path | None = typer.Option(\n        None,\n        \"\
    --output\",\n        \"-o\",\n        help=(\n            \"Indexing pipeline\
    \ output directory. \"\n            \"Overrides output.base_dir in the configuration\
    \ file.\"\n        ),\n        dir_okay=True,\n        writable=True,\n      \
    \  resolve_path=True,\n    ),\n) -> None"
  decorators:
  - '@app.command("update")'
  raises: []
  visibility: protected
  docstring: "Update an existing knowledge graph index.\n\nApplies a default output\
    \ configuration (if not provided by config), saving the new index to the local\
    \ file system in the update_output folder. If an explicit output path is provided,\
    \ it overrides the base output directory specified in the configuration.\n\nArgs:\n\
    \    config: Path | None - The configuration file to use. If None, the configuration\
    \ is located via the project root.\n    root: Path - The project root directory.\
    \ Directory containing the configuration and output structure; used to locate\
    \ the config when not provided.\n    method: IndexingMethod - The indexing method\
    \ to use. Defaults to Standard.\n    verbose: bool - Run the indexing pipeline\
    \ with verbose logging for detailed output.\n    memprofile: bool - Run the indexing\
    \ pipeline with memory profiling.\n    cache: bool - Use LLM cache during processing.\n\
    \    skip_validation: bool - Skip any preflight validation. Useful when running\
    \ no LLM steps.\n    output: Path | None - Output directory for the indexing pipeline.\
    \ Overrides output.base_dir in the configuration file. If None, the configuration\u2019\
    s defaults are used.\n\nReturns:\n    None\n\nRaises:\n    FileNotFoundError -\
    \ If the specified config or root path does not exist.\n    PermissionError -\
    \ If the configured or chosen output locations are not writable.\n    ValueError\
    \ - If an invalid combination of options is provided.\n    OSError - For generic\
    \ I/O errors encountered during update.\n    Other exceptions may propagate from\
    \ the underlying update_cli call."
  code_example: null
  example_source: null
  line_start: 207
  line_end: 285
  dependencies:
  - graphrag/cli/index.py::update_cli
  called_by: []
- node_id: graphrag/cli/prompt_tune.py::prompt_tune
  file: graphrag/cli/prompt_tune.py
  name: prompt_tune
  signature: "def prompt_tune(\n    root: Path,\n    config: Path | None,\n    domain:\
    \ str | None,\n    verbose: bool,\n    selection_method: api.DocSelectionType,\n\
    \    limit: int,\n    max_tokens: int,\n    chunk_size: int,\n    overlap: int,\n\
    \    language: str | None,\n    discover_entity_types: bool,\n    output: Path,\n\
    \    n_subset_max: int,\n    k: int,\n    min_examples_required: int,\n)"
  decorators: []
  raises: []
  visibility: public
  docstring: "Prompt tune the model asynchronously.\n\nNote: This coroutine must be\
    \ awaited. It loads the configuration, applies any chunking overrides, initializes\
    \ the root logger according to the verbose flag, and generates indexing prompts.\
    \ It writes the resulting prompts to the specified output directory if an output\
    \ path is provided; otherwise it logs an error and skips writing. Returns None\
    \ upon successful completion.\n\nArgs:\n    root: Path \u2014 The root directory\
    \ to resolve relative paths from.\n    config: Path | None \u2014 Optional path\
    \ to the configuration file.\n    domain: str | None \u2014 The domain to map\
    \ the input documents to.\n    verbose: bool \u2014 Enable verbose logging.\n\
    \    selection_method: api.DocSelectionType \u2014 The chunk selection method.\n\
    \    limit: int \u2014 The limit of chunks to load.\n    max_tokens: int \u2014\
    \ The maximum number of tokens to use on entity extraction prompts.\n    chunk_size:\
    \ int \u2014 The chunk token size to use.\n    overlap: int \u2014 The number\
    \ of tokens to overlap between consecutive chunks.\n    language: str | None \u2014\
    \ The language to use for the prompts.\n    discover_entity_types: bool \u2014\
    \ Generate entity types.\n    output: Path \u2014 The output folder to store the\
    \ prompts.\n    n_subset_max: int \u2014 The number of text chunks to embed when\
    \ using auto selection method.\n    k: int \u2014 The number of documents to select\
    \ when using auto selection method.\n    min_examples_required: int \u2014 The\
    \ minimum number of examples required for entity extraction prompts.\n\nReturns:\n\
    \    None\n\nRaises:\n    Exceptions raised by underlying IO or configuration\
    \ loading may propagate."
  code_example: null
  example_source: null
  line_start: 25
  line_end: 117
  dependencies:
  - graphrag.api::generate_indexing_prompts
  - graphrag/config/load_config.py::load_config
  - graphrag/logger/standard_logging.py::init_loggers
  - graphrag/utils/cli.py::redact
  called_by:
  - graphrag/cli/main.py::_prompt_tune_cli
- node_id: graphrag/utils/cli.py::redact
  file: graphrag/utils/cli.py
  name: redact
  signature: 'def redact(config: dict) -> str'
  decorators: []
  raises: []
  visibility: public
  docstring: "Sanitize secrets in a configuration object by redacting sensitive fields.\n\
    \nThis function traverses the input configuration and redacts values for keys\
    \ identified as sensitive. The redaction keys are currently hard-coded as api_key,\
    \ connection_string, container_name, and organization. If a sensitive key's value\
    \ is None, that key is omitted from the resulting JSON instead of being redacted.\
    \ The function recursively processes nested dictionaries and lists. The set of\
    \ sensitive keys is hard-coded but designed to be extendable.\n\nReturns:\n  \
    \  str: A JSON-formatted string with sensitive values replaced by \"==== REDACTED\
    \ ====\" and with None-valued sensitive keys omitted.\n\nRaises:\n    TypeError:\
    \ If the resulting object cannot be serialized to JSON (for example, when non-serializable\
    \ values are present in the input).\n\nNotes:\n    json.dumps is used for serialization;\
    \ non-serializable input values will trigger a TypeError. To customize redaction\
    \ behavior, modify the sensitive-keys set or implement a separate configuration\
    \ mechanism."
  code_example: null
  example_source: null
  line_start: 27
  line_end: 54
  dependencies:
  - graphrag/utils/cli.py::redact_dict
  called_by:
  - graphrag/api/query.py::local_search_streaming
  - graphrag/api/query.py::drift_search_streaming
  - graphrag/api/query.py::basic_search_streaming
  - graphrag/cli/index.py::_run_index
  - graphrag/cli/prompt_tune.py::prompt_tune
- node_id: graphrag/utils/cli.py::redact_dict
  file: graphrag/utils/cli.py
  name: redact_dict
  signature: 'def redact_dict(config: dict) -> dict'
  decorators: []
  raises: []
  visibility: public
  docstring: "Redact sensitive values in a dictionary.\n\nArgs:\n    config (dict):\
    \ The configuration dictionary to redact.\n\nReturns:\n    dict: A new dictionary\
    \ with sensitive keys redacted. Keys in {\"api_key\", \"connection_string\", \"\
    container_name\", \"organization\"} will have their values replaced with \"====\
    \ REDACTED ====\" when not None. Nested dictionaries and lists are processed recursively;\
    \ non-dictionary/list values are preserved."
  code_example: null
  example_source: null
  line_start: 31
  line_end: 51
  dependencies:
  - graphrag/utils/cli.py::redact_dict
  called_by:
  - graphrag/utils/cli.py::redact
  - graphrag/utils/cli.py::redact_dict
