{
  "repo": "graphrag",
  "docs_path": "/Users/martinsc/PycharmProjects/DocGen/.docs/graphrag/site/docs",
  "total_files": 19,
  "total_sections": 2,
  "structure": {
    "sections": [
      {
        "name": "guide",
        "files": [
          {
            "path": "guide/index.md",
            "title": "graphrag Guide",
            "frontmatter": {
              "sidebar_position": 1
            },
            "content": "# graphrag Guide\n\nWelcome to the graphrag documentation guide.\n\n## Sections\n\n- [Introduction](introduction)\n- [Quick Start](quick-start)\n- [Architecture](architecture)\n- [API Reference](api-reference)\n- [CLI Reference](cli-reference)\n- [Data Model & Ontology](data-model-ontology)\n- [AI Orchestration & LLM Providers](ai-llm-providers)\n- [Storage & Vector Stores](storage-vector-stores)\n- [Indexing & Query Workflows](workflows-indexing-query)"
          },
          {
            "path": "guide/introduction.md",
            "title": "Introduction",
            "frontmatter": {
              "sidebar_position": 2
            },
            "content": "# Introduction\n\nGraphRAG is a Python-based data pipeline and knowledge-graph augmentation framework designed to help you index unstructured private text, extract entities and relations, and build a hierarchical knowledge graph. It then uses large language models (LLMs) to answer questions with context-augmented prompts, enabling retrieval-augmented reasoning over private datasets. The project provides modular components for indexing, prompt tuning, querying, and visualization, making it suitable for researchers and developers who want to experiment with graph-enabled reasoning on proprietary text.\n\nKey concepts and workflow at a glance:\n- Indexing: Slice input text into fine-grained TextUnits, use LLM-driven prompts to extract entities (with configurable entity types), relationships, and optionally claims (factual assertions linked to source text), and organize them into a knowledge graph.\n- Prompt templates: Customizable prompt files for entity extraction, relationship extraction, and community report generation. Templates use token substitution (e.g., {entity_types}, {tuple_delimiter}) for domain adaptation.\n- Ontology: Represent documents, entities, relationships, and communities with a coherent data model and schemas.\n- Graph structure: Build a hierarchical community graph with summaries, enabling holistic and domain-focused reasoning.\n- Prompt augmentation: Leverage the graph, community summaries, and entity context to augment LLM prompts at query time. Default prompts can be overridden via custom prompt files.\n- AI orchestration: Abstracted LLM providers and tokenization backends enable flexible model backends and lifecycle management. Supports OpenAI-compatible endpoints via api_base and proxy configuration for non-OpenAI models.\n- Storage and retrieval: Pluggable storage backends and vector stores support private data handling and scalable retrieval.\n- End-to-end workflows: Clear workflows for data ingestion, graph construction, embedding, updates, and answering questions.\n\nThis documentation summarizes the project, its goals, and the core concepts behind GraphRAG, with a focus on how its modular components come together to enable sophisticated retrieval-augmented reasoning over private text."
          },
          {
            "path": "guide/quick-start.md",
            "title": "Quick Start",
            "frontmatter": {
              "sidebar_position": 3
            },
            "content": "# Quick Start\n\nThis Quick Start guide provides a practical tour to install and begin using GraphRAG, covering setup, indexing, prompt tuning, and querying workflows. It is designed to get you from zero to an actionable index and query flow as quickly as possible.\n\nPrerequisites\n- Python 3.10–3.12\n- uv (the task runner) for dependency management and workflow orchestration\n\n1) Install dependencies\n```\nuv sync\n```\n\n2) Initialize a project/workspace\nThis creates a settings file and environment configuration. Replace [path] with your working directory.\n```\ngraphrag init --root [path] --force\n```\n\n3) Prepare your dataset\nCreate a folder with input text files, e.g. a simple example:\n```\n[path]/input/book.txt\n```\nYou can adapt this to your private dataset; the indexing engine will process documents into TextUnits and extract entities/relationships.\n\n4) Configure a backend (OpenAI or Azure OpenAI)\n- OpenAI: set GRAPHRAG_API_KEY in the generated .env.\n- Azure OpenAI: configure chat and embedding model blocks in settings.yaml (api_base, api_version, deployment_name, etc.).\n\n5) Run the indexing workflow\n```\ngraphrag index --root [path]\n```\nOr, depending on your setup, you may see project-specific commands via uv/poethepoet integration.\n\n6) Prompt tuning (optional but recommended for best results)\n```\nuv run poe prompt_tune --root [path]\n```\n\n7) Query the index\n```\nuv run poe query --root [path] --query \"What are the top themes in this dataset?\"\n```\n\n8) Next steps\n- Inspect the output directory for graph data and parquet artifacts.\n- Try different query modes (global, local, DRIFT, or basic) to compare results.\n- Use the Unified Search Demo UI (if available) for interactive exploration.\n\nFor hands-on examples and a walkthrough, see the Getting Started guide in the docs."
          },
          {
            "path": "guide/architecture.md",
            "title": "Architecture",
            "frontmatter": {
              "sidebar_position": 4
            },
            "content": "# Architecture\n\nGraphRAG is composed of modular components that interact to index data, construct a knowledge graph, and answer questions with LLMs. The architecture emphasizes separation of concerns, extensibility, and pluggable backends.\n\nCore modular components\n- Public API Layer: Public interfaces for indexing, prompt tuning, and querying GraphRAG. File set: graphrag/graphrag/api/__init__.py, index.py, prompt_tune.py, query.py. This layer defines the library-facing API used by clients and subsystems.\n- GraphRAG Command-Line Interface (CLI): Typer-based CLI that orchestrates indexing, prompt tuning, and querying workflows via uv/poethepoet. Files: graphrag/graphrag/cli/__init__.py, main.py, index.py, initialize.py, prompt_tune.py, query.py.\n- Indexing Engine Core & Workflows: End-to-end data ingestion, graph construction, embedding, and update workflows that build and maintain the knowledge graph index. Key files include run_pipeline.py, utils.py, factory.py, and several workflows like load_input_documents.py, update_text_embeddings.py, create_base_text_units.py, update_entities_relationships.py, and operations for graph creation and embedding.\n- Query Engine Core & Interfaces: Query-time context construction, data retrieval, and LLM orchestration utilities used to answer questions against the graph. Files cover context builders, local/entity extraction, global search, and LLM prompts.\n- Knowledge Graph Data Model: Core data structures representing documents, entities, relationships, communities, and metadata. Includes models such as document.py, entity.py, relationship.py, community.py, text_unit.py, schemas.py, and types.py.\n- AI Orchestration & LLM Providers: Abstractions and backends for language models and tokenization. A registry-based framework decouples providers from GraphRAG’s stack, enabling on-demand creation and lifecycle management. Implementations live under graphrag/language_model and graphrag/language_model/providers.\n- Storage & Vector Stores: Storage abstractions and concrete implementations for file/blob storage and vector stores (LanceDB, Cosmos, Azure AI Search, etc.). Core components include FilePipelineStorage, BlobPipelineStorage, and vector_store backends behind a factory. See graphrag/storage and graphrag/vector_stores.\n- Unified Search App (Demo UI): A demo UI built around the GraphRAG index for visualization and interactive search, tying together data loading, knowledge loading, multiple search strategies, and result visualization.\n\nHow the components interact\n1) Ingestion: The Indexing Engine ingests documents, splits them into TextUnits, and extracts entities/relationships to populate the Knowledge Graph Data Model.\n2) Graph construction: The indexing flow builds a hierarchical community graph using the Leiden algorithm with configurable parameters, computes degrees, and stores graph snapshots (e.g., graphml) for visualization.\n3) Embedding & storage: Embeddings are computed and written to the selected vector store; document/text units and graph artifacts are stored via the Storage layer.\n4) Prompt augmentation: During queries, the Query Engine constructs DRIFT/global/local contexts and uses system prompts to guide the LLM, augmented with graph structure and community summaries.\n5) Query & reasoning: The LLMs generate answers with access to the context built from the graph and embeddings, enabling retrieval-augmented reasoning.\n6) Visualization: The Unified Search App provides a demo UI for exploring the index and querying results.\n\nThis architecture supports modular replacement of providers and storage backends, enabling experimentation across different model backends, vector stores, and storage solutions while maintaining a consistent data model and workflow.\n\n## System Diagram\n\n```mermaid\ngraph TB\n  subgraph \"Public Facing\"\n    api[Public API Layer]\n    cli[GraphRAG CLI]\n  end\n  subgraph \"Indexing & Query Engine\"\n    index[Indexing Engine]\n    query[Query Engine]\n  end\n  subgraph \"Knowledge & AI Stack\"\n    kg[Knowledge Graph Data Model]\n    llm[AI Orchestration & LLM Providers]\n  end\n  subgraph \"Storage & UI\"\n    storage[Storage & Vector Stores]\n    ui[Unified Search App]\n  end\n\n  api -- \"start/indexing\" --> index\n  cli -- \"start/indexing\" --> index\n  index -- \"builds/updates graph\" --> kg\n  index -- \"persist graph/vectors\" --> storage\n  index -- \"embed/augment\" --> llm\n  llm -- \"embedding results\" --> index\n  query -- \"retrieve vectors\" --> storage\n  query -- \"query graph\" --> kg\n  query -- \"generate prompt\" --> llm\n  llm -- \"final answer\" --> query\n  api -- \"handle queries\" --> query\n  query -- \"response\" --> api\n  ui -- \"user requests\" --> api\n  api -- \"render results\" --> ui\n```\n\n## Components\n\n**Public API Layer**: Public interfaces for indexing, prompt tuning, and querying GraphRAG exposed to clients.\n\n**GraphRAG CLI**: Command-line tooling to orchestrate indexing, querying, and prompt tuning workflows.\n\n**Indexing Engine**: Core data processing pipeline that ingests documents, builds/upgrades embeddings and the knowledge graph.\n\n**Query Engine**: Context construction, retrieval, and LLM orchestration to answer questions with graph context.\n\n**Knowledge Graph Data Model**: Data structures for documents, entities, relationships, and graphs that form the knowledge base.\n\n**AI Orchestration & LLM Providers**: Abstractions and backends for language models, tokenization, and prompt management.\n\n**Storage & Vector Stores**: Storage abstractions and concrete vector stores for graphs, embeddings, and documents.\n\n**Unified Search App**: Demo UI for searching, loading data, and visualizing the knowledge graph results."
          },
          {
            "path": "guide/api-reference.md",
            "title": "API Reference",
            "frontmatter": {
              "sidebar_position": 5
            },
            "content": "# API Reference\n\nPublic interfaces for indexing, prompt tuning, and querying GraphRAG are organized under the Public API Layer. These interfaces define the library-facing API used by clients and subsystems. The primary entry points are:\n\n- Indexing API (graphrag/graphrag/api/index.py): Public interfaces for ingesting documents, creating text units, and building the knowledge graph index. This layer abstracts the end-to-end indexing workflow so that researchers and applications can programmatically drive indexing with custom data sources.\n- Prompt Tuning API (graphrag/graphrag/api/prompt_tune.py): Public interfaces for generating and applying prompt-tuning configurations. This enables domain-specific tuning of prompts used during query time or during index-time prompts.\n- Query API (graphrag/graphrag/api/query.py): Public interfaces for composing and executing questions against the knowledge graph. This layer orchestrates the creation of retrieval contexts, optional global/local DRIFT reasoning, and LLM-backed answers.\n\nUsage patterns (high level)\n- Initialize or obtain an API client, configure data sources, and call the indexing API to ingest documents and build the graph.\n- Provide tuning configurations or prompts, and use the prompt tuning API to adapt prompts for specific domains.\n- Use the query API to run questions against the index, leveraging context from the graph and vector store for augmented reasoning.\n\nNotes\n- The exact function names and signatures are provided in the corresponding files: graphrag/graphrag/api/index.py, graphrag/graphrag/api/prompt_tune.py, graphrag/graphrag/api/query.py. This section documents the public API surface and its intended usage rather than concrete implementation details."
          },
          {
            "path": "guide/cli-reference.md",
            "title": "CLI Reference",
            "frontmatter": {
              "sidebar_position": 6
            },
            "content": "# CLI Reference\n\nGraphRAG ships a Command-Line Interface (CLI) that drives indexing, prompt tuning, and querying workflows. The CLI is designed to orchestrate project initialization, index construction and updates, knowledge-graph queries, and prompt tuning workflows, integrating configuration, storage, logging, and the API.\n\nKey concepts\n- Typer-based command structure for a clean, scriptable interface.\n- Commands to initialize a project, run the indexer, tune prompts, and query the index from the command line.\n\nCommon commands (examples)\n- Initialize a project and config:\n```\ngraphrag init --root ./christmas --force\n```\n\n- Run the indexing workflow for a dataset:\n```\ngraphrag index --root ./christmas\n```\n\n- Tune prompts for a dataset:\n```\nuv run poe prompt_tune --root ./christmas\n```\n\n- Execute a query against the index:\n```\nuv run poe query --root ./christmas --query \"What are the top themes in this dataset?\"\n```\n\nNote: The CLI relies on the underlying workflow orchestration configured in settings.yaml and uses the same modular components as the API layer. For more detailed command semantics and available options, see the CLI documentation pages in docs/cli.md and the source files listed under graphrag/graphrag/cli/.*."
          },
          {
            "path": "guide/data-model-ontology.md",
            "title": "Data Model & Ontology",
            "frontmatter": {
              "sidebar_position": 7
            },
            "content": "# Data Model & Ontology\n\nCore data structures and schemas define how GraphRAG represents content inside the knowledge graph. The ontology enables serialization/deserialization and consistent data modeling across the GraphRAG domain. Key data model components and their roles include:\n\n- Document: Represents a source document or input corpus with metadata and text content. Provides a factory to build Document instances from dictionaries and maps to the internal storage/graph structures.\n- TextUnit: Fine-grained analyzable units derived from documents, used for extraction and indexing granularity.\n- Entity: Represents an identified entity such as a person, place, organization, or object, with attributes including title/name and identifiers.\n- Relationship: Captures connections between entities with semantic typing (e.g., description of the relationship), edge weights for ranking, and source/target references.\n- Community: Represents clusters or groups within the graph discovered via hierarchical Leiden algorithm, used to summarize and structure the graph hierarchy.\n- CommunityReport: LLM-generated summaries of each community containing key entities, relationships, and claims. Used as context for global search (map-reduce) queries.\n- Schemas & Types: Shared schemas and type definitions that ensure consistent data structures across modules.\n\nFiles involved (examples):\n- graphrag/graphrag/data_model/document.py\n- graphrag/graphrag/data_model/entity.py\n- graphrag/graphrag/data_model/relationship.py\n- graphrag/graphrag/data_model/community.py\n- graphrag/graphrag/data_model/community_report.py\n- graphrag/graphrag/data_model/text_unit.py\n- graphrag/graphrag/data_model/schemas.py\n- graphrag/graphrag/data_model/types.py\n\nHow the ontology is used\n- Ingested documents are broken into TextUnits and mapped to Document metadata.\n- Entities and relationships populate a graph, which is organized into communities with summaries.\n- The graph and community information are leveraged to construct context windows for LLM prompts during queries, enabling more grounded and interpretable answers.\n\nOutput tables\n- Documents table: Source documents with metadata and provenance.\n- Entities table: Extracted entities with attributes, descriptions, and embeddings.\n- Relationships table: Entity connections with semantic types and weights.\n- Communities table: Hierarchical community clustering data from Leiden.\n- Community reports table: LLM-generated summaries with metadata for each community."
          },
          {
            "path": "guide/ai-llm-providers.md",
            "title": "AI Orchestration & LLM Providers",
            "frontmatter": {
              "sidebar_position": 8
            },
            "content": "# AI Orchestration & LLM Providers\n\nGraphRAG abstracts language models, tokenization, and LLM service management to decouple model backends from the rest of the system. This enables flexible selection of providers, lifecycle management, and plug-and-play experimentation with different model families.\n\nArchitectural purpose\n- Registry-based framework to register, instantiate, and manage chat-based and embedding LLM backends.\n- Decoupling of model providers from GraphRAG’s core logic to support on-demand creation and lifecycle management.\n\nKey components and responsibilities\n- language_model/__init__.py: Public surface exposing the language-model stack.\n- language_model/factory.py: Factory for creating model backends based on configuration.\n- language_model/manager.py: Orchestrates model lifecycles, hot swapping, and resource management.\n- language_model/providers/fnllm/models.py and utils.py: Implementations and utilities for the FNLLM provider (functions for model interaction, tokenization, etc.).\n- language_model/providers/litellm/*: Providers and utilities for LiteLLM-based backends.\n- Callback integration: Hooks for LLM responses, streaming, and logging (e.g., graphrag/callbacks/*).\n\nWhat this enables\n- Runtime choice of LLM provider (OpenAI, Azure OpenAI, local/offline models, etc.).\n- Independent tokenization and prompt handling suited to different backends.\n- Safe lifecycle management including initialization, warm-up, and shutdown of model instances.\n\nNote: The actual provider implementations live under graphrag/language_model and graphrag/language_model/providers. This section documents the intended role and integration points rather than concrete provider details."
          },
          {
            "path": "guide/storage-vector-stores.md",
            "title": "Storage & Vector Stores",
            "frontmatter": {
              "sidebar_position": 9
            },
            "content": "# Storage & Vector Stores\n\nGraphRAG uses pluggable storage backends for both pipeline data and results, along with vector stores that enable semantic retrieval in tandem with the knowledge graph.\n\nStorage abstractions\n- File-based storage: A filesystem-backed pipeline storage that manages a root directory for artifacts and outputs.\n- Blob storage: BlobPipelineStorage for cloud-based blob storage integration.\n- Factory and adapters: A storage factory to create endpoints based on configuration, allowing users to plug in their own storage backend.\n\nVector stores and retrieval backends\n- LanceDB: Local vector store backend optimized for fast experimentation and small to medium datasets.\n- Cosmos DB: Cloud-based vector store integration for scalable deployments.\n- Azure AI Search: Cloud-based search service integration for hybrid text/semantic search.\n\nWhere these are defined\n- graphrag/storage/file_pipeline_storage.py\n- graphrag/graphrag/storage/blob_pipeline_storage.py\n- graphrag/graphrag/vector_stores/__init__.py\n- graphrag/graphrag/vector_stores/factory.py\n- graphrag/graphrag/vector_stores/lancedb.py\n- graphrag/graphrag/vector_stores/cosmosdb.py\n- graphrag/graphrag/vector_stores/azure_ai_search.py\n\nConfiguration hints\n- Storage type and base_dir are configured in settings.yaml or environment variables as part of the Storage configuration section.\n- Vector store configuration is similarly supplied as part of the index/query settings, enabling the system to write embeddings directly to the chosen vector store during indexing."
          },
          {
            "path": "guide/workflows-indexing-query.md",
            "title": "Indexing & Query Workflows",
            "frontmatter": {
              "sidebar_position": 10
            },
            "content": "# Indexing & Query Workflows\n\nEnd-to-end workflows define how data flows from ingestion to answering questions against the knowledge graph. The workflows coordinate multiple components to build, update, and query the index. Incremental indexing is supported for updating indices with new data while preserving existing outputs.\n\nCore workflows and components\n- load_input_documents.py: Ingests input sources (Plain Text, CSV, JSON) with schema validation via InputConfig and prepares documents for indexing.\n- create_base_text_units.py: Splits documents into TextUnits with configurable chunk size and overlap; supports document boundary alignment.\n- update_text_embeddings.py: Computes and updates embeddings for text units, feeding the vector store.\n- update_entities_relationships.py: Extracts and updates entities and relationships in the graph, maintaining graph integrity.\n- create_graph.py: Creates the graph structure from entities and relationships, including community hierarchies.\n- embed_graph/embed_graph.py: Embeds graph-level representations and prepares graph-aware context for queries.\n- embed_graph/embed_node2vec.py: Optional node2vec embeddings for enhanced graph embeddings.\n- compute_degree.py: Computes node degrees and centrality metrics for ranking and visualization.\n- prune_graph.py: Prunes the graph to remove duplicates or low-quality edges/nodes.\n- snapshot_graphml.py: Outputs a GraphML snapshot for visualization tools.\n- validate_config.py: Validates configuration in settings to ensure end-to-end workflows can run.\n- state.py: Internal state definitions for workflow orchestration.\n\nEnd-to-end lifecycle\n1) Ingestion: Load input documents and create base text units.\n2) Embedding: Compute embeddings and push to the vector store; store text/metadata artifacts.\n3) Graph construction: Build the knowledge graph from entities, relationships, and communities; update degrees and prune if necessary.\n4) Description summarization: Summarize entity and relationship descriptions using an LLM. Since entities/relationships may be found in many text units, each with its own description, these are combined into a single concise summary per entity/relationship.\n5) Graph enrichment: Generate community reports (LLM-generated summaries of each community's entities, relationships, and claims) and graph-level features.\n6) Query-time preparation: Assemble context using global (map-reduce over community reports), local (entity-based context), or DRIFT strategies.\n7) Question answering: Use LLM prompts augmented with context to generate answers.\n8) Visualization and inspection: Export graph snapshots and enable exploration via UI or GraphML files.\n\nThis orchestration enables researchers and developers to iterate on indexing strategies, graph structures, and prompt design while maintaining a consistent workflow and data model."
          }
        ]
      },
      {
        "name": "modules",
        "files": [
          {
            "path": "modules/public-api-layer.md",
            "title": "Public API Layer",
            "frontmatter": {
              "sidebar_position": 1
            },
            "content": "# Public API Layer\n\nPublic interfaces for indexing, prompt tuning, and querying GraphRAG. These define the library-facing API used by clients and other subsystems.\n\n## Overview\n\nPublic API surface for GraphRAG. This module defines the library-facing entry points that enable clients to index data, generate indexing prompts, and perform queries against GraphRAG. It coordinates the public interfaces exposed by graphrag.api.index, graphrag.api.prompt_tune, and graphrag.api.query, providing a cohesive surface for library users.\n\nArchitectural purpose\n- Expose and coordinate the GraphRAG public API across indexing, prompt tuning, and querying subsystems.\n- Re-export and document the primary entry points so clients interact with a stable surface rather than internal implementations.\n- Centralize high-level orchestration while delegating concrete work to the underlying submodules.\n\nKey components and responsibilities\n- graphrag.api.index: Utilities to configure and run the GraphRAG indexing pipeline. Responsibilities include resolving the final indexing method (_get_method) and executing the indexing workflow against a GraphRagConfig (build_index).\n- graphrag.api.prompt_tune: Utilities to generate indexing prompts for GraphRAG prompt tuning. Exposes generate_indexing_prompts to assemble prompts from domain content, entity types, entity relationships, and community reports.\n- graphrag.api.query: Query interfaces for the GraphRAG API. Provides high-level entry points to perform global, local, and drift searches, with both streaming and non-streaming variants, coordinating with a GraphRagConfig and related data structures.\n\nMain entry points / public APIs\n- _get_method, build_index (from graphrag.api.index): determine and execute the appropriate indexing workflow.\n- generate_indexing_prompts (from graphrag.api.prompt_tune): construct indexing prompts by coordinating multiple prompt-generation components.\n- on_context, global_search_streaming, global_search, multi_index_global_search, basic_search, basic_search_streaming, drift_search, drift_search_streaming (from graphrag.api.query): provide diverse query capabilities across contexts, global and drift searches, with streaming and non-streaming variants.\n\nUsage notes / side effects\n- APIs rely on GraphRagConfig and related data structures; they may perform I/O, streaming, and data transformations as part of their operations.\n- Functions may raise library-defined runtime errors to signal misconfiguration or invalid usage.\n\nArgs\n- This module does not accept module-level arguments. Client usage occurs via the exported functions described above.\n\nReturns\n- This module itself returns no value. Public API functions return results appropriate to their operations (results, streams, or data structures) as implemented in their respective components.\n\nRaises\n- RuntimeError and library-specific exceptions defined by graphrag.api may be raised to indicate misconfiguration, invalid inputs, or usage errors.\n\n## Files in this Module\n\n- [`graphrag/api/__init__.py`](../files/graphrag-api-init)\n- [`graphrag/api/index.py`](../files/graphrag-api-index)\n- [`graphrag/api/prompt_tune.py`](../files/graphrag-api-prompt-tune)\n- [`graphrag/api/query.py`](../files/graphrag-api-query)"
          },
          {
            "path": "modules/graphrag-command-line-interface.md",
            "title": "GraphRAG Command-Line Interface",
            "frontmatter": {
              "sidebar_position": 2
            },
            "content": "# GraphRAG Command-Line Interface\n\nCLI entry points and tooling to run indexing, querying, and prompt tuning workflows via uv/poethepoet.\n\n## Overview\n\nGraphRAG Command-Line Interface (CLI) for indexing, querying, and prompt tuning workflows via uv/poethepoet.\n\nArchitectural purpose:\n- Provide a Typer-based CLI layer that orchestrates project initialization, index construction and updates, knowledge-graph queries, and prompt tuning workflows, integrating with configuration, storage, logging, and the GraphRAG API.\n\nKey components and responsibilities:\n- graphrag.cli.main: Implements the core Typer-based command implementations and helpers used by Graphrag's CLI. It wires together project initialization, index construction and updates, knowledge-graph queries, and prompt tuning workflows, while also exposing the primary CLI entry points.\n- graphrag.cli.index: GraphRag CLI indexing utilities. This module provides command-line interfaces to run the GraphRag indexing and update pipelines, integrating with the GraphRag API, console workflow callbacks, configuration loading and validation, redaction utilities, and logging. It also defines signal handling and related tooling.\n- graphrag.cli.initialize: GraphRag CLI initialization utilities. Purpose: module that provides the CLI entry point functionality to initialize a GraphRag project at a given filesystem path by creating initial configuration files and preparing prompt templates.\n- graphrag.cli.prompt_tune: Asynchronous prompt tuning orchestration for the GraphRag CLI. Purpose: this module exposes the prompt_tune coroutine which coordinates configuration loading, chunking overrides, logging initialization, and indexing-prompt generation for prompt tuning.\n- graphrag.cli.query: GraphRag CLI query module. Overview: this module provides the command-line interfaces to run GraphRag queries in multiple modes (global, local, drift, and basic) with optional streaming and integrated configuration and storage support.\n\nMain entry points / public APIs:\n- main.py: wildcard_match, path_autocomplete, completer, _initialize_cli, _query_cli, _index_cli, _prompt_tune_cli, _update_cli\n- index.py: handle_signal, _register_signal_handlers, _run_index, index_cli, update_cli\n- initialize.py: initialize_project_at\n- prompt_tune.py: prompt_tune\n- query.py: on_context, run_streaming_search, _resolve_output_files, run_global_search, run_local_search, run_drift_search, run_basic_search\n\n## Files in this Module\n\n- [`graphrag/cli/__init__.py`](../files/graphrag-cli-init)\n- [`graphrag/cli/main.py`](../files/graphrag-cli-main)\n- [`graphrag/cli/index.py`](../files/graphrag-cli-index)\n- [`graphrag/cli/initialize.py`](../files/graphrag-cli-initialize)\n- [`graphrag/cli/prompt_tune.py`](../files/graphrag-cli-prompt-tune)\n- [`graphrag/cli/query.py`](../files/graphrag-cli-query)"
          },
          {
            "path": "modules/indexing-engine-core-workflows.md",
            "title": "Indexing Engine Core & Workflows",
            "frontmatter": {
              "sidebar_position": 3
            },
            "content": "# Indexing Engine Core & Workflows\n\nCore indexing pipeline, graph construction, embedding, and update workflows that build and maintain the knowledge graph index.\n\n## Overview\n\nIndexing Engine Core & Workflows: Core indexing pipeline, graph construction, embedding, and update workflows that build and maintain the knowledge graph index.\n\nArchitectural purpose\n- This package implements the end-to-end GraphRAG-based indexing platform, including the core pipeline, graph construction, embedding, and incremental update workflows to build and maintain a knowledge graph index.\n\nKey components and responsibilities\n- graphrag/index/run/run_pipeline.py: Utilities for running the GraphRag index run pipeline, including execution helpers, run state persistence, and output transfer between storages. Public: run_pipeline; private: _dump_json, _copy_previous_output, _run_pipeline.\n- graphrag/index/run/utils.py: Utilities to assemble a run context, create a callback chain, and derive update-related storage objects. Exports: create_callback_chain, create_run_context, get_update_storages.\n- graphrag/index/workflows/factory.py: GraphRag workflows factory for building pipelines of workflow functions. Coordinates registration and construction of pipelines, maintains a class-level registry of named WorkflowFunction callables, and can assemble pipelines. Public: register, register_all, create_pipeline, register_pipeline; Class: PipelineFactory.\n- graphrag/index/workflows/load_input_documents.py: Load and manage input documents for the GraphRag index workflow; supports multi-format ingestion (Plain Text, CSV, JSON) with schema validation via InputConfig; parses into a standard pandas DataFrame and persists to storage. Public API: load_input_documents, run_workflow.\n- graphrag/index/workflows/update_text_embeddings.py: Module for updating text embeddings during incremental index runs; defines run_workflow to update embeddings based on incremental updates and persists results to storage, leveraging generate_text_embeddings and write_table_to_storage.\n- graphrag/index/workflows/create_base_text_units.py: Module to generate base text units for GraphRAG indexing; utilities to convert input documents into base text units by grouping texts, chunking, and applying optional metadata preprocessing; exposes run_workflow.\n- graphrag/index/workflows/update_entities_relationships.py: Utilities to update entities and relationships during incremental index runs; defines _update_entities_and_relationships and run_workflow to merge previous state with delta updates and update/merge relationships.\n- graphrag/index/operations/create_graph.py: Utilities to construct NetworkX graphs from tabular data; create_graph builds a Graph from a DataFrame of edges and optional node attributes, with support for edge attributes and a node identifier column parameter.\n- graphrag/index/operations/embed_graph/embed_graph.py: Embed graphs into vector space using node2vec; exposes embed_graph to map a NetworkX graph to embedding vectors, with configuration support.\n- graphrag/index/operations/embed_graph/embed_node2vec.py: Node2Vec-based embedding for NetworkX graphs; function embed_node2vec computes node embeddings for Graphs suitable for downstream tasks.\n- graphrag/index/operations/compute_degree.py: Compute the degree of each node in a NetworkX graph and return a pandas DataFrame.\n- graphrag/index/operations/prune_graph.py: Prune graphs by filtering nodes and edges by frequency, degree, and edge weights; exports _get_upper_threshold_by_std and prune_graph.\n- graphrag/index/validate_config.py: GraphRag configuration validation utility; function validate_config_names.\n- graphrag/index/typing/state.py: Typing/state module describing common state representations used by the workflows.\n- graphrag/index/operations/snapshot_graphml.py: Snapshot GraphMLs of graphs to a storage backend; function snapshot_graphml.\n\nPublic entry points / main APIs\n- Public entry points include run_pipeline (graphrag/index/run/run_pipeline.py) and internal helpers (_dump_json, _copy_previous_output, _run_pipeline).\n- Run utilities: create_callback_chain, create_run_context, get_update_storages (graphrag/index/run/utils.py).\n- Workflow factory and assembly: PipelineFactory and functions in graphrag/index/workflows/factory.py (register, register_all, create_pipeline, register_pipeline).\n- Public workflow entry points: load_input_documents.run_workflow, create_base_text_units.run_workflow, update_text_embeddings.run_workflow, update_entities_relationships.run_workflow.\n- Graph construction and embeddings: create_graph, embed_graph, embed_node2vec.\n- Graph analysis: compute_degree.\n- Graph pruning: prune_graph and helper _get_upper_threshold_by_std.\n- Configuration validation: validate_config_names.\n- GraphML snapshot: snapshot_graphml.\n\n## Files in this Module\n\n- [`graphrag/index/run/run_pipeline.py`](../files/graphrag-index-run-run-pipeline)\n- [`graphrag/index/run/utils.py`](../files/graphrag-index-run-utils)\n- [`graphrag/index/workflows/factory.py`](../files/graphrag-index-workflows-factory)\n- [`graphrag/index/workflows/load_input_documents.py`](../files/graphrag-index-workflows-load-input-documents)\n- [`graphrag/index/workflows/update_text_embeddings.py`](../files/graphrag-index-workflows-update-text-embeddings)\n- [`graphrag/index/workflows/create_base_text_units.py`](../files/graphrag-index-workflows-create-base-text-units)\n- [`graphrag/index/workflows/update_entities_relationships.py`](../files/graphrag-index-workflows-update-entities-relationships)\n- [`graphrag/index/operations/create_graph.py`](../files/graphrag-index-operations-create-graph)\n- [`graphrag/index/operations/embed_graph/embed_graph.py`](../files/graphrag-index-operations-embed-graph-embed-graph)\n- [`graphrag/index/operations/embed_graph/embed_node2vec.py`](../files/graphrag-index-operations-embed-graph-embed-node2vec)\n- [`graphrag/index/operations/compute_degree.py`](../files/graphrag-index-operations-compute-degree)\n- [`graphrag/index/operations/prune_graph.py`](../files/graphrag-index-operations-prune-graph)\n- [`graphrag/index/validate_config.py`](../files/graphrag-index-validate-config)\n- [`graphrag/index/typing/state.py`](../files/graphrag-index-typing-state)\n- [`graphrag/index/operations/snapshot_graphml.py`](../files/graphrag-index-operations-snapshot-graphml)"
          },
          {
            "path": "modules/query-engine-core-interfaces.md",
            "title": "Query Engine Core & Interfaces",
            "frontmatter": {
              "sidebar_position": 4
            },
            "content": "# Query Engine Core & Interfaces\n\nQuery-time context construction, data retrieval, and LLM orchestration utilities used to answer questions against the knowledge graph.\n\n## Overview\n\nQuery Engine Core & Interfaces for Graphrag: query-time context construction, data retrieval, and LLM orchestration utilities used to answer questions against the knowledge graph.\n\nArchitectural overview\n- Provides a cohesive framework to construct DRIFT-ready context, extract and map entities from queries, orchestrate structured global search workflows, and support LLM-based answer generation against a knowledge graph.\n- Comprises builders, utilities, and prompts that together enable end-to-end query answering from context construction to result synthesis. It relies on pandas for context representation (DataFrame) where appropriate.\n\nKey components and responsibilities\n- graphrag/query/context_builder/builders.py: Module for constructing DRIFT context used to prime downstream search actions for a given query. This module defines an abstract interface and concrete builders that assemble the context required by DRIFT-based search processes. It relies on pandas for context representation (DataFrame).\n- graphrag/query/context_builder/local_context.py: Utilities for constructing context data for graph-based prompt systems. Purpose: This module provides helper functions to assemble context data tables (entities, covariates, and relationships) into text blocks and structured DataFrames suitable for inclusion in system prompts.\n  Functions: build_entity_context, build_covariates_context, get_candidate_context, _filter_relationships, build_relationship_context\n- graphrag/query/context_builder/entity_extraction.py: Graphrag query context: entity extraction utilities for mapping user queries to Entity objects using vector stores and a relationship graph. Overview This module provides utilities to extract entities from a user query and map them to Entity objects within Graphrag's query context.\n  Functions: from_string, find_nearest_neighbors_by_entity_rank, map_query_to_entities\n  Classes: EntityVectorStoreKey\n- graphrag/query/structured_search/global_search/search.py: Module implementing a structured global search workflow that orchestrates parallel batches of community report summaries, maps each batch to an answer, and reduces the results into a final user-facing response using a language model. This module exposes the GlobalSearch class, which coordinates initialization, streaming, and reduction of search results.\n  Functions: __init__, search, _map_response_single_batch, _stream_reduce_response, stream_search, _reduce_response, _parse_search_response\n  Classes: GlobalSearch\n- graphrag/query/llm/text_utils.py: Utilities for batching, JSON cleaning/parsing, and token-based text chunking to support LLM workflows. This module provides helpers to: - batch data into fixed-size chunks for batched LLM prompts or processing (batched) - repair and parse JSON-like content produced by language models into native Py...\n  Functions: batched, try_parse_json_object, chunk_text\n- graphrag/prompts/query/global_search_knowledge_system_prompt.py: Prompt definition for the knowledge system in global search scenarios.\n- graphrag/prompts/query/local_search_system_prompt.py: Prompt definition for local search system prompts.\n- graphrag/prompts/query/question_gen_system_prompt.py: Prompt definition for question generation to guide LLMs.\n\nMain entry points / public APIs\n- Context builders\n  - DRIFTContextBuilder, BasicContextBuilder, GlobalContextBuilder, LocalContextBuilder (graphrag/query/context_builder/builders.py)\n  - build_context (as part of the DRIFT context construction workflow)\n- Local context utilities\n  - build_entity_context, build_covariates_context, get_candidate_context, _filter_relationships, build_relationship_context (graphrag/query/context_builder/local_context.py)\n- Entity extraction and mapping\n  - from_string, find_nearest_neighbors_by_entity_rank, map_query_to_entities (graphrag/query/context_builder/entity_extraction.py)\n  - EntityVectorStoreKey (graphrag/query/context_builder/entity_extraction.py)\n- Structured global search orchestration\n  - GlobalSearch class (graphrag/query/structured_search/global_search/search.py)\n  - __init__, search, stream_search, _parse_search_response, _map_response_single_batch, _reduce_response, _stream_reduce_response (graphrag/query/structured_search/global_search/search.py)\n- LLM text utilities\n  - batched, try_parse_json_object, chunk_text (graphrag/query/llm/text_utils.py)\n- Prompts\n  - graphrag/prompts/query/global_search_knowledge_system_prompt.py\n  - graphrag/prompts/query/local_search_system_prompt.py\n  - graphrag/prompts/query/question_gen_system_prompt.py\n\nThis module serves as the architectural hub for the Graphrag query experience, wiring together context construction, entity grounding, structured search orchestration, and LLM-driven response synthesis for answering questions against the knowledge graph.\n\n## Files in this Module\n\n- [`graphrag/query/__init__.py`](../files/graphrag-query-init)\n- [`graphrag/query/context_builder/builders.py`](../files/graphrag-query-context-builder-builders)\n- [`graphrag/query/context_builder/local_context.py`](../files/graphrag-query-context-builder-local-context)\n- [`graphrag/query/context_builder/entity_extraction.py`](../files/graphrag-query-context-builder-entity-extraction)\n- [`graphrag/query/structured_search/global_search/search.py`](../files/graphrag-query-structured-search-global-search-search)\n- [`graphrag/query/llm/text_utils.py`](../files/graphrag-query-llm-text-utils)\n- [`graphrag/prompts/query/global_search_knowledge_system_prompt.py`](../files/graphrag-prompts-query-global-search-knowledge-system-prompt)\n- [`graphrag/prompts/query/local_search_system_prompt.py`](../files/graphrag-prompts-query-local-search-system-prompt)\n- [`graphrag/prompts/query/question_gen_system_prompt.py`](../files/graphrag-prompts-query-question-gen-system-prompt)"
          },
          {
            "path": "modules/knowledge-graph-data-model.md",
            "title": "Knowledge Graph Data Model",
            "frontmatter": {
              "sidebar_position": 5
            },
            "content": "# Knowledge Graph Data Model\n\nCore data structures and schemas that represent documents, entities, relationships, communities, and associated metadata in the knowledge graph.\n\n## Overview\n\nKnowledge Graph Data Model\n\nArchitectural purpose:\nCore data structures and schemas that represent documents, entities, relationships, communities, and associated metadata in the knowledge graph, enabling serialization/deserialization and consistent data modeling across the GraphRag domain.\n\nKey components and responsibilities:\n- graphrag/data_model/document.py: Document dataclass model; factory to build Document instances from dictionaries with configurable key mappings; exports Document and from_dict.\n- graphrag/data_model/entity.py: Entity dataclass; from_dict for deserializing Entity from dictionaries with configurable key mappings for identifiers, metadata, embeddings, and more.\n- graphrag/data_model/relationship.py: Relationship dataclass; from_dict constructor to instantiate from external data formats; public API includes Relationship and from_dict.\n- graphrag/data_model/community.py: Community dataclass; extends Named; represents a Community with identity, naming semantics, metadata and relationships.\n- graphrag/data_model/community_report.py: CommunityReport dataclass; inherits Named; from_dict constructor for building instances representing community reports.\n- graphrag/data_model/text_unit.py: TextUnit dataclass; encapsulates a unit of text with associated identifiers linking to entities, relationships, covariates, and related documents; inherits from Id...\n- graphrag/data_model/schemas.py and graphrag/data_model/types.py: Schema and type definitions that underpin the data models.\n\nMain entry points / public APIs:\n- Document.from_dict\n- Entity.from_dict\n- Relationship.from_dict\n- Community.from_dict\n- CommunityReport.from_dict\n- TextUnit.from_dict\n- The Document, Entity, Relationship, Community, CommunityReport, and TextUnit dataclasses\n\nNotes:\n- This module aggregates the core data models used to represent graph constructs and their metadata in the GraphRag system.\n\n## Files in this Module\n\n- [`graphrag/data_model/document.py`](../files/graphrag-data-model-document)\n- [`graphrag/data_model/entity.py`](../files/graphrag-data-model-entity)\n- [`graphrag/data_model/relationship.py`](../files/graphrag-data-model-relationship)\n- [`graphrag/data_model/community.py`](../files/graphrag-data-model-community)\n- [`graphrag/data_model/community_report.py`](../files/graphrag-data-model-community-report)\n- [`graphrag/data_model/text_unit.py`](../files/graphrag-data-model-text-unit)\n- [`graphrag/data_model/schemas.py`](../files/graphrag-data-model-schemas)\n- [`graphrag/data_model/types.py`](../files/graphrag-data-model-types)"
          },
          {
            "path": "modules/ai-orchestration-llm-providers.md",
            "title": "AI Orchestration & LLM Providers",
            "frontmatter": {
              "sidebar_position": 6
            },
            "content": "# AI Orchestration & LLM Providers\n\nAbstractions and backends for language models, tokenization, and LLM service management used across indexing and querying.\n\n## Overview\n\nAbstractions and backends for language models, tokenization, and LLM service orchestration used across indexing and querying.\n\nArchitectural purpose\nProvide an extensible, registry-based framework to register, instantiate, and manage chat and embedding LLM backends, decoupling model providers from Graphrag's language-model stack and enabling on-demand creation and lifecycle management.\n\nKey components and responsibilities\n- graphrag/language_model/__init__.py: module surface that exposes the language_model package.\n- graphrag/language_model/factory.py: ModelFactory with registries for embedding and chat models; exposes API to register model backends and instantiate models by type.\n- graphrag/language_model/manager.py: ModelManager singleton for on-demand creation, registration, retrieval, and listing of ChatModel and EmbeddingModel instances; delegates instantiation to ModelFactory.\n- graphrag/language_model/providers/fnllm/models.py: FNLLM-based providers for embeddings and chat (OpenAI and Azure OpenAI); concrete classes: OpenAIEmbeddingFNLLM, OpenAIChatFNLLM, AzureOpenAIEmbeddingFNLLM, AzureOpenAIChatFNLLM.\n- graphrag/language_model/providers/fnllm/utils.py: Utilities for FNLLM OpenAI provider; helpers for error handling, coroutine execution, model parameter derivation, and OpenAI config.\n- graphrag/language_model/providers/litellm/chat_model.py: Graphrag Litellm chat model wrapper with streaming, caching, and resilience features; class: LitellmChatModel.\n- graphrag/language_model/providers/litellm/embedding_model.py: Litellm embedding wrapper for vector endpoints; class: LitellmEmbeddingModel.\n- graphrag/language_model/response/base.py: Typed containers for LLM provider responses; classes: ModelResponse, ModelOutput; helpers: output, history, parsed_response, content, full_response.\n\nMain entry points / public APIs\n- ModelFactory: registry-based factory for creating chat and embedding backends. Exposed API surface includes:\n  - register_embedding\n  - get_embedding_models\n  - create_chat_model\n  - is_supported_model\n  - is_supported_chat_model\n  - create_embedding_model\n  - is_supported_embedding_model\n  - get_chat_models\n- ModelManager: singleton coordinating lifecycle of model instances; exposed methods include:\n  - get_or_create_chat_model\n  - list_chat_models\n  - remove_chat\n  - list_embedding_models\n  - get_chat_model\n  - get_or_create_embedding_model\n  - get_instance\n  - register_embedding\n- Provider classes:\n  - OpenAIEmbeddingFNLLM\n  - OpenAIChatFNLLM\n  - AzureOpenAIEmbeddingFNLLM\n  - AzureOpenAIChatFNLLM\n  - LitellmEmbeddingModel\n  - LitellmChatModel\n- Response containers and helpers:\n  - ModelResponse\n  - ModelOutput\n  - output\n  - history\n  - parsed_response\n  - content\n  - full_response\n\nNotes\n- Public API methods document parameter expectations, return values, and possible errors in their own docstrings; this module-level docstring provides architectural context and a high-level API surface.\n- Implementations may raise runtime errors such as unsupported model types or invalid configurations; refer to individual components for exact exceptions and handling.\n\nImplementation details\n- Implementation specifics are trimmed from this docstring; see the respective module and class docstrings for behavior, parameters, returns, and error handling.\n\n## Files in this Module\n\n- [`graphrag/language_model/__init__.py`](../files/graphrag-language-model-init)\n- [`graphrag/language_model/factory.py`](../files/graphrag-language-model-factory)\n- [`graphrag/language_model/manager.py`](../files/graphrag-language-model-manager)\n- [`graphrag/language_model/providers/fnllm/models.py`](../files/graphrag-language-model-providers-fnllm-models)\n- [`graphrag/language_model/providers/fnllm/utils.py`](../files/graphrag-language-model-providers-fnllm-utils)\n- [`graphrag/language_model/providers/litellm/chat_model.py`](../files/graphrag-language-model-providers-litellm-chat-model)\n- [`graphrag/language_model/providers/litellm/embedding_model.py`](../files/graphrag-language-model-providers-litellm-embedding-model)\n- [`graphrag/language_model/response/base.py`](../files/graphrag-language-model-response-base)"
          },
          {
            "path": "modules/storage-vector-store-integrations.md",
            "title": "Storage & Vector Store Integrations",
            "frontmatter": {
              "sidebar_position": 7
            },
            "content": "# Storage & Vector Store Integrations\n\nStorage abstractions and concrete implementations for file/blob storage and vector stores (LanceDB, Cosmos, Azure AI Search, etc.).\n\n## Overview\n\nStorage and vector store integrations for GraphRAG.\n\nArchitectural purpose\nProvide pluggable storage backends for pipeline data and results, including filesystem-based and Azure Blob Storage-backed storage, together with vector store backends that support text- and vector-based retrieval for GraphRAG.\n\nKey components and responsibilities\n- graphrag.storage.file_pipeline_storage.FilePipelineStorage: filesystem-backed implementation of the PipelineStorage interface. Manages a root directory (creating it if necessary) and offers operations to read, write, clear, list keys, and filter or traverse items.\n- graphrag.storage.blob_pipeline_storage.BlobPipelineStorage: Azure Blob Storage-backed implementation of the PipelineStorage interface used to cache pipeline results and data. Stores dataframe exports as JSON (and Parquet) where applicable; provides helpers like _abfs_url, _set_df_json, _set_df_parquet, and operations such as find, clear, get, and _create_container.\n- graphrag.vector_stores.lancedb.LanceDBVectorStore: LanceDB-backed vector store for GraphRAG. Supports similarity search by text and by vector, loading documents, filtering by id, and connection management.\n- graphrag.vector_stores.cosmosdb.CosmosDBVectorStore: Cosmos DB-backed vector store. Supports text- and vector-based retrieval, loading documents, filtering by id, and internal utilities like cosine_similarity, _create_database, _create_container.\n- graphrag.vector_stores.azure_ai_search.AzureAISearchVectorStore: Azure AI Search-backed vector store. Supports similarity search by vector and text, loading documents, and id-based filtering; connects to the Azure Cognitive Search index.\n\n- graphrag.vector_stores.factory.VectorStoreFactory: registry-based factory to construct vector store instances from registered implementations. Maintains a registry mapping vector_store_type keys to creator callables. Provides create_vector_store, get_vector_store_types, is_supported_type, and register.\n\nMain entry points / public APIs\n- FilePipelineStorage and BlobPipelineStorage as storage backends for GraphRAG pipelines.\n- LanceDBVectorStore, CosmosDBVectorStore, AzureAISearchVectorStore as concrete vector store implementations.\n- VectorStoreFactory (and its public methods) to instantiate vector stores from registered types.\n\n## Files in this Module\n\n- [`graphrag/storage/__init__.py`](../files/graphrag-storage-init)\n- [`graphrag/storage/file_pipeline_storage.py`](../files/graphrag-storage-file-pipeline-storage)\n- [`graphrag/storage/blob_pipeline_storage.py`](../files/graphrag-storage-blob-pipeline-storage)\n- [`graphrag/vector_stores/__init__.py`](../files/graphrag-vector-stores-init)\n- [`graphrag/vector_stores/factory.py`](../files/graphrag-vector-stores-factory)\n- [`graphrag/vector_stores/lancedb.py`](../files/graphrag-vector-stores-lancedb)\n- [`graphrag/vector_stores/cosmosdb.py`](../files/graphrag-vector-stores-cosmosdb)\n- [`graphrag/vector_stores/azure_ai_search.py`](../files/graphrag-vector-stores-azure-ai-search)"
          },
          {
            "path": "modules/unified-search-app-demo-ui.md",
            "title": "Unified Search App (Demo UI)",
            "frontmatter": {
              "sidebar_position": 8
            },
            "content": "# Unified Search App (Demo UI)\n\nDemo UI and data loading components for Unified Search, built around the GraphRAG index and results visualization.\n\n## Overview\n\nUnified Search App (Demo UI) - Demo UI and data loading components for Unified Search, built around the GraphRAG index and results visualization.\n\nPurpose:\n    Provide a cohesive client-side dashboard and data-loading stack that orchestrates dataset loading, knowledge model provisioning, multiple search strategies (global, local, drift, and basic), question generation, and visualization using the GraphRAG framework. This module ties together the demo UI (Streamlit-based) with the knowledge-loader stack and GraphRAG visualization components to enable end-to-end experimentation and demonstration.\n\nArchitecture:\n    - app: Streamlit-based UI and orchestration layer that wires the app logic to UI components (GraphRAG UI, questions, reports, side bar).\n    - knowledge_loader: data loading layer with support for blob and local storage, dataset discovery, and prompts.\n    - rag: GraphRAG integration typings used across the app.\n    - ui: rendering utilities for search results, citations, HTML rendering, and graph visualizations.\n    - data sources: default, blob_source, and loader utilities to construct and read knowledge data.\n\nPublic APIs:\n    - app_logic: load_knowledge_model; dataset_name; run_global_search_question_generation; run_global_search; run_drift_search; run_local_search; run_basic_search; run_generate_questions\n    - home_page: main; on_click_reset; on_change\n    - knowledge_loader.model: load_entities; load_entity_relationships; load_communities; load_covariates; load_community_reports; load_text_units; load_model\n    - knowledge_loader.data_sources.blob_source: BlobDatasource class with __init__, _get_container, load_blob_prompt_config, load_blob_file, read, read_settings\n    - knowledge_loader.data_sources.loader: _get_base_path; create_datasource; load_dataset_listing; load_prompts\n    - rag.typing: type definitions for GraphRAG integration\n    - ui.search: init_search_ui; render_html_table; convert_numbered_list_to_array; format_response_hyperlinks_by_key; get_ids_per_key; format_suggested_questions; format_response_hyperlinks; display_citations\n    - ui.sidebar: update_basic_rag; reset_app; update_global_search; lookup_label; update_drift_search; update_local_search; create_side_bar; update_dataset\n    - ui.full_graph: create_full_graph_ui\n\n## Files in this Module\n\n- [`unified-search-app/app/__init__.py`](../files/unified-search-app-app-init)\n- [`unified-search-app/app/app_logic.py`](../files/unified-search-app-app-app-logic)\n- [`unified-search-app/app/home_page.py`](../files/unified-search-app-app-home-page)\n- [`unified-search-app/app/knowledge_loader/model.py`](../files/unified-search-app-app-knowledge-loader-model)\n- [`unified-search-app/app/knowledge_loader/data_sources/default.py`](../files/unified-search-app-app-knowledge-loader-data-sources-default)\n- [`unified-search-app/app/knowledge_loader/data_sources/blob_source.py`](../files/unified-search-app-app-knowledge-loader-data-sources-blob-source)\n- [`unified-search-app/app/knowledge_loader/data_sources/loader.py`](../files/unified-search-app-app-knowledge-loader-data-sources-loader)\n- [`unified-search-app/app/rag/typing.py`](../files/unified-search-app-app-rag-typing)\n- [`unified-search-app/app/ui/search.py`](../files/unified-search-app-app-ui-search)\n- [`unified-search-app/app/ui/sidebar.py`](../files/unified-search-app-app-ui-sidebar)\n- [`unified-search-app/app/ui/full_graph.py`](../files/unified-search-app-app-ui-full-graph)"
          }
        ]
      }
    ],
    "files": [
      {
        "path": "index.md",
        "title": "graphrag Documentation",
        "frontmatter": {
          "sidebar_position": 1,
          "slug": "/"
        },
        "content": "# graphrag Documentation\n\nGraphRAG is a Python-based data pipeline and knowledge-graph augmentation framework designed to help you index unstructured private text, extract entities and relations, and build a hierarchical knowledge graph. It then uses large language models (LLMs) to answer questions with context-augmented prompts, enabling retrieval-augmented reasoning over private datasets. The project provides modular components for indexing, prompt tuning, querying, and visualization, making it suitable for researchers...\n\n## Navigation\n\n- **[Guide](guide/)** - Introduction, Quick Start, and How-To guides\n- **[Modules](modules/)** - High-level architectural components\n- **[Files](files/)** - File-level documentation  \n- **[API Reference](api/)** - Classes and functions\n\n## Modules\n\nThis codebase is organized into the following architectural modules:\n\n### [Public API Layer](modules/public-api-layer)\n\nPublic interfaces for indexing, prompt tuning, and querying GraphRAG. These define the library-facing API used by clients and other subsystems.\n\n### [GraphRAG Command-Line Interface](modules/graphrag-command-line-interface)\n\nCLI entry points and tooling to run indexing, querying, and prompt tuning workflows via uv/poethepoet.\n\n### [Indexing Engine Core & Workflows](modules/indexing-engine-core-workflows)\n\nCore indexing pipeline, graph construction, embedding, and update workflows that build and maintain the knowledge graph index.\n\n### [Query Engine Core & Interfaces](modules/query-engine-core-interfaces)\n\nQuery-time context construction, data retrieval, and LLM orchestration utilities used to answer questions against the knowledge graph.\n\n### [Knowledge Graph Data Model](modules/knowledge-graph-data-model)\n\nCore data structures and schemas that represent documents, entities, relationships, communities, and associated metadata in the knowledge graph.\n\n### [AI Orchestration & LLM Providers](modules/ai-orchestration-llm-providers)\n\nAbstractions and backends for language models, tokenization, and LLM service management used across indexing and querying.\n\n### [Storage & Vector Store Integrations](modules/storage-vector-store-integrations)\n\nStorage abstractions and concrete implementations for file/blob storage and vector stores (LanceDB, Cosmos, Azure AI Search, etc.).\n\n### [Unified Search App (Demo UI)](modules/unified-search-app-demo-ui)\n\nDemo UI and data loading components for Unified Search, built around the GraphRAG index and results visualization."
      }
    ]
  }
}