- class_id: tests/integration/storage/test_factory.py::CustomStorage
  file: tests/integration/storage/test_factory.py
  name: CustomStorage
  methods:
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.get
    name: get
    signature: "def get(\n            self, key: str, as_bytes: bool | None = None,\
      \ encoding: str | None = None\n        ) -> Any"
    docstring: "Asynchronous method to retrieve the value for a given key from storage.\n\
      \nThis interface method is intended for storage backends to implement. The actual\
      \ return\nvalue is implementation-dependent. In the base implementation available\
      \ here, this call\nreturns None.\n\nArgs:\n    key (str): The key to retrieve.\n\
      \    as_bytes (bool | None): If True, return the value as bytes. If None, use\
      \ the backend's\n        default behavior. The exact semantics are backend-dependent.\n\
      \    encoding (str | None): Optional encoding to apply when returning the value\
      \ as a string.\n        Its interpretation is backend-dependent.\n\nReturns:\n\
      \    None: The value retrieval is not implemented in the base class and returns\
      \ None. Concrete\n    backends may return the stored value, or may raise an\
      \ error if the key is not found.\n\nRaises:\n    Exception: Backend-specific\
      \ errors may be raised for not-found conditions or other I/O errors.\n     \
      \   Implementations define their own exception types."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "\"\"\"Get the creation date for the given key.\n\nArgs:\n    key (str):\
      \ The key for which to retrieve the creation date.\n\nReturns:\n    str: The\
      \ creation date as a string.\n\"\"\""
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Deletes the item associated with the specified key from storage.\n\
      \nArgs:\n    key: The key of the item to delete.\n\nReturns:\n    None"
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.find
    name: find
    signature: "def find(\n            self,\n            file_pattern: re.Pattern[str],\n\
      \            base_dir: str | None = None,\n            file_filter: dict[str,\
      \ Any] | None = None,\n            max_count=-1,\n        ) -> Iterator[tuple[str,\
      \ dict[str, Any]]]"
    docstring: "Find files in the storage that match a compiled file_pattern, with\
      \ optional base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern\
      \ (re.Pattern[str]): A compiled regular expression to match file paths.\n  \
      \  base_dir (str | None): The base directory to search within. If None, search\
      \ starts from the storage root.\n    file_filter (dict[str, Any] | None): Optional\
      \ dictionary of metadata-based filters to apply when selecting files.\n    max_count\
      \ (int): Maximum number of results to return. A value of -1 means no limit.\n\
      \nReturns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator yielding tuples\
      \ of (path, metadata) for each matching file."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: 'Return a list of keys stored in the storage.


      Args:

      self: The storage instance.


      Returns:

      list[str]: A list of keys as strings.


      Raises:

      This implementation does not raise any exceptions.'
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "\"\"\"Return the current storage instance (no new child is created).\n\
      \nThis method accepts an optional name parameter for API compatibility but does\
      \ not create a new child. It returns the current instance (self).\n\nArgs:\n\
      \    name (str | None): Optional name for the child storage. This parameter\
      \ is accepted for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self).\n\"\"\""
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously checks whether the given key exists in storage.\n\n\
      This coroutine should be awaited to obtain the result.\n\nArgs:\n    key: The\
      \ key to check for existence.\n\nReturns:\n    bool: True if the key exists,\
      \ False otherwise. Note: This is a placeholder implementation that always returns\
      \ False. Subclasses should override this method to provide a real existence\
      \ check.\n\nRaises:\n    Exception: If a storage backend encounters an error\
      \ during the check."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage backend. This no-op\
      \ implementation does not modify any stored data.\n\nThis coroutine completes\
      \ without returning a value.\n\nParameters:\n    None\n\nReturns:\n    None\n\
      \nRaises:\n    None: This no-op implementation does not raise exceptions."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set the value for the given key.\n\nAsynchronously set the value for\
      \ the given key.\n\nArgs:\n    key (str): The key to set the value for.\n  \
      \  value (Any): The value to set.\n    encoding (str | None): Optional encoding\
      \ to apply when serializing the value.\n\nReturns:\n    None: This coroutine\
      \ completes when the value has been set."
  - node_id: tests/integration/storage/test_factory.py::CustomStorage.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
      \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
      \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None"
- class_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry
  file: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py
  name: RandomWaitRetry
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    docstring: "\"\"\"Initialize a RandomWaitRetry instance with retry configuration.\n\
      \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
      \    max_retries: The maximum number of retry attempts (int). Must be greater\
      \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n   \
      \ None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n   \
      \ ValueError: max_retry_wait must be greater than 0.\n\"\"\""
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function with a random delay between retries\
      \ until it succeeds or the maximum number of retries is reached.\n\nArgs:\n\
      \    func: The asynchronous function to retry.\n    kwargs: Additional keyword\
      \ arguments to pass to the function.\n\nReturns:\n    The result of the awaited\
      \ function.\n\nRaises:\n    Exception: If the wrapped function keeps raising\
      \ and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/random_wait_retry.py::RandomWaitRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function until it succeeds or the maximum number\
      \ of retries is reached.\n\nArgs:\n    func: Callable[..., Any] - The function\
      \ to invoke. It will be called as func(**kwargs) and its result will be returned\
      \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
      \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
      \    Exception - The last exception raised by func if the maximum number of\
      \ retries is exceeded."
- class_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage
  file: graphrag/storage/cosmosdb_pipeline_storage.py
  name: CosmosDBPipelineStorage
  methods:
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_database
    name: _delete_database
    signature: def _delete_database(self) -> None
    docstring: "Delete the database if it exists.\n\nDeletes the database associated\
      \ with this storage object if it exists. If the database is deleted, the internal\
      \ container reference is cleared to reflect that there is no active container\
      \ available until a new database/container is created.\n\nReturns:\n    None:\
      \ This method does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError:\
      \ If the database to delete does not exist.\n\nSide effects:\n    self._container_client\
      \ is set to None.\n    self._database_client is updated with the value returned\
      \ by delete_database."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a CosmosDB storage instance.\n\nArgs:\n  cosmosdb_account_url:\
      \ The URL of the Cosmos DB account. Used to initialize CosmosClient when a connection\
      \ string is not provided.\n  connection_string: The Cosmos DB connection string.\
      \ Used to initialize CosmosClient when provided.\n  base_dir: The database name\
      \ to create/use.\n  container_name: The container name to create/use.\n  encoding:\
      \ Encoding to use for data (default \"utf-8\").\nReturns:\n  None\nRaises:\n\
      \  ValueError: If no base_dir is provided for database name or if neither connection_string\
      \ nor cosmosdb_account_url is provided."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear all contents from storage.\n\nThis currently deletes the database,\
      \ including all containers and data within it.\nTODO: We should decide what\
      \ granularity of deletion is the ideal behavior (e.g. delete all items within\
      \ a container, delete the current container, delete the current database)\n\n\
      Returns:\n    None: The function does not return a value.\n\nRaises:\n    Exception:\
      \ If deletion fails."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"Keys listing is not supported for CosmosDB storage.\n\nArgs:\n\
      \    self: The instance of the storage.\n\nReturns:\n    None: This method does\
      \ not return any keys; it raises NotImplementedError.\n\nRaises:\n    NotImplementedError:\
      \ CosmosDB storage does not support listing keys.\n\"\"\""
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "Delete the container with the current container name if it exists.\n\
      \nArgs:\n    self: Any. The storage instance.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    CosmosResourceNotFoundError: If the\
      \ container to delete does not exist."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> PipelineStorage'
    docstring: "Return the current storage instance (no new child is created).\n\n\
      This method accepts an optional name parameter for API compatibility but does\
      \ not create a new child. It returns the current instance (self).\n\nArgs:\n\
      \    name: str | None, optional name for the child storage. This parameter is\
      \ accepted for API compatibility but is ignored.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self)."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create a Cosmos DB container for the current container name if it\
      \ doesn't exist.\n\nThis method creates or retrieves the container using the\
      \ current container name as the id and a partition key on the path \"/id\" (Hash).\
      \ It assigns the resulting container proxy to self._container_client. The operation\
      \ only proceeds if a database client exists.\n\nArgs:\n    self: The instance\
      \ of the class containing the Cosmos client references.\n\nReturns:\n    None.\
      \ Updates the internal _container_client attribute with the container proxy.\n\
      \nRaises:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Insert the contents of a file into a Cosmos DB container for the given\
      \ filename key.\n\nFor better optimization, the file is destructured such that\
      \ each row is a unique Cosmos DB item.\n\nArgs:\n  key (str): The filename key\
      \ under which to insert the item in the Cosmos DB container.\n  value (Any):\
      \ The content to insert. If value is bytes, it is treated as a parquet file\
      \ and each row becomes a separate item (with an id derived from the key prefix\
      \ and the row index or the existing row id). If the input row lacks an id, a\
      \ unique id is constructed as \"<prefix>:<index>\" and the prefix is tracked\
      \ for downstream handling. If value is not bytes, it is treated as a cache output\
      \ or stats.json and stored as a single item with id=key and body parsed from\
      \ JSON.\n  encoding (str | None): Optional encoding to use. This parameter is\
      \ unused by this method and is ignored.\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
      \ If the database or container are not initialized. This exception is raised\
      \ internally but is caught and logged within the method and is not propagated\
      \ to callers."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._create_database
    name: _create_database
    signature: def _create_database(self) -> None
    docstring: "Create the database if it doesn't exist.\n\nReturns:\n    None: This\
      \ method does not return a value.\n\nRaises:\n    CosmosHttpResponseError: If\
      \ the Cosmos DB service returns an HTTP error."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Delete all cosmosdb items belonging to the given filename key.\n\n\
      This coroutine does nothing if the database or container client is not initialized.\
      \ For keys containing \".parquet\", it deletes all items whose id starts with\
      \ the prefix of the key; otherwise it deletes the single item with id equal\
      \ to the key and the corresponding partition key.\n\nIf a CosmosResourceNotFoundError\
      \ is raised, it is ignored. Any other exception is logged.\n\nArgs:\n    key:\
      \ The filename key identifying the items to delete.\n\nReturns:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Asynchronously fetch items from the Cosmos DB container that match\
      \ the given key.\n\nThis coroutine supports two modes:\n\n- as_bytes is truthy:\
      \ returns matching items as Parquet-encoded data. The data are collected by\
      \ querying items whose id starts with the derived prefix, converted to a DataFrame,\
      \ and then serialized to Parquet using DataFrame.to_parquet(). The return value\
      \ is Parquet data only if in-memory writing is supported by the underlying to_parquet\
      \ implementation; otherwise the exact return type may vary depending on the\
      \ runtime.\n- as_bytes is None or False: returns the body of a single item as\
      \ a JSON string.\n\nArgs:\n    key (str): The key used to query the container.\
      \ If as_bytes is True, this key is treated as a prefix for filtering by id.\n\
      \    as_bytes (bool | None): If True, return matching items as Parquet-encoded\
      \ data. If None or False, return the body of a single item as a JSON string.\n\
      \    encoding (str | None): Encoding to use for the returned data if applicable.\
      \ This parameter is currently unused.\n\nReturns:\n    Any: If as_bytes is True\
      \ and in-memory Parquet writing is supported, returns a Parquet-encoded bytes-like\
      \ object representing the matching items. If as_bytes is None or False, returns\
      \ a JSON string of the item body for the given key, or None if the database\
      \ or container client is not initialized or if an error occurs.\n\nRaises:\n\
      \    None: This coroutine handles exceptions internally, logs a warning, and\
      \ returns None on error."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict.\n\nReturns:\n    bool:\
      \ True if no file_filter is defined or if all key/value pairs in file_filter\
      \ match the corresponding fields in item using re.search.\n\nRaises:\n    None"
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage._get_prefix
    name: _get_prefix
    signature: 'def _get_prefix(self, key: str) -> str'
    docstring: "\"Get the prefix of the filename key.\"\n\nArgs:\n  key: The filename\
      \ key as a string. The prefix is the substring before the first dot.\n\nReturns:\n\
      \  str: The prefix portion of the key (substring before the first dot)."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously determine whether the contents for the given filename\
      \ key exist in Cosmos DB storage.\n\nThis coroutine checks existence by querying\
      \ the Cosmos DB container. For parquet files, existence is determined by a prefix\
      \ match on the item id using STARTSWITH, since parquet data is stored with a\
      \ prefixed id. For non-parquet keys, existence is determined by an exact id\
      \ match (c.id == key).\n\nInitialization/readiness: If the storage has not been\
      \ initialized (database or container client not available), the method returns\
      \ False without performing I/O.\n\nAsync context: This is an async function\
      \ and should be awaited by callers. In this implementation it does not contain\
      \ explicit await expressions; the actual I/O occurs through the Cosmos client.\n\
      \nArgs:\n    key (str): The filename key to check for existence in Cosmos DB\
      \ storage.\n\nReturns:\n    bool: True if the item(s) exist in Cosmos DB storage,\
      \ otherwise False. Parquet keys return True if any item exists with an id starting\
      \ with the generated prefix; non-parquet keys return True only when exactly\
      \ one matching item exists.\n\nRaises:\n    Exception: Propagates exceptions\
      \ raised by the underlying Cosmos DB SDK during query/communication with the\
      \ service."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find documents in a Cosmos DB container using a file pattern regex\
      \ and optional file filter.\n\nArgs:\n    file_pattern (re.Pattern[str]): The\
      \ file pattern to use.\n    base_dir (str | None): The name of the base directory\
      \ (not used in Cosmos DB context).\n    file_filter (dict[str, Any] | None):\
      \ A dictionary of key-value pairs to filter the documents.\n    max_count (int):\
      \ The maximum number of documents to return. If -1, all documents are returned.\n\
      \nReturns:\n    Iterator[tuple[str, dict[str, Any]]]: An iterator of document\
      \ IDs and their corresponding regex matches."
  - node_id: graphrag/storage/cosmosdb_pipeline_storage.py::CosmosDBPipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date for the given key from Cosmos DB storage and\
      \ format it with the local time zone.\n\nArgs:\n    key (str): The key for which\
      \ to retrieve the creation date.\n\nReturns:\n    str: The creation date as\
      \ a string formatted with the local time zone. Returns an empty string if the\
      \ key is not found or an error occurs.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; it returns an empty string on error."
- class_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks
  file: graphrag/callbacks/workflow_callbacks.py
  name: WorkflowCallbacks
  methods:
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "\"\"\"Handle when progress occurs.\n\nArgs:\n    progress: Progress\
      \ object representing the current progress event.\n\nReturns:\n    None\n\"\"\
      \""
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n   \
      \ name (str): The name of the workflow starting.\n    instance (object): The\
      \ workflow instance object associated with this start event.\n\nReturns:\n \
      \   None\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Parameters:\n    results: list[PipelineRunResult]. A list of PipelineRunResult\
      \ objects representing the results of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks.py::WorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow ends.\n\nArgs:\n    name:\
      \ str\n        The name of the workflow.\n    instance: object\n        The\
      \ workflow instance object.\n\nReturns:\n    None...\n\"\"\""
- class_id: graphrag/query/structured_search/drift_search/action.py::DriftAction
  file: graphrag/query/structured_search/drift_search/action.py
  name: DriftAction
  methods:
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.compute_score
    name: compute_score
    signature: 'def compute_score(self, scorer: Any)'
    docstring: "Compute the score for the action using the provided scorer.\n\nArgs:\n\
      \    scorer (Any): The scorer to compute the score.\n\nReturns:\n    None: The\
      \ method updates self.score with the value returned by scorer.compute_score(self.query,\
      \ self.answer); if the result is None, self.score is set to -inf."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__hash__
    name: __hash__
    signature: def __hash__(self) -> int
    docstring: "Return a hash value for the DriftAction object to enable hashing in\
      \ networkx.MultiDiGraph.\n\nAssumes queries are unique.\n\nArgs:\n    self:\
      \ The DriftAction instance.\n\nReturns:\n    int: Hash based on the query."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        query: str,\n        answer:\
      \ str | None = None,\n        follow_ups: list[\"DriftAction\"] | None = None,\n\
      \    )"
    docstring: "Initialize the DriftAction with a query, optional answer, and follow-up\
      \ actions.\n\nArgs:\n    query (str): The query for the action.\n    answer\
      \ (Optional[str]): The answer to the query, if available.\n    follow_ups (Optional[list[DriftAction]]):\
      \ A list of follow-up actions.\n\nReturns:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.serialize
    name: serialize
    signature: 'def serialize(self, include_follow_ups: bool = True) -> dict[str,
      Any]'
    docstring: "Serialize the action to a dictionary.\n\nSerializes the DriftAction\
      \ into a dictionary representation, including the core fields and, optionally,\
      \ serialized follow-up actions.\n\nArgs:\n    include_follow_ups (bool): Whether\
      \ to include follow-up actions in the serialization. The default is True; when\
      \ True, the result includes a \"follow_ups\" key containing a list of serialized\
      \ follow-up actions, each serialized by its own serialize() method.\n\nReturns:\n\
      \    dict[str, Any]: Serialized action as a dictionary with the following keys:\n\
      \        - \"query\": The query string.\n        - \"answer\": The answer.\n\
      \        - \"score\": The score.\n        - \"metadata\": Metadata dictionary.\n\
      \        - \"follow_ups\": (optional) List of serialized follow-up actions if\
      \ include_follow_ups is True (empty list if none)."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.is_complete
    name: is_complete
    signature: def is_complete(self) -> bool
    docstring: "Check if the action is complete (i.e., an answer is available).\n\n\
      Returns:\n    bool: True if an answer is available, False otherwise."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.from_primer_response
    name: from_primer_response
    signature: "def from_primer_response(\n        cls, query: str, response: str\
      \ | dict[str, Any] | list[dict[str, Any]]\n    ) -> \"DriftAction\""
    docstring: "Create a DriftAction from a DRIFTPrimer response.\n\nArgs:\n    query\
      \ (str): The query string.\n    response (str | dict[str, Any]): Primer response\
      \ data. The runtime accepts:\n        - a dictionary with keys:\n          \
      \  - follow_up_queries (list[dict[str, Any]]): actions to follow up with\n \
      \           - intermediate_answer: the answer to present\n            - score\
      \ (optional, numeric): a score for the action\n        - a JSON string that\
      \ decodes to such a dictionary.\n\nReturns:\n    DriftAction: A new DriftAction\
      \ instance populated from the response. The instance's\n    query is set to\
      \ the provided query; follow_ups, answer, and score are populated\n    from\
      \ the corresponding keys in the response.\n\nRaises:\n    ValueError: If the\
      \ response is not a dictionary or a JSON string that decodes to a dictionary.\n\
      \    ValueError: If a JSON string cannot be parsed or decodes to a non-dictionary\
      \ value."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.deserialize
    name: deserialize
    signature: 'def deserialize(cls, data: dict[str, Any]) -> "DriftAction"'
    docstring: "Deserialize the action from a dictionary.\n\nArgs:\n    data (dict[str,\
      \ Any]): Serialized action data.\n\nReturns:\n    DriftAction: A deserialized\
      \ instance of DriftAction.\n\nRaises:\n    ValueError: If the 'query' key is\
      \ missing in serialized data."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.__eq__
    name: __eq__
    signature: 'def __eq__(self, other: object) -> bool'
    docstring: "Check equality based on the query string.\n\nArgs:\n    other (object):\
      \ Another object to compare with.\n\nReturns:\n    bool: True if the other object\
      \ is a DriftAction with the same query, False otherwise."
  - node_id: graphrag/query/structured_search/drift_search/action.py::DriftAction.search
    name: search
    signature: 'def search(self, search_engine: Any, global_query: str, scorer: Any
      = None)'
    docstring: "Execute an asynchronous search using the search engine, and update\
      \ the action with the results.\n\nIf a scorer is provided, compute the score\
      \ for the action.\n\nArgs:\n    search_engine (Any): The search engine to execute\
      \ the query.\n    global_query (str): The global query string.\n    scorer (Any,\
      \ optional): Scorer to compute scores for the action.\n\nReturns:\n    DriftAction:\
      \ The updated action with the search results.\n\nRaises:\n    Exception: If\
      \ an error occurs during search execution or processing."
- class_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC
  file: tests/unit/indexing/graph/utils/test_stable_lcc.py
  name: TestStableLCC
  methods:
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph
    name: _create_strongly_connected_graph
    signature: def _create_strongly_connected_graph(self, digraph=False)
    docstring: "Create and return a test graph with a linear chain of edges and node\
      \ attributes.\n\nCreates an undirected Graph by default; if digraph is True,\
      \ a DiGraph is created. Adds nodes \"1\", \"2\", \"3\", \"4\" with node_name\
      \ attributes 1, 2, 3, 4, and adds edges (\"4\",\"5\") with degree=4, (\"3\"\
      ,\"4\") with degree=3, (\"2\",\"3\") with degree=2, and (\"1\",\"2\") with degree=1.\n\
      \nArgs:\n    self: The instance of the test class.\n    digraph: bool, whether\
      \ to create a directed graph (DiGraph) instead of an undirected Graph.\n\nReturns:\n\
      \    graph: networkx.Graph or networkx.DiGraph, created according to the digraph\
      \ flag."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC._create_strongly_connected_graph_with_edges_flipped
    name: _create_strongly_connected_graph_with_edges_flipped
    signature: def _create_strongly_connected_graph_with_edges_flipped(self, digraph=False)
    docstring: "Create and return a test graph used for stability tests of the largest\
      \ connected component. This variant builds a five-node graph where the first\
      \ edge introduces the new node 5 (edge 5-4). The remaining edges form a path:\
      \ 4-3, 3-2, 2-1, with edge attributes degree=4, degree=3, degree=2, and degree=1\
      \ respectively. If digraph is False, an undirected Graph is created; if digraph\
      \ is True, a directed DiGraph is created and the edges are oriented accordingly.\
      \ The function name indicates the first edge's orientation is flipped relative\
      \ to the non-flipped version (which uses 4-5). The graph contains five nodes\
      \ labeled 1 through 5; nodes 1-4 have node_name attributes 1-4, and node 5 is\
      \ introduced by the first edge.\n\nArgs:\n    self: The instance of the test\
      \ class. Type: unittest.TestCase\n    digraph (bool): If True, create a directed\
      \ graph (nx.DiGraph); otherwise an undirected graph (nx.Graph).\n\nReturns:\n\
      \    graph (networkx.Graph or networkx.DiGraph): The constructed graph.\n\n\
      Raises:\n    None"
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_undirected_graph_run_twice_produces_same_graph
    name: test_undirected_graph_run_twice_produces_same_graph
    signature: def test_undirected_graph_run_twice_produces_same_graph(self)
    docstring: "Verify that running stable_largest_connected_component on an undirected\
      \ graph twice yields the same graph, even if the input edges are flipped.\n\n\
      Args:\n    self (TestStableLCC): Instance of the test class.\n\nReturns:\n \
      \   None: This test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If the graphs produced by stable_largest_connected_component differ."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_keeps_source_target_intact
    name: test_directed_graph_keeps_source_target_intact
    signature: def test_directed_graph_keeps_source_target_intact(self)
    docstring: "Test that a directed graph keeps source and target intact when computing\
      \ the stable largest connected component.\n\nArgs:\n    self (TestStableLCC):\
      \ Instance of the test class.\n\nReturns:\n    None: This test does not return\
      \ a value.\n\nRaises:\n    AssertionError: If the directed edges in the input\
      \ and output graphs differ, indicating the edge directions were not preserved."
  - node_id: tests/unit/indexing/graph/utils/test_stable_lcc.py::TestStableLCC.test_directed_graph_run_twice_produces_same_graph
    name: test_directed_graph_run_twice_produces_same_graph
    signature: def test_directed_graph_run_twice_produces_same_graph(self)
    docstring: "Test that running stable_largest_connected_component on a directed\
      \ graph twice yields the same graph.\n\nArgs:\n    self (TestStableLCC): Instance\
      \ of the test class.\n\nReturns:\n    None: This test does not return a value.\n\
      \nRaises:\n    AssertionError: If the graphs produced by stable_largest_connected_component\
      \ differ between runs."
- class_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore
  file: graphrag/vector_stores/azure_ai_search.py
  name: AzureAISearchVectorStore
  methods:
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search.\n\nThis method constructs\
      \ a VectorizedQuery using the provided query_embedding and k, and issues a search\
      \ via the configured database connection. It returns a list of VectorStoreSearchResult\
      \ objects, each containing a VectorStoreDocument and the corresponding similarity\
      \ score.\n\nArgs:\n  query_embedding: list[float] - Embedding vector to search\
      \ with\n  k: int - Number of nearest neighbors to retrieve\n  kwargs: Any -\
      \ Additional keyword arguments (passed through)\n\nReturns:\n  list[VectorStoreSearchResult]\
      \ - List of matching documents with scores\n\nRaises:\n  Exception - If the\
      \ underlying search operation fails or the Azure client raises an error"
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: 'Connect to AI search vector storage.


      Args:

      - url: The endpoint URL for the Azure AI Search service.

      - api_key: Optional API key for authentication. If provided, AzureKeyCredential
      is used; otherwise DefaultAzureCredential is used.

      - audience: Optional audience to pass to the client.

      - vector_search_profile_name: Optional name for the vector search profile. Defaults
      to "vectorSearchProfile".


      Returns:

      - None


      Raises:

      - ValueError: Azure AI Search expects url.'
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a text-based similarity search.\n\nArgs:\n    text (str):\
      \ The input text to search for similar documents.\n    text_embedder (TextEmbedder):\
      \ The callable used to compute an embedding for the input text.\n    k (int):\
      \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
      \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
      \ VectorStoreSearchResult objects.\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Builds a query filter to filter documents by a list of IDs.\n\nArgs:\n\
      \  include_ids: list[str] | list[int] The IDs to include in the filter. If None\
      \ or an empty\n    list is provided, the filter is cleared (set to None) and\
      \ None is returned.\n\nReturns:\n  Any The constructed query filter string to\
      \ filter documents by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize Azure AI Search vector store by delegating to the base\
      \ class constructor.\n\nArgs:\n    vector_store_schema_config: VectorStoreSchemaConfig\
      \ - The schema configuration for the vector store.\n    **kwargs: Any - Additional\
      \ keyword arguments forwarded to the base class initializer.\n\nReturns:\n \
      \   None\n\nRaises:\n    Exceptions raised by the base class __init__ are propagated."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id with its id, text, vector, and attributes\
      \ populated from the stored document.\n\nRaises:\n    json.JSONDecodeError:\
      \ If the attributes field cannot be decoded as JSON.\n    Exception: If the\
      \ underlying database connection fails to retrieve the document."
  - node_id: graphrag/vector_stores/azure_ai_search.py::AzureAISearchVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into an Azure AI Search index.\n\nArgs:\n    documents\
      \ (list[VectorStoreDocument]): Documents to load into the index. Each document\
      \ has id, vector, text, and attributes. Only documents with a non-null vector\
      \ are uploaded.\n    overwrite (bool): If True, delete and recreate the index\
      \ before loading.\n\nReturns:\n    None: This method does not return a value.\n\
      \nRaises:\n    Exception: If an error occurs during index creation or document\
      \ upload (propagates from underlying Azure SDK calls)."
- class_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks
  file: graphrag/callbacks/noop_query_callbacks.py
  name: NoopQueryCallbacks
  methods:
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_start
    name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    docstring: "Handle the start of a map operation.\n\nArgs:\n    map_response_contexts:\
      \ A list of strings representing contexts for the map response operation to\
      \ begin processing.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_llm_new_token
    name: on_llm_new_token
    signature: def on_llm_new_token(self, token)
    docstring: "Handle when a new token is generated.\n\nThis is a no-op callback\
      \ in NoopQueryCallbacks; calling this method does not modify state, perform\
      \ work, or produce side effects.\n\nArgs:\n    token: str\n        The new token\
      \ generated by the LLM.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_context
    name: on_context
    signature: 'def on_context(self, context: Any) -> None'
    docstring: "\"\"\"Handle when context data is constructed.\n\nThis no-op implementation\
      \ does not modify, store, or otherwise affect the given context.\n\nArgs:\n\
      \    context (Any): The context data provided to the callback. This implementation\
      \ performs no operations on it.\n\nReturns:\n    None: The function returns\
      \ no value and has no side effects.\n\nRaises:\n    None: This function does\
      \ not raise exceptions by itself.\n\"\"\""
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_start
    name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
      \ Context for the reduce response (str | dict[str, Any]).\n\nReturns:\n    None:\
      \ The function does not return a value."
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_map_response_end
    name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    docstring: "Handle the end of map operation.\n\nArgs:\n    map_response_outputs:\
      \ list[SearchResult] - The outputs produced by the map operation.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/noop_query_callbacks.py::NoopQueryCallbacks.on_reduce_response_end
    name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    docstring: "No-op callback for the end of the reduce operation in NoopQueryCallbacks.\n\
      \nThis method intentionally performs no action and exists solely to conform\
      \ to the NoopQueryCallbacks interface as a placeholder without side effects.\n\
      \nArgs:\n    reduce_response_output: str\n        The output produced by the\
      \ end of the reduce operation.\n\nReturns:\n    None: The function does not\
      \ return a value."
- class_id: graphrag/query/structured_search/drift_search/state.py::QueryState
  file: graphrag/query/structured_search/drift_search/state.py
  name: QueryState
  methods:
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.action_token_ct
    name: action_token_ct
    signature: def action_token_ct(self) -> dict[str, int]
    docstring: "Return the token counts across all actions in the graph.\n\nArgs:\n\
      \    self: The instance containing a graph attribute; each node in the graph\
      \ has metadata with keys 'llm_calls', 'prompt_tokens', and 'output_tokens'.\n\
      Returns:\n    dict[str, int]: A dictionary with keys 'llm_calls', 'prompt_tokens',\
      \ and 'output_tokens' mapping to the summed counts across all nodes."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize the drift query state with an empty graph.\n\nArgs:\n \
      \   self: The instance being initialized.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.serialize
    name: serialize
    signature: "def serialize(\n        self, include_context: bool = True\n    )\
      \ -> dict[str, Any] | tuple[dict[str, Any], dict[str, Any], str]"
    docstring: "Serialize the graph to a dictionary representation, optionally including\
      \ contextual information for nodes.\n\nArgs:\n- include_context (bool): If True,\
      \ return a 3-tuple consisting of the graph dictionary, a context_data dictionary,\
      \ and a string representation of the context_data. If False, return only the\
      \ graph dictionary.\n\nReturns:\n- If include_context is False: a dictionary\
      \ with the keys \"nodes\" and \"edges\". \"nodes\" is a list of dictionaries\
      \ for each node, each containing an \"id\" (int), fields from node.serialize(include_follow_ups=False),\
      \ and all attributes from self.graph.nodes[node]. \"edges\" is a list of dictionaries\
      \ with \"source\" (int), \"target\" (int), and \"weight\" (float, defaults to\
      \ 1.0).\n- If include_context is True: a tuple of three elements: (graph_dict,\
      \ context_data, context_text).\n  - graph_dict is the same dictionary as above\
      \ (with keys \"nodes\" and \"edges\").\n  - context_data is a dictionary mapping\
      \ a node query (string) to its context_data (any), built from nodes that have\
      \ a non-empty metadata.context_data and a query.\n  - context_text is the string\
      \ representation of context_data.\n\nNotes and edge cases:\n- The function returns\
      \ different shapes depending on include_context. Callers must handle both possibilities.\n\
      - If no context data exists, context_data will be {}, and context_text will\
      \ be \"{}\".\n- If the graph has no nodes, nodes and edges lists are empty;\
      \ IDs are assigned by enumeration order starting at 0.\n- We rely on node.serialize(include_follow_ups=False)\
      \ for the per-node payload; exact fields depend on the DriftAction implementation.\n\
      - Edge weights default to 1.0 if not present in edge_data."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.find_incomplete_actions
    name: find_incomplete_actions
    signature: def find_incomplete_actions(self) -> list[DriftAction]
    docstring: "Find all unanswered actions in the graph.\n\n        Args:\n     \
      \       self: The instance containing the graph where actions reside.\n\n  \
      \      Returns:\n            list[DriftAction]: A list of DriftAction objects\
      \ that are not complete."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_action
    name: add_action
    signature: 'def add_action(self, action: DriftAction, metadata: dict[str, Any]
      | None = None)'
    docstring: "Add an action node to the graph with optional metadata.\n\nArgs:\n\
      \    action: DriftAction to add as a node in the graph.\n    metadata: Optional\
      \ dict[str, Any] of node attributes to attach to the action. If None, no attributes\
      \ are added.\n\nReturns:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.add_all_follow_ups
    name: add_all_follow_ups
    signature: "def add_all_follow_ups(\n        self,\n        action: DriftAction,\n\
      \        follow_ups: list[DriftAction] | list[str],\n        weight: float =\
      \ 1.0,\n    )"
    docstring: "Add all follow-up actions and link them to the given action.\n\nArgs:\n\
      \    action: The parent DriftAction to link follow-up actions to.\n    follow_ups:\
      \ A list of follow-up actions to add. Each item can be a DriftAction or a string\
      \ query.\n    weight: The weight to apply to each relationship when linking\
      \ to the parent.\n\nReturns:\n    None. The follow-up actions are added to the\
      \ graph and connected to the given action.\n\nRaises:\n    None. The method\
      \ logs warnings for invalid input types but does not raise exceptions."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.deserialize
    name: deserialize
    signature: 'def deserialize(self, data: dict[str, Any])'
    docstring: "Deserialize the dictionary back to a graph.\n\nArgs:\n    data: Serialized\
      \ representation of the graph to deserialize. Contains \"nodes\" and \"edges\"\
      .\n\nReturns:\n    None: This method updates the graph in place and does not\
      \ return a value.\n\nRaises:\n    KeyError: If any node entry in data's \"nodes\"\
      \ list is missing the required \"id\" field."
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.relate_actions
    name: relate_actions
    signature: "def relate_actions(\n        self, parent: DriftAction, child: DriftAction,\
      \ weight: float = 1.0\n    )"
    docstring: "Relate two actions in the graph.\n\nArgs:\n    self: The QueryState\
      \ instance.\n    parent: The parent DriftAction in the relation.\n    child:\
      \ The child DriftAction to be related to the parent.\n    weight: The weight\
      \ of the edge to add between the actions.\n\nReturns:\n    None. The function\
      \ mutates the internal graph by adding an edge with the specified weight.\n\n\
      Raises:\n    None"
  - node_id: graphrag/query/structured_search/drift_search/state.py::QueryState.rank_incomplete_actions
    name: rank_incomplete_actions
    signature: "def rank_incomplete_actions(\n        self, scorer: Callable[[DriftAction],\
      \ float] | None = None\n    ) -> list[DriftAction]"
    docstring: "\"\"\"Rank all incomplete actions in the graph, optionally by a scorer.\n\
      \nArgs:\n    scorer: Callable[[DriftAction], float] | None. A function that\
      \ takes a DriftAction and returns a numeric score. If provided, actions are\
      \ scored and returned sorted by score in descending order. If None, actions\
      \ are returned in a random order.\n\nReturns:\n    list[DriftAction]: The incomplete\
      \ actions, either ranked by score or shuffled.\n\nRaises:\n    None\n\"\"\""
- class_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider
  file: graphrag/language_model/providers/fnllm/cache.py
  name: FNLLMCacheProvider
  methods:
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear the cache.\n\nClears all entries from the underlying cache managed\
      \ by this provider.\n\nReturns:\n    None\nRaises:\n    Exception: if the underlying\
      \ cache clear operation fails."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.get
    name: get
    signature: 'def get(self, key: str) -> Any | None'
    docstring: "Asynchronous retrieval of a value from the underlying cache.\n\nThis\
      \ method is part of the FNLLMCacheProvider and delegates to the underlying cache\
      \ (self._cache).\n\nArgs:\n    key: The key to retrieve from the cache.\n\n\
      Returns:\n    Any | None: The value associated with the key, or None if not\
      \ present.\n\nNotes:\n    Exceptions may propagate from the underlying cache."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously check if the cache has a value for the given key.\n\
      \nArgs:\n    key: The cache key to check.\n\nReturns:\n    bool: True if a value\
      \ exists for the key in the cache, otherwise False."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.remove
    name: remove
    signature: 'def remove(self, key: str) -> None'
    docstring: "Remove a value from the cache.\n\nArgs:\n    key (str): The key to\
      \ remove from the cache.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    Exceptions may propagate from the underlying cache."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.__init__
    name: __init__
    signature: 'def __init__(self, cache: PipelineCache)'
    docstring: "Initialize FNLLMCacheProvider with a PipelineCache.\n\nArgs:\n  cache:\
      \ The underlying PipelineCache instance used by this provider.\n\nReturns:\n\
      \  None"
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.set
    name: set
    signature: "def set(\n        self, key: str, value: Any, metadata: dict[str,\
      \ Any] | None = None\n    ) -> None"
    docstring: "Write a value into the cache.\n\nArgs:\n    key: str \u2014 The key\
      \ under which to store the value.\n    value: Any \u2014 The value to store\
      \ in the cache.\n    metadata: dict[str, Any] | None \u2014 Optional metadata\
      \ associated with the value.\n\nReturns:\n    None \u2014 The method does not\
      \ return a value.\n\nRaises:\n    Exceptions raised by the underlying cache\
      \ operation are propagated to the caller."
  - node_id: graphrag/language_model/providers/fnllm/cache.py::FNLLMCacheProvider.child
    name: child
    signature: 'def child(self, key: str) -> "FNLLMCacheProvider"'
    docstring: "Create a child cache.\n\nArgs:\n    key: The key used to create the\
      \ child cache.\n\nReturns:\n    FNLLMCacheProvider: A new FNLLMCacheProvider\
      \ wrapping the child cache created for the given key."
- class_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch
  file: graphrag/query/structured_search/drift_search/search.py
  name: DRIFTSearch
  methods:
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response
    name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        responses: str | dict[str,\
      \ Any],\n        query: str,\n        llm_calls: dict[str, int],\n        prompt_tokens:\
      \ dict[str, int],\n        output_tokens: dict[str, int],\n        **llm_kwargs,\n\
      \    ) -> str"
    docstring: "Reduce the response to a single comprehensive response.\n\nParameters\n\
      ----------\nresponses : str|dict[str, Any]\n    The responses to reduce. If\
      \ a string, it is treated as a single response; if a\n    dict, the function\
      \ extracts the \"answer\" field from each node in responses[\"nodes\"].\nquery\
      \ : str\n    The original query.\nllm_calls : dict[str, int]\n    Counter for\
      \ LLM calls performed during reduction. This dictionary is updated in\n    place;\
      \ after execution, llm_calls[\"reduce\"] will be set to 1.\nprompt_tokens :\
      \ dict[str, int]\n    Token counts for prompts used during reduction. This dictionary\
      \ is updated in\n    place; after execution, prompt_tokens[\"reduce\"] will\
      \ equal the total number of\n    tokens in the constructed search prompt plus\
      \ the encoded query.\noutput_tokens : dict[str, int]\n    Token counts for outputs\
      \ produced during reduction. This dictionary is updated in\n    place; after\
      \ execution, output_tokens[\"reduce\"] will equal the number of tokens in\n\
      \    the reduced response.\nllm_kwargs : dict[str, Any]\n    Additional keyword\
      \ arguments to pass to the LLM (passed to model.achat via\n    model_parameters).\n\
      \nReturns\n-------\nstr\n    The reduced (consolidated) response."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._process_primer_results
    name: _process_primer_results
    signature: "def _process_primer_results(\n        self, query: str, search_results:\
      \ SearchResult\n    ) -> DriftAction"
    docstring: "Process the results from the primer search to extract intermediate\
      \ answers and follow-up queries.\n\nArgs:\n    query (str): The original search\
      \ query.\n    search_results (SearchResult): The results from the primer search.\n\
      \nReturns:\n    DriftAction: Action generated from the primer response.\n\n\
      Raises:\n    RuntimeError: If no intermediate answers or follow-up queries are\
      \ found in the primer response.\n    ValueError: If the primer response is not\
      \ a list of dictionaries."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._search_step
    name: _search_step
    signature: "def _search_step(\n        self, global_query: str, search_engine:\
      \ LocalSearch, actions: list[DriftAction]\n    ) -> list[DriftAction]"
    docstring: "Perform an asynchronous search step (internal API) by executing each\
      \ DriftAction concurrently and collecting the results.\n\nArgs:\n    global_query\
      \ (str): The global query for the search.\n    search_engine (LocalSearch):\
      \ The local search engine instance.\n    actions (list[DriftAction]): A list\
      \ of actions to perform.\n\nReturns:\n    list[DriftAction]: The results from\
      \ executing the search actions asynchronously.\n\nRaises:\n    Exception: May\
      \ propagate exceptions raised by the underlying action searches."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch._reduce_response_streaming
    name: _reduce_response_streaming
    signature: "def _reduce_response_streaming(\n        self,\n        responses:\
      \ str | dict[str, Any],\n        query: str,\n        model_params: dict[str,\
      \ Any],\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream a reduced response by constructing a reduced prompt from the\
      \ provided responses and streaming tokens from the model.\n\nParameters\n----------\n\
      responses : str | dict[str, Any]\n    The responses to reduce. If a string,\
      \ treated as a single response; otherwise, if a dict,\n    extract the \"answer\"\
      \ value from each node in responses.get(\"nodes\", []) that has an \"answer\"\
      .\nquery : str\n    The original query.\nmodel_params : dict[str, Any]\n   \
      \ Parameters for the underlying model used during streaming.\n\nReturns\n-------\n\
      AsyncGenerator[str, None]\n    An asynchronous generator yielding streamed tokens\
      \ (strings) from the model. Each yielded token\n    corresponds to a portion\
      \ of the reduced response. Tokens are also emitted to registered callbacks\n\
      \    via on_llm_new_token as they arrive.\n\nRaises\n------\nException\n   \
      \ Propagates exceptions raised by the underlying model streaming or by registered\
      \ callbacks."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ DRIFTSearchContextBuilder,\n        tokenizer: Tokenizer | None = None,\n\
      \        query_state: QueryState | None = None,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n    )"
    docstring: "Initialize the DRIFTSearch class.\n\nThis constructor wires core components\
      \ for DRIFT-style search, including the\nlanguage model interface, context,\
      \ token handling, and query lifecycle.\n\nArgs:\n    model (ChatModel): The\
      \ chat-based language model used for searching.\n    context_builder (DRIFTSearchContextBuilder):\
      \ Builder that holds DRIFT configuration and context.\n    tokenizer (Tokenizer,\
      \ optional): Token encoder used to tokenize input and manage tokens.\n     \
      \   If not provided, a default tokenizer is obtained via get_tokenizer().\n\
      \    query_state (QueryState, optional): State tracked for the current search\
      \ query.\n        If not provided, a new QueryState is created.\n    callbacks\
      \ (list[QueryCallbacks], optional): Callback handlers for query events.\n  \
      \      If not provided, an empty list is used.\n\nReturns:\n    None\n\nSide\
      \ effects:\n    - Assigns instance attributes for model, context_builder, tokenizer,\
      \ and query_state.\n    - Creates a DRIFTPrimer instance configured with the\
      \ current context and tokenizer.\n    - Initializes the local search component\
      \ by calling init_local_search(),\n      preparing a LocalSearch with parameters\
      \ derived from the DRIFT configuration."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.init_local_search
    name: init_local_search
    signature: def init_local_search(self) -> LocalSearch
    docstring: "Initialize a LocalSearch instance configured from the DRIFT search\
      \ configuration.\n\nThis method reads values from the DRIFTSearch instance's\
      \ configuration (via self.context_builder.config) to build the LocalSearch parameters,\
      \ including how context is built, how many tokens to consider, and how the OpenAI\
      \ model is prompted.\n\nParameters\n    None: This method does not accept explicit\
      \ parameters and relies on the DRIFTSearch instance state\n    (model, context_builder,\
      \ tokenizer, and config) to construct and configure the LocalSearch.\n\nReturns\n\
      \    LocalSearch: A LocalSearch instance configured with:\n        model: the\
      \ current ChatModel\n        system_prompt: the DRIFT context's system prompt\n\
      \        context_builder: the DRIFT local mixed context\n        tokenizer:\
      \ the tokenizer in use\n        model_params: OpenAI API parameters derived\
      \ from the local_search fields\n        context_builder_params: local context\
      \ parameters derived from the local_search fields\n        response_type: multiple\
      \ paragraphs\n        callbacks: any provided callbacks\n\nConfiguration details\
      \ used from DRIFT config\n    local_search_text_unit_prop: text unit property\
      \ used by the local search\n    local_search_community_prop: property for community\
      \ weighting\n    local_search_top_k_mapped_entities: maximum number of mapped\
      \ entities\n    local_search_top_k_relationships: maximum number of relationships\n\
      \    local_search_max_data_tokens: maximum tokens for local context\n    local_search_llm_max_gen_tokens:\
      \ maximum tokens for LLM generation\n    local_search_temperature: sampling\
      \ temperature\n    local_search_n: number of candidate generations\n    local_search_top_p:\
      \ nucleus sampling parameter\n    local_search_llm_max_gen_completion_tokens:\
      \ maximum tokens for LLM completion\n\nNotes\n    This method relies on the\
      \ presence and validity of the above config fields.\n    Missing or invalid\
      \ values may raise exceptions when constructing LocalSearch or its parameters.\n\
      \nRaises\n    ValueError: if required configuration values are missing or invalid\n\
      \    TypeError: if internal components are not properly initialized\n    Exception:\
      \ propagates any exception raised by LocalSearch construction or parameter derivation"
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ Any = None,\n        reduce: bool = True,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Perform an asynchronous DRIFT search.\n\nArgs:\n    query (str): The\
      \ query to search for.\n    conversation_history (Any, optional): The conversation\
      \ history, if any.\n    reduce (bool, optional): Whether to reduce the response\
      \ to a single comprehensive response.\n    **kwargs: Additional keyword arguments\
      \ that may be used by the search implementation.\n\nReturns:\n    SearchResult:\
      \ The search result containing the response and context data.\n\nRaises:\n \
      \   ValueError: If the query is empty."
  - node_id: graphrag/query/structured_search/drift_search/search.py::DRIFTSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self, query: str, conversation_history:\
      \ ConversationHistory | None = None\n    ) -> AsyncGenerator[str, None]"
    docstring: "Perform a streaming DRIFT search asynchronously.\n\nArgs:\n    query\
      \ (str): The query to search for.\n    conversation_history (ConversationHistory\
      \ | None, optional): The conversation history.\n\nReturns:\n    AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding pieces of the reduced response as\
      \ they are produced.\n\nRaises:\n    Exception: If the underlying search or\
      \ streaming operations fail."
- class_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore
  file: graphrag/vector_stores/lancedb.py
  name: LanceDBVectorStore
  methods:
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a similarity search using a given input text.\n\nArgs:\n\
      \    text: The input text to search for similar documents.\n    text_embedder:\
      \ TextEmbedder used to compute an embedding for the input text.\n    k: The\
      \ number of top results to return.\n    **kwargs: Additional keyword arguments.\n\
      \nReturns:\n    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
      \ objects.\n\nRaises:\n    Exception: If text_embedder or the underlying similarity\
      \ search raise.\n\"\"\""
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id: The identifier of the\
      \ document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document corresponding\
      \ to the provided id. If a matching document is found, its id, text, vector,\
      \ and attributes are populated; otherwise a VectorStoreDocument with id set\
      \ to the provided id and text=None, vector=None is returned."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into LanceDB vector storage.\n\nArgs:\n    documents:\
      \ List of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
      \ If True, overwrite existing table data; otherwise, append to the table.\n\n\
      Returns:\n    None\n\nRaises:\n    May raise exceptions from LanceDB operations\
      \ during table creation or data insertion."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float]\
      \ | np.ndarray, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform a vector-based similarity search against the LanceDB\
      \ document collection.\n\nQuery the underlying document collection for documents\
      \ whose embeddings are close to the provided query_embedding. If a query filter\
      \ has been configured (via filter_by_id), the search results are restricted\
      \ to that subset using a prefilter.\n\nThe top-k results are returned in order\
      \ of increasing distance. Each result is a VectorStoreSearchResult containing:\n\
      - document: VectorStoreDocument with id, text, vector, and attributes (attributes\
      \ parsed from JSON in the attributes field)\n- score: 1 - abs(float(doc[\"_distance\"\
      ])) (a similarity score in [0, 1])\n\nArgs:\n  query_embedding: list[float]\
      \ | np.ndarray - Embedding vector to search with\n  k: int - Number of top results\
      \ to return\n  **kwargs: Any - Additional keyword arguments for compatibility;\
      \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]\
      \ - Top-k results with associated documents and similarity scores\n\nRaises:\n\
      \  ValueError, TypeError, RuntimeError - If the input embedding is invalid or\
      \ the search operation fails due to backend issues.\n\nNotes:\n  - If self.query_filter\
      \ is set, results are filtered by the provided condition before applying the\
      \ top-k limit.\n  - The attributes field is parsed from JSON; ensure the underlying\
      \ column contains valid JSON.\n\n\"\"\""
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
      \ list[str] | list[int]\n    The IDs to include in the filter. If the list is\
      \ empty, the filter is cleared\n    (set to None) and None is returned.\n\n\
      Returns:\n  Any\n    The constructed query filter string to filter documents\
      \ by the provided IDs, or None\n    if no IDs are provided."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize LanceDB vector store by delegating to the base class constructor.\n\
      \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema\
      \ configuration for the vector store.\n  **kwargs: Any - Additional keyword\
      \ arguments forwarded to the base class initializer.\n\nReturns:\n  None\n\n\
      Raises:\n  Exceptions raised by the base class __init__ are propagated...."
  - node_id: graphrag/vector_stores/lancedb.py::LanceDBVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to LanceDB vector storage.\n\nArgs:\n    db_uri (str): The\
      \ LanceDB database URI to connect to. This value must be supplied via kwargs\
      \ with the key 'db_uri'.\n\nNotes:\n    - If self.index_name is set and a table\
      \ with that name exists in the connected database, the function will open that\
      \ table and assign it to self.document_collection.\n\nReturns:\n    None\n\n\
      Raises:\n    KeyError: If 'db_uri' is not provided in kwargs.\n    Exception:\
      \ If the underlying LanceDB library raises an exception during connect or table\
      \ open."
- class_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel
  file: graphrag/language_model/providers/litellm/embedding_model.py
  name: LitellmEmbeddingModel
  methods:
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel._get_kwargs
    name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    docstring: "Get model arguments supported by litellm.\n\nArgs:\n    kwargs: Arbitrary\
      \ keyword arguments. Only those keys in [\"name\", \"dimensions\", \"encoding_format\"\
      , \"timeout\", \"user\"] will be included in the returned dictionary.\n\nReturns:\n\
      \    dict[str, Any]: A dictionary containing the subset of keyword arguments\
      \ that litellm supports."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
      \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    A list of embeddings, where each\
      \ embedding is a list of floats."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Batch generate embeddings.\n\nArgs:\n    text_list: A batch of text\
      \ inputs to generate embeddings for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    list[list[float]]: A batch of embeddings."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Async embed.\n\nArgs:\n    text: The text to generate an embedding\
      \ for.\n    kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    list[float]: The embedding."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Embed a single text input.\n\nArgs:\n    text: The text to generate\
      \ an embedding for.\n    **kwargs: Additional keyword arguments passed to the\
      \ embedding model. These are forwarded to the underlying embedding request and\
      \ may influence the resulting embedding.\n\nReturns:\n    list[float]: The embedding\
      \ for the input text as a list of floating-point numbers. If no embedding is\
      \ produced or the response contains no data, returns [].\n\nNotes:\n    This\
      \ function does not raise an exception on its own. If the underlying embedding\
      \ call fails, the exception will propagate to the caller."
  - node_id: graphrag/language_model/providers/litellm/embedding_model.py::LitellmEmbeddingModel.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    docstring: "Initialize the Litellm embedding model with the given name and configuration.\n\
      \nArgs:\n    name: str\n        The name of the model instance.\n    config:\
      \ LanguageModelConfig\n        The configuration for the language model.\n \
      \   cache: PipelineCache | None\n        Optional cache to use for embeddings.\
      \ If provided, a child cache scoped to this model's name is created.\n    **kwargs:\
      \ Any\n        Additional keyword arguments accepted for compatibility. They\
      \ are not used to configure the embedding model at initialization and will be\
      \ ignored here; some kwargs may be processed by the embedding methods via _get_kwargs.\n\
      \nReturns:\n    None\n\nRaises:\n    Exception\n        If the underlying embedding\
      \ initialization fails (e.g., invalid configuration or cache-related errors)."
- class_id: tests/mock_provider.py::MockChatLLM
  file: tests/mock_provider.py
  name: MockChatLLM
  methods:
  - node_id: tests/mock_provider.py::MockChatLLM.achat
    name: achat
    signature: "def achat(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    docstring: "Return the next response in the predefined list, cycling through available\
      \ responses using modulo arithmetic. If there are no configured responses, returns\
      \ an empty content response.\n\nArgs:\n    prompt: The input prompt to process.\n\
      \    history: Optional list of previous messages for context.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler.\n\n\
      Returns:\n    ModelResponse: The next response in the predefined sequence. If\
      \ the next item is a BaseModel, it will be used as the response payload. If\
      \ the item is a plain string, it will be wrapped in a response object (the wrapper\
      \ BaseModelResponse) containing that string as content. When no responses are\
      \ configured, a BaseModelResponse with empty content is returned.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying chat logic or input validation."
  - node_id: tests/mock_provider.py::MockChatLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        responses: list[str | BaseModel]\
      \ | None = None,\n        config: LanguageModelConfig | None = None,\n     \
      \   json: bool = False,\n        **kwargs: Any,\n    )"
    docstring: "Initialize a mock chat LLM provider with optional responses and configuration.\n\
      \nArgs:\n    responses: List[str | BaseModel] | None. A list of responses to\
      \ return in sequence. Each item can be a string or a BaseModel.\n    config:\
      \ LanguageModelConfig | None. Optional configuration. If provided and it has\
      \ a.responses attribute, those will be used instead of the responses argument.\n\
      \    json: bool. JSON serialization option (present for compatibility; not used\
      \ by this initializer).\n    kwargs: Any. Additional keyword arguments passed\
      \ to the initializer.\n\nReturns:\n    None. This constructor initializes internal\
      \ state and does not return a value.\n\nRaises:\n    None. This initializer\
      \ does not raise exceptions by itself."
  - node_id: tests/mock_provider.py::MockChatLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Asynchronously stream the configured responses from the mock provider.\n\
      \nThis generator yields each configured response in order. It does not use the\
      \ input\nprompt or history for generation.\n\nArgs:\n    prompt (str): The input\
      \ prompt to process. This implementation ignores it.\n    history (list | None):\
      \ Optional conversation history. This implementation ignores it.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying handler.\n\nReturns:\n\
      \    AsyncGenerator[str, None]: An asynchronous generator yielding response\
      \ strings. If a configured response is a BaseModel, it is converted to JSON\
      \ using model_dump_json(); otherwise the response is yielded as-is.\n\nRaises:\n\
      \    None"
  - node_id: tests/mock_provider.py::MockChatLLM.chat
    name: chat
    signature: "def chat(\n        self,\n        prompt: str,\n        history: list\
      \ | None = None,\n        **kwargs,\n    ) -> ModelResponse"
    docstring: "Return the next response in the configured sequence.\n\nThis mock\
      \ chat provider cycles through configured responses and returns them one at\
      \ a time. If no responses are configured, an empty response with content \"\"\
      \ is returned.\n\nArgs:\n    prompt (str): The input prompt to process. The\
      \ mock uses no prompt data to generate the response.\n    history (list | None):\
      \ Optional history for context. Not used by this mock implementation.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler. These\
      \ are ignored by this mock implementation but accepted for compatibility.\n\n\
      Returns:\n    ModelResponse: The next response in the configured list as a BaseModelResponse.\
      \ The response content is accessible via response.output.content, and if the\
      \ stored response was a BaseModel it will be serialized to JSON for the content\
      \ and exposed via parsed_response.\n\nNotes:\n    - The next response is selected\
      \ using a modulo operation on the internal index and then the index is incremented.\n\
      \    - If a response is a Pydantic BaseModel, its JSON representation is used\
      \ as the content (via model_dump_json()). The original BaseModel is exposed\
      \ in parsed_response.\n    - If no responses are configured, the content is\
      \ an empty string \"\" and parsed_response is None."
  - node_id: tests/mock_provider.py::MockChatLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self,\n        prompt: str,\n        history:\
      \ list | None = None,\n        **kwargs,\n    ) -> Generator[str, None]"
    docstring: "Not yet implemented: chat_stream for the mock provider.\n\nThis mock\
      \ chat_stream is not implemented and will raise NotImplementedError if called.\
      \ When implemented, it would yield strings from an internal responses list,\
      \ in order, ignoring the prompt and history.\n\nArgs:\n    prompt (str): The\
      \ input prompt to process. This mock ignores the prompt data.\n    history (list\
      \ | None): Optional conversation history. This mock ignores history.\n    **kwargs:\
      \ Additional keyword arguments forwarded to the underlying chat handler.\n\n\
      Returns:\n    None\n    Type: None\n\nRaises:\n    NotImplementedError"
- class_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource
  file: unified-search-app/app/knowledge_loader/data_sources/blob_source.py
  name: BlobDatasource
  methods:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.__init__
    name: __init__
    signature: 'def __init__(self, database: str)'
    docstring: "Initialize the BlobDatasource with the given database identifier.\n\
      \nArgs:\n    database: The database identifier used to access the blob storage.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "\"\"\"Read parquet file for a given table from blob storage.\n\nArgs:\n\
      \    table: The table name to read (without the .parquet extension).\n    throw_on_missing:\
      \ If True, raise FileNotFoundError when the table file does not exist.\n   \
      \ columns: Optional list of column names to read from the parquet file. If None,\
      \ all columns are read.\n\nReturns:\n    pd.DataFrame: A DataFrame containing\
      \ the data from the parquet file. If the table file does not exist and throw_on_missing\
      \ is False, an empty DataFrame is returned. If columns are provided, the empty\
      \ DataFrame will have those columns.\n\nRaises:\n    FileNotFoundError: If the\
      \ table does not exist and throw_on_missing is True.\n\"\"\""
  - node_id: unified-search-app/app/knowledge_loader/data_sources/blob_source.py::BlobDatasource.read_settings
    name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    docstring: "Read settings from container.\n\nArgs:\n    self: The BlobDatasource\
      \ instance.\n    file: The blob file name containing the settings.\n    throw_on_missing:\
      \ If True, raise FileNotFoundError when the file does not exist.\n\nReturns:\n\
      \    GraphRagConfig | None: The graphrag configuration loaded from the settings\
      \ file, or None if not found.\n\nRaises:\n    FileNotFoundError: If the file\
      \ does not exist and throw_on_missing is True."
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/base.py
  name: BaseNounPhraseExtractor
  methods:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.load_spacy_model
    name: load_spacy_model
    signature: "def load_spacy_model(\n        model_name: str, exclude: list[str]\
      \ | None = None\n    ) -> spacy.language.Language"
    docstring: "Load a SpaCy model.\n\nArgs:\n    model_name: Name of the SpaCy model\
      \ to load.\n    exclude: Optional list of components to exclude from loading.\n\
      \nReturns:\n    spacy.language.Language: The loaded SpaCy language object.\n\
      \nRaises:\n    OSError: If the model cannot be loaded (after attempting to download\
      \ if necessary)."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str | None,\n  \
      \      exclude_nouns: list[str] | None = None,\n        max_word_length: int\
      \ = 15,\n        word_delimiter: str = \" \",\n    ) -> None"
    docstring: "Initialize the base noun phrase extractor.\n\nArgs:\n    model_name:\
      \ The name of the SpaCy model to use, or None.\n    exclude_nouns: List of nouns\
      \ to exclude from extraction. If None, an empty list is used. Excluded nouns\
      \ are stored in uppercase.\n    max_word_length: Maximum length of a word to\
      \ consider when forming noun phrases.\n    word_delimiter: Delimiter used to\
      \ join words within a noun phrase.\n\nReturns:\n    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.extract
    name: extract
    signature: 'def extract(self, text: str) -> list[str]'
    docstring: "Extract noun phrases from text.\n\nArgs:\n    text (str): Text.\n\n\
      Returns:\n    list[str]: List of noun phrases."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/base.py::BaseNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the extractor.\n\nThis is an abstract\
      \ method and must be implemented by concrete subclasses. The base\nclass cannot\
      \ be instantiated due to ABCMeta and abstractmethod usage; the actual\nstring\
      \ representation is therefore defined by subclass implementations. At runtime,\n\
      the behavior depends on the subclass implementation rather than raising an error\
      \ in\nthe base class.\n\nArgs:\n    self (BaseNounPhraseExtractor): The instance\
      \ of the extractor.\n\nReturns:\n    str: The string representation used for\
      \ cache key generation, encoding the extractor's\n    configuration (e.g., model_name,\
      \ max_word_length, exclude_nouns, word_delimiter)."
- class_id: graphrag/query/context_builder/conversation_history.py::QATurn
  file: graphrag/query/context_builder/conversation_history.py
  name: QATurn
  methods:
  - node_id: graphrag/query/context_builder/conversation_history.py::QATurn.get_answer_text
    name: get_answer_text
    signature: def get_answer_text(self) -> str | None
    docstring: "Return the concatenated text of the assistant answers.\n\nArgs:\n\
      \    self: The QATurn instance containing assistant answers.\n\nReturns:\n \
      \   str | None: The assistant answers contents joined by newline characters,\
      \ or None if there are no assistant answers."
  - node_id: graphrag/query/context_builder/conversation_history.py::QATurn.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the QA turn.\n\nArgs:\n    self: QATurn\
      \ instance to stringify.\n\nReturns:\n    str: The string representation of\
      \ the QA turn. If there are assistant answers, the string is\n        \"Question:\
      \ <user_query.content>\\nAnswer: <answers>\"; otherwise, it's\n        \"Question:\
      \ <user_query.content>\"."
- class_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore
  file: graphrag/vector_stores/cosmosdb.py
  name: CosmosDBVectorStore
  methods:
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._database_exists
    name: _database_exists
    signature: def _database_exists(self) -> bool
    docstring: "Check if the configured Cosmos DB database exists.\n\nReturns:\n \
      \   bool: True if the database exists, False otherwise.\n\nRaises:\n    CosmosHttpResponseError:\
      \ If there is an HTTP error while listing databases."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a text-based similarity search.\n\nArgs:\n    text (str):\
      \ The input text to search for similar documents.\n    text_embedder (TextEmbedder):\
      \ The callable used to compute an embedding for the input text.\n    k (int):\
      \ The number of top results to return.\n    **kwargs (Any): Additional keyword\
      \ arguments.\n\nReturns:\n    list[VectorStoreSearchResult]: A list of matching\
      \ VectorStoreSearchResult objects.\n\nRaises:\n    ..."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_database
    name: _delete_database
    signature: def _delete_database(self) -> None
    docstring: "Delete the database if it exists.\n\nReturns:\n    None: This method\
      \ does not return a value."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Builds a query filter to filter documents by a list of ids.\n\nArgs:\n\
      \  include_ids: list[str] | list[int]\n    The IDs to include in the filter.\
      \ If None or an empty list is provided, the filter is cleared (set to None)\
      \ and None is returned.\n\nReturns:\n  Any\n    The constructed query filter\
      \ string to filter documents by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self, vector_store_schema_config: VectorStoreSchemaConfig,\
      \ **kwargs: Any\n    ) -> None"
    docstring: "Initialize CosmosDB vector store by delegating to the base class constructor.\n\
      \nArgs:\n  vector_store_schema_config: VectorStoreSchemaConfig - The schema\
      \ configuration for the vector store.\n  **kwargs: Any - Additional keyword\
      \ arguments forwarded to the base class initializer.\n\nReturns:\n  None\n\n\
      Raises:\n  Exceptions raised by the base class __init__ are propagated."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.cosine_similarity
    name: cosine_similarity
    signature: def cosine_similarity(a, b)
    docstring: "Cosine similarity between two vectors.\n\nArgs:\n  a: First vector.\n\
      \  b: Second vector.\n\nReturns:\n  float: The cosine similarity between a and\
      \ b. If either vector has zero magnitude, returns 0.0."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_database
    name: _create_database
    signature: def _create_database(self) -> None
    docstring: "Create the database if it doesn't exist.\n\nThis method uses the Cosmos\
      \ client to ensure the database exists and then obtains a database client for\
      \ subsequent operations.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    CosmosHttpResponseError: If the Cosmos DB service\
      \ returns an HTTP error."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create the Cosmos DB container for the current container name if it\
      \ doesn't exist.\n\nConfigures a partition key based on the id field, a vector\
      \ embedding policy for the vector field and size, and an indexing policy. It\
      \ first attempts to apply a diskANN vector index; if that raises CosmosHttpResponseError,\
      \ it retries without vector indexes to ensure compatibility.\n\nArgs:\n    self:\
      \ Any. The instance containing the Cosmos DB client and vector store configuration.\n\
      \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
      \ Cosmos client operations may be raised."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to CosmosDB vector storage.\n\nArgs:\n    connection_string:\
      \ The Cosmos DB connection string. If provided, a CosmosClient is created from\
      \ it.\n    url: The Cosmos DB account URL. Used when connection_string is not\
      \ provided.\n    database_name: The name of the database to use. This must be\
      \ provided.\n\nReturns:\n    None\n\nRaises:\n    ValueError: If neither connection_string\
      \ nor url is provided.\n    ValueError: If database_name is not provided.\n\
      \    ValueError: If index_name is empty or not provided."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the CosmosDB vector store.\n\nIf overwrite is\
      \ True, the existing container will be deleted and recreated before loading.\
      \ Documents with non-null vectors are stored. Each document is stored as an\
      \ item with fields corresponding to the configured id_field, vector_field, text_field,\
      \ and attributes_field (JSON-serialized). Upload uses upsert semantics; existing\
      \ items with the same id will be updated.\n\nArgs:\n    documents (list[VectorStoreDocument]):\
      \ List of VectorStoreDocument objects to load into CosmosDB. Only documents\
      \ with a non-null vector are stored. Each stored item includes fields corresponding\
      \ to the configured id_field, vector_field, text_field, and attributes_field.\n\
      \    overwrite (bool): If True, delete and recreate the container before loading\
      \ documents; otherwise preserve existing data.\n\nReturns:\n    None\n\nRaises:\n\
      \    ValueError: If the container client is not initialized."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._container_exists
    name: _container_exists
    signature: def _container_exists(self) -> bool
    docstring: "Check if the configured Cosmos DB container exists in the database.\n\
      \nArgs:\n    self: The instance that holds _container_name and _database_client\
      \ used to query Cosmos DB.\n\nReturns:\n    bool: True if a container with id\
      \ equal to self._container_name exists, otherwise False. The check is performed\
      \ against the 'id' field of containers returned by list_containers.\n\nRaises:\n\
      \    CosmosHttpResponseError: If an HTTP error occurs while listing containers.\
      \ This method does not catch it."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id in the Cosmos DB vector store.\n\nArgs:\n\
      \    id: The identifier of the document to retrieve.\n\nReturns:\n    VectorStoreDocument:\
      \ The document corresponding to the provided id with its id, vector, text, and\
      \ attributes populated from the stored item.\n\nRaises:\n    ValueError: If\
      \ the container client is not initialized."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "\"\"\"Delete the vector store container from the database if it exists.\n\
      \nArgs:\n    self: The instance of the class containing the vector store and\
      \ database client.\n\nReturns:\n    None\n\nRaises:\n    CosmosHttpResponseError:\
      \ If the underlying Cosmos DB client call to delete the container fails.\n\"\
      \"\""
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Clear the vector store by deleting its container and database.\n\n\
      Deletes the underlying container and database used to store vectors by delegating\
      \ to _delete_container() and _delete_database().\n\nRaises:\n    CosmosHttpResponseError:\
      \ If the underlying Cosmos DB client call to delete the container or database\
      \ fails."
  - node_id: graphrag/vector_stores/cosmosdb.py::CosmosDBVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search against the CosmosDB container.\n\
      \nArgs:\n  query_embedding: Embedding vector to search with\n  k: Number of\
      \ top results to return\n  kwargs: Additional keyword arguments for compatibility;\
      \ not used directly by this method\n\nReturns:\n  list[VectorStoreSearchResult]:\
      \ Top-k results as VectorStoreSearchResult objects\n\nRaises:\n  ValueError:\
      \ If the container client is not initialized."
- class_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage
  file: graphrag/storage/blob_pipeline_storage.py
  name: BlobPipelineStorage
  methods:
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._abfs_url
    name: _abfs_url
    signature: 'def _abfs_url(self, key: str) -> str'
    docstring: "Get the ABFS URL for the given key.\n\nArgs:\n    key: The key identifying\
      \ the blob within the container and path prefix.\n\nReturns:\n    str: The ABFS\
      \ URL for the given key, formatted as abfs://{path}."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nArgs:\n    self: The instance of\
      \ the BlobPipelineStorage.\n\nReturns:\n    list[str]: The keys currently stored\
      \ in the storage.\n\nRaises:\n    NotImplementedError: Blob storage does yet\
      \ not support listing keys."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_json
    name: _set_df_json
    signature: 'def _set_df_json(self, key: str, dataframe: Any) -> None'
    docstring: "Store a dataframe as JSON at the specified key in storage.\n\nThe\
      \ dataframe is serialized to JSON in records format with one JSON object per\
      \ line and written to the path derived from the provided key. Depending on configuration,\
      \ the write uses either a storage account with DefaultAzureCredential or a connection\
      \ string.\n\nArgs:\n    key: The key under which to store the JSON export of\
      \ the dataframe.\n    dataframe: The dataframe to serialize to JSON.\n\nReturns:\n\
      \    None\n\nRaises:\n    Exception: If an error occurs during serialization\
      \ or storage write."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._set_df_parquet
    name: _set_df_parquet
    signature: 'def _set_df_parquet(self, key: str, dataframe: Any) -> None'
    docstring: "\"\"\"Set a parquet dataframe.\n\nStores the provided dataframe as\
      \ a Parquet file at the path derived from the key. If a storage account name\
      \ is configured and no connection string is provided, the Parquet file is written\
      \ using Azure storage options with the storage account name and DefaultAzureCredential;\
      \ otherwise, the Parquet file is written using the provided connection string.\n\
      \nArgs:\n    key: The key under which to store the parquet export of the dataframe.\n\
      \    dataframe: The dataframe to serialize and store.\n\nReturns:\n    None\n\
      \nRaises:\n    Exceptions raised by the dataframe serialization or the storage\
      \ write operations.\n\"\"\""
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find blobs in a container using a file pattern, as well as a custom\
      \ filter function.\n\nArgs:\n    base_dir: The name of the base container.\n\
      \    file_pattern: The file pattern to use.\n    file_filter: A dictionary of\
      \ key-value pairs to filter the blobs.\n    max_count: The maximum number of\
      \ blobs to return. If -1, all blobs are returned.\n\nReturns:\n    An iterator\
      \ of blob names and their corresponding regex matches."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the blob storage cache. This\
      \ implementation is a no-op and does not remove any blobs from the container.\
      \ To perform an actual clear, implement iteration over the container's blobs\
      \ and delete each one.\n\nReturns:\n    None: The coroutine completes without\
      \ returning a value."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get a value from the cache.\n\nArgs:\n    key: The key to retrieve\
      \ from the cache.\n    as_bytes: If True, return the raw bytes stored for the\
      \ key; if False, decode the data to a string using encoding.\n    encoding:\
      \ Encoding to use when decoding the value if as_bytes is False.\n\nReturns:\n\
      \    Any: The value associated with the key. If as_bytes is False, the value\
      \ is decoded to a string using the provided encoding or the object's default\
      \ encoding. If an error occurs, returns None.\n\nRaises:\n    None: This method\
      \ does not raise any exceptions; it logs a warning and returns None on error."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._create_container
    name: _create_container
    signature: def _create_container(self) -> None
    docstring: "Create the blob container if it does not exist.\n\nArgs:\n    self:\
      \ The instance of the class containing the BlobServiceClient and container configuration.\n\
      \nReturns:\n    None\n\nRaises:\n    Exceptions raised by the underlying Azure\
      \ Blob Storage client operations may be raised."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Delete a key from the cache.\n\nArgs:\n    key (str): The key to delete.\n\
      \nReturns:\n    None"
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._container_exists
    name: _container_exists
    signature: def _container_exists(self) -> bool
    docstring: "Check if the configured blob container exists.\n\nArgs:\n    self:\
      \ The instance of the class that holds _container_name and _blob_service_client.\n\
      \nReturns:\n    bool: True if the container exists, otherwise False.\n\nRaises:\n\
      \    Exception: If the underlying blob service client call to list_containers\
      \ raises an exception."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set a value in the cache.\n\nArgs:\n  key: str \u2014 The key under\
      \ which to store the value.\n  value: Any \u2014 The value to store in the cache.\n\
      \  encoding: str | None \u2014 Optional encoding to use when encoding non-bytes\
      \ to bytes. If None, uses the default encoding.\n\nReturns:\n  None \u2014 The\
      \ method does not return a value.\n\nRaises:\n  Exception \u2014 Exceptions\
      \ raised by the underlying cache operation are caught and logged; they are not\
      \ propagated to the caller."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._keyname
    name: _keyname
    signature: 'def _keyname(self, key: str) -> str'
    docstring: "Get the key name.\n\nArgs:\n    key: The key to be joined with the\
      \ path prefix to form the full key path.\n\nReturns:\n    The full key name\
      \ as a string."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a new BlobPipelineStorage instance.\n\nThis constructor\
      \ selects the initialization flow as follows:\n- If a connection_string is provided,\
      \ create the BlobServiceClient from the connection string.\n- Otherwise, if\
      \ storage_account_blob_url is provided, create the BlobServiceClient using the\
      \ account URL and DefaultAzureCredential.\n- If neither is provided, raise a\
      \ ValueError.\n\ncontainer_name is required. Providing container_name as None\
      \ raises ValueError. If the container_name key is missing from kwargs, a KeyError\
      \ is raised by Python.\n\nbase_dir is optional and sets the path prefix within\
      \ the container (defaults to an empty string). encoding defaults to 'utf-8'.\n\
      \nArgs:\n  container_name: The container name to use for blob storage. This\
      \ key is required. If the key is missing from kwargs, a KeyError is raised.\
      \ If the value is None, a ValueError is raised.\n  connection_string: The Azure\
      \ Blob Storage connection string. If provided, used to initialize the client.\n\
      \  storage_account_blob_url: The URL of the storage account blob endpoint. Used\
      \ with DefaultAzureCredential when connection_string is not provided.\n  base_dir:\
      \ The base directory (path prefix) within the container. Optional. Defaults\
      \ to ''.\n  encoding: Encoding to use. Defaults to 'utf-8'.\n\nReturns:\n  None\n\
      \nRaises:\n  KeyError: If container_name is not provided in kwargs.\n  ValueError:\
      \ If container_name is None.\n  ValueError: If neither connection_string nor\
      \ storage_account_blob_url is provided.\n  ValueError: If storage_account_blob_url\
      \ is None when connection_string is not provided."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Asynchronously check if a key exists in the cache.\n\nArgs:\n    key\
      \ (str): The cache key to check.\n\nReturns:\n    bool: True if a value exists\
      \ for the key in the cache, otherwise False.\n\nRaises:\n    Exception: If an\
      \ error occurs while performing the underlying blob storage operations."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict. If a key referenced\
      \ by file_filter is missing from item, a KeyError may be raised. Values should\
      \ be strings (or objects compatible with re.search).\n\nReturns:\n    bool:\
      \ True if no file_filter is defined. Otherwise, True if all key/value patterns\
      \ in file_filter match the corresponding fields in item using re.search; False\
      \ otherwise.\n\nRaises:\n    KeyError: If a key referenced by file_filter is\
      \ missing from the item."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._delete_container
    name: _delete_container
    signature: def _delete_container(self) -> None
    docstring: "\"\"\"Delete the container if it exists.\n\nThis protected method\
      \ deletes the container using the internal container name (self._container_name).\
      \ Deletion is conditional on existence, as determined by _container_exists().\
      \ The deletion is performed via the BlobServiceClient stored on self._blob_service_client\
      \ and may raise Azure SDK exceptions if the operation fails.\n\nArgs:\n    None:\
      \ This method does not accept explicit parameters. It relies on internal state\
      \ (self._container_name, self._blob_service_client).\n\nReturns:\n    None:\
      \ This method does not return a value.\n\nRaises:\n    AzureError: Azure SDK\
      \ exceptions may be raised during the delete operation.\n\"\"\""
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. If None, the current instance is returned.\n\
      \nReturns:\n    PipelineStorage: The current instance when name is None; otherwise\
      \ a new BlobPipelineStorage configured with base_dir set to the path formed\
      \ by joining the current path prefix and name."
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage._blobname
    name: _blobname
    signature: 'def _blobname(blob_name: str) -> str'
    docstring: "Normalize a blob name by removing the internal path prefix and any\
      \ leading slash.\n\nArgs:\n    blob_name: The original blob name as a string.\n\
      \nReturns:\n    The blob name with the path prefix (self._path_prefix) removed\
      \ if present, and any leading slash stripped.\n\nRaises:\n    None"
  - node_id: graphrag/storage/blob_pipeline_storage.py::BlobPipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "\"\"\"Get the creation date for the given key from Azure Blob storage\
      \ and format it with the local time zone.\n\nArgs:\n    key (str): The key for\
      \ which to retrieve the creation date from blob storage.\n\nReturns:\n    str:\
      \ The creation date as a string formatted with the local time zone. Returns\
      \ an empty string if an error occurs.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; it returns an empty string if an error occurs.\n\"\"\
      \""
- class_id: graphrag/vector_stores/base.py::BaseVectorStore
  file: graphrag/vector_stores/base.py
  name: BaseVectorStore
  methods:
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform ANN search by vector.\n\nArgs:\n    self: The instance of\
      \ the class.\n    query_embedding: list[float] The embedding vector to search\
      \ with.\n    k: int The number of top results to return.\n    **kwargs: Any\
      \ Additional keyword arguments that may influence the search.\n\nReturns:\n\
      \    list[VectorStoreSearchResult]: The top-k search results as VectorStoreSearchResult\
      \ objects.\n\nRaises:\n    NotImplementedError: If the method is not implemented\
      \ by a subclass."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
    docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
      \ arguments for connecting to the vector storage.\n\nReturns:\n    None"
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids\
      \ (Optional[list[str] | list[int]]): The IDs to include in the filter. If None\
      \ or an empty list is provided, the filter is cleared (set to None) and None\
      \ is returned. Concrete implementations may define different handling for empty\
      \ input.\n\nReturns:\n  Any: The constructed query filter to filter documents\
      \ by the provided IDs, or None if no IDs are provided."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector-store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
      \ - List of VectorStoreDocument objects to load into the vector store.\n   \
      \ overwrite: bool - If True, overwrite existing data in the vector store; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        vector_store_schema_config:\
      \ VectorStoreSchemaConfig,\n        db_connection: Any | None = None,\n    \
      \    document_collection: Any | None = None,\n        query_filter: Any | None\
      \ = None,\n        **kwargs: Any,\n    )"
    docstring: "Initialize the base vector store with the provided configuration and\
      \ optional resources.\n\nThis initializer assigns the given resources to instance\
      \ attributes and extracts\nconfiguration fields from vector_store_schema_config\
      \ to set the store's metadata\nand vector properties. It does not call super().__init__.\n\
      \nArgs:\n  vector_store_schema_config (VectorStoreSchemaConfig): The schema\
      \ configuration for the vector store. This is used to set index_name, id_field,\
      \ text_field, vector_field, attributes_field, and vector_size on the instance.\n\
      \  db_connection (Any | None): Optional database connection.\n  document_collection\
      \ (Any | None): Optional document collection.\n  query_filter (Any | None):\
      \ Optional query filter to apply to queries.\n  kwargs (Any): Additional keyword\
      \ arguments captured for later use and stored in self.kwargs.\n\nReturns:\n\
      \  None\n\nRaises:\n  None"
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id."
  - node_id: graphrag/vector_stores/base.py::BaseVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "\"\"\"Perform ANN search by text.\n\nArgs:\n    self: The instance\
      \ of the class.\n    text: str The input text to search for similar documents.\n\
      \    text_embedder: TextEmbedder The callable used to compute an embedding for\
      \ the input text.\n    k: int The number of top results to return.\n    **kwargs:\
      \ Any Additional keyword arguments.\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ A list of matching VectorStoreSearchResult objects.\n\nRaises:\n    NotImplementedError:\
      \ If the method is not implemented.\n\"\"\""
- class_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer
  file: graphrag/tokenizer/tiktoken_tokenizer.py
  name: TiktokenTokenizer
  methods:
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "\"\"\"Decode a list of tokens back into a string.\n\nArgs:\n    tokens\
      \ (list[int]): A list of tokens to decode.\n\nReturns:\n    str: The decoded\
      \ string from the list of tokens.\n\nRaises:\n    Exception: If decoding fails\
      \ due to an underlying error in the encoding.\n\"\"\""
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.__init__
    name: __init__
    signature: 'def __init__(self, encoding_name: str) -> None'
    docstring: "Initialize the Tiktoken Tokenizer.\n\nArgs:\n    encoding_name (str):\
      \ The name of the Tiktoken encoding to use for tokenization.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/tokenizer/tiktoken_tokenizer.py::TiktokenTokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "Encode the given text into a list of tokens.\n\nArgs:\n    text (str):\
      \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
      \ the encoded text."
- class_id: graphrag/query/structured_search/local_search/search.py::LocalSearch
  file: graphrag/query/structured_search/local_search/search.py
  name: LocalSearch
  methods:
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Builds a local search context that fits a single context window and\
      \ generates an answer for the user query.\n\nArgs:\n    query: The user query\
      \ to process.\n    conversation_history: Optional conversation history to incorporate\
      \ into the search context.\n    **kwargs: Additional keyword arguments passed\
      \ to the context builder and the model. May include drift_query to override\
      \ the query for drift.\n\nReturns:\n    SearchResult: The constructed search\
      \ result containing the response text, context data and text, completion time,\
      \ and token usage metadata.\n\nRaises:\n    None. All exceptions are caught\
      \ within the method and result in an empty response rather than propagating\
      \ errors."
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    docstring: "Initialize a LocalSearch instance for local search orchestration.\n\
      \nArgs:\n    model: ChatModel - The language model interface used for this local\
      \ search.\n    context_builder: LocalContextBuilder - The builder that constructs\
      \ the context for the local search.\n    tokenizer: Tokenizer | None - Optional\
      \ tokenizer to use.\n    system_prompt: str | None - System prompt for the local\
      \ search. If None, uses LOCAL_SEARCH_SYSTEM_PROMPT.\n    response_type: str\
      \ - The type of response formatting, e.g., \"multiple paragraphs\".\n    callbacks:\
      \ list[QueryCallbacks] | None - Optional list of query callbacks.\n    model_params:\
      \ dict[str, Any] | None - Additional parameters for the model.\n    context_builder_params:\
      \ dict | None - Additional parameters for the context builder.\n\nReturns:\n\
      \    None - The instance is initialized and nothing is returned."
  - node_id: graphrag/query/structured_search/local_search/search.py::LocalSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator"
    docstring: "Build local search context that fits a single context window and generate\
      \ answer for the user query.\n\nArgs:\n    query (str): The user query to process.\n\
      \    conversation_history (ConversationHistory | None): Optional conversation\
      \ history to incorporate into the search context.\n\nReturns:\n    AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding strings representing chunks of the\
      \ generated answer.\n\nRaises:\n    Exception: If an error occurs during streaming\
      \ or within the model or callbacks."
- class_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry
  file: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py
  name: IncrementalWaitRetry
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retry_wait:\
      \ float,\n        max_retries: int = 5,\n        **kwargs: Any,\n    )"
    docstring: "Initialize an Incremental Wait Retry instance with retry configuration.\n\
      \nArgs:\n    max_retry_wait: The maximum wait time between retries (float).\n\
      \    max_retries: The maximum number of retry attempts (int). Must be greater\
      \ than 0.\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n   \
      \ None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n   \
      \ ValueError: max_retry_wait must be greater than 0."
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function with incremental delay until it succeeds\
      \ or the maximum number of retries is reached.\n\nArgs:\n  func: The asynchronous\
      \ function to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword\
      \ arguments to pass to the function. (Any)\n\nReturns:\n  Any: The result of\
      \ the awaited function.\n\nRaises:\n  Exception: If the wrapped function keeps\
      \ raising and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/incremental_wait_retry.py::IncrementalWaitRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function.\n\nRetries the provided function until\
      \ it succeeds or the maximum number of retries is reached, applying an incremental\
      \ delay between retries.\n\nArgs:\n    func: Callable[..., Any] - The function\
      \ to invoke. It will be called as func(**kwargs) and its result will be returned\
      \ on success.\n    kwargs: Any - Keyword arguments to pass to func.\n\nReturns:\n\
      \    Any - The value returned by func on a successful invocation.\n\nRaises:\n\
      \    Exception - The last exception raised by func when the maximum number of\
      \ retries is exceeded."
- class_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks
  file: graphrag/callbacks/query_callbacks.py
  name: QueryCallbacks
  methods:
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_start
    name: on_reduce_response_start
    signature: "def on_reduce_response_start(\n        self, reduce_response_context:\
      \ str | dict[str, Any]\n    ) -> None"
    docstring: "Handle the start of reduce operation.\n\nArgs:\n    reduce_response_context:\
      \ Context for the reduce response (str | dict[str, Any]).\nReturns:\n    None:\
      \ The function does not return a value."
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_llm_new_token
    name: on_llm_new_token
    signature: def on_llm_new_token(self, token) -> None
    docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n \
      \       The new token generated by the LLM.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_end
    name: on_map_response_end
    signature: 'def on_map_response_end(self, map_response_outputs: list[SearchResult])
      -> None'
    docstring: "End of map operation callback. This default implementation is a no-op\
      \ and does not mutate state or produce side effects. Subclasses may override\
      \ this method to handle the map outputs as needed.\n\nArgs:\n    map_response_outputs\
      \ (list[SearchResult]): The outputs produced by the map operation.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_map_response_start
    name: on_map_response_start
    signature: 'def on_map_response_start(self, map_response_contexts: list[str])
      -> None'
    docstring: "Handle the start of map response operation.\n\nArgs:\n    map_response_contexts:\
      \ A list of strings representing contexts for the map response operation to\
      \ begin processing.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_context
    name: on_context
    signature: 'def on_context(self, context: Any) -> None'
    docstring: "Handle when context data is constructed.\n\nArgs:\n    context (Any):\
      \ The context data provided to the callback. This implementation performs no\
      \ operations on it.\n\nReturns:\n    None: The function returns no value.\n\n\
      Raises:\n    None: This function does not raise exceptions by itself."
  - node_id: graphrag/callbacks/query_callbacks.py::QueryCallbacks.on_reduce_response_end
    name: on_reduce_response_end
    signature: 'def on_reduce_response_end(self, reduce_response_output: str) -> None'
    docstring: "Handle the end of reduce operation.\n\nArgs:\n    reduce_response_output:\
      \ str\n        The output produced by the end of the reduce operation.\n\nReturns:\n\
      \    None: The function does not return a value."
- class_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore
  file: tests/integration/vector_stores/test_factory.py
  name: CustomVectorStore
  methods:
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: def similarity_search_by_text(self, text, text_embedder, k=10, **kwargs)
    docstring: "Performs a similarity search using the provided text.\n\nArgs:\n \
      \   text (str): The query text to search.\n    text_embedder (Any): The embedder\
      \ used to convert the text into embeddings.\n    k (int): The number of results\
      \ to return. Defaults to 10.\n    **kwargs: Additional keyword arguments passed\
      \ to the underlying search implementation.\n\nReturns:\n    list: A list of\
      \ search results. In this implementation, returns an empty list.\n\nRaises:\n\
      \    None: This function does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.filter_by_id
    name: filter_by_id
    signature: def filter_by_id(self, include_ids)
    docstring: "Filter vector store results by a set of IDs.\n\nArgs:\n    include_ids:\
      \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    Any\
      \ - The filtered results. This implementation returns a dictionary (empty by\
      \ default in the test)."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.connect
    name: connect
    signature: def connect(self, **kwargs)
    docstring: "Connect to the vector store.\n\nThis base implementation is a placeholder/no-op\
      \ and should be overridden by subclasses to establish an actual connection.\n\
      \nArgs:\n    kwargs: Arbitrary keyword arguments.\n\nReturns:\n    None\n\n\
      Raises:\n    None: This base implementation does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: def similarity_search_by_vector(self, query_embedding, k=10, **kwargs)
    docstring: "Perform a similarity search by vector and return the top-k results.\n\
      \nArgs:\n  self: The instance of the class.\n  query_embedding: list[float]\
      \ The embedding vector to search with.\n  k: int The number of top results to\
      \ return.\n  **kwargs: Any Additional keyword arguments that may influence the\
      \ search.\n\nReturns:\n  list: The top-k search results. This placeholder implementation\
      \ returns an empty list."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Internal API: Initialize the CustomVectorStore by forwarding keyword\
      \ arguments to the base class initializer.\n\nArgs:\n  kwargs: dict of keyword\
      \ arguments forwarded to BaseVectorStore.__init__\n\nReturns:\n  None\n\nRaises:\n\
      \  Propagates exceptions raised by BaseVectorStore.__init__."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.load_documents
    name: load_documents
    signature: def load_documents(self, documents, overwrite=True)
    docstring: "Load documents into the vector store.\n\nArgs:\n  documents (list[VectorStoreDocument]):\
      \ List of VectorStoreDocument objects to load into the vector store.\n  overwrite\
      \ (bool): If True, overwrite existing data in the vector store; otherwise, preserve\
      \ existing data.\n\nReturns:\n  None\n\nNotes:\n  - This base implementation\
      \ is a placeholder and intentionally does nothing. Subclasses should override\
      \ to provide concrete loading behavior.\n  - No input validation is performed\
      \ in this base method.\n  - If documents is empty, the method performs no action.\n\
      \  - Overwrite semantics are intended for the concrete implementation; callers\
      \ should ensure documents meet preconditions (e.g., required fields, IDs) as\
      \ required by the concrete store."
  - node_id: tests/integration/vector_stores/test_factory.py::CustomVectorStore.search_by_id
    name: search_by_id
    signature: def search_by_id(self, id)
    docstring: "\"\"\"\nSearch for a document by id.\n\nArgs:\n    id (str): The identifier\
      \ of the document to retrieve.\n\nReturns:\n    VectorStoreDocument: The document\
      \ corresponding to the provided id.\n\"\"\""
- class_id: graphrag/config/enums.py::ModelType
  file: graphrag/config/enums.py
  name: ModelType
  methods:
  - node_id: graphrag/config/enums.py::ModelType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes."
- class_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: TokenTextSplitter
  methods:
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.num_tokens
    name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
    docstring: "Return the number of tokens in a string.\n\nArgs:\n    text: The input\
      \ string to count tokens in.\n\nReturns:\n    int: The number of tokens in text."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> list[str]'
    docstring: "Split text into chunks using a token-based tokenizer.\n\nIf text is\
      \ a list of strings, it is joined with spaces to form a single string before\
      \ splitting. If the input is NaN or an empty string, an empty list is returned.\n\
      \nArgs:\n    text: str | list[str]\n        The input text to split. If a list\
      \ of strings is provided, they are joined with spaces prior to splitting.\n\n\
      Returns:\n    list[str]\n        The list of chunked text strings produced.\n\
      \nRaises:\n    TypeError: If a non-string value is encountered during processing."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TokenTextSplitter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        tokenizer: Tokenizer | None\
      \ = None,\n        **kwargs: Any,\n    )"
    docstring: "Init method for TokenTextSplitter with an optional tokenizer.\n\n\
      This initializer sets the tokenizer to use for tokenization. If no tokenizer\
      \ is provided,\na default tokenizer is obtained via get_tokenizer(). Any additional\
      \ keyword arguments are\nforwarded to the base class initializer via super().__init__(**kwargs).\n\
      \nArgs:\n    tokenizer (Tokenizer | None): Tokenizer to use for tokenization.\
      \ If None, a default tokenizer\n        is obtained via get_tokenizer().\n \
      \   **kwargs (Any): Additional keyword arguments forwarded to the base class\
      \ initializer.\n\nReturns:\n    None"
- class_id: graphrag/query/structured_search/base.py::BaseSearch
  file: graphrag/query/structured_search/base.py
  name: BaseSearch
  methods:
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Asynchronously search the given query.\n\nThis abstract method must\
      \ be implemented by subclasses. It performs an asynchronous\nsearch given a\
      \ query string and optional conversation history, returning a SearchResult.\n\
      \nArgs:\n    query (str): The search query to execute.\n    conversation_history\
      \ (ConversationHistory | None): Optional conversation history to consider during\
      \ the search. If provided, prior turns may influence results.\n    **kwargs:\
      \ Additional keyword arguments passed to the search implementation.\n\nReturns:\n\
      \    SearchResult: The result of the asynchronous search operation.\n\nRaises:\n\
      \    NotImplementedError: Subclasses must implement this method."
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream search for the given query.\nArgs:\n    query (str): The search\
      \ query to execute.\n    conversation_history (ConversationHistory | None):\
      \ Optional conversation history to consider during the search.\nReturns:\n \
      \   AsyncGenerator[str, None]: An asynchronous generator yielding strings representing\
      \ streamed search results.\nRaises:\n    NotImplementedError: Subclasses must\
      \ implement this method."
  - node_id: graphrag/query/structured_search/base.py::BaseSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ T,\n        tokenizer: Tokenizer | None = None,\n        model_params: dict[str,\
      \ Any] | None = None,\n        context_builder_params: dict[str, Any] | None\
      \ = None,\n    )"
    docstring: "Initialize a BaseSearch instance with the provided model and context\
      \ builder.\n\nArgs:\n    model (ChatModel): The language model interface used\
      \ for this base search.\n    context_builder (T): The builder that constructs\
      \ the context for the search.\n    tokenizer (Tokenizer | None): Optional tokenizer\
      \ to use. If None, a tokenizer is selected via get_tokenizer().\n    model_params\
      \ (dict[str, Any] | None): Optional configuration parameters for the language\
      \ model.\n    context_builder_params (dict[str, Any] | None): Optional configuration\
      \ parameters for the context builder.\n\nReturns:\n    None: This constructor\
      \ initializes internal state and does not return a value."
- class_id: graphrag/utils/api.py::MultiVectorStore
  file: graphrag/utils/api.py
  name: MultiVectorStore
  methods:
  - node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a text-based similarity search.\n\nThis method computes an\
      \ embedding for the input text using the provided text_embedder. If the resulting\
      \ embedding is truthy (i.e., not None or an empty result), it delegates to similarity_search_by_vector\
      \ with that embedding and the specified k. If the embedding is falsy, it returns\
      \ an empty list, indicating no results.\n\nArgs:\n    text (str): The input\
      \ text to search for similar documents.\n    text_embedder (TextEmbedder): A\
      \ callable that returns a list of floats representing the embedding of the input\
      \ text.\n    k (int): The number of top results to return. Defaults to 10.\n\
      \    **kwargs: Additional keyword arguments passed to downstream search methods.\n\
      \nReturns:\n    list[VectorStoreSearchResult]: A list of matching VectorStoreSearchResult\
      \ objects, sorted by score in descending order and truncated to k results.\n\
      \nRaises:\n    Propagates exceptions raised by the text_embedder or by the underlying\
      \ similarity_search_by_vector call.\n\nNotes:\n    - If text is None or empty,\
      \ text_embedder(text) may raise or return a falsy value, in which case this\
      \ method returns [].\n    - The internal flow is: compute the embedding via\
      \ text_embedder, then perform a vector search only if the embedding is truthy;\
      \ otherwise, return an empty list."
  - node_id: graphrag/utils/api.py::MultiVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Perform a vector-based similarity search across all configured embedding\
      \ stores and merge results.\n\nArgs:\n  query_embedding: list[float] - Embedding\
      \ vector to search with\n  k: int - Number of top results to return\n  kwargs:\
      \ Any - Additional keyword arguments for compatibility; not used directly by\
      \ this method\n\nReturns:\n  list[VectorStoreSearchResult] - Top-k results merged\
      \ from all stores, sorted by score in descending order\n\nRaises:\n  Exception\
      \ - Exceptions raised by the underlying embedding stores may propagate to the\
      \ caller."
  - node_id: graphrag/utils/api.py::MultiVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Search for a document by id across the configured vector stores.\n\
      \nArgs:\n    id: The composite identifier used to locate the document. It should\
      \ be formatted as \"<internal_id>-<index_name>\", where\n        <internal_id>\
      \ is the id within the specific vector store and <index_name> is the name of\
      \ that store.\n\nReturns:\n    VectorStoreDocument: The document corresponding\
      \ to the provided id from the matching vector store.\n\nRaises:\n    ValueError:\
      \ If the index name extracted from the id is not found among the configured\
      \ index stores."
  - node_id: graphrag/utils/api.py::MultiVectorStore.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        embedding_stores: list[BaseVectorStore],\n\
      \        index_names: list[str],\n    )"
    docstring: "Initialize a MultiVectorStore with embedding stores and index names.\n\
      \nArgs:\n    embedding_stores: list[BaseVectorStore]\n        The vector stores\
      \ to combine in this multi-store.\n    index_names: list[str]\n        The corresponding\
      \ index names for each embedding store.\n\nReturns:\n    None"
  - node_id: graphrag/utils/api.py::MultiVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector store.\n\nArgs:\n    documents: List\
      \ of VectorStoreDocument objects to load into the vector store.\n    overwrite:\
      \ Whether to overwrite existing data in the vector store if True; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented"
  - node_id: graphrag/utils/api.py::MultiVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Build a query filter to filter documents by id.\n\nArgs:\n  include_ids:\
      \ list[str] | list[int]\n    The IDs to include in the filter.\n\nReturns:\n\
      \  Any\n    The constructed query filter to filter documents by the provided\
      \ IDs.\n\nRaises:\n  NotImplementedError\n    If the method is not implemented."
  - node_id: graphrag/utils/api.py::MultiVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> Any'
    docstring: "Connect to vector storage.\n\nArgs:\n    kwargs: Additional keyword\
      \ arguments for connecting to the vector storage.\n\nReturns:\n    Any: The\
      \ result of the connection operation.\n\nRaises:\n    NotImplementedError: If\
      \ the method is not implemented."
- class_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage
  file: graphrag/storage/memory_pipeline_storage.py
  name: MemoryPipelineStorage
  methods:
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nArgs:\n    self (MemoryPipelineStorage):\
      \ The instance of the MemoryPipelineStorage.\n\nReturns:\n    list[str]: The\
      \ keys currently stored in the storage."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Asynchronously delete the given key from the storage.\n\nArgs:\n \
      \   key (str): The key to delete.\n\nReturns:\n    None\n\nRaises:\n    KeyError:\
      \ If the key does not exist in the storage."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "\"\"\"Return True if the given key exists in the storage.\n\nArgs:\n\
      \    key (str): The key to check for.\n\nReturns:\n    bool: True if the key\
      \ exists in the storage, False otherwise.\n\"\"\""
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Asynchronously set the value for the given key in memory storage.\n\
      \nThis method updates the in-memory storage dictionary by assigning the provided\
      \ value to the specified key. The encoding parameter is accepted for compatibility\
      \ but is not used in this implementation.\n\nArgs:\n    key (str): The key to\
      \ set the value for.\n    value (Any): The value to set.\n    encoding (str\
      \ | None): Optional encoding to apply when serializing the value (unused).\n\
      \nReturns:\n    None: The coroutine completes when the value has been set.\n\
      \nRaises:\n    None: This coroutine does not raise any exceptions."
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize in-memory storage backend.\n\nThis constructor initializes\
      \ the internal storage by creating an empty\ndictionary bound to self._storage\
      \ and by calling the base class initializer.\n\nArgs:\n    self (MemoryPipelineStorage):\
      \ The instance being initialized. No additional\n        parameters.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "\"\"\"Get the value for the given key from in-memory storage.\n\n\
      Args:\n    key (str): The key to retrieve the value for.\n    as_bytes (bool\
      \ | None): Unused. This parameter is accepted for API compatibility but is ignored\
      \ by this backend.\n    encoding (str | None): Unused. This parameter is accepted\
      \ for API compatibility but is ignored by this backend.\n\nReturns:\n    Any:\
      \ The value associated with the key if present; None if the key is not in storage.\"\
      \"\""
  - node_id: graphrag/storage/memory_pipeline_storage.py::MemoryPipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create a child storage instance.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. This parameter is accepted for API compatibility\
      \ but is ignored.\n\nReturns:\n    PipelineStorage: A new MemoryPipelineStorage\
      \ instance representing the child storage."
- class_id: graphrag/tokenizer/tokenizer.py::Tokenizer
  file: graphrag/tokenizer/tokenizer.py
  name: Tokenizer
  methods:
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "\"\"\"Encode the given text into a list of tokens.\n\nArgs:\n    text\
      \ (str): The input text to encode.\n\nReturns:\n    list[int]: A list of tokens\
      \ representing the encoded text.\n\nRaises:\n    NotImplementedError: The encode\
      \ method must be implemented by subclasses.\n\"\"\""
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.num_tokens
    name: num_tokens
    signature: 'def num_tokens(self, text: str) -> int'
    docstring: "Return the number of tokens in the given text.\n\nParameters\n   \
      \ text (str): The input text to analyze.\n\nReturns\n    int: The number of\
      \ tokens in the input text.\n\nRaises\n    NotImplementedError: If the encode\
      \ method is not implemented by a subclass."
  - node_id: graphrag/tokenizer/tokenizer.py::Tokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
      \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from\
      \ the list of tokens.\n\nRaises:\n    NotImplementedError: If the decode method\
      \ has not been implemented by subclasses."
- class_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch
  file: graphrag/query/structured_search/global_search/search.py
  name: GlobalSearch
  methods:
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n     \
      \   map_system_prompt: str | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str = \"multiple paragraphs\",\n  \
      \      allow_general_knowledge: bool = False,\n        general_knowledge_inclusion_prompt:\
      \ str | None = None,\n        json_mode: bool = True,\n        callbacks: list[QueryCallbacks]\
      \ | None = None,\n        max_data_tokens: int = 8000,\n        map_llm_params:\
      \ dict[str, Any] | None = None,\n        reduce_llm_params: dict[str, Any] |\
      \ None = None,\n        map_max_length: int = 1000,\n        reduce_max_length:\
      \ int = 2000,\n        context_builder_params: dict[str, Any] | None = None,\n\
      \        concurrent_coroutines: int = 32,\n    )"
    docstring: "Initialize a GlobalSearch instance (internal API).\n\nArgs:\n    model:\
      \ ChatModel - The language model interface used for this global search.\n  \
      \  context_builder: GlobalContextBuilder - The builder that constructs the context\
      \ for the search.\n    tokenizer: Tokenizer | None - Optional tokenizer to use;\
      \ if None, a default tokenizer will be used.\n    map_system_prompt: str | None\
      \ - System prompt for the map stage; if None, MAP_SYSTEM_PROMPT is used.\n \
      \   reduce_system_prompt: str | None - System prompt for the reduce stage; if\
      \ None, REDUCE_SYSTEM_PROMPT is used.\n    response_type: str - How to format\
      \ the response, e.g., \"multiple paragraphs\".\n    allow_general_knowledge:\
      \ bool - Whether to allow incorporating general knowledge beyond the provided\
      \ context.\n    general_knowledge_inclusion_prompt: str | None - Prompt guiding\
      \ inclusion of general knowledge; if None, GENERAL_KNOWLEDGE_INSTRUCTION is\
      \ used.\n    json_mode: bool - Whether to request responses in JSON format.\n\
      \    callbacks: list[QueryCallbacks] | None - Optional callbacks to handle search\
      \ lifecycle events.\n    max_data_tokens: int - Maximum number of tokens allocated\
      \ for data in the mapping stage.\n    map_llm_params: dict[str, Any] | None\
      \ - Parameters for the map LLM call.\n    reduce_llm_params: dict[str, Any]\
      \ | None - Parameters for the reduce LLM call.\n    map_max_length: int - Maximum\
      \ token length for map responses.\n    reduce_max_length: int - Maximum token\
      \ length for reduce responses.\n    context_builder_params: dict[str, Any] |\
      \ None - Additional parameters forwarded to the context builder.\n    concurrent_coroutines:\
      \ int - Maximum number of concurrent coroutines running during mapping.\n\n\
      Returns:\n    None"
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs: Any,\n    ) -> GlobalSearchResult"
    docstring: "Perform a global search.\n\nGlobal search mode includes two steps:\n\
      \n- Step 1: Run parallel LLM calls on communities' short summaries to generate\
      \ answer for each batch\n- Step 2: Combine the answers from step 2 to generate\
      \ the final answer\n\nArgs:\n    query: The search query.\n    conversation_history:\
      \ Optional conversation history to provide context for the search.\n    kwargs:\
      \ Additional keyword arguments for the search.\n\nReturns:\n    GlobalSearchResult:\
      \ The result of the global search."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._map_response_single_batch
    name: _map_response_single_batch
    signature: "def _map_response_single_batch(\n        self,\n        context_data:\
      \ str,\n        query: str,\n        max_length: int,\n        **llm_kwargs,\n\
      \    ) -> SearchResult"
    docstring: "Generate an answer for a single chunk of community reports.\n\nArgs:\n\
      \    context_data (str): Contextual data for the current chunk of community\
      \ reports.\n    query (str): The query to be answered based on the provided\
      \ context.\n    max_length (int): Maximum length allowed for the generated response.\n\
      \    llm_kwargs (Any): Additional keyword arguments forwarded to the language\
      \ model (model_parameters).\n\nReturns:\n    SearchResult: The processed response\
      \ for this batch, including the parsed response, context data, timing, and token\
      \ usage.\n\nRaises:\n    None: This function handles exceptions internally and\
      \ does not propagate exceptions to the caller."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._stream_reduce_response
    name: _stream_reduce_response
    signature: "def _stream_reduce_response(\n        self,\n        map_responses:\
      \ list[SearchResult],\n        query: str,\n        max_length: int,\n     \
      \   **llm_kwargs,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream and reduce multiple map responses into a single streamed output\
      \ by ranking key points and querying the LLM.\n\nArgs:\n    map_responses (list[SearchResult]):\
      \ List of SearchResult objects to extract key points from. Each result may contain\
      \ a response that is a list of dictionaries with keys 'answer' and 'score'.\n\
      \    query (str): User query string to pass as the prompt for the LLM during\
      \ streaming.\n    max_length (int): Maximum length to request in the reduce\
      \ system prompt (limits the generated content).\n    llm_kwargs (dict[str, Any]):\
      \ Additional keyword arguments forwarded to the language model streaming method\
      \ (e.g., model_parameters). This function forwards these to the underlying streaming\
      \ API via the async generator.\n\nReturns:\n    AsyncGenerator[str, None]: An\
      \ asynchronous generator yielding chunks of text produced by the streaming LLM.\n\
      \nNotes:\n- Key points are collected from all map_responses, filtered to keep\
      \ only entries with a positive 'score', and labeled with the originating analyst\
      \ index (Analyst 1, Analyst 2, ...).\n- If no positive-scoring key points exist\
      \ and allow_general_knowledge is False, the function yields NO_DATA_ANSWER and\
      \ terminates. This provides a canned empty answer to avoid hallucinations when\
      \ no relevant data is available.\n- If general knowledge is allowed (allow_general_knowledge\
      \ is True) and no data points exist, the NO_DATA_ANSWER path is skipped and\
      \ the LLM may supplement with generic knowledge.\n- The function enforces a\
      \ token budget via self.max_data_tokens, constructing text_data by concatenating\
      \ stacked analyst blocks until the token limit would be exceeded.\n- The constructed\
      \ report (text_data) feeds reduce_system_prompt via its format with report_data,\
      \ response_type, and max_length. If allow_general_knowledge is enabled, an additional\
      \ general knowledge inclusion prompt is appended.\n- The final prompt used to\
      \ query the LLM is built from search_prompt, and the function streams chunks\
      \ from self.model.achat_stream, yielding each chunk while notifying registered\
      \ callbacks through on_llm_new_token.\n- Analytic ordering is determined by\
      \ descending score after filtering, while analysts are preserved by their original\
      \ indices for labeling in the response."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream the global search response.\n\nArgs:\n    query: str\n    \
      \    The search query to process.\n    conversation_history: ConversationHistory\
      \ | None\n        Optional conversation history to provide context for the search.\n\
      \nReturns:\n    AsyncGenerator[str, None]\n        An asynchronous generator\
      \ yielding string fragments that represent streaming portions of the final answer.\
      \ Fragments are produced by the streaming reduction step as results become available.\n\
      \nRaises:\n    Exception\n        Propagates exceptions raised by the context\
      \ builder, mapping, and streaming components (e.g., I/O or LLM errors)."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._reduce_response
    name: _reduce_response
    signature: "def _reduce_response(\n        self,\n        map_responses: list[SearchResult],\n\
      \        query: str,\n        **llm_kwargs,\n    ) -> SearchResult"
    docstring: "Combine all intermediate responses from multiple batches into a final\
      \ answer to the user query.\n\nArgs:\n    self: The instance of the containing\
      \ class.\n    map_responses: list[SearchResult]\n        The intermediate responses\
      \ collected from each batch.\n    query: str\n        The original user query.\n\
      \    llm_kwargs: dict\n        Additional keyword arguments to pass to the LLM\
      \ model during reduction.\n\nReturns:\n    SearchResult\n        The reduced\
      \ final response containing the generated answer, context data,\n        and\
      \ performance metrics such as completion time and token usage.\n\nRaises:\n\
      \    None\n        This method handles exceptions internally and does not raise\
      \ to callers."
  - node_id: graphrag/query/structured_search/global_search/search.py::GlobalSearch._parse_search_response
    name: _parse_search_response
    signature: 'def _parse_search_response(self, search_response: str) -> list[dict[str,
      Any]]'
    docstring: "Parse the search response json and return a list of key points.\n\n\
      Parameters\n----------\nsearch_response : str\n    The search response json\
      \ string. Expected to contain a top-level object with a\n    \"points\" field\
      \ that is a list of key point objects. Each point should include\n    a \"description\"\
      \ and a \"score\".\n\nReturns\n-------\nlists[dict[str, Any]]\n    A list of\
      \ key points, where each point is a dictionary with keys \"answer\" and\n  \
      \  \"score\". The value for \"answer\" comes from the point's \"description\"\
      . The value\n    for \"score\" is the integer value of the point's \"score\"\
      . If the input cannot be\n    parsed or does not contain a valid \"points\"\
      \ list, a single default point is returned:\n    {\"answer\": \"\", \"score\"\
      : 0}.\n\nNotes\n-----\nNo exceptions are raised by this function. Parsing errors\
      \ or missing data result in the\ndefault return above."
- class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: OpenAIEmbeddingFNLLM
  methods:
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed_batch
    name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "Embed the given texts using the Model.\n\nArgs:\n    text_list: The\
      \ texts to embed.\n    kwargs: Additional arguments to pass to the LLM.\n\n\
      Returns:\n    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n\
      \    ValueError: If no embeddings are found in the response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"Embed the given text using the Model.\n\nArgs:\n    text: The\
      \ text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\n\
      Returns:\n    The embeddings of the text.\n\nRaises:\n    ValueError: If no\
      \ embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
      \ (list[str]): The texts to embed.\n    kwargs (dict[str, Any]): Additional\
      \ arguments to pass to the LLM.\n\nReturns:\n    list[list[float]]: The embeddings\
      \ for the input texts.\n\nRaises:\n    ValueError: If no embeddings are found\
      \ in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
      \ The text to embed.\n    kwargs (dict[str, Any]): Additional arguments to pass\
      \ to the Model.\n\nReturns:\n    list[float]: The embeddings of the text.\n\n\
      Raises:\n    ValueError: If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIEmbeddingFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an OpenAI Embedding FNLLM provider using the given configuration\
      \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
      \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
      \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
      \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided,\
      \ an error handler will be created to log errors.\n    cache: PipelineCache\
      \ | None\n        The pipeline cache to wrap. If None, no cache is used.\n\n\
      Returns:\n    None\n        This initializer does not return a value.\n\nRaises:\n\
      \    Exception\n        Propagates exceptions raised by the underlying helper\
      \ functions and OpenAI client initialization."
- class_id: graphrag/config/errors.py::AzureApiBaseMissingError
  file: graphrag/config/errors.py
  name: AzureApiBaseMissingError
  methods:
  - node_id: graphrag/config/errors.py::AzureApiBaseMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
    docstring: "Init method for AzureApiBaseMissingError (internal API).\n\nArgs:\n\
      \    llm_type: The LLM type for which the API Base is required.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationHistory
  methods:
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.to_qa_turns
    name: to_qa_turns
    signature: def to_qa_turns(self) -> list[QATurn]
    docstring: "\"\"\"Convert conversation history to a list of QA turns.\n\nReturns:\n\
      \    list[QATurn]: A list of QA turns created from the conversation history,\
      \ where each QA turn contains a user_query from a USER turn and a list of assistant_answers\
      \ collected from subsequent turns until the next USER turn.\n\"\"\""
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.from_list
    name: from_list
    signature: "def from_list(\n        cls, conversation_turns: list[dict[str, str]]\n\
      \    ) -> \"ConversationHistory\""
    docstring: "Create a ConversationHistory from a list of conversation turns.\n\n\
      Each turn is a dictionary in the form of {\"role\": \"<conversation_role>\"\
      , \"content\": \"<turn content>\"}.\n\nArgs:\n    cls: The class object, used\
      \ to instantiate a new ConversationHistory.\n    conversation_turns: A list\
      \ of dictionaries representing turns. Each dictionary has keys \"role\" and\
      \ \"content\".\n\nReturns:\n    ConversationHistory: A new ConversationHistory\
      \ containing the parsed turns."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize a ConversationHistory with an empty list of turns.\n\n\
      Returns:\n    None\n        This initializer does not return a value. It initializes\
      \ the turns attribute to an empty list."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.add_turn
    name: add_turn
    signature: 'def add_turn(self, role: ConversationRole, content: str)'
    docstring: "Add a new turn to the conversation history.\n\nArgs:\n    role (ConversationRole):\
      \ The role for the new turn.\n    content (str): The content of the turn.\n\n\
      Returns:\n    None"
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.get_user_turns
    name: get_user_turns
    signature: 'def get_user_turns(self, max_user_turns: int | None = 1) -> list[str]'
    docstring: '"""Get the last user turns in the conversation history.\n\nArgs:\n    max_user_turns:
      Maximum number of user turns to include from history. If None, include all user
      turns. Default is 1.\n\nReturns:\n    list[str]: A list of user turn contents,
      in reverse chronological order (most recent first), up to max_user_turns.\n\nRaises:\n    None\n"""'
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationHistory.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        tokenizer: Tokenizer |\
      \ None = None,\n        include_user_turns_only: bool = True,\n        max_qa_turns:\
      \ int | None = 5,\n        max_context_tokens: int = 8000,\n        recency_bias:\
      \ bool = True,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Conversation History\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Prepare conversation history as context data for system prompt.\n\n\
      Parameters\n----------\ntokenizer : Tokenizer | None\n    The tokenizer to use.\
      \ If None, a default tokenizer retrieved by get_tokenizer() will be used.\n\
      include_user_turns_only : bool\n    If True, only user queries (not assistant\
      \ responses) will be included in the context. Default is True.\nmax_qa_turns\
      \ : int | None\n    Maximum number of QA turns to include in the context. If\
      \ None, there is no explicit limit. Default is 5.\nmax_context_tokens : int\n\
      \    Maximum number of tokens allowed for the context data. Default is 8000.\n\
      recency_bias : bool\n    If True, reverse the order of the conversation history\
      \ to prioritize the most recent QA turn. Default is True.\ncolumn_delimiter\
      \ : str\n    Delimiter to use for separating columns in the context data. Default\
      \ is \"|\".\ncontext_name : str\n    Name of the context, default is \"Conversation\
      \ History\".\n\nReturns\n-------\ntuple[str, dict[str, pd.DataFrame]]\n    A\
      \ tuple containing:\n    - context_text: the context data text (string) including\
      \ a header line and the QA turns formatted as a table.\n    - context_dict:\
      \ a mapping from the lowercase context name (context_name.lower()) to a DataFrame\
      \ containing the turns included in the context. The key is consistently lowercase\
      \ in both empty and non-empty cases."
- class_id: graphrag/language_model/protocol/base.py::ChatModel
  file: graphrag/language_model/protocol/base.py
  name: ChatModel
  methods:
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    docstring: "Generate a response for the given text.\n\nArgs:\n    prompt: The\
      \ text to generate a response for.\n    history: The conversation history.\n\
      \    **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    ModelResponse: The response for the given text."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.chat
    name: chat
    signature: "def chat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> ModelResponse"
    docstring: "Generate a response for the given text.\n\nArgs:\n    prompt (str):\
      \ The text to generate a response for.\n    history (list | None): The conversation\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    ModelResponse: The response for the given text.\n\nRaises:\n\
      \    Exception: If an error occurs during generation."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    docstring: "Generate a response for the given text using a streaming interface.\n\
      \nArgs:\n    prompt: str \u2014 The text to generate a response for.\n    history:\
      \ list | None \u2014 The conversation history.\n    **kwargs: Any \u2014 Additional\
      \ keyword arguments (e.g., model parameters).\n\nReturns:\n    Generator[str,\
      \ None] \u2014 The generator that yields strings representing the response."
  - node_id: graphrag/language_model/protocol/base.py::ChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream the given prompt via an asynchronous streaming interface. This\
      \ generator yields partial results over time as the model streams its response.\n\
      \nArgs:\n  prompt: The text to generate a response for.\n  history: The conversation\
      \ history. Optional prior messages that may influence generation.\n  **kwargs:\
      \ Additional keyword arguments (e.g., model parameters, streaming controls).\n\
      \nReturns:\n  AsyncGenerator[str, None]: An asynchronous generator that yields\
      \ strings representing portions of the response as they become available.\n\n\
      Raises:\n  Propagates exceptions raised by the underlying model call or streaming\
      \ backend (e.g., network errors, timeouts, invalid parameters).\n\nUsage:\n\
      \  async for chunk in model.achat_stream(\"Hello, world!\", history=None):\n\
      \      print(chunk)"
- class_id: graphrag/storage/factory.py::StorageFactory
  file: graphrag/storage/factory.py
  name: StorageFactory
  methods:
  - node_id: graphrag/storage/factory.py::StorageFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, storage_type: str) -> bool'
    docstring: "Check if the given storage type is supported.\n\nArgs:\n    storage_type\
      \ (str): The type identifier for the storage.\n\nReturns:\n    bool: True if\
      \ the storage type is registered in the registry, False otherwise."
  - node_id: graphrag/storage/factory.py::StorageFactory.create_storage
    name: create_storage
    signature: 'def create_storage(cls, storage_type: str, kwargs: dict) -> PipelineStorage'
    docstring: "Create a storage object from the provided type.\n\nArgs:\n    storage_type:\
      \ The type of storage to create.\n    kwargs: Additional keyword arguments for\
      \ the storage constructor.\n\nReturns:\n    A PipelineStorage instance.\n\n\
      Raises:\n    ValueError: If the storage type is not registered."
  - node_id: graphrag/storage/factory.py::StorageFactory.register
    name: register
    signature: "def register(\n        cls, storage_type: str, creator: Callable[...,\
      \ PipelineStorage]\n    ) -> None"
    docstring: "Register a custom storage implementation.\n\nArgs:\n    storage_type\
      \ (str): The type identifier for the storage.\n    creator (Callable[..., PipelineStorage]):\
      \ A class or callable that creates an instance of PipelineStorage.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/storage/factory.py::StorageFactory.get_storage_types
    name: get_storage_types
    signature: def get_storage_types(cls) -> list[str]
    docstring: "Get the registered storage implementations.\n\nArgs:\n    cls: The\
      \ class on which this classmethod is invoked.\n\nReturns:\n    list[str]: The\
      \ list of registered storage type keys (i.e., the keys of cls._registry)."
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py
  name: CFGNounPhraseExtractor
  methods:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "String representation of the extractor, used for cache key generation.\n\
      \nArgs:\n    self: The instance of the extractor.\n\nReturns:\n    The string\
      \ representation used for cache key generation."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n        noun_phrase_grammars: dict[tuple, str],\n\
      \        noun_phrase_tags: list[str],\n    )"
    docstring: "\"\"\"Noun phrase extractor combining CFG-based noun-chunk extraction\
      \ and NER.\n\nCFG-based extraction was based on TextBlob's fast NP extractor\
      \ implementation:\nThis extractor tends to be faster than the dependency-parser-based\
      \ extractors but grammars may need to be changed for different languages.\n\n\
      Args:\n    model_name (str): SpaCy model name.\n    max_word_length (int): Maximum\
      \ length (in character) of each extracted word.\n    include_named_entities\
      \ (bool): Whether to include named entities in noun phrases\n    exclude_entity_tags\
      \ (list[str]): list of named entity tags to exclude in noun phrases.\n    exclude_pos_tags\
      \ (list[str]): List of POS tags to remove in noun phrases.\n    word_delimiter\
      \ (str): Delimiter for joining words.\n    noun_phrase_grammars (dict[tuple,\
      \ str]): CFG for matching noun phrases.\n\nReturns:\n    None\n\nRaises:\n \
      \   None\n\"\"\"\n}"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text using a combination of CFG-based noun\
      \ phrase matching and optional named-entity recognition (NER), with post-filtering\
      \ rules applied to remove unlikely phrases.\n\nThis extractor is configurable\
      \ via instance attributes that influence its behavior and the NLP model loaded:\n\
      \n- include_named_entities (bool): If True, include named entities in the noun\
      \ phrase results; otherwise, NER is disabled by loading the model with the ner\
      \ component excluded.\n- exclude_entity_tags (list[str]): Named entity labels\
      \ to exclude from consideration (e.g., PERSON, ORG).\n- exclude_pos_tags (list[str]):\
      \ POS tags to remove from consideration when forming CFG-based noun phrases.\n\
      - noun_phrase_grammars (dict[tuple[str,str], str]): CFG rules used to merge\
      \ adjacent words into noun phrases based on their POS tags.\n- noun_phrase_tags\
      \ (set[str]): The POS-like labels that qualify a token as a noun phrase after\
      \ CFG merging.\n- word_delimiter (str): Delimiter used to join tokens when merging\
      \ matched words.\n- model_name, max_word_length, exclude_nouns, etc.: Other\
      \ configuration inherited from the base extractor that affect tokenization,\
      \ length filtering, and joining.\n\nReturns:\n    list[str]: A deduplicated\
      \ list of noun phrases extracted from the input text. Duplicates are removed\
      \ by using a set; the resulting order is not guaranteed and may appear in arbitrary\
      \ order. If you need deterministic ordering, post-process the results (e.g.,\
      \ by preserving original offsets or sorting).\n\nRaises:\n    RuntimeError,\
      \ ValueError, or spaCy-related exceptions that may be raised by the underlying\
      \ NLP pipeline if the configured model cannot be loaded, if processing fails,\
      \ or if input text is not valid."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor.extract_cfg_matches
    name: extract_cfg_matches
    signature: 'def extract_cfg_matches(self, doc: Doc) -> list[tuple[str, str]]'
    docstring: "Return noun phrases that match a given context-free grammar.\n\nArgs:\n\
      \    doc (Doc): The spaCy Doc to process for noun phrase extraction.\n\nReturns:\n\
      \    list[tuple[str, str]]: A list of noun phrases as (text, tag) pairs, where\
      \ text is the merged noun phrase string and tag is the corresponding noun phrase\
      \ tag."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/cfg_extractor.py::CFGNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: tuple[str, str],\
      \ entities: set[str] | None = None\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\n\
      Args:\n  noun_chunk: tuple[str, str] - The noun chunk as (text, pos) where text\
      \ is the chunk text and pos is its part-of-speech tag.\n  entities: set[str]\
      \ | None - Optional set of entity strings to consider when validating the noun\
      \ chunk. If provided and the noun_chunk[0] is in entities, is_valid_entity is\
      \ computed.\n\nReturns:\n  dict[str, Any] - A dictionary containing the following\
      \ keys:\n      cleaned_tokens: List[str] - Tokens after removing excluded nouns.\n\
      \      cleaned_text: str - The cleaned tokens joined by self.word_delimiter,\
      \ with newline characters removed, and converted to upper-case.\n      is_valid_entity:\
      \ bool - True if the noun chunk corresponds to a valid entity given the tokens\
      \ and optional entities.\n      has_proper_nouns: bool - True if noun_chunk[1]\
      \ == \"PROPN\".\n      has_compound_words: bool - True if cleaned_tokens form\
      \ a hyphenated compound.\n      has_valid_tokens: bool - True if all cleaned_tokens\
      \ pass the maximum word length constraint.\n\nRaises:\n  None"
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py
  name: RegexENNounPhraseExtractor
  methods:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text using TextBlob's noun phrase extractor\
      \ with post-filtering.\n\nThis English-only extractor relies on TextBlob and\
      \ NLTK data and downloads required corpora on first use\n(brown, treebank, averaged_perceptron_tagger_eng)\
      \ and tokenizers (punkt, punkt_tab). It uses instance\nproperties from the base\
      \ extractor (exclude_nouns, max_word_length, word_delimiter) to filter and format\
      \ results.\n\nThe method collects noun phrases from TextBlob and applies filtering\
      \ based on:\n- presence of a proper noun within the phrase\n- number of cleaned\
      \ tokens\n- presence of a compound word\nand ensures all tokens are valid and\
      \ within max_word_length. The resulting phrases are normalized by removing\n\
      excluded tokens, joining with the configured delimiter, converting to uppercase,\
      \ and deduplicating.\n\nArgs:\n    text: The input text to extract noun phrases\
      \ from.\n\nReturns:\n    List[str]: A list of cleaned noun phrases. Duplicates\
      \ are removed; the order is not guaranteed.\n\nRaises:\n    May propagate exceptions\
      \ from TextBlob or NLTK if required resources cannot be downloaded or loaded."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_phrase: str, all_proper_nouns:\
      \ list[str] | None = None\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun phrase for filtering.\n\nArgs:\n    noun_phrase:\
      \ The noun phrase to analyze.\n    all_proper_nouns: Optional list of proper\
      \ nouns (uppercase) to consider when detecting proper nouns within the phrase.\n\
      \nReturns:\n    dict[str, Any]: A dictionary containing the following keys:\n\
      \        cleaned_tokens: List[str] of tokens after removing excluded nouns.\n\
      \        cleaned_text: str of cleaned tokens joined by the configured delimiter,\
      \ with newline removed and uppercased.\n        has_proper_nouns: True if any\
      \ cleaned token matches an entry in all_proper_nouns.\n        has_compound_words:\
      \ True if any cleaned token is a hyphenated compound word.\n        has_valid_tokens:\
      \ True if all cleaned tokens are valid per regex and length constraints."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the regex extractor, used for cache\
      \ key generation.\n\nArgs:\n    self: The instance of the extractor.\n\nReturns:\n\
      \    str: The cache key string encoding the extractor's configuration, built\
      \ from exclude_nouns, max_word_length, and word_delimiter.\n\nRaises:\n    None"
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/regex_extractor.py::RegexENNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        exclude_nouns: list[str],\n\
      \        max_word_length: int,\n        word_delimiter: str,\n    )"
    docstring: "Initialize a regular-expression-based noun phrase extractor for English.\n\
      \nNOTE: This is the extractor used in the first benchmarking of LazyGraphRAG\
      \ but it only works for English. It is much faster but likely less accurate\
      \ than the syntactic parser-based extractor. TODO: Reimplement this using SpaCy\
      \ to remove TextBlob dependency.\n\nArgs:\n    exclude_nouns: list[str] \u2014\
      \ List of nouns to exclude from extraction.\n    max_word_length: int \u2014\
      \ Maximum length (in characters) of each extracted word.\n    word_delimiter:\
      \ str \u2014 Delimiter for joining words.\n\nReturns:\n    None"
- class_id: graphrag/config/environment_reader.py::EnvironmentReader
  file: graphrag/config/environment_reader.py
  name: EnvironmentReader
  methods:
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader._read_env
    name: _read_env
    signature: "def _read_env(\n        self, env_key: str | list[str], default_value:\
      \ T, read: Callable[[str, T], T]\n    ) -> T | None"
    docstring: "Read environment value(s) using a reader function and return the first\
      \ non-default result.\n\nArgs:\n    env_key: str | list[str]. Environment key\
      \ or keys to look up. If a single string is provided, it will be treated as\
      \ a one-element list. Keys are checked in order and converted to upper-case\
      \ before reading.\n    default_value: T. The default value to return if no key\
      \ yields a non-default result.\n    read: Callable[[str, T], T]. A function\
      \ that takes an environment key (uppercase) and a default value, and returns\
      \ a value of type T.\n\nReturns:\n    T | None. The value returned by read for\
      \ the first key that yields a value different from default_value; otherwise\
      \ returns default_value.\n\nRaises:\n    Any exception raised by the read callable\
      \ is propagated to the caller."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.__init__
    name: __init__
    signature: 'def __init__(self, env: Env)'
    docstring: "Initialize the EnvironmentReader with the provided environment.\n\n\
      This constructor stores the given environment for later reads and initializes\
      \ an internal configuration stack to an empty list.\n\nArgs:\n    env: Environment\
      \ instance used to read configuration values.\n\nReturns:\n    None"
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.config_context
    name: config_context
    signature: def config_context()
    docstring: "Create a context manager to push a value into the config_stack for\
      \ the duration of the context.\n\nReturns:\n    A context manager that appends\
      \ the value (or {} if the value is falsy) to the internal _config_stack upon\
      \ entry, and pops it on exit."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.section
    name: section
    signature: def section(self) -> dict
    docstring: "Get the current section.\n\nReturns:\n    dict: The current section\
      \ dictionary from the configuration stack, or an empty\n    dict if there is\
      \ no active section."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.use
    name: use
    signature: 'def use(self, value: Any | None)'
    docstring: "Create a context manager to push the value into the config_stack.\n\
      \nArgs:\n    value: Any | None. The value to push onto the internal config stack;\
      \ if None, an empty dict is pushed.\n\nReturns:\n    contextmanager. A context\
      \ manager that pushes the provided value (or an empty dict) onto the internal\
      \ config stack for the duration of the context and pops it on exit."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.env
    name: env
    signature: def env(self)
    docstring: "\"\"\"Get the environment object.\n\nReturns:\n    Env: The environment\
      \ object stored on this reader.\n\"\"\""
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.envvar_prefix
    name: envvar_prefix
    signature: 'def envvar_prefix(self, prefix: KeyValue)'
    docstring: "Set the environment variable prefix.\n\nThe provided prefix is normalized\
      \ via read_key and then transformed to uppercase with a trailing underscore,\
      \ and applied to the underlying Env instance using Env.prefixed.\n\nArgs:\n\
      \    prefix (KeyValue): The key to use as the environment variable prefix. It\
      \ can be a string or Enum. Strings are converted to lowercase; Enum values are\
      \ converted to their value and lowercased. The result is suffixed with an underscore\
      \ and uppercased before being applied as the prefix.\n\nReturns:\n    Env: An\
      \ environment object with the specified prefix applied."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.str
    name: str
    signature: "def str(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: str | None = None,\n    )\
      \ -> str | None"
    docstring: "Read a configuration value from the current section or environment.\n\
      \nArgs:\n    key: KeyValue\n        The key to read. It can be a string or an\
      \ Enum. It is normalized to a string using read_key.\n    env_key: EnvKeySet\
      \ | None\n        Optional environment variable name(s) to check if the key\
      \ is not found in the current section. If None, the environment key used is\
      \ the key.\n    default_value: str | None\n        Default value to return if\
      \ the key is not found in the current section or environment.\n\nReturns:\n\
      \    Any | None\n    The value read from the current section (returned as stored,\
      \ which may be non-string) or from the environment (typically a string), or\
      \ None if not found."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.int
    name: int
    signature: "def int(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: int | None = None,\n    )\
      \ -> int | None"
    docstring: "Read an integer configuration value.\n\nArgs:\n  key: KeyValue\n \
      \     The key to read. It is normalized to a string using read_key.\n  env_key:\
      \ EnvKeySet | None\n      Optional environment variable name(s) to check if\
      \ the key is not found in the current section. If None, the environment key\
      \ used is the key.\n  default_value: int | None\n      The default value to\
      \ return if the key is not found in either the current section or environment.\n\
      \nReturns:\n  int | None\n      The resulting integer value, or None if no value\
      \ is found and no default_value is provided.\n\nRaises:\n  AttributeError\n\
      \      If the provided key cannot be normalized to a string via read_key."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.bool
    name: bool
    signature: "def bool(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: bool | None = None,\n   \
      \ ) -> bool | None"
    docstring: "Read a boolean configuration value.\n\nArgs:\n  key: KeyValue\n  \
      \    The key to read. It is normalized to a string using read_key.\n  env_key:\
      \ EnvKeySet | None\n      Optional environment variable name(s) to check if\
      \ the key is not found in the current section. If None, the environment key\
      \ used is the key.\n  default_value: bool | None\n      The default value to\
      \ return if the key is not found in either the section or environment.\nReturns:\n\
      \  bool | None\n      The boolean value read from the configuration, or None\
      \ if not found and no default is provided.\nRaises:\n  AttributeError\n    \
      \  If value is not a string and Enum (as raised by read_key)."
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.float
    name: float
    signature: "def float(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: float | None = None,\n  \
      \  ) -> float | None"
    docstring: "\"\"\"Read a float configuration value.\n\nArgs:\n    key: KeyValue\n\
      \        The key to read. It can be a string or an Enum. It is normalized to\
      \ a string using read_key.\n    env_key: EnvKeySet | None\n        Optional\
      \ environment variable name(s) to check if the key is not found in the current\
      \ section. If None, the environment key used is the key.\n    default_value:\
      \ float | None\n        The default value to return if the key is not found\
      \ in either the current section or environment.\n\nReturns:\n    float | None\n\
      \        The configured float value, or None if not found and no default is\
      \ provided.\n\nRaises:\n    AttributeError\n        If the provided key cannot\
      \ be normalized to a string.\n\"\"\""
  - node_id: graphrag/config/environment_reader.py::EnvironmentReader.list
    name: list
    signature: "def list(\n        self,\n        key: KeyValue,\n        env_key:\
      \ EnvKeySet | None = None,\n        default_value: list | None = None,\n   \
      \ ) -> list | None"
    docstring: "Parse a list configuration value.\n\nArgs:\n  key: KeyValue\n    \
      \  The key to read. It can be a string or an Enum. It is normalized to a string\
      \ using read_key.\n  env_key: EnvKeySet | None\n      Optional environment variable\
      \ name(s) to check if the key is not found in the current section. If None,\
      \ the environment key used is the key.\n  default_value: list | None\n     \
      \ The default value to return if the key is not found and no environment value\
      \ is available.\n\nReturns:\n  list | None\n      The parsed list from the configuration,\
      \ or default_value if not found.\n\nRaises:\n  AttributeError\n      If value\
      \ is not a string and ..."
- class_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage
  file: graphrag/storage/file_pipeline_storage.py
  name: FilePipelineStorage
  methods:
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "\"\"\"Clear all entries under the root directory.\n\nRemoves directories\
      \ recursively and unlinks files directly under the root directory, effectively\
      \ clearing the storage contents.\n\nArgs:\n    self: The FilePipelineStorage\
      \ instance to operate on.\n\nReturns:\n    None\n\nRaises:\n    OSError: If\
      \ a filesystem operation fails during removal of files or directories.\n\"\"\
      \""
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "Return the keys in the storage.\n\nThe keys represent file names located\
      \ directly in the root directory (self._root_dir). Only files are included;\
      \ directories are ignored. This operation is non-recursive and may return an\
      \ empty list if no files are present. The keys are provided as file names (not\
      \ full paths).\n\nArgs:\n  self (FilePipelineStorage): The instance of the FilePipelineStorage.\n\
      \nReturns:\n  list[str]: The file names (not full paths) of files directly under\
      \ root_dir; directories are excluded. The list may be empty if no files exist."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Return a storage instance for a child or the current instance if no\
      \ name is provided.\n\nArgs:\n    name: str | None, optional name for the child\
      \ storage. If None, the current instance is returned.\n\nReturns:\n    PipelineStorage:\
      \ The current instance (self) when name is None; otherwise a new FilePipelineStorage\
      \ representing the child storage located at the path formed by joining the current\
      \ root_dir with the provided name."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.item_filter
    name: item_filter
    signature: 'def item_filter(item: dict[str, Any]) -> bool'
    docstring: "Determine whether the given item matches the current file_filter or\
      \ if no filter is set.\n\nArgs:\n    item (dict[str, Any]): The item to evaluate.\
      \ The keys used by file_filter are read from this dict. If a key referenced\
      \ by file_filter is missing from item, a KeyError may be raised. Values should\
      \ be strings (or objects compatible with re.search).\n\nReturns:\n    bool:\
      \ True if no file_filter is defined; otherwise, True only if all re.search(value,\
      \ item[key]) checks succeed for every key, value pair in file_filter.\n\nRaises:\n\
      \    KeyError: If item lacks a key referenced by file_filter.\n    TypeError:\
      \ If item[key] is not a string (and thus not compatible with re.search).\n \
      \   re.error: If a regex pattern from file_filter is invalid.\n\nNotes:\n  \
      \  This function relies on file_filter from outer scope. To improve robustness\
      \ and testability, consider passing file_filter as an explicit parameter to\
      \ the function."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find files in the storage that match a compiled file pattern and optional\
      \ metadata-based filtering.\n\nArgs:\n    file_pattern (re.Pattern[str]): A\
      \ compiled regular expression to match file paths.\n    base_dir (str | None):\
      \ Base directory to search within. If None, search starts from the storage root.\n\
      \    file_filter (dict[str, Any] | None): Optional dictionary mapping named\
      \ group keys to regular expressions; only items whose corresponding fields match\
      \ are yielded.\n    max_count (int): The maximum number of results to yield.\
      \ If -1, yield all matches.\n\nReturns:\n    Iterator[tuple[str, dict[str, Any]]]:\
      \ An iterator yielding tuples of (filename, group) where filename is the path\
      \ relative to the storage root (without a leading path separator) and group\
      \ is the dictionary of named groups extracted by file_pattern.\n\nRaises:\n\
      \    KeyError: If file_filter references a key not present in the named groups\
      \ produced by file_pattern."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any) -> None'
    docstring: "Initialize a file-based storage backend.\n\nThis constructor prepares\
      \ the storage root by creating the base_dir if it does not exist and sets the\
      \ encoding used for file I/O.\n\nArgs:\n  base_dir (str): Directory path where\
      \ files are stored. Defaults to the empty string (uses the current working directory).\n\
      \  encoding (str): Text encoding for file operations. Defaults to \"utf-8\"\
      .\n\nReturns:\n  None\n\nRaises:\n  OSError: If the root directory cannot be\
      \ created or accessed due to filesystem errors.\n  Other exceptions may be raised\
      \ for permission issues or unexpected filesystem failures."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage._read_file
    name: _read_file
    signature: "def _read_file(\n        self,\n        path: str | Path,\n      \
      \  as_bytes: bool | None = False,\n        encoding: str | None = None,\n  \
      \  ) -> Any"
    docstring: "Read the contents of a file asynchronously.\n\nArgs:\n    path: The\
      \ path to the file to read. (str | Path)\n    as_bytes: When True, read in binary\
      \ mode and return bytes; otherwise read in text mode. (bool | None)\n    encoding:\
      \ Encoding to use when reading as text. If None and as_bytes is False, uses\
      \ self._encoding. (str | None)\n\nReturns:\n    The contents of the file. Returns\
      \ bytes if as_bytes is True, otherwise a string.\n\nRaises:\n    Exceptions\
      \ raised by the underlying file I/O (e.g., open or read errors) may be propagated."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = False, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get the contents of a file identified by key from the storage root\
      \ or directly from the provided path if present.\n\nArgs:\n    key: str - The\
      \ file key or path relative to the root storage.\n    as_bytes: bool | None\
      \ - If True, read the file as bytes; otherwise read as text.\n    encoding:\
      \ str | None - Encoding to use when reading text. Ignored when reading bytes.\n\
      \nReturns:\n    Any - The file contents, read as bytes when as_bytes is True,\
      \ otherwise as text, or None if no file is found.\n\nRaises:\n    Propagates\
      \ exceptions raised by has() or _read_file() during I/O operations."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set a value for the given key in the file-based storage.\n\nArgs:\n\
      \    key: str\n        The key, used as the relative path under the storage\
      \ root.\n    value: Any\n        The value to store. If value is bytes, write\
      \ in binary mode; otherwise write as text.\n    encoding: str | None\n     \
      \   Encoding to use when writing text. If None, uses the default encoding.\n\
      \nReturns:\n    None\n        This coroutine completes after the value has been\
      \ written."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Check whether a file for the given key exists in the storage.\n\n\
      This coroutine checks the existence of the file located at the path formed by\
      \ joining the storage root directory with the provided key.\n\nArgs:\n    key\
      \ (str): The key (relative path) to check within the storage root directory.\n\
      \nReturns:\n    bool: True if the file exists, False otherwise."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "Asynchronously delete the item associated with the given key from\
      \ storage.\n\nIf the key exists, delete the corresponding file; if the key does\
      \ not exist, this operation is a no-op. This method is asynchronous.\n\nArgs:\n\
      \    key (str): The key of the item to delete.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    OSError: If a filesystem operation\
      \ fails during removal of the file."
  - node_id: graphrag/storage/file_pipeline_storage.py::FilePipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date of a file.\n\nArgs:\n    key (str): The key\
      \ of the file for which to retrieve the creation date.\n\nReturns:\n    str:\
      \ The creation date as a string formatted with the local time zone.\n\nRaises:\n\
      \    FileNotFoundError: If the file does not exist at the constructed path.\n\
      \    OSError: If an OS error occurs while accessing file metadata."
- class_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig
  file: graphrag/config/models/graph_rag_config.py
  name: GraphRagConfig
  methods:
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_base_dir
    name: _validate_input_base_dir
    signature: def _validate_input_base_dir(self) -> None
    docstring: "Validate the input base directory.\n\nArgs:\n    self: The instance\
      \ of the configuration model containing input configuration and root_dir.\n\n\
      Returns:\n    None. This method updates input.storage.base_dir to an absolute\
      \ path derived from root_dir joined with the provided base_dir.\n\nRaises:\n\
      \    ValueError: If the input storage type is file and the input storage base_dir\
      \ is empty."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_rate_limiter_services
    name: _validate_rate_limiter_services
    signature: def _validate_rate_limiter_services(self) -> None
    docstring: "Validate the rate limiter services configuration.\n\nThis method checks\
      \ each model's rate_limit_strategy. For each model with a configured strategy,\
      \ it verifies the strategy is registered with RateLimiterFactory and, if rpm\
      \ or tpm values are provided, creates a corresponding rate limiter instance\
      \ to validate configuration.\n\nArgs:\n  self: The instance containing the models\
      \ configuration to validate.\n\nReturns:\n  None: This method does not return\
      \ a value.\n\nRaises:\n  ValueError: If a rate limiter strategy for a model\
      \ is not registered."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_reporting_base_dir
    name: _validate_reporting_base_dir
    signature: def _validate_reporting_base_dir(self) -> None
    docstring: "Validate the reporting base directory.\n\nArgs:\n    self (GraphRagConfig):\
      \ The instance of the configuration model containing the reporting configuration\
      \ and root_dir.\n\nReturns:\n    None. When the reporting type is file, this\
      \ method validates that the base_dir is non-empty and then converts it to an\
      \ absolute path using root_dir.\n\nRaises:\n    ValueError: If the reporting\
      \ type is file and the reporting.base_dir is empty or whitespace."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_factories
    name: _validate_factories
    signature: def _validate_factories(self) -> None
    docstring: "Validate the factories used in the configuration.\n\nArgs:\n    self:\
      \ The GraphRagConfig instance.\n\nReturns:\n    None: This method does not return\
      \ a value.\n\nRaises:\n    Exception: If validation fails in the underlying\
      \ retry or rate limiter validations."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model configuration after the initial schema validation.\n\
      \nThis is a post-schema, after-hook validator that returns the same instance\
      \ after performing a series of internal checks to ensure the model configuration\
      \ is consistent and ready for use.\n\nArgs:\n    self: The GraphRagConfig instance\
      \ being validated.\n\nReturns:\n    GraphRagConfig: The same instance after\
      \ validation.\n\nRaises:\n    ValueError: If a required configuration value\
      \ is missing or invalid during validation.\n\nValidations performed:\n- root_dir\
      \ is valid\n- models configuration is present and valid\n- input_pattern is\
      \ correct\n- input_base_dir is valid\n- reporting_base_dir is valid\n- output_base_dir\
      \ is valid\n- multi_output_base_dirs are valid\n- update_index_output_base_dir\
      \ is valid\n- vector_store_db_uri is valid\n- factories configuration is valid"
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_output_base_dir
    name: _validate_output_base_dir
    signature: def _validate_output_base_dir(self) -> None
    docstring: "Validate the output base directory.\n\nArgs:\n    self: The instance\
      \ of the configuration model containing the output configuration and root_dir.\n\
      \nReturns:\n    None. This method updates output.base_dir to an absolute path\
      \ derived from root_dir joined with the provided base_dir when the output type\
      \ is file.\n\nRaises:\n    ValueError: If the output storage type is file and\
      \ the output base_dir is empty."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_retry_services
    name: _validate_retry_services
    signature: def _validate_retry_services(self) -> None
    docstring: "\"\"\"Validate the retry services configuration.\n\nThis method iterates\
      \ over all language model configurations contained in self.models\nand validates\
      \ each model's retry_strategy. If a model's retry_strategy is \"none\",\n the\
      \ strategy is skipped. For any other strategy, the strategy must be registered\
      \ with\nRetryFactory; otherwise a ValueError is raised. The error message lists\
      \ the available\nretry strategies. When a registered strategy is found, a retry\
      \ object is instantiated by\nRetryFactory.create using the model's max_retries\
      \ and max_retry_wait to validate the\nconfiguration.\n\nArgs:\n  self: GraphRagConfig.\
      \ The instance containing the models dictionary to validate.\n\nReturns:\n \
      \ None\n\nRaises:\n  ValueError: If a model's retry_strategy is not registered\
      \ with RetryFactory. The exception\n    message includes the model id and the\
      \ list of available strategies.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "\"\"\"\nGet a string representation of this GraphRagConfig as a JSON\
      \ string.\n\nThis representation is produced by Pydantic's model_dump_json with\
      \ an indentation of 4 spaces, yielding a pretty-printed JSON string.\n\nArgs:\n\
      \    self: GraphRagConfig. The instance to serialize. Note: self is implicit\
      \ in Python method calls; this description is informational.\n\nReturns:\n \
      \   str: JSON string representation of the GraphRagConfig with indent=4.\n\n\
      Raises:\n    ValueError, TypeError: If the underlying serialization fails due\
      \ to non-serializable fields or data.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_vector_store_config
    name: get_vector_store_config
    signature: 'def get_vector_store_config(self, vector_store_id: str) -> VectorStoreConfig'
    docstring: '"""Get a vector store configuration by ID.


      Args:

      vector_store_id: The ID of the vector store to get. Should match an ID in the
      vector_store list.


      Returns:

      VectorStoreConfig: The vector store configuration if found.


      Raises:

      ValueError: If the vector store ID is not found in the configuration.

      """'
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_vector_store_db_uri
    name: _validate_vector_store_db_uri
    signature: def _validate_vector_store_db_uri(self) -> None
    docstring: "Validate the vector store configuration for LanceDB vector stores\
      \ and normalize their db_uri paths to absolute paths.\n\nArgs:\n    self: The\
      \ Graph Rag configuration instance being validated.\n\nReturns:\n    None\n\n\
      Raises:\n    ValueError: If a LanceDB vector store has a missing or empty db_uri.\
      \ The error message is: Vector store URI is required for LanceDB. Please rerun\
      \ graphrag init and set the vector store configuration."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_input_pattern
    name: _validate_input_pattern
    signature: def _validate_input_pattern(self) -> None
    docstring: "Mutates the input file_pattern when it is empty by applying a default\
      \ pattern based on the input type.\n\nIf the pattern is empty:\n- for text input:\
      \ sets the pattern to \".*\\\\.txt$\"\n- for other input types: sets the pattern\
      \ to \".*\\\\.{extension}$\" where extension is the value of input.file_type.value\n\
      \nArgs:\n    self: The GraphRagConfig instance.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_multi_output_base_dirs
    name: _validate_multi_output_base_dirs
    signature: def _validate_multi_output_base_dirs(self) -> None
    docstring: "Validate the outputs dict base directories.\n\nArgs:\n    self: The\
      \ instance of the configuration model containing the outputs dictionary and\
      \ root_dir.\n\nReturns:\n    None. This method updates file outputs' base_dir\
      \ to an absolute path by resolving it relative to root_dir.\n\nRaises:\n   \
      \ ValueError: If any file-type output has an empty base_dir."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.get_language_model_config
    name: get_language_model_config
    signature: 'def get_language_model_config(self, model_id: str) -> LanguageModelConfig'
    docstring: "\"\"\"Get a model configuration by ID.\n\nArgs:\n    model_id (str):\
      \ The ID of the model to get. Should match an ID in the models list.\n\nReturns:\n\
      \    LanguageModelConfig: The model configuration if found.\n\nRaises:\n   \
      \ ValueError: If the model ID is not found in the configuration.\n\"\"\""
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig.__repr__
    name: __repr__
    signature: def __repr__(self) -> str
    docstring: "Get a string representation of this GraphRagConfig instance.\n\nArgs:\n\
      \  self: GraphRagConfig, the GraphRagConfig instance to represent as a string.\n\
      \nReturns:\n  str: The string representation of this GraphRagConfig instance."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_models
    name: _validate_models
    signature: def _validate_models(self) -> None
    docstring: "Validate the models configuration.\n\nEnsure both a default chat model\
      \ and default embedding model have been defined. Other models may also be defined\
      \ but defaults are required for the time being as places of the code fallback\
      \ to default model configs instead of specifying a specific model.\n\nTODO:\
      \ Don't fallback to default models elsewhere in the code. Forcing code to specify\
      \ a model to use and allowing for any names for model configurations.\n\nArgs:\n\
      \    self: GraphRagConfig instance being validated.\n\nReturns:\n    None: This\
      \ method does not return a value.\n\nRaises:\n    LanguageModelConfigMissingError:\
      \ If the default chat model ID or the default embedding model ID is not present\
      \ in self.models."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_update_index_output_base_dir
    name: _validate_update_index_output_base_dir
    signature: def _validate_update_index_output_base_dir(self) -> None
    docstring: "Validate the update index output base directory.\n\nArgs:\n  self\
      \ (GraphRagConfig): The instance of the graph rag configuration model containing\
      \ the update_index_output and root_dir attributes.\n\nReturns:\n  None. This\
      \ method updates update_index_output.base_dir to an absolute path derived from\
      \ root_dir when the update_index_output.type is file.\n\nRaises:\n  ValueError:\
      \ If update_index_output.type is file and update_index_output.base_dir is blank\
      \ or whitespace."
  - node_id: graphrag/config/models/graph_rag_config.py::GraphRagConfig._validate_root_dir
    name: _validate_root_dir
    signature: def _validate_root_dir(self) -> None
    docstring: "Validate the root directory.\n\nArgs:\n  self: GraphRagConfig. The\
      \ instance containing the root_dir attribute.\n\nReturns:\n  None. This method\
      \ updates self.root_dir to an absolute path of an existing directory, defaulting\
      \ to the current working directory if root_dir is blank.\n\nRaises:\n  FileNotFoundError:\
      \ If the resolved root_dir is not an existing directory."
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor
  file: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py
  name: SyntacticNounPhraseExtractor
  methods:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.extract
    name: extract
    signature: "def extract(\n        self,\n        text: str,\n    ) -> list[str]"
    docstring: "Extract noun phrases from text. Noun phrases may include named entities\
      \ and noun chunks, which are filtered based on some heuristics.\n\nArgs:\n \
      \   text: Text.\n\nReturns:\n    List of noun phrases."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Returns the string representation used for cache key generation.\n\
      \nThe returned string encodes the extractor's configuration to uniquely identify\n\
      cache entries. It is constructed from the following attributes: model_name,\n\
      max_word_length, include_named_entities, exclude_entity_tags, exclude_pos_tags,\n\
      exclude_nouns, and word_delimiter.\n\nParameters:\n    self: The instance of\
      \ the extractor.\n\nReturns:\n    str: The cache-key string in the form:\n \
      \       syntactic_<model_name>_<max_word_length>_<include_named_entities>_<exclude_entity_tags>_<exclude_pos_tags>_<exclude_nouns>_<word_delimiter>\n\
      \nThere are no other parameters beyond self."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_name: str,\n        max_word_length:\
      \ int,\n        include_named_entities: bool,\n        exclude_entity_tags:\
      \ list[str],\n        exclude_pos_tags: list[str],\n        exclude_nouns: list[str],\n\
      \        word_delimiter: str,\n    )"
    docstring: "Initialize the SyntacticNounPhraseExtractor.\n\nThis initializer configures\
      \ the base class and prepares the SpaCy NLP pipeline used for noun phrase extraction\
      \ via dependency parsing and optional NER.\n\nIt delegates to the base class\
      \ BaseNounPhraseExtractor to set common configuration, including excluding nouns.\
      \ The call to the base initializer passes model_name, max_word_length, exclude_nouns,\
      \ and word_delimiter.\n\nArgs:\n    model_name: SpaCy model name.\n    max_word_length:\
      \ Maximum length in character of each extracted word.\n    include_named_entities:\
      \ Whether to include named entities in noun phrases. When True, named entities\
      \ are loaded and considered; when False, NER is disabled in the pipeline.\n\
      \    exclude_entity_tags: List of named entity tags to exclude in noun phrases.\n\
      \    exclude_pos_tags: List of POS tags to remove in noun phrases.\n    exclude_nouns:\
      \ List of nouns to exclude (passed to the base initializer).\n    word_delimiter:\
      \ Delimiter for joining words.\n\nReturns:\n    None\n\nNotes:\n    This initializer\
      \ sets up the self.nlp pipeline and stores configuration attributes for later\
      \ use.\n\nDynamic model loading behavior:\n    If include_named_entities is\
      \ False: load_spacy_model with exclude including lemmatizer and ner to disable\
      \ NER.\n    If include_named_entities is True: load_spacy_model with exclude\
      \ including lemmatizer to keep NER enabled."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/syntactic_parsing_extractor.py::SyntacticNounPhraseExtractor._tag_noun_phrases
    name: _tag_noun_phrases
    signature: "def _tag_noun_phrases(\n        self, noun_chunk: Span, entities:\
      \ list[Span]\n    ) -> dict[str, Any]"
    docstring: "Extract attributes of a noun chunk, to be used for filtering.\n\n\
      Args:\n  noun_chunk: Span - The noun chunk to analyze.\n  entities: list[Span]\
      \ - Entities to consider when validating the noun chunk; used to determine if\
      \ the chunk matches an entity and, if so, whether it's a valid entity.\n\nReturns:\n\
      \  dict[str, Any] - A dictionary containing the following keys:\n    cleaned_tokens:\
      \ List[Token] - The tokens from the noun_chunk after filtering out tokens based\
      \ on excluded POS tags, excluded nouns, spaces, and punctuation.\n    cleaned_text:\
      \ str - The tokens joined using the configured delimiter (word_delimiter), with\
      \ newline characters removed and converted to uppercase.\n    is_valid_entity:\
      \ bool - True if the noun_chunk corresponds to a valid entity among the provided\
      \ entities; otherwise False.\n    has_proper_nouns: bool - True if any of the\
      \ cleaned tokens is a proper noun.\n    has_compound_words: bool - True if the\
      \ cleaned token texts form a hyphenated compound (as determined by is_compound).\n\
      \    has_valid_tokens: bool - True if all cleaned token texts satisfy the maximum\
      \ token length constraint (has_valid_token_length with self.max_word_length).\n\
      \nRaises:\n  None."
- class_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger
  file: graphrag/logger/blob_workflow_logger.py
  name: BlobWorkflowLogger
  methods:
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._write_log
    name: _write_log
    signature: 'def _write_log(self, log: dict[str, Any])'
    docstring: "Write log data to blob storage.\n\nThis method appends the provided\
      \ log data as a JSON-formatted line to the blob, and reinitializes the internal\
      \ client when the accumulated block count reaches the configured maximum.\n\n\
      Args:\n    log: dict[str, Any] Log data to serialize as JSON and append as a\
      \ line in the blob.\n\nReturns:\n    None\n\nRaises:\n    OSError: If an I/O\
      \ error occurs during blob operations or during client reinitialization.\n \
      \   ValueError: If the log data cannot be serialized to JSON."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger._get_log_type
    name: _get_log_type
    signature: 'def _get_log_type(self, level: int) -> str'
    docstring: "Get log type string for a given numeric log level.\n\nArgs:\n    level:\
      \ int - The numeric log level (e.g., logging.INFO, logging.WARNING, logging.ERROR).\n\
      \nReturns:\n    str - The log type: \"error\" if level >= logging.ERROR, \"\
      warning\" if level >= logging.WARNING, otherwise \"log\"."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        connection_string: str | None,\n\
      \        container_name: str | None,\n        blob_name: str = \"\",\n     \
      \   base_dir: str | None = None,\n        storage_account_blob_url: str | None\
      \ = None,\n        level: int = logging.NOTSET,\n    )"
    docstring: "Create a new instance of the BlobWorkflowLogger class.\n\nArgs:\n\
      \  connection_string: Connection string for the blob storage, or None\n  container_name:\
      \ Name of the blob container\n  blob_name: Name of the blob to create; if empty,\
      \ a timestamped default will be used\n  base_dir: Base directory to prepend\
      \ to the blob name, or None\n  storage_account_blob_url: URL of the storage\
      \ account blob service, or None\n  level: Logging level\n\nReturns:\n  None\n\
      \nRaises:\n  ValueError: No container name provided for blob storage.\n  ValueError:\
      \ No storage account blob url provided for blob storage.\n  ValueError: Either\
      \ connection_string or storage_account_blob_url must be provided."
  - node_id: graphrag/logger/blob_workflow_logger.py::BlobWorkflowLogger.emit
    name: emit
    signature: def emit(self, record) -> None
    docstring: "Emit a log record to blob storage.\n\nCreates a JSON structure from\
      \ the given log record, including type (\"log\", \"warning\", or \"error\")\
      \ based on the record level, and the main message. Optional fields such as details,\
      \ cause (from exc_info), and stack (if present) are added if they exist. The\
      \ resulting payload is passed to _write_log for persistence in Azure Blob storage.\
      \ If writing fails with OSError or ValueError, those exceptions are not propagated;\
      \ they are handled by self.handleError(record). This method is part of the BlobWorkflowLogger\
      \ class and interacts with _write_log and the logic that reinitializes the blob\
      \ client when the block counter reaches the maximum.\n\nArgs:\n    record: logging.LogRecord\
      \ The log record to emit to blob storage.\n\nReturns:\n    None\n\nRaises:\n\
      \    OSError: Not raised; errors are caught and delegated to self.handleError(record)\
      \ instead.\n    ValueError: Not raised; errors are caught and delegated to self.handleError(record)\
      \ instead."
- class_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks
  file: graphrag/callbacks/console_workflow_callbacks.py
  name: ConsoleWorkflowCallbacks
  methods:
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.__init__
    name: __init__
    signature: def __init__(self, verbose=False)
    docstring: "Initialize ConsoleWorkflowCallbacks with an optional verbose mode.\n\
      \nArgs:\n  verbose: Enable verbose logging to the console.\n\nReturns:\n  None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Args:\n    results: A list of PipelineRunResult objects representing the results\
      \ of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "\"\"\"Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the workflows that started.\n\nReturns:\n\
      \    None\n\"\"\""
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
      \ The name of the workflow starting.\n    instance (object): The workflow instance\
      \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "\"\"\"Writes a live progress bar to stdout that updates in place as\
      \ progress events occur.\n\nThis callback renders a simple, in-place progress\
      \ bar on a single stdout line and\noverwrites the previous line using a carriage\
      \ return. It flushes the output to\nensure timely updates.\n\nProgress calculation:\n\
      - completed_items: number of items completed (defaults to 0 if None or falsy)\n\
      - total_items: total items to process (defaults to 1 if None or falsy to avoid\
      \ division by zero)\n- percent: integer percentage of completion, computed as\
      \ round((completed / total) * 100)\n\nOutput behavior:\n- Prints a line showing\
      \ the current progress as \"completed / total\" followed by a dot-filled\n \
      \ bar whose width is proportional to percent. The bar is created by left-justifying\
      \ the\n  start string within a field of width percent using '.' as the fill\
      \ character.\n- The line uses end=\"\\r\" to return the cursor to the start\
      \ of the line for in-place updates.\n\nArgs:\n    progress: Progress object\
      \ containing completed_items and total_items attributes.\n\nReturns:\n    None\n\
      \"\"\""
  - node_id: graphrag/callbacks/console_workflow_callbacks.py::ConsoleWorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None..."
- class_id: graphrag/config/models/language_model_config.py::LanguageModelConfig
  file: graphrag/config/models/language_model_config.py
  name: LanguageModelConfig
  methods:
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_key
    name: _validate_api_key
    signature: def _validate_api_key(self) -> None
    docstring: "Validate the API key.\n\nAPI Key is required when using OpenAI API\
      \ or when using Azure API with API Key authentication.\nFor the time being,\
      \ this check is extra verbose for clarity.\nIt will also raise an exception\
      \ if an API Key is provided when one is not expected such as the case of using\
      \ Azure Managed Identity.\n\nArgs:\n    self: The LanguageModelConfig instance.\n\
      \nReturns:\n    None\n\nRaises:\n    ApiKeyMissingError: If the API key is missing\
      \ and is required.\n    ConflictingSettingsError: If an API Key is provided\
      \ when using Azure Managed Identity."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_encoding_model
    name: _validate_encoding_model
    signature: def _validate_encoding_model(self) -> None
    docstring: 'Validate the encoding model.


      If encoding_model is not provided (empty or whitespace) and the type is not
      Chat or Embedding, derive

      the encoding model name for the configured LLM model using tiktoken.encoding_name_for_model
      and assign it

      to self.encoding_model. This updates the attribute in place and returns None.'
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_deployment_name
    name: _validate_deployment_name
    signature: def _validate_deployment_name(self) -> None
    docstring: "Validate the deployment name for Azure-hosted models.\n\nThis internal\
      \ validator checks whether a deployment_name is provided when using Azure OpenAI\
      \ (AOI) configurations. If the deployment_name is missing or consists only of\
      \ whitespace for Azure-hosted models (AzureOpenAIChat, AzureOpenAIEmbedding,\
      \ or when model_provider == \"azure\"), it logs a debug message stating that\
      \ deployment_name is not set and that the service will default to the model\
      \ name. No exception is raised; the behavior is a soft default.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_azure_settings
    name: _validate_azure_settings
    signature: def _validate_azure_settings(self) -> None
    docstring: "Validate the Azure settings.\n\nArgs:\n    self: The instance of the\
      \ language model configuration.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
      \ If the API base is missing and is required.\n    AzureApiVersionMissingError:\
      \ If the API version is missing and is required.\n    AzureDeploymentNameMissingError:\
      \ If the deployment name is missing and is required."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model_provider
    name: _validate_model_provider
    signature: def _validate_model_provider(self) -> None
    docstring: "Validate the model provider.\n\nThis validation applies only when\
      \ the model type is Chat or Embedding. If the model type is Chat or Embedding\
      \ and the model_provider is missing or blank, a KeyError is raised indicating\
      \ that a model provider must be specified for that type. For other model types,\
      \ this method performs no validation.\n\nArgs:\n    self: The instance of the\
      \ language model configuration. Note that self is an implicit parameter for\
      \ Python methods.\n\nReturns:\n    None\n\nRaises:\n    KeyError: If the model\
      \ provider is missing when the model type is Chat or Embedding."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_requests_per_minute
    name: _validate_requests_per_minute
    signature: def _validate_requests_per_minute(self) -> None
    docstring: "Validate the requests per minute.\n\nThis is a Pydantic model validator\
      \ invoked on the LanguageModelConfig instance. Accepted values are integers\
      \ >= 1, the string 'auto', or null. When the type is 'Chat' or 'Embedding' and\
      \ rate_limit_strategy is not None, 'auto' is not allowed.\n\nRaises\n------\n\
      ValueError\n    If the requests_per_minute is less than 1.\n    If requests_per_minute\
      \ is 'auto' when using type 'Chat' or 'Embedding' with a non-null rate_limit_strategy."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_max_retries
    name: _validate_max_retries
    signature: def _validate_max_retries(self) -> None
    docstring: "\"\"\"Validate the maximum retries.\n\nArgs:\n    self: The instance\
      \ of the language model configuration.\n\nReturns:\n    None: This method does\
      \ not return a value.\n\nRaises:\n    ValueError: If the maximum retries is\
      \ less than 1.\n\"\"\""
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_tokens_per_minute
    name: _validate_tokens_per_minute
    signature: def _validate_tokens_per_minute(self) -> None
    docstring: "Validate the tokens per minute.\n\nThis in-place validator ensures\
      \ tokens_per_minute is one of: an integer >= 1, the string \"auto\", or None.\
      \ It also enforces that tokens_per_minute cannot be set to 'auto' when using\
      \ a Chat or Embedding model type and a rate_limit_strategy is provided.\n\n\
      Returns:\n    None: This method validates in place on the LanguageModelConfig\
      \ instance and returns None.\n\nRaises:\n    ValueError: If tokens_per_minute\
      \ is an integer and less than 1.\n    ValueError: If tokens_per_minute is 'auto'\
      \ when using type 'Chat' or 'Embedding' and rate_limit_strategy is not None."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_base
    name: _validate_api_base
    signature: def _validate_api_base(self) -> None
    docstring: "Validate the API base.\n\nRequired when using AOI.\n\nArgs:\n    self:\
      \ The LanguageModelConfig instance.\n\nReturns:\n    None\n\nRaises:\n    AzureApiBaseMissingError:\
      \ If the API base is missing and is required."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_type
    name: _validate_type
    signature: def _validate_type(self) -> None
    docstring: "Validate the model type.\n\nArgs:\n    self: The instance being validated.\n\
      \nReturns:\n    None\n        The function does not return a value.\n\nRaises:\n\
      \    KeyError: If the model name is not recognized."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_auth_type
    name: _validate_auth_type
    signature: def _validate_auth_type(self) -> None
    docstring: "Validate the authentication type.\n\nauth_type must be api_key when\
      \ using OpenAI and\ncan be either api_key or azure_managed_identity when using\
      \ AOI.\n\nArgs:\n    self: The instance being validated.\n\nReturns:\n    None\n\
      \        The function does not return a value.\n\nRaises:\n    ConflictingSettingsError:\
      \ If the Azure authentication type conflicts with the model being used."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the LanguageModelConfig after the initial Pydantic schema\
      \ validation as a model_validator(mode='after') post-hook; this reflects the\
      \ actual validation sequence (after initial schema validation) and may raise\
      \ additional errors.\n\nArgs:\n    self (LanguageModelConfig): The LanguageModelConfig\
      \ instance being validated.\n\nReturns:\n    LanguageModelConfig: The same instance\
      \ after validation.\n\nRaises:\n    ApiKeyMissingError: If the API key is missing\
      \ when required.\n    AzureApiBaseMissingError: If the Azure API base is missing\
      \ when required.\n    AzureApiVersionMissingError: If the Azure API version\
      \ is missing when required.\n    AzureDeploymentNameMissingError: If the Azure\
      \ deployment name is missing when required.\n    ConflictingSettingsError: If\
      \ there are conflicting settings detected during validation."
  - node_id: graphrag/config/models/language_model_config.py::LanguageModelConfig._validate_api_version
    name: _validate_api_version
    signature: def _validate_api_version(self) -> None
    docstring: "Validate the API version.\n\n        Required when using AOI.\n\n\
      \        Args:\n            self: The LanguageModelConfig instance.\n\n    \
      \    Returns:\n            None\n\n        Raises:\n            AzureApiVersionMissingError:\
      \ If the API version is missing and is required."
- class_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter
  file: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py
  name: RateLimiter
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        /,\n        **kwargs: Any,\n\
      \    ) -> None"
    docstring: "Abstract initializer for rate limiters. Subclasses must implement\
      \ their own initialization logic; this method should not perform concrete initialization.\n\
      \nArgs:\n    kwargs: Additional keyword arguments passed to initialization.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/rate_limiter.py::RateLimiter.acquire
    name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    docstring: "\"\"\"Acquire Rate Limiter.\n\nParameters\n    token_count (int):\
      \ The estimated number of tokens for the current request.\n\nReturns\n    Iterator[None]:\
      \ A context manager that yields None and does not return any value.\n\nRaises\n\
      \    NotImplementedError: RateLimiter subclasses must implement the acquire\
      \ method.\n\"\"\""
- class_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore
  file: tests/integration/vector_stores/test_azure_ai_search.py
  name: TestAzureAISearchVectorStore
  methods:
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store
    name: vector_store
    signature: def vector_store(self, mock_search_client, mock_index_client)
    docstring: "Create an Azure AI Search vector store fixture.\n\nArgs:\n    self:\
      \ The test class instance.\n    mock_search_client: Mock Azure AI Search client\
      \ to be assigned to vector_store.db_connection.\n    mock_index_client: Mock\
      \ Azure AI Search index client to be assigned to vector_store.index_client.\n\
      \nReturns:\n    AzureAISearchVectorStore: The configured Azure AI Search vector\
      \ store instance."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.none_embedder
    name: none_embedder
    signature: 'def none_embedder(text: str) -> None'
    docstring: "A placeholder embedder function used in tests that accepts a text\
      \ string and returns None.\n\nArgs:\n    text: str\n        The input text to\
      \ be embedded. The function does not perform embedding and returns None.\n\n\
      Returns:\n    None\n        The function returns no value."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.vector_store_custom
    name: vector_store_custom
    signature: def vector_store_custom(self, mock_search_client, mock_index_client)
    docstring: "Create an Azure AI Search vector store fixture with custom field mappings.\n\
      \nArgs:\n    self: The test class instance.\n    mock_search_client: Mock Azure\
      \ AI Search client to be assigned to vector_store.db_connection.\n    mock_index_client:\
      \ Mock Azure AI Search index client to be assigned to vector_store.index_client.\n\
      \nReturns:\n    AzureAISearchVectorStore: The configured Azure AI Search vector\
      \ store instance."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_embedder
    name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
    docstring: "A simple text embedder used for testing that returns a fixed embedding\
      \ vector. The embedding is independent of the input text and always returns\
      \ [0.1, 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text: Input text to embed.\n\nReturns:\n\
      \    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4, 0.5].\n\n\
      Raises:\n    None: This function does not raise any exceptions."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_customization
    name: test_vector_store_customization
    signature: "def test_vector_store_customization(\n        self,\n        vector_store_custom,\n\
      \        sample_documents,\n        mock_search_client,\n        mock_index_client,\n\
      \    )"
    docstring: "Test vector store customization with Azure AI Search.\n\nArgs:\n \
      \   self: The test case instance.\n    vector_store_custom: Custom Azure AI\
      \ Search vector store used in the test.\n    sample_documents: Documents used\
      \ to load into the vector store.\n    mock_search_client: Mock for the Azure\
      \ AI Search search client.\n    mock_index_client: Mock for the Azure AI Search\
      \ index client.\n\nReturns:\n    None."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_vector_store_operations
    name: test_vector_store_operations
    signature: "def test_vector_store_operations(\n        self, vector_store, sample_documents,\
      \ mock_search_client, mock_index_client\n    )"
    docstring: "Test basic vector store operations with Azure AI Search.\n\nArgs:\n\
      \  self: The test case instance.\n  vector_store: AzureAISearchVectorStore used\
      \ in the test.\n  sample_documents: Documents loaded into the vector store for\
      \ testing.\n  mock_search_client: Mock for the Azure AI Search search client.\n\
      \  mock_index_client: Mock for the Azure AI Search index client.\n\nReturns:\n\
      \  None."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_search_client
    name: mock_search_client
    signature: def mock_search_client(self)
    docstring: "Create and yield a mock Azure AI Search client for tests.\n\nArgs:\n\
      \    self: TestAzureAISearchVectorStore instance.\n\nReturns:\n    MagicMock:\
      \ The mocked Azure AI Search client (SearchClient) instance produced by patch."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.sample_documents
    name: sample_documents
    signature: def sample_documents(self)
    docstring: "Create sample documents for testing.\n\nParameters:\n    self: Instance\
      \ of the test class used by pytest to provide fixture context.\n\nReturns:\n\
      \    List[VectorStoreDocument]: A list of VectorStoreDocument objects with ids\
      \ \"doc1\" and \"doc2\",\n        texts \"This is document 1\" and \"This is\
      \ document 2\",\n        vectors [0.1, 0.2, 0.3, 0.4, 0.5] and [0.2, 0.3, 0.4,\
      \ 0.5, 0.6],\n        and attributes {\"title\": \"Doc 1\", \"category\": \"\
      test\"} and {\"title\": \"Doc 2\", \"category\": \"test\"}."
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.test_empty_embedding
    name: test_empty_embedding
    signature: def test_empty_embedding(self, vector_store, mock_search_client)
    docstring: "Test similarity search by text with empty embedding.\n\nArgs:\n  \
      \  self: The test method's instance.\n    vector_store: The AzureAISearchVectorStore\
      \ instance under test.\n    mock_search_client: Mock search client used to verify\
      \ interactions.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: tests/integration/vector_stores/test_azure_ai_search.py::TestAzureAISearchVectorStore.mock_index_client
    name: mock_index_client
    signature: def mock_index_client(self)
    docstring: "Create a mock Azure AI Search index client.\n\nArgs:\n    self (TestAzureAISearchVectorStore):\
      \ The test class instance.\n\nReturns:\n    MagicMock: The mocked Azure AI Search\
      \ index client instance produced by patching SearchIndexClient.\n\nRaises:\n\
      \    None"
- class_id: graphrag/storage/pipeline_storage.py::PipelineStorage
  file: graphrag/storage/pipeline_storage.py
  name: PipelineStorage
  methods:
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.has
    name: has
    signature: 'def has(self, key: str) -> bool'
    docstring: "Return True if the given key exists in the storage.\n\nArgs:\n   \
      \ key: The key to check for.\n\nReturns:\n    output - True if the key exists\
      \ in the storage, False otherwise."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.find
    name: find
    signature: "def find(\n        self,\n        file_pattern: re.Pattern[str],\n\
      \        base_dir: str | None = None,\n        file_filter: dict[str, Any] |\
      \ None = None,\n        max_count=-1,\n    ) -> Iterator[tuple[str, dict[str,\
      \ Any]]]"
    docstring: "Find files in the storage that match a compiled file_pattern, with\
      \ optional base_dir and metadata-based filtering.\n\nArgs:\n    file_pattern\
      \ (re.Pattern[str]): A compiled regular expression to match file paths.\n  \
      \  base_dir (str | None): The base directory to search within. If None, search\
      \ starts from the storage root.\n    file_filter (dict[str, Any] | None): Optional\
      \ dictionary of metadata field names to values to filter on. Implementations\
      \ may perform exact-value matching against the file's metadata; keys correspond\
      \ to metadata attributes present in the returned dictionaries. If None, no additional\
      \ filtering is applied.\n    max_count (int): Maximum number of results to yield.\
      \ -1 means no limit. When a non-negative value is provided, at most that many\
      \ results are yielded.\n\nReturns:\n    Iterator[tuple[str, dict[str, Any]]]:\
      \ An iterator yielding (path, metadata) pairs where path is the matched file\
      \ path as a string and metadata is a dictionary of attributes describing the\
      \ file. The exact metadata keys are implementation-dependent and may vary between\
      \ storage backends.\n\nNotes:\n    This is an abstract method. Concrete subclasses\
      \ must provide an implementation.\n\nRaises:\n    Exceptions raised depend on\
      \ the concrete subclass implementation (e.g., I/O or filesystem errors)."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get_creation_date
    name: get_creation_date
    signature: 'def get_creation_date(self, key: str) -> str'
    docstring: "Get the creation date for the given key.\n\nArgs:\n    key (str):\
      \ The key for which to retrieve the creation date.\n\nReturns:\n    str: The\
      \ creation date as a string."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.clear
    name: clear
    signature: def clear(self) -> None
    docstring: "Asynchronously clear all entries from the storage.\n\nThis coroutine\
      \ clears all data stored in the storage backend and does not return a value.\
      \ It must be awaited.\n\nArgs:\n    self: The storage instance.\n\nReturns:\n\
      \    None: The coroutine completes without returning a value.\n\nRaises:\n \
      \   Exception: If the storage backend fails to clear the storage."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.set
    name: set
    signature: 'def set(self, key: str, value: Any, encoding: str | None = None) ->
      None'
    docstring: "Set the value for the given key.\n\nThis is an asynchronous operation\
      \ that stores the provided value under the given key. If the key already exists,\
      \ its value will be overwritten.\n\nArgs:\n    key (str): The key to set the\
      \ value for.\n    value (Any): The value to set.\n    encoding (str | None):\
      \ Optional encoding to apply when serializing the value.\n\nReturns:\n    None:\
      \ This coroutine completes when the value has been stored.\n\nRaises:\n    StorageError:\
      \ If a storage-related error occurs during storage."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"List all keys in the storage.\n\nArgs:\n    self (PipelineStorage):\
      \ The instance of the PipelineStorage.\n\nReturns:\n    list[str]: The keys\
      \ currently stored in the storage.\n\nRaises:\n    NotImplementedError: If key\
      \ listing is not supported by the storage backend.\n\"\"\""
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.get
    name: get
    signature: "def get(\n        self, key: str, as_bytes: bool | None = None, encoding:\
      \ str | None = None\n    ) -> Any"
    docstring: "Get the value for the given key.\n\n        Args:\n            key\
      \ (str): The key to retrieve the value for.\n            as_bytes (bool | None):\
      \ If True, return the value as bytes. If None, use the backend's default representation.\n\
      \            encoding (str | None): The text encoding to use when decoding bytes\
      \ to str. If None, the backend's default encoding is used.\n\n        Returns:\n\
      \            Any: The value for the given key. The concrete return type depends\
      \ on as_bytes and encoding:\n                - If as_bytes is True: bytes\n\
      \                - If as_bytes is False or None and encoding is not None: str\
      \ decoded using the provided encoding\n                - If as_bytes is None\
      \ and encoding is None: backend-specific type or None\n\n        Raises:\n \
      \           KeyError: If the key does not exist in storage.\n            ValueError:\
      \ If the provided encoding is invalid or not supported for the stored value.\n\
      \            RuntimeError: If a backend-specific error occurs."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.child
    name: child
    signature: 'def child(self, name: str | None) -> "PipelineStorage"'
    docstring: "Create or return a child storage instance.\n\nThis method creates\
      \ and returns a dedicated child storage instance. The optional name parameter\
      \ is accepted for API compatibility but may be ignored by the implementing class.\
      \ The behavior with the name (whether it selects a specific child or is ignored)\
      \ is determined by the concrete implementation.\n\nArgs:\n    name (str | None):\
      \ Optional name for the child storage. This parameter is accepted for API compatibility\
      \ but may be ignored by the implementation.\n\nReturns:\n    PipelineStorage:\
      \ The child storage instance corresponding to the provided name."
  - node_id: graphrag/storage/pipeline_storage.py::PipelineStorage.delete
    name: delete
    signature: 'def delete(self, key: str) -> None'
    docstring: "\"\"\"Delete the given key from the storage.\n\nArgs:\n    key (str):\
      \ The key to delete.\n\nReturns:\n    None\n\"\"\""
- class_id: graphrag/factory/factory.py::Factory
  file: graphrag/factory/factory.py
  name: Factory
  methods:
  - node_id: graphrag/factory/factory.py::Factory.create
    name: create
    signature: 'def create(self, *, strategy: str, **kwargs: Any) -> T'
    docstring: "Create a service instance based on the strategy.\n\nArgs:\n    strategy\
      \ (str): The name of the strategy.\n    kwargs (Any): Additional arguments to\
      \ pass to the service initializer.\n\nReturns:\n    T: An instance of T.\n\n\
      Raises:\n    ValueError: If the strategy is not registered."
  - node_id: graphrag/factory/factory.py::Factory.__contains__
    name: __contains__
    signature: 'def __contains__(self, strategy: str) -> bool'
    docstring: "\"\"\"Check if a strategy is registered.\n\nArgs:\n    strategy: str\
      \ The name of the strategy.\n\nReturns:\n    bool: True if the strategy is registered,\
      \ False otherwise.\n\"\"\""
  - node_id: graphrag/factory/factory.py::Factory.__new__
    name: __new__
    signature: 'def __new__(cls, *args: Any, **kwargs: Any) -> "Factory"'
    docstring: "Return the per-subclass singleton instance for the class that invokes\
      \ __new__.\n\nIf an instance for this class has not been created yet, create\
      \ one using the base object's __new__ and store it on the class. Subsequent\
      \ calls return the same instance.\n\nArgs\n----\n    cls: The class for which\
      \ to obtain the singleton instance. This is the class that invoked __new__,\
      \ and may be a subclass of Factory.\n    *args: Additional positional arguments\
      \ forwarded to the class's constructor.\n    **kwargs: Additional keyword arguments\
      \ forwarded to the class's constructor.\n\nReturns\n----\n    The singleton\
      \ instance of the calling class (cls). Each subclass maintains its own _instance,\
      \ so the return value is an instance of the subclass rather than the base Factory."
  - node_id: graphrag/factory/factory.py::Factory.register
    name: register
    signature: 'def register(self, *, strategy: str, service_initializer: Callable[...,
      T]) -> None'
    docstring: "Register a new service factory for a strategy.\n\nStores a factory\
      \ (callable) under the given strategy name. The factory is not invoked at registration\
      \ time; it will be called later by create(**kwargs) to produce an instance of\
      \ T.\n\nIf a factory is already registered under the same strategy name, it\
      \ will be overwritten with the new factory.\n\nArgs:\n    strategy (str): The\
      \ name of the strategy.\n    service_initializer (Callable[..., T]): A callable\
      \ that, when invoked via create(**kwargs), returns an instance of T.\n\nReturns:\n\
      \    None: This method does not return a value."
  - node_id: graphrag/factory/factory.py::Factory.__init__
    name: __init__
    signature: def __init__(self)
    docstring: "Initialize internal state for the Factory singleton instance on first\
      \ initialization.\n\nArgs\n----\nself: The Factory instance being initialized.\
      \ The internal state is created only on the first initialization due to the\
      \ singleton __new__-based pattern.\n\nReturns\n-------\nNone\n\nRaises\n-------\n\
      None\n\nAttributes\n- _services: dict[str, Callable[..., T]] \u2014 registry\
      \ mapping strategy names to callables that return T.\n- _initialized: bool \u2014\
      \ initialization flag."
  - node_id: graphrag/factory/factory.py::Factory.keys
    name: keys
    signature: def keys(self) -> list[str]
    docstring: "\"\"\"Get a list of registered strategy names.\n\nArgs:\n    self:\
      \ The instance containing registered strategies.\n\nReturns:\n    list[str]:\
      \ A list of the registered strategy names.\n\"\"\""
- class_id: graphrag/language_model/manager.py::ModelManager
  file: graphrag/language_model/manager.py
  name: ModelManager
  methods:
  - node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_chat_model
    name: get_or_create_chat_model
    signature: "def get_or_create_chat_model(\n        self, name: str, model_type:\
      \ str, **chat_kwargs: Any\n    ) -> ChatModel"
    docstring: "Get or create the ChatLLM instance registered under the given name.\n\
      \nIf the ChatLLM does not exist, it is created and registered.\n\nArgs:\n  \
      \  name: Unique identifier for the ChatLLM instance.\n    model_type: Key for\
      \ the ChatModel implementation in LLMFactory.\n    chat_kwargs: Additional keyword\
      \ arguments for instantiation.\n\nReturns:\n    ChatModel: The ChatLLM instance\
      \ associated with the given name.\n\nRaises:\n    Exception: Any error raised\
      \ during creation via register_chat or the underlying factory."
  - node_id: graphrag/language_model/manager.py::ModelManager.list_chat_models
    name: list_chat_models
    signature: def list_chat_models(self) -> dict[str, ChatModel]
    docstring: "Return a copy of all registered ChatModel instances.\n\nReturns:\n\
      \    dict[str, ChatModel]: A dictionary mapping model names to ChatModel instances."
  - node_id: graphrag/language_model/manager.py::ModelManager.remove_chat
    name: remove_chat
    signature: 'def remove_chat(self, name: str) -> None'
    docstring: "Remove the ChatLLM instance registered under the given name.\n\nArgs:\n\
      \    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n    None"
  - node_id: graphrag/language_model/manager.py::ModelManager.list_embedding_models
    name: list_embedding_models
    signature: def list_embedding_models(self) -> dict[str, EmbeddingModel]
    docstring: "Return a shallow copy of all registered EmbeddingModel instances.\n\
      \nThis returns a shallow copy of the internal mapping of embedding models keyed\
      \ by\ntheir registration name. Modifications to the returned dictionary do not\
      \ affect\nthe internal registry.\n\nReturns:\n    dict[str, EmbeddingModel]:\
      \ A shallow copy mapping model names to EmbeddingModel\n    instances."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_chat_model
    name: get_chat_model
    signature: 'def get_chat_model(self, name: str) -> ChatModel | None'
    docstring: "Retrieve the ChatLLM instance registered under the given name.\n\n\
      Args:\n    name: Unique identifier for the ChatLLM instance.\n\nReturns:\n \
      \   ChatModel: The ChatLLM instance registered under the given name.\n\nRaises:\n\
      \    ValueError: If no ChatLLM is registered under the name."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_or_create_embedding_model
    name: get_or_create_embedding_model
    signature: "def get_or_create_embedding_model(\n        self, name: str, model_type:\
      \ str, **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    docstring: "Retrieve the EmbeddingsLLM instance registered under the given name.\n\
      \nIf the EmbeddingsLLM does not exist, it is created and registered.\n\nArgs:\n\
      \    name: Unique identifier for the EmbeddingsLLM instance.\n    model_type:\
      \ Key for the EmbeddingsLLM implementation in LLMFactory.\n    **embedding_kwargs:\
      \ Additional parameters for instantiation.\n\nReturns:\n    EmbeddingModel:\
      \ The EmbeddingModel instance associated with the given name."
  - node_id: graphrag/language_model/manager.py::ModelManager.get_instance
    name: get_instance
    signature: def get_instance(cls) -> ModelManager
    docstring: "Return the singleton instance of ModelManager.\n\nThis is a classmethod\
      \ that returns the existing ModelManager singleton by delegating to the class's\
      \ __new__ method. No additional parameters are required beyond cls.\n\nArgs:\n\
      \    cls: The ModelManager class used to access the singleton instance.\n\n\
      Returns:\n    ModelManager: The singleton ModelManager instance."
  - node_id: graphrag/language_model/manager.py::ModelManager.register_embedding
    name: register_embedding
    signature: "def register_embedding(\n        self, name: str, model_type: str,\
      \ **embedding_kwargs: Any\n    ) -> EmbeddingModel"
    docstring: "Register an EmbeddingsLLM instance under a unique name.\n\nRegisters\
      \ a new EmbeddingModel in self.embedding_models using the specified model_type\
      \ and the provided keyword arguments.\n\nArgs:\n    name (str): Unique identifier\
      \ for the EmbeddingsLLM instance.\n    model_type (str): Key for the EmbeddingsLLM\
      \ implementation in the factory (ModelFactory).\n    **embedding_kwargs: Additional\
      \ keyword arguments for instantiation, passed to the model factory.\n\nReturns:\n\
      \    EmbeddingModel: The EmbeddingModel instance registered under the given\
      \ name.\n\nRaises:\n    ValueError: If the provided model_type is invalid or\
      \ if the underlying factory encounters an error constructing the model.\n\n\
      Notes:\n    The function assigns the given name into embedding_kwargs before\
      \ creation, and stores the resulting model in self.embedding_models under the\
      \ provided name."
  - node_id: graphrag/language_model/manager.py::ModelManager.__new__
    name: __new__
    signature: def __new__(cls) -> Self
    docstring: "Create a new singleton instance of ModelManager if it does not exist.\n\
      \nArgs:\n    cls: Type[ModelManager] The ModelManager class used to access the\
      \ singleton instance.\n\nReturns:\n    Self: The singleton ModelManager instance.\n\
      \nRaises:\n    None: This method does not raise any exceptions."
  - node_id: graphrag/language_model/manager.py::ModelManager.register_chat
    name: register_chat
    signature: "def register_chat(\n        self, name: str, model_type: str, **chat_kwargs:\
      \ Any\n    ) -> ChatModel"
    docstring: "Register a ChatModel instance under a unique name.\n\nThis method\
      \ injects the provided name into the chat_kwargs before instantiation,\ncreates\
      \ the ChatModel using ModelFactory.create_chat_model, and registers it in\n\
      self.chat_models under the given name.\n\nArgs:\n    name (str): Unique identifier\
      \ for the ChatLLM/ChatModel instance.\n    model_type (str): Key for the ChatLLM\
      \ implementation in LLMFactory.\n    chat_kwargs (dict[str, Any]): Additional\
      \ keyword arguments for instantiation.\n        The dictionary will have the\
      \ key 'name' added prior to the factory call.\n\nReturns:\n    ChatModel: The\
      \ ChatModel instance registered under the given name.\n\nRaises:\n    Exception\
      \ types raised by the underlying factory call (ModelFactory.create_chat_model)\n\
      \    or input/validation errors may propagate to the caller."
  - node_id: graphrag/language_model/manager.py::ModelManager.__init__
    name: __init__
    signature: def __init__(self) -> None
    docstring: "\"\"\"Initialize the singleton LLM manager's internal state on first\
      \ instantiation.\n\nArgs:\n    self: The instance being initialized. Sets up\
      \ internal dictionaries for chat_models and embedding_models and marks the instance\
      \ as initialized to avoid reinitialization.\n\nReturns:\n    None: This method\
      \ does not return a value.\n\nRaises:\n    None: This method does not raise\
      \ any exceptions.\n\"\"\""
  - node_id: graphrag/language_model/manager.py::ModelManager.remove_embedding
    name: remove_embedding
    signature: 'def remove_embedding(self, name: str) -> None'
    docstring: "Remove the EmbeddingsLLM instance registered under the given name.\n\
      \nArgs:\n    name: str \u2014 Unique identifier for the EmbeddingsLLM instance.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/manager.py::ModelManager.get_embedding_model
    name: get_embedding_model
    signature: 'def get_embedding_model(self, name: str) -> EmbeddingModel | None'
    docstring: "\"\"\"\nRetrieve the EmbeddingsLLM instance registered under the given\
      \ name.\n\nArgs:\n    name (str): Unique identifier for the EmbeddingsLLM instance.\n\
      \nReturns:\n    EmbeddingModel: The EmbeddingModel instance registered under\
      \ the name.\n\nRaises:\n    ValueError: If no EmbeddingsLLM is registered under\
      \ the name.\n\"\"\""
- class_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor
  file: graphrag/index/operations/extract_covariates/claim_extractor.py
  name: ClaimExtractor
  methods:
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._clean_claim
    name: _clean_claim
    signature: "def _clean_claim(\n        self, claim: dict, document_id: str, resolved_entities:\
      \ dict\n    ) -> dict"
    docstring: 'Update a claim''s object and subject identifiers in place using a
      resolved_entities mapping. This function does not filter by status and does
      not remove claims with status = False.


      Args:

      - claim (dict): The claim dictionary to update. The function reads the object_id
      (or object) and subject_id (or subject), substitutes them using resolved_entities,
      and writes the resulting values back to object_id and subject_id in the input
      dict. If keys are missing, existing values are preserved.

      - document_id (str): Identifier of the document containing the claim. This parameter
      is unused by this function.

      - resolved_entities (dict): Mapping of original entity identifiers to resolved
      identifiers; used to substitute the object and subject when present.


      Returns:

      - dict: The updated claim dictionary (the same object, mutated in place).


      Notes:

      - The function updates only the object_id and subject_id keys.

      - No status filtering is performed; claims with status = False are not removed.

      - When reading, object is considered as a fallback for object_id and subject
      as a fallback for subject_id.'
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._process_document
    name: _process_document
    signature: "def _process_document(\n        self, prompt_args: dict, doc, doc_index:\
      \ int\n    ) -> list[dict]"
    docstring: "Process a single document to extract claims from the text using the\
      \ configured extraction prompt and parse the results into dictionaries.\n\n\
      Args:\n  prompt_args: A dictionary of additional arguments used to configure\
      \ the extraction prompts and behavior.\n  doc: The document content to process.\n\
      \  doc_index: The zero-based index of the document within the input collection.\n\
      \nReturns:\n  A list of dictionaries representing parsed claims as produced\
      \ by _parse_claim_tuples. Each dictionary typically includes keys such as subject_id,\
      \ object_id, and type.\n\nRaises:\n  Exceptions raised by the underlying language\
      \ model interactions (e.g., self._model.achat) or by parsing/processing logic\
      \ can propagate to callers."
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor._parse_claim_tuples
    name: _parse_claim_tuples
    signature: "def _parse_claim_tuples(\n        self, claims: str, prompt_variables:\
      \ dict\n    ) -> list[dict[str, Any]]"
    docstring: "Parse claim tuples.\n\nArgs:\n    claims: The raw claims text to parse\
      \ into structured claim dictionaries. The text is\n        expected to contain\
      \ multiple claims separated by the configured record delimiter,\n        with\
      \ fields within each claim separated by the configured tuple delimiter. The\n\
      \        trailing completion delimiter is ignored during parsing.\n    prompt_variables:\
      \ A mapping used to determine the delimiters for records, tuples, and\n    \
      \    completions. The method looks up keys corresponding to internal delimiter\
      \ keys and\n        falls back to DEFAULT_RECORD_DELIMITER, DEFAULT_TUPLE_DELIMITER,\
      \ and\n        DEFAULT_COMPLETION_DELIMITER when not present.\n\nReturns:\n\
      \    list[dict[str, Any]]: A list of dictionaries where each dictionary represents\
      \ a parsed claim\n        with the following keys: subject_id, object_id, type,\
      \ status, start_date, end_date,\n        description, source_text. Values are\
      \ strings or None depending on whether a field was\n        present.\n\nRaises:\n\
      \    None: This method does not raise any documented exceptions during normal\
      \ operation."
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        input_text_key: str\
      \ | None = None,\n        input_entity_spec_key: str | None = None,\n      \
      \  input_claim_description_key: str | None = None,\n        input_resolved_entities_key:\
      \ str | None = None,\n        tuple_delimiter_key: str | None = None,\n    \
      \    record_delimiter_key: str | None = None,\n        completion_delimiter_key:\
      \ str | None = None,\n        max_gleanings: int | None = None,\n        on_error:\
      \ ErrorHandlerFn | None = None,\n    )"
    docstring: "Initialize ClaimExtractor.\n\nArgs:\n  model_invoker: ChatModel\n\
      \      The model invoker used to run prompts.\n  extraction_prompt: str | None\n\
      \      Custom prompt for extraction. If None, defaults to EXTRACT_CLAIMS_PROMPT.\n\
      \  input_text_key: str | None\n      Key in inputs for the input text. Defaults\
      \ to \"input_text\".\n  input_entity_spec_key: str | None\n      Key for the\
      \ entity specifications. Defaults to \"entity_specs\".\n  input_claim_description_key:\
      \ str | None\n      Key for the claim description. Defaults to \"claim_description\"\
      .\n  input_resolved_entities_key: str | None\n      Key for resolved entities.\
      \ Defaults to \"resolved_entities\".\n  tuple_delimiter_key: str | None\n  \
      \    Key for the tuple delimiter. Defaults to \"tuple_delimiter\".\n  record_delimiter_key:\
      \ str | None\n      Key for the record delimiter. Defaults to \"record_delimiter\"\
      .\n  completion_delimiter_key: str | None\n      Key for the completion delimiter.\
      \ Defaults to \"completion_delimiter\".\n  max_gleanings: int | None\n     \
      \ Maximum number of gleanings to perform. If None, uses graphrag_config_defaults.extract_claims.max_gleanings.\n\
      \  on_error: ErrorHandlerFn | None\n      Error handler function. If None, a\
      \ no-op handler is used.\nReturns:\n  None"
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.pull_field
    name: pull_field
    signature: 'def pull_field(index: int, fields: list[str]) -> str | None'
    docstring: "\"\"\"Pull a field from a list of strings by index.\n\nArgs:\n   \
      \ index: The position of the field to extract from fields.\n    fields: The\
      \ list of string fields from which to pull the value.\n\nReturns:\n    str |\
      \ None: The trimmed field at the given index if present; otherwise None.\n\"\
      \"\""
  - node_id: graphrag/index/operations/extract_covariates/claim_extractor.py::ClaimExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self, inputs: dict[str, Any], prompt_variables:\
      \ dict | None = None\n    ) -> ClaimExtractorResult"
    docstring: "Process a collection of input texts to extract claims and return the\
      \ structured results.\n\nArgs:\n  inputs: dict[str, Any] - The inputs containing\
      \ the texts to process and related fields such as input_text key, entity specs,\
      \ and claim descriptions.\n  prompt_variables: dict | None - Optional mapping\
      \ of prompt variables to customize extraction prompts and delimiters. If None,\
      \ defaults are used.\n\nReturns:\n  ClaimExtractorResult - The result object\
      \ containing:\n    output: list[dict] - The cleaned claim dictionaries.\n  \
      \  source_docs: dict[str, str] - Mapping from document_id to the original text\
      \ for each processed document.\n\nRaises:\n  KeyError - If required input keys\
      \ are missing from inputs."
- class_id: graphrag/language_model/response/base.pyi::ModelOutput
  file: graphrag/language_model/response/base.pyi
  name: ModelOutput
  methods:
  - node_id: graphrag/language_model/response/base.pyi::ModelOutput.full_response
    name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
    docstring: "Return the full response payload as a dictionary, or None if not available.\n\
      \nArgs:\n    self: The instance of the class.\n\nReturns:\n    dict[str, Any]\
      \ | None: The full response payload as a dictionary, or None if not present."
  - node_id: graphrag/language_model/response/base.pyi::ModelOutput.content
    name: content
    signature: def content(self) -> str
    docstring: "Content of the model output as a string.\n\nReturns:\n    str: The\
      \ content of the model output as a string."
- class_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore
  file: tests/unit/query/context_builder/test_entity_extraction.py
  name: MockBaseVectorStore
  methods:
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.connect
    name: connect
    signature: 'def connect(self, **kwargs: Any) -> None'
    docstring: "Connect to the vector store.\n\nArgs:\n    kwargs: Arbitrary keyword\
      \ arguments.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError: Raised\
      \ to indicate the method is not implemented."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.__init__
    name: __init__
    signature: 'def __init__(self, documents: list[VectorStoreDocument]) -> None'
    docstring: "Initialize the MockBaseVectorStore with the provided documents for\
      \ testing.\n\nArgs:\n    documents: list[VectorStoreDocument] - Documents to\
      \ initialize the mock vector store with.\n\nReturns:\n    None"
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.filter_by_id
    name: filter_by_id
    signature: 'def filter_by_id(self, include_ids: list[str] | list[int]) -> Any'
    docstring: "Filter vector store documents by a set of IDs.\n\nArgs:\n    include_ids:\
      \ list[str] | list[int] - IDs to include when filtering.\n\nReturns:\n    list[VectorStoreDocument]\
      \ - The documents from self.documents whose id is in include_ids."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_text
    name: similarity_search_by_text
    signature: "def similarity_search_by_text(\n        self, text: str, text_embedder:\
      \ TextEmbedder, k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Performs a length-based similarity search over stored documents (embeddings\
      \ are ignored).\n\nArgs:\n    text (str): The query text to compare against\
      \ documents by their character length.\n    text_embedder (TextEmbedder): Provided\
      \ for API compatibility but not used by this implementation; embeddings are\
      \ not computed.\n    k (int): The number of top results to return. Defaults\
      \ to 10.\n    **kwargs (Any): Additional keyword arguments passed to the underlying\
      \ search implementation (not used).\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ A list of VectorStoreSearchResult objects sorted by increasing score; the\
      \ score is the absolute difference between the length of the input text and\
      \ the document's text length. If document.text is None, it is treated as an\
      \ empty string. Only the top k results are returned.\n\nRaises:\n    Not specified.\
      \ This method does not document any specific exceptions."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.load_documents
    name: load_documents
    signature: "def load_documents(\n        self, documents: list[VectorStoreDocument],\
      \ overwrite: bool = True\n    ) -> None"
    docstring: "Load documents into the vector store.\n\nArgs:\n    documents: list[VectorStoreDocument]\
      \ - List of VectorStoreDocument objects to load into the vector store.\n   \
      \ overwrite: bool - If True, overwrite existing data in the vector store; otherwise,\
      \ preserve existing data.\n\nReturns:\n    None\n\nRaises:\n    NotImplementedError:\
      \ load_documents method not implemented."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.similarity_search_by_vector
    name: similarity_search_by_vector
    signature: "def similarity_search_by_vector(\n        self, query_embedding: list[float],\
      \ k: int = 10, **kwargs: Any\n    ) -> list[VectorStoreSearchResult]"
    docstring: "Return the top-k documents using a deterministic mock search that\
      \ ignores the query embedding. This implementation does not perform real vector\
      \ similarity; instead it returns the first k documents with a fixed score of\
      \ 1. If k exceeds the number of available documents, all documents up to that\
      \ limit are returned.\n\nArgs:\n    self: The instance of the class containing\
      \ the documents.\n    query_embedding: list[float] The embedding vector to search\
      \ with. This value is ignored in the current implementation.\n    k: int The\
      \ number of top results to return.\n    **kwargs: Any additional keyword arguments\
      \ that may influence compatibility but are not used.\n\nReturns:\n    list[VectorStoreSearchResult]:\
      \ The top-k search results. Each result contains the associated document and\
      \ a fixed score of 1."
  - node_id: tests/unit/query/context_builder/test_entity_extraction.py::MockBaseVectorStore.search_by_id
    name: search_by_id
    signature: 'def search_by_id(self, id: str) -> VectorStoreDocument'
    docstring: "Return the first document with its id set to the provided id.\n\n\
      Args:\n    id (str): The identifier to assign to the first stored document before\
      \ returning.\n\nReturns:\n    VectorStoreDocument: The document whose id is\
      \ set to the provided id."
- class_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource
  file: unified-search-app/app/knowledge_loader/data_sources/typing.py
  name: Datasource
  methods:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read_settings
    name: read_settings
    signature: 'def read_settings(self, file: str) -> GraphRagConfig | None'
    docstring: "Read settings for a datasource from a file.\n\nArgs:\n    file: Path\
      \ to the settings file.\n\nReturns:\n    GraphRagConfig | None: The GraphRagConfig\
      \ read from the file, or None if no settings could be read.\n\nRaises:\n   \
      \ NotImplementedError: If the method is not implemented by a subclass."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "Read method for a datasource.\n\nArgs:\n    table: str - The name\
      \ of the table to read from.\n    throw_on_missing: bool - If True, raise an\
      \ error when the table is missing; otherwise, handle gracefully.\n    columns:\
      \ list[str] | None - Specific columns to read from the table, or None to read\
      \ all columns.\n\nReturns:\n    pd.DataFrame - The data read from the specified\
      \ table.\n\nRaises:\n    NotImplementedError - If the method is not implemented."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.__call__
    name: __call__
    signature: 'def __call__(self, table: str, columns: list[str] | None) -> pd.DataFrame'
    docstring: "Call method definition for a datasource to retrieve a DataFrame for\
      \ the given table and optional columns.\n\nArgs:\n  table: name of the table\
      \ to query\n  columns: optional list of column names to include; if None, all\
      \ columns are returned\n\nReturns:\n  pd.DataFrame: the DataFrame resulting\
      \ from the call\n\nRaises:\n  NotImplementedError"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.write
    name: write
    signature: "def write(\n        self, table: str, df: pd.DataFrame, mode: WriteMode\
      \ | None = None\n    ) -> None"
    docstring: "Write data to a table.\n\nArgs:\n    table (str): The name of the\
      \ target table.\n    df (pd.DataFrame): The data to write to the table.\n  \
      \  mode (WriteMode | None): The write mode to use; None indicates default behavior.\n\
      \nReturns:\n    None\n\nRaises:\n    NotImplementedError"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/typing.py::Datasource.has_table
    name: has_table
    signature: 'def has_table(self, table: str) -> bool'
    docstring: "\"\"\"Check if table exists.\n\nArgs:\n    table: The name of the\
      \ table to check for existence.\n\nReturns:\n    bool: True if the table exists,\
      \ otherwise False.\n\nRaises:\n    NotImplementedError: If the method is not\
      \ implemented.\n\"\"\""
- class_id: graphrag/language_model/factory.py::ModelFactory
  file: graphrag/language_model/factory.py
  name: ModelFactory
  methods:
  - node_id: graphrag/language_model/factory.py::ModelFactory.register_embedding
    name: register_embedding
    signature: "def register_embedding(\n        cls, model_type: str, creator: Callable[...,\
      \ EmbeddingModel]\n    ) -> None"
    docstring: "Register an EmbeddingModel implementation.\n\nStores the given creator\
      \ in the internal _embedding_registry mapping for the specified model_type.\n\
      \nArgs:\n    model_type: The type identifier for the EmbeddingModel to register.\n\
      \    creator: A callable that returns an EmbeddingModel instance when invoked.\n\
      \nReturns:\n    None"
  - node_id: graphrag/language_model/factory.py::ModelFactory.get_embedding_models
    name: get_embedding_models
    signature: def get_embedding_models(cls) -> list[str]
    docstring: "Get the registered EmbeddingModel implementations.\n\nArgs:\n    cls:\
      \ The class that maintains the _embedding_registry mapping of EmbeddingModel\
      \ implementations.\n\nReturns:\n    list[str]: A list of the registered EmbeddingModel\
      \ implementation names."
  - node_id: graphrag/language_model/factory.py::ModelFactory.create_chat_model
    name: create_chat_model
    signature: 'def create_chat_model(cls, model_type: str, **kwargs: Any) -> ChatModel'
    docstring: "Create a ChatModel instance from a registered implementation.\n\n\
      Args:\n    model_type: The type of ChatModel to create.\n    **kwargs: Additional\
      \ keyword arguments for the ChatModel constructor.\n\nReturns:\n    A ChatModel\
      \ instance.\n\nRaises:\n    ValueError: If the provided model_type is not registered."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_model
    name: is_supported_model
    signature: 'def is_supported_model(cls, model_type: str) -> bool'
    docstring: "Determine whether the provided model_type is supported by any registered\
      \ model backends (chat or embedding).\n\nArgs:\n    model_type: The type of\
      \ model to check.\n\nReturns:\n    bool: True if the model_type is registered\
      \ as either a chat model or an embedding model; otherwise False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_chat_model
    name: is_supported_chat_model
    signature: 'def is_supported_chat_model(cls, model_type: str) -> bool'
    docstring: "Check if the given chat model type is supported by registered chat\
      \ model implementations.\n\nArgs:\n    cls: type The class reference (classmethod\
      \ parameter).\n    model_type: str The type identifier for the chat model to\
      \ check.\n\nReturns:\n    bool: True if model_type is registered as a chat model,\
      \ otherwise False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.create_embedding_model
    name: create_embedding_model
    signature: 'def create_embedding_model(cls, model_type: str, **kwargs: Any) ->
      EmbeddingModel'
    docstring: "Create an EmbeddingModel instance.\n\nArgs:\n    model_type: str The\
      \ type of EmbeddingModel to create.\n    **kwargs: Any Additional keyword arguments\
      \ for the EmbeddingModel constructor.\n\nReturns:\n    EmbeddingModel: The EmbeddingModel\
      \ instance.\n\nRaises:\n    ValueError: If the provided model_type is not registered."
  - node_id: graphrag/language_model/factory.py::ModelFactory.is_supported_embedding_model
    name: is_supported_embedding_model
    signature: 'def is_supported_embedding_model(cls, model_type: str) -> bool'
    docstring: "Check if the given embedding model type is supported.\n\nArgs:\n \
      \   cls: type The class reference (classmethod parameter).\n    model_type:\
      \ str The type identifier for the embedding model to check.\n\nReturns:\n  \
      \  bool: True if model_type is registered in the embedding registry, otherwise\
      \ False."
  - node_id: graphrag/language_model/factory.py::ModelFactory.get_chat_models
    name: get_chat_models
    signature: def get_chat_models(cls) -> list[str]
    docstring: "Get the registered ChatModel implementations.\n\nArgs:\n    cls: The\
      \ class that maintains the _chat_registry mapping of ChatModel implementations.\n\
      \nReturns:\n    list[str]: A list of the registered ChatModel implementation\
      \ names."
  - node_id: graphrag/language_model/factory.py::ModelFactory.register_chat
    name: register_chat
    signature: 'def register_chat(cls, model_type: str, creator: Callable[..., ChatModel])
      -> None'
    docstring: "Register a ChatModel implementation in the registry.\n\nRegisters\
      \ a ChatModel implementation in the internal _chat_registry mapping for the\
      \ specified model_type.\n\nArgs:\n    cls: The ModelFactory class.\n    model_type:\
      \ str\n        The unique identifier for the ChatModel implementation to register.\n\
      \    creator: Callable[..., ChatModel]\n        A callable that returns a ChatModel\
      \ instance when invoked.\n\nReturns:\n    None"
- class_id: tests/integration/language_model/test_factory.py::CustomChatModel
  file: tests/integration/language_model/test_factory.py
  name: CustomChatModel
  methods:
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat
    name: achat
    signature: "def achat(\n            self, prompt: str, history: list | None =\
      \ None, **kwargs: Any\n        ) -> ModelResponse"
    docstring: "Asynchronous achat that returns a fixed content response for the given\
      \ prompt.\n\nArgs:\n    prompt (str): The input prompt. The content is fixed\
      \ and does not depend on the prompt.\n    history (list | None): Optional conversation\
      \ history. May be unused by this placeholder implementation.\n    **kwargs (Any):\
      \ Additional keyword arguments. May be ignored.\n\nReturns:\n    ModelResponse:\
      \ The response for the prompt. The BaseModelResponse contains output with content=\"\
      content\". The output.full_response is None.\n\nRaises:\n    None documented\
      \ for this placeholder implementation.\n\nNotes:\n    This is a placeholder\
      \ implementation. It does not engage a real chat model; it always returns the\
      \ fixed content string."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat
    name: chat
    signature: "def chat(\n            self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n        ) -> ModelResponse"
    docstring: "Process a chat prompt and return the corresponding model response.\n\
      \nArgs:\n    self: The instance of the class that contains this chat method.\n\
      \    prompt (str): The prompt to send to the model.\n    history (list | None):\
      \ Optional chat history to provide context for the model.\n    **kwargs: Additional\
      \ keyword arguments passed to the underlying model.\n\nReturns:\n    ModelResponse:\
      \ The model response object. In this implementation, it is a BaseModelResponse\n\
      \        containing a BaseModelOutput with content set to \"content\" and full_response\
      \ set to {\"key\": \"value\"}."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n            self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n        ) -> Generator[str, None]"
    docstring: "Stream a chat response for the given prompt.\n\nArgs:\n  prompt: str\
      \ \u2014 The text to generate a response for.\n  history: list | None \u2014\
      \ The conversation history.\n  **kwargs: Any \u2014 Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n  Generator[str, None] \u2014 The generator\
      \ that yields strings representing the response."
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n            self, prompt: str, history: list |\
      \ None = None, **kwargs: Any\n        ) -> AsyncGenerator[str, None]"
    docstring: "\"\"\"Stream Chat with the Model using the given prompt.\n\nArgs:\n\
      \  prompt: The prompt to chat with.\n  history: The conversation history.\n\
      \  kwargs: Additional arguments to pass to the Model.\n\nReturns:\n  An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \  Propagates exceptions raised by the underlying model call or streaming response.\n\
      \"\"\""
  - node_id: tests/integration/language_model/test_factory.py::CustomChatModel.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with optional keyword arguments.\n\nThis __init__\
      \ accepts arbitrary keyword arguments (kwargs) but does not store or use them.\
      \ No state is initialized or modified, and no side effects occur.\n\nArgs:\n\
      \  kwargs: dict[str, Any] - keyword arguments provided to the initializer. They\
      \ are ignored.\n\nReturns:\n  None"
- class_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager
  file: graphrag/callbacks/workflow_callbacks_manager.py
  name: WorkflowCallbacksManager
  methods:
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "\"\"\"Execute this callback when a workflow starts.\n\nArgs:\n   \
      \ name (str): The name of the workflow starting.\n    instance (object): The\
      \ workflow instance object associated with this start event.\n\nReturns:\n \
      \   None\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.__init__
    name: __init__
    signature: def __init__(self)
    docstring: 'Initialize a new WorkflowCallbacksManager.


      This manager holds registered WorkflowCallbacks instances in the _callbacks
      list and dispatches relevant lifecycle events to them (pipeline_start, pipeline_end,
      workflow_start, workflow_end, progress). The _callbacks list is initialized
      to an empty list during construction.


      Returns: None (implicit)'
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Dispatch the pipeline_start event to all registered callbacks.\n\n\
      As a manager, this forwards the pipeline_start event to every registered WorkflowCallbacks\
      \ instance that implements a pipeline_start method. The names argument is passed\
      \ unchanged to each callback's pipeline_start.\n\nArgs:\n    names: list[str]\
      \ The names of the pipelines that started.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "Forward progress events to registered callbacks that implement a progress\
      \ method.\n\nThis method iterates over the internal _callbacks collection and\
      \ propagates the\nprovided Progress event to each callback by invoking its progress\
      \ method when\npresent.\n\nArgs:\n    progress: Progress object representing\
      \ the current progress event.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.register
    name: register
    signature: 'def register(self, callbacks: WorkflowCallbacks) -> None'
    docstring: "Register a new WorkflowCallbacks instance.\n\nAppends the provided\
      \ WorkflowCallbacks instance to the internal registry (self._callbacks). This\
      \ allows multiple callbacks to be registered and receive lifecycle event callbacks.\
      \ There is no deduplication; registering the same instance more than once will\
      \ result in duplicates in the registry.\n\nArgs:\n    callbacks (WorkflowCallbacks):\
      \ The WorkflowCallbacks instance to register.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/workflow_callbacks_manager.py::WorkflowCallbacksManager.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback when the entire pipeline ends.\n\nArgs:\n  \
      \  results: A list of PipelineRunResult objects representing the results of\
      \ the pipeline runs.\n\nReturns:\n    None..."
- class_id: graphrag/index/operations/layout_graph/typing.py::NodePosition
  file: graphrag/index/operations/layout_graph/typing.py
  name: NodePosition
  methods:
  - node_id: graphrag/index/operations/layout_graph/typing.py::NodePosition.to_pandas
    name: to_pandas
    signature: def to_pandas(self) -> tuple[str, float, float, str, float]
    docstring: "\"\"\"Convert this NodePosition to a pandas-friendly 5-tuple.\n\n\
      Args:\n    self (NodePosition): The NodePosition instance to convert.\n\nReturns:\n\
      \    tuple[str, float, float, str, float]: The tuple containing label, x, y,\
      \ cluster, and size.\n\n\"\"\""
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationTurn
  methods:
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationTurn.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the conversation turn.\n\nArgs:\n\
      \    self: The ConversationTurn instance for which to obtain the string representation.\n\
      \nReturns:\n    str: The string representation of the turn in the format \"\
      <role>: <content>\".\n\nRaises:\n    None"
- class_id: tests/mock_provider.py::MockEmbeddingLLM
  file: tests/mock_provider.py
  name: MockEmbeddingLLM
  methods:
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n  \
      \  text_list: A batch of text inputs to generate embeddings for.\n    **kwargs:\
      \ Additional keyword arguments (e.g., model parameters).\n\nReturns:\n    list[list[float]]:\
      \ A batch of embeddings corresponding to the input texts."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "Batch generate embeddings for a list of input texts.\n\nArgs:\n  text_list:\
      \ A batch of input texts to generate embeddings for.\n  **kwargs: Additional\
      \ keyword arguments (e.g., model parameters).\n\nReturns:\n  list[list[float]]:\
      \ A batch of embeddings corresponding to the input texts."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The\
      \ input text to generate the embedding for.\n    kwargs: Additional keyword\
      \ arguments passed to the embedding model.\n\nReturns:\n    list[float]: A list\
      \ of floating-point numbers representing the embedding.\n\nRaises:\n    This\
      \ function does not raise any exceptions."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n    text: The\
      \ input text to generate the embedding for.\n    kwargs: Additional keyword\
      \ arguments passed to the embedding model.\n\nReturns:\n    list[float]: A list\
      \ of floating-point numbers representing the embedding.\n\nRaises:\n    This\
      \ function does not raise any exceptions...."
  - node_id: tests/mock_provider.py::MockEmbeddingLLM.__init__
    name: __init__
    signature: 'def __init__(self, **kwargs: Any)'
    docstring: "Initialize a mock embedding LLM provider.\n\nArgs:\n    kwargs: Additional\
      \ keyword arguments (ignored).\n\nReturns:\n    None"
- class_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext
  file: graphrag/query/structured_search/basic_search/basic_context.py
  name: BasicSearchContext
  methods:
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext._map_ids
    name: _map_ids
    signature: def _map_ids(self) -> dict[str, str]
    docstring: "Map id to short_id in the text units.\n\nArgs:\n    self (object):\
      \ The instance containing the text_units attribute used to build the mapping.\n\
      \nReturns:\n    dict[str, str]: A mapping from each text unit's id to its short_id.\n\
      \nRaises:\n    AttributeError: If any text unit is missing 'id' or 'short_id'\
      \ attributes."
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        k: int = 10,\n        max_context_tokens:\
      \ int = 12_000,\n        context_name: str = \"Sources\",\n        column_delimiter:\
      \ str = \"|\",\n        text_id_col: str = \"source_id\",\n        text_col:\
      \ str = \"text\",\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "\"\"\"Build the context for the basic search mode.\n\nArgs:\n  query\
      \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
      \ | None): Optional conversation history to consider while constructing the\
      \ context.\n  k (int): Number of top related texts to retrieve.\n  max_context_tokens\
      \ (int): Maximum total number of tokens allowed for the context.\n  context_name\
      \ (str): Name assigned to the context section in the results (default \"Sources\"\
      ).\n  column_delimiter (str): Delimiter used to separate columns in the generated\
      \ context text.\n  text_id_col (str): Name of the column containing text identifiers.\n\
      \  text_col (str): Name of the column containing the text content.\n  **kwargs:\
      \ Additional keyword arguments that may influence how the context is built.\n\
      \nReturns:\n  ContextBuilderResult: The result containing the built context\
      \ for the basic search mode.\n\"\"\""
  - node_id: graphrag/query/structured_search/basic_search/basic_context.py::BasicSearchContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        text_embedder: EmbeddingModel,\n\
      \        text_unit_embeddings: BaseVectorStore,\n        text_units: list[TextUnit]\
      \ | None = None,\n        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = \"id\",\n    )"
    docstring: "Initialize a BasicSearchContext with the provided embedding model\
      \ and text unit embeddings and prepare internal mappings.\n\nArgs:\n    text_embedder:\
      \ EmbeddingModel The embedding model used to embed text for similarity search.\n\
      \    text_unit_embeddings: BaseVectorStore The vector store containing embeddings\
      \ for text units.\n    text_units: list[TextUnit] | None Optional list of text\
      \ units to consider.\n    tokenizer: Tokenizer | None Optional tokenizer to\
      \ use; if not provided, get_tokenizer() is used.\n    embedding_vectorstore_key:\
      \ str Key in the vector store for identifying text units (default: \"id\").\n\
      \nReturns:\n    None\n\nRaises:\n    None"
- class_id: graphrag/logger/factory.py::LoggerFactory
  file: graphrag/logger/factory.py
  name: LoggerFactory
  methods:
  - node_id: graphrag/logger/factory.py::LoggerFactory.create_logger
    name: create_logger
    signature: 'def create_logger(cls, reporting_type: str, kwargs: dict) -> logging.Handler'
    docstring: "Create a logger handler for the requested type using the built-in\
      \ registry.\n\nThis method looks up the given reporting_type in the internal\
      \ registry and invokes the registered\ncreator with the provided kwargs to create\
      \ and return a logging.Handler instance.\n\nArgs:\n    reporting_type (str):\
      \ The type identifier of the logger/handler to create.\n    kwargs (dict): Keyword\
      \ arguments forwarded to the registered creator to configure the handler.\n\n\
      Returns:\n    logging.Handler: The configured handler instance for the requested\
      \ type.\n\nRaises:\n    ValueError: If the reporting_type is not registered\
      \ in the registry."
  - node_id: graphrag/logger/factory.py::LoggerFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, reporting_type: str) -> bool'
    docstring: "Check if the given logger type is supported.\n\nArgs:\n    cls: The\
      \ class reference (classmethod parameter).\n    reporting_type (str): The type\
      \ identifier for the logger.\n\nReturns:\n    bool: True if the reporting type\
      \ is registered in the registry, False otherwise."
  - node_id: graphrag/logger/factory.py::LoggerFactory.register
    name: register
    signature: "def register(\n        cls, reporting_type: str, creator: Callable[...,\
      \ logging.Handler]\n    ) -> None"
    docstring: "Register a custom logger implementation.\n\nThis is a classmethod\
      \ on LoggerFactory. It updates the internal registry (cls._registry) by storing\
      \ a mapping from the provided reporting_type to the given creator callable.\
      \ The registry is consulted by create_logger to instantiate loggers for the\
      \ requested type.\n\nArgs:\n    reporting_type: The type identifier for the\
      \ logger.\n    creator: A class or callable that initializes and returns a logging.Handler\
      \ instance.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/logger/factory.py::LoggerFactory.get_logger_types
    name: get_logger_types
    signature: def get_logger_types(cls) -> list[str]
    docstring: "\"\"\"Get the registered logger implementations.\n\nArgs:\n    cls:\
      \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
      \ The list of registered logger implementation names.\n\"\"\""
- class_id: graphrag/query/question_gen/base.py::BaseQuestionGen
  file: graphrag/query/question_gen/base.py
  name: BaseQuestionGen
  methods:
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.generate
    name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate questions.\n\nArgs:\n    question_history: History of previously\
      \ generated questions.\n    context_data: Optional context data used to influence\
      \ generation; None if unavailable.\n    question_count: Number of questions\
      \ to generate.\n    kwargs: Additional keyword arguments for extensibility.\n\
      \nReturns:\n    QuestionResult: The generated results including the response\
      \ list, context_data, completion_time, llm_calls, and prompt_tokens.\n\nRaises:\n\
      \    NotImplementedError: If the method is not implemented by a subclass."
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.agenerate
    name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "\"\"\"Generate questions asynchronously.\"\"\"\n\nArgs:\n    question_history:\
      \ History of previously asked questions.\n    context_data: Optional context\
      \ data used to influence generation; None if unavailable.\n    question_count:\
      \ Number of questions to generate.\n    kwargs: Additional keyword arguments\
      \ for extensibility.\n\nReturns:\n    QuestionResult: The generated result containing\
      \ the questions and related data.\n\""
  - node_id: graphrag/query/question_gen/base.py::BaseQuestionGen.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ GlobalContextBuilder | LocalContextBuilder,\n        tokenizer: Tokenizer\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize a Base Question Gen with the provided model and context\
      \ builder.\n\nArgs:\n    model (ChatModel): The language model interface used\
      \ for this base question generator.\n    context_builder (GlobalContextBuilder\
      \ | LocalContextBuilder): The builder that constructs the context for the questions.\n\
      \    tokenizer (Tokenizer | None): Optional tokenizer to use. If None, a tokenizer\
      \ appropriate for the model will be selected.\n    model_params (dict[str, Any]\
      \ | None): Optional parameters for the language model. If None, defaults to\
      \ an empty dict.\n    context_builder_params (dict[str, Any] | None): Optional\
      \ parameters for the context builder. If None, defaults to an empty dict.\n\n\
      Returns:\n    None"
- class_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel
  file: tests/integration/language_model/test_factory.py
  name: CustomEmbeddingModel
  methods:
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n            self, text_list: list[str], **kwargs\n\
      \        ) -> list[list[float]]"
    docstring: "Asynchronously compute embeddings for a batch of input texts.\n\n\
      Args:\n    text_list: A batch of text inputs to generate embeddings for.\n\n\
      Returns:\n    list[list[float]]: A batch of embeddings corresponding to the\
      \ input texts."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "Asynchronously generate an embedding for the input text.\n\nArgs:\n\
      \  text: The input text to generate the embedding for.\n  kwargs: Additional\
      \ keyword arguments passed to the embedding model.\n\nReturns:\n  list[float]:\
      \ A list of floating-point numbers representing the embedding.\n\nRaises:\n\
      \  This function does not raise any exceptions."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "Compute embeddings for a batch of input texts.\n\nArgs:\n    text_list:\
      \ list[str] - A batch of text inputs to generate embeddings for.\n    **kwargs:\
      \ Any - Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    list[list[float]] - A batch of embeddings corresponding to the input texts."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "Generate an embedding for the input text.\n\nArgs:\n  text: The input\
      \ text to generate the embedding for.\n  kwargs: Additional keyword arguments\
      \ passed to the embedding model.\n\nReturns:\n  list[float]: A list of floating-point\
      \ numbers representing the embedding.\n\nRaises:\n  This function does not raise\
      \ any exceptions...."
  - node_id: tests/integration/language_model/test_factory.py::CustomEmbeddingModel.__init__
    name: __init__
    signature: def __init__(self, **kwargs)
    docstring: "Initialize the instance with arbitrary keyword arguments; no initialization\
      \ is performed.\n\nArgs:\n  kwargs: dict[str, Any] - keyword arguments provided\
      \ to the initializer. They are ignored.\n\nReturns:\n  None\n\nRaises:\n  None..."
- class_id: graphrag/config/models/storage_config.py::StorageConfig
  file: graphrag/config/models/storage_config.py
  name: StorageConfig
  methods:
  - node_id: graphrag/config/models/storage_config.py::StorageConfig.validate_base_dir
    name: validate_base_dir
    signature: def validate_base_dir(cls, value, info)
    docstring: "Normalize base_dir to a filesystem path string for local storage.\n\
      \nThis validator does not verify that the path exists or is valid beyond conversion\
      \ to a string. It only performs normalization when the storage type is local\
      \ (StorageType.file); for all other storage types, the input value is returned\
      \ unchanged.\n\nArgs:\n    cls (type): The class that defines the validator.\n\
      \    value (Any): The input base_dir value to normalize.\n    info (object):\
      \ Validation context containing other field values, including the 'type' field.\n\
      \nReturns:\n    str: The normalized path string when using local storage; otherwise\
      \ returns the input value.\n\nRaises:\n    None"
- class_id: unified-search-app/app/state/session_variable.py::SessionVariable
  file: unified-search-app/app/state/session_variable.py
  name: SessionVariable
  methods:
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__init__
    name: __init__
    signature: 'def __init__(self, default: Any = "", prefix: str = "")'
    docstring: "Create a managed session variable with a default value and an optional\
      \ prefix.\n\nThe prefix is used to avoid collisions between variables with the\
      \ same name.\n\nArgs:\n    default: The initial/default value for the session\
      \ variable.\n    prefix: Optional prefix to prepend to the key to differentiate\
      \ this variable from others with the same name.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.value
    name: value
    signature: 'def value(self, value: Any) -> None'
    docstring: "Set the session variable value.\n\nArgs:\n    value: The new value\
      \ to assign to the session variable. Can be of any type.\n\nReturns:\n    None"
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.__repr__
    name: __repr__
    signature: def __repr__(self) -> Any
    docstring: "Return a string representation of the managed session variable value.\n\
      \nArgs:\n  self: The instance of the class containing the key used to index\
      \ session_state.\n\nReturns:\n  str: The string representation of the value\
      \ stored in st.session_state for this key.\n\nRaises:\n  KeyError: If the key\
      \ is not present in st.session_state."
  - node_id: unified-search-app/app/state/session_variable.py::SessionVariable.key
    name: key
    signature: def key(self) -> str
    docstring: "Key property that returns the session_state key for this variable.\n\
      \nReturns:\n    str: The key used to access the session_state dictionary for\
      \ this variable."
- class_id: graphrag/data_model/text_unit.py::TextUnit
  file: graphrag/data_model/text_unit.py
  name: TextUnit
  methods:
  - node_id: graphrag/data_model/text_unit.py::TextUnit.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        text_key: str = \"text\",\n        entities_key: str = \"entity_ids\"\
      ,\n        relationships_key: str = \"relationship_ids\",\n        covariates_key:\
      \ str = \"covariate_ids\",\n        n_tokens_key: str = \"n_tokens\",\n    \
      \    document_ids_key: str = \"document_ids\",\n        attributes_key: str\
      \ = \"attributes\",\n    ) -> \"TextUnit\""
    docstring: "Create a new TextUnit from the dict data.\n\nArgs:\n    cls: The class.\n\
      \    d (dict[str, Any]): The source dictionary containing the values for the\
      \ TextUnit fields.\n    id_key (str): Key in d for the text unit's identifier.\
      \ Defaults to \"id\".\n    short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n    text_key (str): Key in\
      \ d for the text content. Defaults to \"text\".\n    entities_key (str): Key\
      \ in d for the related entity IDs. Defaults to \"entity_ids\".\n    relationships_key\
      \ (str): Key in d for the related relationship IDs. Defaults to \"relationship_ids\"\
      .\n    covariates_key (str): Key in d for covariate IDs. Defaults to \"covariate_ids\"\
      .\n    n_tokens_key (str): Key in d for the number of tokens. Defaults to \"\
      n_tokens\".\n    document_ids_key (str): Key in d for the document IDs. Defaults\
      \ to \"document_ids\".\n    attributes_key (str): Key in d for additional attributes.\
      \ Defaults to \"attributes\".\n\nReturns:\n    TextUnit: A new TextUnit instance\
      \ populated with values from d.\n\nRaises:\n    KeyError: If id_key is not present\
      \ in d."
- class_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer
  file: tests/unit/indexing/text_splitting/test_text_splitting.py
  name: MockTokenizer
  methods:
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.encode
    name: encode
    signature: def encode(self, text)
    docstring: "Encode the input text as a list of Unicode code points.\n\nArgs:\n\
      \    text: str\n        The input text to encode as Unicode code points.\n\n\
      Returns:\n    list[int]\n        A list of integers where each integer is the\
      \ Unicode code point of the\n        corresponding character in text.\n\nRaises:\n\
      \    TypeError:\n        If text is None or not iterable."
  - node_id: tests/unit/indexing/text_splitting/test_text_splitting.py::MockTokenizer.decode
    name: decode
    signature: def decode(self, token_ids)
    docstring: "Decode token ids to string by converting each integer to a character\
      \ and concatenating.\n\nArgs:\n    token_ids: An iterable of integers representing\
      \ token IDs to decode into a string.\n\nReturns:\n    str: The decoded string.\n\
      \nRaises:\n    TypeError: If token_ids contains non-integer elements or elements\
      \ cannot be processed by chr.\n    ValueError: If token_id is outside the valid\
      \ range for chr (0 <= id <= 0x10FFFF)."
- class_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: OpenAIChatFNLLM
  methods:
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    docstring: "\"\"\"\nStream Chat with the Model asynchronously using the given\
      \ prompt.\n\nThis is an asynchronous generator that streams chunks of the model's\
      \ response as they become available. Each yielded value is a non-None string\
      \ from response.output.content.\n\nArgs:\n    prompt (str): The prompt to chat\
      \ with.\n    history (list[str] | None): Optional history to pass to the Model.\
      \ If provided, the model will consider this history when generating streamed\
      \ output.\n    kwargs: Additional keyword arguments to pass to the Model. (type:\
      \ dict[str, Any])\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous\
      \ generator yielding the streamed response chunks as non-None strings.\n\"\"\
      \""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The chat history to include in the\
      \ chat, or None for no history.\n    kwargs: Additional arguments to pass to\
      \ the Model.\n\nReturns:\n    ModelResponse: The response from the Model.\n\n\
      Raises:\n    Exception: Exceptions raised by the underlying model call are propagated."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    docstring: "\"\"\"\nStream Chat with the Model using the given prompt.\n\nArgs:\n\
      \    prompt (str): The prompt to chat with.\n    history (list[str] | None):\
      \ Optional history to pass to the Model. If provided, the model will consider\
      \ it.\n    kwargs (dict[str, Any], optional): Additional keyword arguments to\
      \ pass to the Model.\n\nReturns:\n    AsyncGenerator[str, None]: An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying model call or streaming\
      \ response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    docstring: "Chat with the Model using the given prompt.\n\nParameters:\n    prompt\
      \ (str): The prompt to chat with.\n    history (list | None): The conversation\
      \ history to include in the chat, or None for no history.\n    kwargs: Additional\
      \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
      \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
      \ model call are propagated to the caller."
  - node_id: graphrag/language_model/providers/fnllm/models.py::OpenAIChatFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an OpenAI Chat FNLLM provider using the given configuration\
      \ and optional components.\n\nArgs:\n    name: str\n        The name to assign\
      \ to the internal cache provider and model instance.\n    config: LanguageModelConfig\n\
      \        The configuration used to derive the OpenAI configuration.\n    callbacks:\
      \ WorkflowCallbacks | None\n        Optional WorkflowCallbacks; if provided,\
      \ an error handler will be created and used.\n    cache: PipelineCache | None\n\
      \        Optional PipelineCache to back the model cache provider.\n\nReturns:\n\
      \    None"
- class_id: graphrag/config/errors.py::ApiKeyMissingError
  file: graphrag/config/errors.py
  name: ApiKeyMissingError
  methods:
  - node_id: graphrag/config/errors.py::ApiKeyMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str, azure_auth_type: str | None = None)
      -> None'
    docstring: "Init method for ApiKeyMissingError (internal API).\n\nArgs:\n    llm_type:\
      \ The LLM type for which the API Key is required.\n    azure_auth_type: Optional\
      \ Azure authentication type; if provided, include in the message.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
- class_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry
  file: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py
  name: ExponentialRetry
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        base_delay: float = 2.0,\n        jitter: bool = True,\n   \
      \     **kwargs: Any,\n    )"
    docstring: "Initialize a LiteLLM Exponential Retry Service with retry configuration.\n\
      \nArgs:\n    max_retries: The maximum number of retry attempts (int). Must be\
      \ greater than 0.\n    base_delay: The base delay between retries in seconds\
      \ (float). Must be greater than 1.0.\n    jitter: Whether to apply jitter to\
      \ the delay (bool).\n    kwargs: Additional keyword arguments (Any).\n\nReturns:\n\
      \    None\n\nRaises:\n    ValueError: max_retries must be greater than 0.\n\
      \    ValueError: base_delay must be greater than 1.0."
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function using exponential backoff.\n\nRetries\
      \ the provided function on failure up to the configured max_retries with an\
      \ exponential backoff delay. The initial delay is 1.0 second and increases by\
      \ the base_delay factor; if jitter is enabled, a small random amount is added.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs) and its return value will be returned on success.\n    kwargs:\
      \ Any - Keyword arguments to pass to func when calling it.\n\nReturns:\n   \
      \ Any: The value returned by func when it succeeds.\n\nRaises:\n    Exception:\
      \ The last exception raised by func when the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/exponential_retry.py::ExponentialRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function.\n\nArgs:\n  func: The asynchronous\
      \ function to retry. (Callable[..., Awaitable[Any]])\n  kwargs: Additional keyword\
      \ arguments to pass to the function. (Any)\n\nReturns:\n  Any: The result of\
      \ the awaited function.\n\nRaises:\n  Exception: If the wrapped function keeps\
      \ raising and the maximum number of retries is exceeded."
- class_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey
  file: graphrag/query/context_builder/entity_extraction.py
  name: EntityVectorStoreKey
  methods:
  - node_id: graphrag/query/context_builder/entity_extraction.py::EntityVectorStoreKey.from_string
    name: from_string
    signature: 'def from_string(value: str) -> "EntityVectorStoreKey"'
    docstring: "\"\"\"Convert string to EntityVectorStoreKey.\n\nArgs:\n    value:\
      \ str. The string key to convert. Expected to be \"id\" or \"title\".\n\nReturns:\n\
      \    EntityVectorStoreKey: The corresponding enum member (EntityVectorStoreKey.ID\
      \ for \"id\", EntityVectorStoreKey.TITLE for \"title\").\n\nRaises:\n    ValueError:\
      \ If value is not a valid EntityVectorStoreKey (i.e., not \"id\" or \"title\"\
      ).\n\"\"\""
- class_id: graphrag/vector_stores/factory.py::VectorStoreFactory
  file: graphrag/vector_stores/factory.py
  name: VectorStoreFactory
  methods:
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.create_vector_store
    name: create_vector_store
    signature: "def create_vector_store(\n        cls,\n        vector_store_type:\
      \ str,\n        vector_store_schema_config: VectorStoreSchemaConfig,\n     \
      \   **kwargs: dict,\n    ) -> BaseVectorStore"
    docstring: "Create a vector store object from the provided type via a registry\
      \ lookup.\n\nThis function looks up the registered vector store implementation\
      \ by vector_store_type and instantiates it by passing vector_store_schema_config\
      \ and any additional keyword arguments to the concrete vector store constructor.\
      \ The concrete vector store may require or accept additional kwargs; these are\
      \ forwarded via kwargs.\n\nArgs:\n    vector_store_type (str): The type identifier\
      \ for the vector store to create.\n    vector_store_schema_config (VectorStoreSchemaConfig):\
      \ Configuration describing the vector store schema; it is forwarded to the concrete\
      \ vector store implementation.\n    **kwargs: Additional keyword arguments for\
      \ the concrete vector store constructor.\n\nReturns:\n    BaseVectorStore: A\
      \ vector store instance.\n\nRaises:\n    ValueError: If the vector store type\
      \ is not registered."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.get_vector_store_types
    name: get_vector_store_types
    signature: def get_vector_store_types(cls) -> list[str]
    docstring: "Get the registered vector store implementations.\n\nArgs:\n    cls:\
      \ The class on which this classmethod is invoked.\n\nReturns:\n    list[str]:\
      \ The list of registered vector store type keys (i.e., the keys of cls._registry)."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.is_supported_type
    name: is_supported_type
    signature: 'def is_supported_type(cls, vector_store_type: str) -> bool'
    docstring: "Check if the given vector store type is supported.\n\nArgs:\n    cls:\
      \ type The class reference (classmethod parameter).\n    vector_store_type:\
      \ str The type identifier for the vector store.\n\nReturns:\n    bool: True\
      \ if vector_store_type is registered in the registry, False otherwise."
  - node_id: graphrag/vector_stores/factory.py::VectorStoreFactory.register
    name: register
    signature: "def register(\n        cls, vector_store_type: str, creator: Callable[...,\
      \ BaseVectorStore]\n    ) -> None"
    docstring: "Register a custom vector store implementation.\n\nStores the provided\
      \ creator in the internal registry under the given vector_store_type. The registration\n\
      does not enforce any factory semantics; the creator is stored as-is and will\
      \ be invoked at runtime by\nVectorStoreFactory.create_vector_store with vector_store_schema_config\
      \ and any additional keyword arguments.\n\nArgs:\n    vector_store_type (str):\
      \ The type identifier for the vector store.\n    creator (Callable[..., BaseVectorStore]):\
      \ A class or callable that creates an instance of BaseVectorStore. The creator\n\
      \        is registered as-is and is invoked later with vector_store_schema_config\
      \ and kwargs.\n\nReturns:\n    None"
- class_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder
  file: graphrag/query/structured_search/drift_search/drift_context.py
  name: DRIFTSearchContextBuilder
  methods:
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.init_local_context_builder
    name: init_local_context_builder
    signature: def init_local_context_builder(self) -> LocalSearchMixedContext
    docstring: "Initialize and return the local search mixed context built from the\
      \ current DRIFT context attributes.\n\nArgs:\n    self: The DRIFT drift context\
      \ builder instance.\n\nReturns:\n    LocalSearchMixedContext: The initialized\
      \ local context constructed from the builder's state, including:\n      - community_reports\n\
      \      - text_units\n      - entities\n      - relationships\n      - covariates\n\
      \      - entity_text_embeddings\n      - embedding_vectorstore_key\n      -\
      \ text_embedder\n      - tokenizer\n\nRaises:\n    None\n\nExamples:\n    context\
      \ = self.init_local_context_builder()"
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self, query: str, **kwargs\n    ) -> tuple[pd.DataFrame,\
      \ dict[str, int]]"
    docstring: "Build DRIFT search context.\n\nArgs\n  query: str\n      Search query\
      \ string.\n\nReturns\n  tuple[pd.DataFrame, dict[str, int]]: Top-k most similar\
      \ documents, and a dictionary containing the number of LLM calls, and prompts\
      \ and output tokens.\n\nRaises\n  ValueError: If no community reports are available,\
      \ or embeddings are incompatible."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.check_query_doc_encodings
    name: check_query_doc_encodings
    signature: 'def check_query_doc_encodings(query_embedding: Any, embedding: Any)
      -> bool'
    docstring: "Check if the embeddings are compatible for direct comparison.\n\n\
      This function enforces concrete compatibility criteria:\n- Both embeddings are\
      \ non-None\n- They have the same container type (type(query_embedding) == type(embedding))\n\
      - They have the same length (len(query_embedding) == len(embedding))\n- The\
      \ inner element types are the same (type(query_embedding[0]) == type(embedding[0]))\n\
      \nNote: The function expects sequences of homogeneous elements (e.g., lists\
      \ of floats, numpy arrays of floats). It assumes non-empty embeddings. If both\
      \ embeddings have length 0, attempting to access the first element (index 0)\
      \ will raise IndexError.\n\nArgs\n----\nquery_embedding : Any\n    Embedding\
      \ of the query.\nembedding : Any\n    Embedding to compare against.\n\nReturns\n\
      -------\nbool: True if embeddings are compatible, otherwise False.\n\nRaises\n\
      ------\nIndexError\n    If embeddings are empty and the code attempts to access\
      \ the first element.\nTypeError\n    If inputs do not support len() or indexing,\
      \ or are not indexable."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.convert_reports_to_df
    name: convert_reports_to_df
    signature: 'def convert_reports_to_df(reports: list[CommunityReport]) -> pd.DataFrame'
    docstring: "Convert a list of CommunityReport objects to a pandas DataFrame.\n\
      \nArgs:\n    reports (list[CommunityReport]): List of CommunityReport objects.\n\
      \nReturns:\n    pd.DataFrame: DataFrame with report data.\n\nRaises:\n    ValueError:\
      \ If some reports are missing full content or full content embeddings."
  - node_id: graphrag/query/structured_search/drift_search/drift_context.py::DRIFTSearchContextBuilder.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        text_embedder:\
      \ EmbeddingModel,\n        entities: list[Entity],\n        entity_text_embeddings:\
      \ BaseVectorStore,\n        text_units: list[TextUnit] | None = None,\n    \
      \    reports: list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n        config: DRIFTSearchConfig | None =\
      \ None,\n        local_system_prompt: str | None = None,\n        local_mixed_context:\
      \ LocalSearchMixedContext | None = None,\n        reduce_system_prompt: str\
      \ | None = None,\n        response_type: str | None = None,\n    )"
    docstring: "Initialize the DRIFT search context builder with necessary components.\n\
      \nThis constructor wires together the core components required for DRIFT-style\n\
      structured search, including the language model interface, embedding model,\n\
      entity context, prompts, and optional metadata. If some optional inputs are\
      \ not\nprovided, sensible defaults are created.\n\nArgs:\n    model (ChatModel):\
      \ The chat-based language model to drive queries.\n    text_embedder (EmbeddingModel):\
      \ Embedding model used to encode text.\n    entities (list[Entity]): Entities\
      \ present in the current context.\n    entity_text_embeddings (BaseVectorStore):\
      \ Vector store containing entity text embeddings.\n    text_units (list[TextUnit]\
      \ | None): Optional list of TextUnit objects for the context.\n    reports (list[CommunityReport]\
      \ | None): Optional list of CommunityReport objects.\n    relationships (list[Relationship]\
      \ | None): Optional relationships among entities.\n    covariates (dict[str,\
      \ list[Covariate]] | None): Optional covariates keyed by name.\n    tokenizer\
      \ (Tokenizer | None): Optional Tokenizer to use; if None, a tokenizer will be\
      \ created via get_tokenizer().\n    embedding_vectorstore_key (str): Key for\
      \ the embedding vector store; defaults to EntityVectorStoreKey.ID.\n    config\
      \ (DRIFTSearchConfig | None): Optional configuration for DRIFT search behavior;\
      \ if None, a default config is created.\n    local_system_prompt (str | None):\
      \ Optional override for the local system prompt; defaults to DRIFT_LOCAL_SYSTEM_PROMPT.\n\
      \    local_mixed_context (LocalSearchMixedContext | None): Optional prebuilt\
      \ local mixed context; if None, a new one is initialized.\n    reduce_system_prompt\
      \ (str | None): Optional prompt reduction instruction; defaults to DRIFT_REDUCE_PROMPT.\n\
      \    response_type (str | None): Optional specifier for the desired response\
      \ type.\n\nReturns:\n    None\n\nSide effects:\n    - Creates a default DRIFTSearchConfig\
      \ if none is provided.\n    - Creates or retrieves a tokenizer if one is not\
      \ supplied.\n    - Sets default local system and reduction prompts if not provided.\n\
      \    - May initialize a new LocalSearchMixedContext via init_local_context_builder()\
      \ when\n      local_mixed_context is not supplied.\n\nRaises:\n    - Propagates\
      \ any exceptions raised by get_tokenizer() or the LocalSearchMixedContext constructor."
- class_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig
  file: graphrag/config/models/vector_store_schema_config.py
  name: VectorStoreSchemaConfig
  methods:
  - node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model after the initial schema validation.\n\nArgs:\n\
      \    self: The instance being validated.\n\nReturns:\n    The instance after\
      \ validation.\n\nRaises:\n    ValueError: If an unsafe or invalid field name\
      \ is encountered during validation."
  - node_id: graphrag/config/models/vector_store_schema_config.py::VectorStoreSchemaConfig._validate_schema
    name: _validate_schema
    signature: def _validate_schema(self) -> None
    docstring: "\"\"\"Validate the schema.\n\nArgs:\n    self (VectorStoreSchemaConfig):\
      \ The instance containing schema field names to validate.\n\nReturns:\n    None\n\
      \nRaises:\n    ValueError: If any of the id_field, vector_field, text_field,\
      \ or attributes_field contains an unsafe or invalid field name.\n\"\""
- class_id: graphrag/language_model/response/base.py::ModelResponse
  file: graphrag/language_model/response/base.py
  name: ModelResponse
  methods:
  - node_id: graphrag/language_model/response/base.py::ModelResponse.output
    name: output
    signature: def output(self) -> ModelOutput
    docstring: "Return the output of the response. This is a property-like member\
      \ of the response object (no parentheses needed).\n\nReturns:\n    ModelOutput:\
      \ The output associated with this response. The returned object provides:\n\
      \        content: str - The textual content of the output.\n        full_response:\
      \ dict[str, Any] | None - The complete JSON response from the LLM provider,\
      \ or None if not available.\n\nNotes:\n    - The ModelOutput is always produced;\
      \ accessing this property does not raise exceptions in normal operation.\n \
      \   - If content is empty, the content field may be an empty string."
  - node_id: graphrag/language_model/response/base.py::ModelResponse.history
    name: history
    signature: def history(self) -> list
    docstring: "Return the history of the response.\n\nReturns:\n    list[Any]: The\
      \ history of the response."
  - node_id: graphrag/language_model/response/base.py::ModelResponse.parsed_response
    name: parsed_response
    signature: def parsed_response(self) -> T | None
    docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The\
      \ instance of the implementing class providing the parsed_response property.\n\
      \nReturns:\n    T | None: The parsed response, or None if not available.\n\n\
      Raises:\n    None..."
- class_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor
  file: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py
  name: SummarizeExtractor
  methods:
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions_with_llm
    name: _summarize_descriptions_with_llm
    signature: "def _summarize_descriptions_with_llm(\n        self, id: str | tuple[str,\
      \ str] | list[str], descriptions: list[str]\n    )"
    docstring: "Summarize descriptions using a large language model (LLM).\n\nArgs:\n\
      \    id: str | tuple[str, str] | list[str] - Identifier(s) for the entity or\
      \ entities.\n    descriptions: list[str] - Descriptions to be summarized.\n\n\
      Returns:\n    str - The summarized descriptions as a string.\n\nRaises:\n  \
      \  Exception - If the underlying LLM call fails or processing encounters an\
      \ error."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor._summarize_descriptions
    name: _summarize_descriptions
    signature: "def _summarize_descriptions(\n        self, id: str | tuple[str, str],\
      \ descriptions: list[str]\n    ) -> str"
    docstring: "Asynchronously summarize descriptions into a single description.\n\
      \nArgs:\n  id: str | tuple[str, str] - Identifier(s) for the entity or entities.\n\
      \  descriptions: list[str] - Descriptions to be summarized.\n\nReturns:\n  str\
      \ - The summarized descriptions as a string.\n\nRaises:\n  Exception - If the\
      \ underlying LLM call fails or processing encounters an error."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        id: str | tuple[str, str],\n\
      \        descriptions: list[str],\n    ) -> SummarizationResult"
    docstring: "Asynchronously process the given descriptions for the specified id\
      \ and return the summarization result.\n\nArgs:\n  id: str | tuple[str, str]\
      \ - The identifier for the summarization target. It can be a string or a tuple\
      \ of two strings.\n  descriptions: list[str] - The list of description strings\
      \ to summarize. If empty, the resulting description will be an empty string;\
      \ if a single element, that element is returned; otherwise a summarization is\
      \ performed.\n\nReturns:\n  SummarizationResult - An object containing the id\
      \ and the resulting description (description is a string, possibly empty)."
  - node_id: graphrag/index/operations/summarize_descriptions/description_summary_extractor.py::SummarizeExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        max_summary_length: int,\n        max_input_tokens: int,\n        summarization_prompt:\
      \ str | None = None,\n        on_error: ErrorHandlerFn | None = None,\n    )"
    docstring: "Initialize a SummarizeExtractor with the given model invoker and configuration.\n\
      \nArgs:\n    model_invoker (ChatModel): The model invoker used to run prompts.\n\
      \    max_summary_length (int): Maximum length of the summary to produce.\n \
      \   max_input_tokens (int): Maximum number of input tokens to consider for summarization.\n\
      \    summarization_prompt (str | None): Custom prompt to use for summarization.\
      \ If None, defaults to SUMMARIZE_PROMPT.\n    on_error (ErrorHandlerFn | None):\
      \ Optional error handler. If None, a no-op is used.\n\nReturns:\n    None: The\
      \ constructor does not return a value."
- class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: AzureOpenAIEmbeddingFNLLM
  methods:
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs) -> list[float]'
    docstring: "Embed the given text using the Model.\n\nArgs:\n    text: The text\
      \ to embed.\n    kwargs: Additional arguments to pass to the Model.\n\nReturns:\n\
      \    The embeddings of the text.\n\nRaises:\n    ValueError: If no embeddings\
      \ are found in the response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.aembed_batch
    name: aembed_batch
    signature: 'def aembed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list:\
      \ The texts to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
      \nReturns:\n    list[list[float]]: The embeddings for the input texts.\n\nRaises:\n\
      \    ValueError: If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs) -> list[list[float]]'
    docstring: "\"\"\"\nEmbed the given texts using the Model.\n\nArgs:\n    text_list\
      \ (list[str]): The texts to embed.\n    kwargs: Additional arguments to pass\
      \ to the Model.\n\nReturns:\n    list[list[float]]: The embeddings for the input\
      \ texts.\n\nRaises:\n    ValueError: If no embeddings are found in the response.\n\
      \"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs) -> list[float]'
    docstring: "\"\"\"\nEmbed the given text using the Model.\n\nArgs:\n    text (str):\
      \ The text to embed.\n    kwargs: Additional arguments to pass to the Model.\n\
      \nReturns:\n    list[float]: The embeddings of the text.\n\nRaises:\n    ValueError:\
      \ If no embeddings are found in the response.\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIEmbeddingFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an Azure OpenAI Embedding FNLLM provider using the given\
      \ configuration and optional components.\n\nArgs:\n    name (str): The name\
      \ to assign to the internal cache provider and model instance.\n    config (LanguageModelConfig):\
      \ The configuration used to derive the OpenAI configuration.\n    callbacks\
      \ (WorkflowCallbacks | None): Optional WorkflowCallbacks; if provided, an error\
      \ handler will be created to log issues.\n    cache (PipelineCache | None):\
      \ Optional PipelineCache to back the embedding cache.\n\nReturns:\n    None"
- class_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: DRIFTContextBuilder
  methods:
  - node_id: graphrag/query/context_builder/builders.py::DRIFTContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        **kwargs,\n\
      \    ) -> tuple[pd.DataFrame, dict[str, int]]"
    docstring: "Build the context used to prime subsequent search actions for the\
      \ given query.\n\nThis asynchronous method constructs a DataFrame of contextual\
      \ items and a metrics dictionary\nthat can be used to warm up or seed downstream\
      \ DRIFT search processes.\n\nArgs:\n    self: The instance of the class.\n \
      \   query (str): The search query for which to build the context.\n    **kwargs:\
      \ Additional keyword arguments to customize context construction.\n\nReturns:\n\
      \    tuple[pd.DataFrame, dict[str, int]]: A pair where the first element is\
      \ a pandas DataFrame containing\n    the contextual items to be used for downstream\
      \ search (columns and contents vary by\n    implementation but typically include\
      \ the text and related metadata), and the second element is\n    a mapping from\
      \ metric names (strings) to integers representing context-related counters or\
      \ scores\n    produced during construction.\n\nRaises:\n    Exception: Implementation-specific\
      \ errors may be raised during context construction. Callers should\n       \
      \        handle broad exceptions and consider fallback or retry as appropriate."
- class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion
  file: graphrag/language_model/providers/litellm/types.py
  name: AFixedModelCompletion
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelCompletion.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        # Optional OpenAI\
      \ params: see https://platform.openai.com/docs/api-reference/chat/create\n \
      \       messages: list = [],  # type: ignore  # noqa: B006\n        stream:\
      \ bool | None = None,\n        stream_options: dict | None = None,  # type:\
      \ ignore\n        stop=None,  # type: ignore\n        max_completion_tokens:\
      \ int | None = None,\n        max_tokens: int | None = None,\n        modalities:\
      \ list[ChatCompletionModality] | None = None,\n        prediction: ChatCompletionPredictionContentParam\
      \ | None = None,\n        audio: ChatCompletionAudioParam | None = None,\n \
      \       logit_bias: dict | None = None,  # type: ignore\n        user: str |\
      \ None = None,\n        # openai v1.0+ new params\n        response_format:\
      \ dict | type[BaseModel] | None = None,  # type: ignore\n        seed: int |\
      \ None = None,\n        tools: list | None = None,  # type: ignore\n       \
      \ tool_choice: str | dict | None = None,  # type: ignore\n        logprobs:\
      \ bool | None = None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    docstring: "Asynchronous chat completion function for litellm integration; calls\
      \ the OpenAI-compatible chat completion API and supports streaming responses.\n\
      \nArgs:\n    messages: list\n        Chat messages to include in the request.\n\
      \    stream: bool | None\n        If True, stream partial responses as they\
      \ arrive.\n    stream_options: dict | None\n        Options for streaming.\n\
      \    stop: Any\n        Stop sequences for the generation.\n    max_completion_tokens:\
      \ int | None\n        Maximum tokens for the completion.\n    max_tokens: int\
      \ | None\n        Maximum tokens to generate.\n    modalities: list[ChatCompletionModality]\
      \ | None\n        Modality configuration.\n    prediction: ChatCompletionPredictionContentParam\
      \ | None\n        Prediction content.\n    audio: ChatCompletionAudioParam |\
      \ None\n        Audio parameters.\n    logit_bias: dict | None\n        Logit\
      \ bias overrides.\n    user: str | None\n        User identifier.\n    response_format:\
      \ dict | type[BaseModel] | None\n        Response format or model.\n    seed:\
      \ int | None\n        Random seed.\n    tools: list | None\n        Tools to\
      \ use.\n    tool_choice: str | dict | None\n        Tool selection.\n    logprobs:\
      \ bool | None\n        Include log probabilities.\n    top_logprobs: int | None\n\
      \        Top logprobs to return.\n    parallel_tool_calls: bool | None\n   \
      \     Run tool calls in parallel.\n    web_search_options: OpenAIWebSearchOptions\
      \ | None\n        Web search options.\n    deployment_id: str | None\n     \
      \   Optional deployment identifier.\n    extra_headers: dict | None\n      \
      \  Extra HTTP headers.\n    functions: list | None\n        Deprecated OpenAI\
      \ functions.\n    function_call: str | None\n        Function call.\n    thinking:\
      \ AnthropicThinkingParam | None\n        LiteLLM thinking parameter.\n    kwargs:\
      \ Any\n        Additional keyword arguments accepted.\n\nReturns:\n    ModelResponse\
      \ | CustomStreamWrapper\n\nRaises:\n    Exception types raised by the underlying\
      \ API client or streaming wrapper (e.g., OpenAI API errors, network errors,\
      \ or litellm runtime errors)."
- class_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer
  file: graphrag/query/structured_search/drift_search/primer.py
  name: DRIFTPrimer
  methods:
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.split_reports
    name: split_reports
    signature: 'def split_reports(self, reports: pd.DataFrame) -> list[pd.DataFrame]'
    docstring: "Split the input reports into folds to enable parallel processing.\n\
      \nArgs:\n    reports (pd.DataFrame): DataFrame of community reports.\n\nReturns:\n\
      \    list[pd.DataFrame]: List of report folds."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        top_k_reports:\
      \ pd.DataFrame,\n    ) -> SearchResult"
    docstring: "Asynchronous search method that processes the query and returns a\
      \ SearchResult.\n\nArgs:\n    query (str): The search query.\n    top_k_reports\
      \ (pd.DataFrame): DataFrame containing the top-k reports.\n\nReturns:\n    SearchResult:\
      \ The search result containing the response and context data."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.decompose_query
    name: decompose_query
    signature: "def decompose_query(\n        self, query: str, reports: pd.DataFrame\n\
      \    ) -> tuple[dict, dict[str, int]]"
    docstring: "Decompose the query into subqueries using global guidance from the\
      \ provided community reports.\n\nArgs:\n    query (str): The original search\
      \ query.\n    reports (pd.DataFrame): DataFrame containing community reports.\
      \ Must include a 'full_content' column used to build the concatenated context\
      \ for the primer prompt.\n\nReturns:\n    tuple[dict, dict[str, int]]: A tuple\
      \ containing:\n        parsed_response (dict): The parsed JSON response produced\
      \ by the language model.\n        token_ct (dict[str, int]): Token usage counters\
      \ with keys:\n            llm_calls (int): number of language model calls performed\n\
      \            prompt_tokens (int): number of tokens in the constructed prompt\n\
      \            output_tokens (int): number of tokens in the model output\n\nRaises:\n\
      \    KeyError: if the reports DataFrame does not contain the required 'full_content'\
      \ column.\n    json.JSONDecodeError: if the model response content is not valid\
      \ JSON.\n    Exception: any exceptions raised by the chat model interaction\
      \ (propagated from the API call).\n\nNotes:\n    The function builds community_reports\
      \ by concatenating the 'full_content' field of all reports with double newlines,\
      \ and passes that into DRIFT_PRIMER_PROMPT."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::DRIFTPrimer.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        config: DRIFTSearchConfig,\n\
      \        chat_model: ChatModel,\n        tokenizer: Tokenizer | None = None,\n\
      \    )"
    docstring: "Initialize the DRIFTPrimer.\n\nArgs:\n    config (DRIFTSearchConfig):\
      \ Configuration settings for DRIFT search.\n    chat_model (ChatModel): The\
      \ language model used for searching.\n    tokenizer (Tokenizer, optional): Tokenizer\
      \ for managing tokens. If not provided, a default tokenizer is obtained.\n\n\
      Returns:\n    None\n\nRaises:\n    None"
- class_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType
  file: graphrag/index/operations/summarize_communities/typing.py
  name: CreateCommunityReportsStrategyType
  methods:
  - node_id: graphrag/index/operations/summarize_communities/typing.py::CreateCommunityReportsStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this CreateCommunityReportsStrategyType\
      \ enum member.\n\nArgs:\n    self: CreateCommunityReportsStrategyType, the enum\
      \ member to represent as a string.\n\nReturns:\n    str: The enum member's value\
      \ enclosed in double quotes."
- class_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig
  file: graphrag/config/models/vector_store_config.py
  name: VectorStoreConfig
  methods:
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_db_uri
    name: _validate_db_uri
    signature: def _validate_db_uri(self) -> None
    docstring: "Validate the database URI. If the vector store type is LanceDB and\
      \ db_uri is missing or empty, set it to the default value from vector_store_defaults.\n\
      \nArgs:\n    self: The VectorStoreConfig instance being validated.\n\nReturns:\n\
      \    None\n\nRaises:\n    ValueError: If vector_store.type is not LanceDB and\
      \ a non-empty db_uri is provided. The error message is: vector_store.db_uri\
      \ is only used when vector_store.type is LanceDB. Please rerun graphrag init\
      \ and select the correct vector store type."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_embeddings_schema
    name: _validate_embeddings_schema
    signature: def _validate_embeddings_schema(self) -> None
    docstring: "Validate the embeddings schema. This method performs two checks:\n\
      1) Each entry in embeddings_schema must correspond to a known embedding schema\
      \ name present in all_embeddings. If any name is invalid, it raises a ValueError\
      \ with the message: vector_store.embeddings_schema contains an invalid embedding\
      \ schema name: {name}. Please update your settings.yaml and select the correct\
      \ embedding schema names.\n2) When vector_store.type == CosmosDB, every key\
      \ in embeddings_schema must be 'id'. If any key differs, it raises a ValueError\
      \ with the message: When using CosmosDB, the id_field in embeddings_schema must\
      \ be 'id'. Please update your settings.yaml and set the id_field to 'id'.\n\n\
      This method does not return a value. It raises ValueError on invalid configurations,\
      \ which are handled by the caller during model validation.\nArgs:\n    self:\
      \ The instance containing embeddings_schema and type attributes.\nReturns:\n\
      \    None\nRaises:\n    ValueError: vector_store.embeddings_schema contains\
      \ an invalid embedding schema name: {name}. Please update your settings.yaml\
      \ and select the correct embedding schema names.\n    ValueError: When using\
      \ CosmosDB, the id_field in embeddings_schema must be 'id'. Please update your\
      \ settings.yaml and set the id_field to 'id'."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_url
    name: _validate_url
    signature: def _validate_url(self) -> None
    docstring: "Validate the database URL.\n\nArgs:\n    self: The VectorStoreConfig\
      \ instance being validated.\n\nReturns:\n    None\n\nRaises:\n    ValueError:\
      \ If vector_store.type == azure_ai_search and vector_store.url is missing or\
      \ empty. The error message is: vector_store.url is required when vector_store.type\
      \ == azure_ai_search. Please rerun graphrag init and select the correct vector\
      \ store type.\n    ValueError: If vector_store.type == cosmos_db and vector_store.url\
      \ is missing or empty. The error message is: vector_store.url is required when\
      \ vector_store.type == cosmos_db. Please rerun graphrag init and select the\
      \ correct vector store type.\n    ValueError: If vector_store.type == LanceDB\
      \ and vector_store.url is provided (non-empty). The error message is: vector_store.url\
      \ is only used when vector_store.type == azure_ai_search or vector_store.type\
      \ == cosmos_db. Please rerun graphrag init and select the correct vector store\
      \ type."
  - node_id: graphrag/config/models/vector_store_config.py::VectorStoreConfig._validate_model
    name: _validate_model
    signature: def _validate_model(self)
    docstring: "Validate the model after the initial schema validation.\n\nArgs:\n\
      \    self: The instance being validated.\n\nReturns:\n    The same instance\
      \ after validation.\n\nRaises:\n    ValueError: If an invalid database URI,\
      \ URL, or embeddings schema is encountered during validation."
- class_id: graphrag/language_model/response/base.pyi::ModelResponse
  file: graphrag/language_model/response/base.pyi
  name: ModelResponse
  methods:
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.parsed_response
    name: parsed_response
    signature: def parsed_response(self) -> _T | None
    docstring: "Return the parsed response, if available.\n\nArgs:\n    self: The\
      \ instance of the implementing class providing the parsed_response property.\n\
      \nReturns:\n    _T | None: The parsed response, or None if not available.\n\n\
      Raises:\n    None"
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.history
    name: history
    signature: def history(self) -> list[Any]
    docstring: "History of the model response.\n\nReturns\n    list[Any]: The history\
      \ as a list of items."
  - node_id: graphrag/language_model/response/base.pyi::ModelResponse.output
    name: output
    signature: def output(self) -> ModelOutput
    docstring: "Return the ModelOutput for this response.\n\nArgs:\n    self: The\
      \ response object.\n\nReturns:\n    ModelOutput: The output associated with\
      \ this response. The returned object provides:\n        content: str - The textual\
      \ content of the output.\n        full_response: dict[str, Any] | None - The\
      \ complete JSON response from the LLM provider, or None if not available."
- class_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel
  file: graphrag/language_model/providers/litellm/chat_model.py
  name: LitellmChatModel
  methods:
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel._get_kwargs
    name: _get_kwargs
    signature: 'def _get_kwargs(self, **kwargs: Any) -> dict[str, Any]'
    docstring: "Get model arguments supported by litellm.\n\nArgs:\n  kwargs (dict[str,\
      \ Any]): Arbitrary keyword arguments. Only keys in the following set will be\
      \ included in the returned dictionary: \"name\", \"modalities\", \"prediction\"\
      , \"audio\", \"logit_bias\", \"metadata\", \"user\", \"response_format\", \"\
      seed\", \"tools\", \"tool_choice\", \"logprobs\", \"top_logprobs\", \"parallel_tool_calls\"\
      , \"web_search_options\", \"extra_headers\", \"functions\", \"function_call\"\
      , \"thinking\".\n\nReturns:\n  dict[str, Any]: A dictionary containing the subset\
      \ of keyword arguments that litellm supports. If a 'json' keyword argument is\
      \ provided, response_format is set to {\"type\": \"json_object\"}. If a 'json_model'\
      \ keyword argument is provided and it is a subclass of pydantic.BaseModel, response_format\
      \ is set to that model."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> AsyncGenerator[str, None]"
    docstring: "Generate a response for the given prompt and history.\n\nArgs:\n \
      \   prompt: The prompt to generate a response for.\n    history: Optional chat\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    AsyncGenerator[str, None]: The generated response as a stream\
      \ of strings."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs:
      Any) -> "MR"'
    docstring: "Generate a response for the given prompt and history.\n\nArgs\n----\n\
      \    prompt: The prompt to generate a response for.\n    history: Optional chat\
      \ history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns\n-------\n    LitellmModelResponse: The generated model response."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs: Any\n    ) -> Generator[str, None]"
    docstring: "Generate a response for the given prompt and history.\n\nArgs:\n \
      \   prompt (str): The prompt to generate a response for.\n    history (list[dict[str,\
      \ str]] | None): Optional chat history represented as a list of messages. Each\
      \ message is a dict with keys such as \"role\" and \"content\".\n    kwargs\
      \ (Any): Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    Generator[str, None]: The generated response as a stream of strings.\n\n\
      Raises:\n    Exception: Exceptions raised by the underlying streaming mechanism\
      \ or model client may propagate to the caller."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs: Any\n    ) -> \"MR\""
    docstring: "Asynchronously generate a response for the given prompt and history.\n\
      \nArgs:\n    prompt: The prompt to generate a response for.\n    history: Optional\
      \ chat history.\n    **kwargs: Additional keyword arguments (e.g., model parameters).\n\
      \nReturns:\n    MR: The generated model response."
  - node_id: graphrag/language_model/providers/litellm/chat_model.py::LitellmChatModel.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        name: str,\n        config:\
      \ \"LanguageModelConfig\",\n        cache: \"PipelineCache | None\" = None,\n\
      \        **kwargs: Any,\n    )"
    docstring: "Initialize the Litellm chat model with the provided name and configuration.\n\
      \nArgs:\n    name: str\n        The name of the model instance.\n    config:\
      \ LanguageModelConfig\n        The configuration for the language model.\n \
      \   cache: PipelineCache | None\n        Optional cache to use for responses.\
      \ If provided, a child cache scoped to this model's name is created.\n    **kwargs:\
      \ Any\n        Additional keyword arguments.\n\nReturns:\n    None"
- class_id: graphrag/config/enums.py::SearchMethod
  file: graphrag/config/enums.py
  name: SearchMethod
  methods:
  - node_id: graphrag/config/enums.py::SearchMethod.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "\"\"\"Return the string representation of the enum value.\n\nArgs:\n\
      \    self: The enum member.\n\nReturns:\n    str: The string representation\
      \ of the enum value.\n\"\"\""
- class_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore
  file: tests/integration/vector_stores/test_lancedb.py
  name: TestLanceDBVectorStore
  methods:
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents_categories
    name: sample_documents_categories
    signature: def sample_documents_categories(self)
    docstring: "Create sample documents with different categories for testing.\n\n\
      Args:\n    self: Instance of the test class used by pytest to provide fixture\
      \ context.\n\nReturns:\n    List[VectorStoreDocument]: A list of VectorStoreDocument\
      \ objects with\n        varying category attributes in the attributes dictionary\
      \ to support\n        category-based tests (e.g., animals and vehicles)."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.sample_documents
    name: sample_documents
    signature: def sample_documents(self)
    docstring: "Create sample documents for testing.\n\nArgs:\n    self: Instance\
      \ of the test class used by pytest to provide fixture context.\n\nReturns:\n\
      \    List[VectorStoreDocument]: A list of three VectorStoreDocument objects\
      \ representing\n        the sample documents with ids \"1\", \"2\", and \"3\"\
      ; texts \"This is document 1\",\n        \"This is document 2\", \"This is document\
      \ 3\"; vectors as [0.1, 0.2, 0.3, 0.4, 0.5],\n        [0.2, 0.3, 0.4, 0.5, 0.6],\
      \ and [0.3, 0.4, 0.5, 0.6, 0.7]; and attributes including\n        \"title\"\
      \ as \"Doc 1\"/\"Doc 2\"/\"Doc 3\" and \"category\" as \"test\" for each.\n\n\
      Raises:\n    None"
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_empty_collection
    name: test_empty_collection
    signature: def test_empty_collection(self)
    docstring: "Test creating an empty LanceDB collection, deleting a loaded document,\
      \ and then adding a new document.\n\nArgs:\n    self: The test case instance.\n\
      \nReturns:\n    None\n\nRaises:\n    AssertionError: If any assertion fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_customization
    name: test_vector_store_customization
    signature: def test_vector_store_customization(self, sample_documents)
    docstring: "Test vector store customization with LanceDB.\n\nArgs:\n    self:\
      \ The test case instance.\n    sample_documents: list[VectorStoreDocument] -\
      \ Documents used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\
      \nRaises:\n    AssertionError: If any assertion in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_filter_search
    name: test_filter_search
    signature: def test_filter_search(self, sample_documents_categories)
    docstring: "Test filtered search with LanceDB to verify that filtering by document\
      \ IDs correctly constrains the results of a vector similarity search to the\
      \ filtered documents. Key steps: load the sample_documents_categories into the\
      \ vector store, apply filter_by_id(['1','2']), run similarity_search_by_vector\
      \ with a sample vector and k=3, and assert that the results respect the filter\
      \ (at most two results, no document with id '3', and all result ids are within\
      \ {'1','2'}).\n\nArgs:\n  self: The test case instance.\n  sample_documents_categories:\
      \ list[VectorStoreDocument] - Documents loaded into the LanceDB vector store\
      \ for this test.\n\nReturns:\n  None.\n\nRaises:\n  AssertionError: If any assertion\
      \ in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.test_vector_store_operations
    name: test_vector_store_operations
    signature: def test_vector_store_operations(self, sample_documents)
    docstring: "Test basic vector store operations with LanceDB.\n\nArgs:\n    self:\
      \ The test case instance.\n    sample_documents: list[VectorStoreDocument] -\
      \ Documents used to load into the LanceDB vector store.\n\nReturns:\n    None.\n\
      \nRaises:\n    AssertionError: If any assertion in the test fails."
  - node_id: tests/integration/vector_stores/test_lancedb.py::TestLanceDBVectorStore.mock_embedder
    name: mock_embedder
    signature: 'def mock_embedder(text: str) -> list[float]'
    docstring: "A simple text embedder used for testing that returns a fixed embedding\
      \ vector. The embedding is independent of the input text and always returns\
      \ [0.1, 0.2, 0.3, 0.4, 0.5].\n\nArgs:\n    text (str): Input text to embed.\n\
      \nReturns:\n    list[float]: The fixed embedding vector [0.1, 0.2, 0.3, 0.4,\
      \ 0.5].\n\nRaises:\n    None: This function does not raise any exceptions."
- class_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch
  file: graphrag/query/structured_search/basic_search/search.py
  name: BasicSearch
  methods:
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.search
    name: search
    signature: "def search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> SearchResult"
    docstring: "Build rag search context that fits a single context window and generates\
      \ an answer for the user query.\n\nArgs:\n  query: The user query to process.\n\
      \  conversation_history: Optional conversation history to incorporate into the\
      \ search context.\n  **kwargs: Additional keyword arguments passed to the context\
      \ builder and the model.\n\nReturns:\n  SearchResult: The search result containing\
      \ the generated response, context data, and timing information."
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.stream_search
    name: stream_search
    signature: "def stream_search(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n    ) -> AsyncGenerator[str, None]"
    docstring: "Build basic search context that fits a single context window and stream\
      \ the answer for the user query.\n\nArgs:\n  query (str): The user query to\
      \ process.\n  conversation_history (ConversationHistory | None): Optional conversation\
      \ history to incorporate into the search context.\n\nReturns:\n  AsyncGenerator[str,\
      \ None]: An asynchronous generator yielding strings representing chunks of the\
      \ generated answer as they are produced."
  - node_id: graphrag/query/structured_search/basic_search/search.py::BasicSearch.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ BasicContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str | None = None,\n        response_type: str = \"multiple\
      \ paragraphs\",\n        callbacks: list[QueryCallbacks] | None = None,\n  \
      \      model_params: dict[str, Any] | None = None,\n        context_builder_params:\
      \ dict | None = None,\n    )"
    docstring: "Initialize a BasicSearch instance for basic search orchestration (internal\
      \ API).\n\nArgs:\n    model (ChatModel): The language model interface used for\
      \ this basic search.\n    context_builder (BasicContextBuilder): The builder\
      \ that constructs the context for the search.\n    tokenizer (Tokenizer | None):\
      \ Optional tokenizer to use.\n    system_prompt (str | None): System prompt\
      \ for the search. If None, uses BASIC_SEARCH_SYSTEM_PROMPT.\n    response_type\
      \ (str): Specifies the format of the response. Defaults to \"multiple paragraphs\"\
      .\n    callbacks (list[QueryCallbacks] | None): Optional list of query callbacks\
      \ to invoke.\n    model_params (dict[str, Any] | None): Optional parameters\
      \ to pass to the model.\n    context_builder_params (dict | None): Optional\
      \ parameters for the context builder.\n\nReturns:\n    None"
- class_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig
  file: graphrag/config/models/text_embedding_config.py
  name: TextEmbeddingConfig
  methods:
  - node_id: graphrag/config/models/text_embedding_config.py::TextEmbeddingConfig.resolved_strategy
    name: resolved_strategy
    signature: 'def resolved_strategy(self, model_config: LanguageModelConfig) ->
      dict'
    docstring: "Get the resolved text embedding strategy.\n\nArgs:\n    model_config:\
      \ The language model configuration used to resolve the strategy.\n\nReturns:\n\
      \    dict: The resolved text embedding strategy. If a custom strategy is provided\
      \ via self.strategy, that is returned; otherwise, a default strategy dictionary\
      \ is returned with the following keys:\n        type: The strategy type (TextEmbedStrategyType.openai)\n\
      \        llm: The serialized language model configuration (model_config.model_dump())\n\
      \        num_threads: The number of concurrent requests (model_config.concurrent_requests)\n\
      \        batch_size: The configured batch size (self.batch_size)\n        batch_max_tokens:\
      \ The configured max tokens per batch (self.batch_max_tokens)\n\nRaises:\n \
      \   ImportError: If the import of TextEmbedStrategyType fails."
- class_id: tests/smoke/test_fixtures.py::TestIndexer
  file: tests/smoke/test_fixtures.py
  name: TestIndexer
  methods:
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__assert_indexer_outputs
    name: __assert_indexer_outputs
    signature: "def __assert_indexer_outputs(\n        self, root: Path, workflow_config:\
      \ dict[str, dict[str, Any]]\n    )"
    docstring: "Assert that the indexer outputs conform to the provided workflow configuration.\n\
      \nArgs:\n    self: The instance of the containing class.\n    root: Path to\
      \ the root directory containing the indexer outputs (expects an output subdirectory\
      \ with stats.json).\n    workflow_config: Mapping of workflow names to their\
      \ configuration dictionaries. Each config may include:\n        - expected_artifacts:\
      \ List[str] of artifact files to validate (parquet files).\n        - max_runtime:\
      \ Optional number specifying the maximum allowed runtime for the workflow.\n\
      \        - row_range: List[int] with two elements [min_rows, max_rows] for the\
      \ number of rows in each artifact.\n        - nan_allowed_columns: Optional[List[str]]\
      \ of column names that may contain NaN values.\n\nReturns:\n    None\n\nRaises:\n\
      \    AssertionError: If the output folder does not exist, if the reported workflows\
      \ do not match the configured ones, if a runtime constraint is violated, or\
      \ if artifact checks fail (row count or NaN values)."
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_indexer
    name: __run_indexer
    signature: "def __run_indexer(\n        self,\n        root: Path,\n        input_file_type:\
      \ str,\n    )"
    docstring: "Run the indexer command for the given root and input file type and\
      \ ensure it completes successfully by invoking uv run poe index, including --verbose\
      \ when a debug flag is set.\n\nArgs:\n    root: Path\n        Path to the root\
      \ directory used for indexing.\n    input_file_type: str\n        The input\
      \ file type. This parameter is accepted for interface compatibility but is not\
      \ used in the function body.\n\nReturns:\n    None\n\nRaises:\n    AssertionError\n\
      \        If the indexer finishes with a non-zero return code."
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.__run_query
    name: __run_query
    signature: 'def __run_query(self, root: Path, query_config: dict[str, str])'
    docstring: "Run a uv run poe query command using the provided root and query_config\
      \ and return the subprocess result.\n\nArgs:\n  root: Path to the root directory\
      \ for the command. The path is resolved to an absolute POSIX string and passed\
      \ to --root.\n  query_config: dict[str, str]. Configuration for the query. Must\
      \ include:\n      method: The value for --method.\n      query: The value for\
      \ --query.\n      community_level: Optional; the value for --community-level.\
      \ If omitted, defaults to 2.\n\nReturns:\n  subprocess.CompletedProcess: The\
      \ result of subprocess.run invoked with capture_output=True and text=True.\n\
      \nRaises:\n  None"
  - node_id: tests/smoke/test_fixtures.py::TestIndexer.test_fixture
    name: test_fixture
    signature: "def test_fixture(\n        self,\n        input_path: str,\n     \
      \   input_file_type: str,\n        workflow_config: dict[str, dict[str, Any]],\n\
      \        query_config: list[dict[str, str]],\n    )"
    docstring: "Prepare Azurite test data for the fixtures. This coroutine uses the\
      \ azure configuration to create or reset a blob storage container, uploads test\
      \ data from the local input directory (txt and csv files), and returns a callable\
      \ that will delete the container when invoked.\n\nArgs:\n  input_path: Path\
      \ on disk containing test input data. The function looks for an input subdirectory\
      \ with .txt and .csv files to upload.\n  azure: Dictionary with Azure/Azurite\
      \ configuration. Expected keys include: ...\n\nReturns:\n  Callable[[], None]:\
      \ A callable that will delete the container when invoked.\n\nRaises:\n  Exceptions\
      \ related to Azure/Azurite operations (not specified in the provided data)."
- class_id: graphrag/logger/progress.py::ProgressTicker
  file: graphrag/logger/progress.py
  name: ProgressTicker
  methods:
  - node_id: graphrag/logger/progress.py::ProgressTicker.__call__
    name: __call__
    signature: 'def __call__(self, num_ticks: int = 1) -> None'
    docstring: "Emit progress.\n\nArgs:\n    num_ticks (int): Number of ticks to advance\
      \ the progress.\n\nReturns:\n    None: This method updates internal counters\
      \ and, if a callback is set, notifies it with a Progress object."
  - node_id: graphrag/logger/progress.py::ProgressTicker.__init__
    name: __init__
    signature: "def __init__(\n        self, callback: ProgressHandler | None, num_total:\
      \ int, description: str = \"\"\n    )"
    docstring: "Initialize a ProgressTicker with the provided callback, total items,\
      \ and optional description.\n\nArgs:\n    callback: ProgressHandler | None\n\
      \        A function to handle progress reports, or None to disable updates.\n\
      \    num_total: int\n        Total number of items to track.\n    description:\
      \ str\n        Optional description for the progress updates.\n\nReturns:\n\
      \    None"
  - node_id: graphrag/logger/progress.py::ProgressTicker.done
    name: done
    signature: def done(self) -> None
    docstring: "Mark the progress as done.\n\nIf a callback was provided (self._callback\
      \ is not None), invoke it with a Progress object whose total_items equals the\
      \ configured total (self._num_total) and whose completed_items equals the same\
      \ value, indicating completion. The description from initialization (self._description)\
      \ is preserved.\n\nThis is a bound method; the self parameter is implicit and\
      \ not documented.\n\nReturns:\n    None"
- class_id: graphrag/data_model/document.py::Document
  file: graphrag/data_model/document.py
  name: Document
  methods:
  - node_id: graphrag/data_model/document.py::Document.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      text_key: str = \"text\",\n        text_units_key: str = \"text_units\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Document\""
    docstring: "Create a new document from the dict data.\n\nArgs:\n    cls (type):\
      \ The class.\n    d (dict[str, Any]): The source dictionary containing the values\
      \ for the Document fields.\n    id_key (str): Key in d for the document's identifier.\
      \ Defaults to \"id\".\n    short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n    title_key (str): Key in\
      \ d for the title. Defaults to \"title\".\n    type_key (str): Key in d for\
      \ the document type. Defaults to \"type\".\n    text_key (str): Key in d for\
      \ the text content. Defaults to \"text\".\n    text_units_key (str): Key in\
      \ d for the list of text unit ids. Defaults to \"text_units\".\n    attributes_key\
      \ (str): Key in d for optional attributes dictionary. Defaults to \"attributes\"\
      .\n\nReturns:\n    Document: A Document instance created from the dictionary\
      \ data.\n\nRaises:\n    KeyError: If a required key is missing from d."
- class_id: graphrag/index/workflows/factory.py::PipelineFactory
  file: graphrag/index/workflows/factory.py
  name: PipelineFactory
  methods:
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register
    name: register
    signature: 'def register(cls, name: str, workflow: WorkflowFunction)'
    docstring: "Register a custom workflow function.\n\nArgs:\n    cls: The class\
      \ that provides access to the registry (PipelineFactory).\n    name: The name\
      \ under which the workflow will be registered.\n    workflow: The workflow function\
      \ to register for the given name.\n\nReturns:\n    None"
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_all
    name: register_all
    signature: 'def register_all(cls, workflows: dict[str, WorkflowFunction])'
    docstring: "Register a dict of custom workflow functions.\n\nArgs:\n    cls: The\
      \ class that provides access to the registry (PipelineFactory).\n    workflows:\
      \ A dictionary mapping workflow names to workflow functions.\n\nReturns:\n \
      \   None"
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.create_pipeline
    name: create_pipeline
    signature: "def create_pipeline(\n        cls,\n        config: GraphRagConfig,\n\
      \        method: IndexingMethod | str = IndexingMethod.Standard,\n    ) -> Pipeline"
    docstring: "Create a pipeline for executing a sequence of workflows.\n\nArgs:\n\
      \    cls: The class reference (provided automatically for classmethod)\n   \
      \ config: GraphRagConfig\n    method: The indexing method or key to select a\
      \ predefined pipeline. Defaults to IndexingMethod.Standard.\n\nReturns:\n  \
      \  Pipeline: The constructed Pipeline object.\n\nRaises:\n    KeyError: If any\
      \ workflow name in the selected workflows is not registered in the class-level\
      \ workflows registry."
  - node_id: graphrag/index/workflows/factory.py::PipelineFactory.register_pipeline
    name: register_pipeline
    signature: 'def register_pipeline(cls, name: str, workflows: list[str])'
    docstring: "Register a new pipeline method as a list of workflow names.\n\nArgs:\n\
      \    name: The name of the pipeline to register.\n    workflows: A list of workflow\
      \ names that constitute the pipeline.\n\nReturns:\n    None"
- class_id: graphrag/data_model/relationship.py::Relationship
  file: graphrag/data_model/relationship.py
  name: Relationship
  methods:
  - node_id: graphrag/data_model/relationship.py::Relationship.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        source_key: str = \"source\",\n        target_key: str = \"target\"\
      ,\n        description_key: str = \"description\",\n        rank_key: str =\
      \ \"rank\",\n        weight_key: str = \"weight\",\n        text_unit_ids_key:\
      \ str = \"text_unit_ids\",\n        attributes_key: str = \"attributes\",\n\
      \    ) -> \"Relationship\""
    docstring: "Create a new Relationship from the dictionary data.\n\nArgs:\n  cls\
      \ (type): The class.\n  d (dict[str, Any]): The source dictionary containing\
      \ the values for the Relationship fields.\n  id_key (str): Key in d for the\
      \ relationship's identifier. Defaults to \"id\".\n  short_id_key (str): Key\
      \ in d for the optional short identifier. Defaults to \"human_readable_id\"\
      .\n  source_key (str): Key in d for the source entity. Defaults to \"source\"\
      .\n  target_key (str): Key in d for the target entity. Defaults to \"target\"\
      .\n  description_key (str): Key in d for the description. Defaults to \"description\"\
      .\n  rank_key (str): Key in d for the rank. Defaults to \"rank\".\n  weight_key\
      \ (str): Key in d for the weight. Defaults to \"weight\".\n  text_unit_ids_key\
      \ (str): Key in d for text unit IDs. Defaults to \"text_unit_ids\".\n  attributes_key\
      \ (str): Key in d for additional attributes. Defaults to \"attributes\".\n\n\
      Returns:\n  Relationship: A Relationship instance constructed from the dictionary\
      \ data.\n\nRaises:\n  KeyError: If id_key is not found in d."
- class_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError
  file: graphrag/index/utils/derive_from_rows.py
  name: ParallelizationError
  methods:
  - node_id: graphrag/index/utils/derive_from_rows.py::ParallelizationError.__init__
    name: __init__
    signature: 'def __init__(self, num_errors: int, example: str | None = None)'
    docstring: "\"\"\"Initialize a ParallelizationError with details about errors\
      \ during parallel transformation.\n\nArgs:\n    num_errors: The number of errors\
      \ that occurred while running parallel transformation.\n    example: Optional\
      \ example error string to include in the message.\n\nReturns:\n    None\n\"\"\
      \""
- class_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig
  file: graphrag/config/models/community_reports_config.py
  name: CommunityReportsConfig
  methods:
  - node_id: graphrag/config/models/community_reports_config.py::CommunityReportsConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "\"\"\"Get the resolved community report extraction strategy.\n\nArgs:\n\
      \    root_dir: The root directory used to resolve the graph and text prompt\
      \ file paths.\n    model_config: The LanguageModelConfig instance containing\
      \ the model configuration; its\n        model_dump() result is included in the\
      \ strategy as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy\
      \ is provided, it is returned as-is; otherwise\n        a default strategy dictionary\
      \ is constructed with the following keys:\n        type, llm, graph_prompt,\
      \ text_prompt, max_report_length, max_input_length.\n        graph_prompt is\
      \ the contents of the file at the path root_dir / self.graph_prompt when\n \
      \       self.graph_prompt is provided, otherwise None. text_prompt similarly\
      \ uses root_dir / self.text_prompt.\n\nRaises:\n    FileNotFoundError: If a\
      \ provided graph_prompt or text_prompt path does not exist.\n    OSError: If\
      \ an I/O error occurs while reading prompt files.\n\"\"\""
- class_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor
  file: graphrag/index/operations/extract_graph/graph_extractor.py
  name: GraphExtractor
  methods:
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__call__
    name: __call__
    signature: "def __call__(\n        self, texts: list[str], prompt_variables: dict[str,\
      \ Any] | None = None\n    ) -> GraphExtractionResult"
    docstring: "Asynchronously run graph extraction on a list of input texts and return\
      \ the results.\n\nArgs:\n  texts: List[str] - List of input texts to process;\
      \ each element is treated as a separate document.\n  prompt_variables: dict[str,\
      \ Any] | None - Optional mapping of prompt variables to customize the extraction\
      \ prompts and delimiters. If None, defaults are used.\n\nReturns:\n  GraphExtractionResult\
      \ - An object containing the aggregated extraction output and a mapping of document\
      \ indices to their source texts.\n\nRaises:\n  Exception - If an error occurs\
      \ during document processing or result aggregation."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_document
    name: _process_document
    signature: "def _process_document(\n        self, text: str, prompt_variables:\
      \ dict[str, str]\n    ) -> str"
    docstring: "Process a single document to extract entities using the configured\
      \ extraction prompts, and accumulate the results. If gleanings are enabled (max_gleanings\
      \ > 0), this may perform multiple continuation prompts to extract additional\
      \ entities until the limit is reached or the model indicates there are no more\
      \ entities.\n\nArgs:\n  text: str - The document content to process.\n  prompt_variables:\
      \ dict[str, str] - A dictionary of prompt variables used to configure the extraction\
      \ prompts and behavior.\n\nReturns:\n  str - The concatenated extraction results\
      \ for the document.\n\nRaises:\n  Exception - If an error occurs calling the\
      \ model or during continuation prompts."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        tuple_delimiter_key: str | None = None,\n        record_delimiter_key:\
      \ str | None = None,\n        input_text_key: str | None = None,\n        entity_types_key:\
      \ str | None = None,\n        completion_delimiter_key: str | None = None,\n\
      \        prompt: str | None = None,\n        join_descriptions=True,\n     \
      \   max_gleanings: int | None = None,\n        on_error: ErrorHandlerFn | None\
      \ = None,\n    )"
    docstring: "Initializes a GraphExtractor with the given configuration.\n\nCreates\
      \ and configures a GraphExtractor instance using the provided model_invoker\
      \ and optional configuration values. This constructor assigns the model to use\
      \ for prompt execution, defines default keys for various prompt variables (e.g.,\
      \ tuple_delimiter, record_delimiter, input_text, entity_types, and completion_delimiter),\
      \ selects the extraction prompt, and establishes max_gleanings and an optional\
      \ on_error handler. It does not return a value.\n\nArgs:\n  model_invoker (ChatModel):\n\
      \      The model invoker used to run prompts.\n  tuple_delimiter_key (str |\
      \ None):\n      Key in prompt_variables for the tuple delimiter. Defaults to\
      \ \"tuple_delimiter\".\n  record_delimiter_key (str | None):\n      Key in prompt_variables\
      \ for the record delimiter. Defaults to \"record_delimiter\".\n  input_text_key\
      \ (str | None):\n      Key in inputs for the input text. Defaults to \"input_text\"\
      .\n  entity_types_key (str | None):\n      Key for the entity types in prompt\
      \ variables. Defaults to \"entity_types\".\n  completion_delimiter_key (str\
      \ | None):\n      Key for the completion delimiter in prompt variables. Defaults\
      \ to \"completion_delimiter\".\n  prompt (str | None):\n      Custom extraction\
      \ prompt to use. If None, defaults to GRAPH_EXTRACTION_PROMPT.\n  join_descriptions\
      \ (bool):\n      Whether to join descriptions in the extraction.\n  max_gleanings\
      \ (int | None):\n      Maximum number of gleanings. If None, defaults to graphrag_config_defaults.extract_graph.max_gleanings.\n\
      \  on_error (ErrorHandlerFn | None):\n      Optional error handler function.\
      \ If None, a no-op handler is used."
  - node_id: graphrag/index/operations/extract_graph/graph_extractor.py::GraphExtractor._process_results
    name: _process_results
    signature: "def _process_results(\n        self,\n        results: dict[int, str],\n\
      \        tuple_delimiter: str,\n        record_delimiter: str,\n    ) -> nx.Graph"
    docstring: "Parse the result string to create an undirected unipartite graph.\n\
      \nArgs:\n    results (dict[int, str]): dict of results from the extraction chain\n\
      \    tuple_delimiter (str): delimiter between tuples in an output record, default\
      \ is '<|>'\n    record_delimiter (str): delimiter between records, default is\
      \ '##'\n\nReturns:\n    nx.Graph: The undirected graph constructed from the\
      \ results."
- class_id: graphrag/language_model/protocol/base.py::EmbeddingModel
  file: graphrag/language_model/protocol/base.py
  name: EmbeddingModel
  methods:
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed_batch
    name: aembed_batch
    signature: "def aembed_batch(\n        self, text_list: list[str], **kwargs: Any\n\
      \    ) -> list[list[float]]"
    docstring: "Asynchronously generate embedding vectors for the given list of strings.\n\
      \nArgs:\n    text_list: The list of strings to generate embeddings for.\n  \
      \  **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns\n\
      -------\n    list[list[float]]: A list of embedding vectors for each input item\
      \ in the batch.\n\nRaises:\n    Exception: If an error occurs during embedding\
      \ generation."
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed
    name: embed
    signature: 'def embed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text:\
      \ The text to generate an embedding for.\n    **kwargs: Additional keyword arguments\
      \ (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding vector.\n\
      \nRaises:\n    Exception: If an error occurs during embedding generation."
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.embed_batch
    name: embed_batch
    signature: 'def embed_batch(self, text_list: list[str], **kwargs: Any) -> list[list[float]]'
    docstring: "\"\"\"\nGenerate embedding vectors for the given list of strings.\n\
      \nArgs:\n    text_list: The list of strings to generate embeddings for.\n  \
      \  **kwargs: Additional keyword arguments (e.g., model parameters).\n\nReturns:\n\
      \    list[list[float]]: A list of embedding vectors for each input item in the\
      \ batch.\n\nRaises:\n    Exception: If an error occurs during embedding generation.\n\
      \"\"\""
  - node_id: graphrag/language_model/protocol/base.py::EmbeddingModel.aembed
    name: aembed
    signature: 'def aembed(self, text: str, **kwargs: Any) -> list[float]'
    docstring: "Generate an embedding vector for the given text.\n\nArgs:\n    text\
      \ (str): The text to generate an embedding for.\n    **kwargs: Additional keyword\
      \ arguments (e.g., model parameters).\n\nReturns:\n    list[float]: The embedding\
      \ vector."
- class_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM
  file: graphrag/language_model/providers/fnllm/models.py
  name: AzureOpenAIChatFNLLM
  methods:
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat
    name: achat
    signature: "def achat(\n        self, prompt: str, history: list | None = None,\
      \ **kwargs\n    ) -> ModelResponse"
    docstring: "Chat with the Model using the given prompt.\n\nThis method supports\
      \ an optional conversation history. If history is None, the\nmodel is called\
      \ with the prompt and any provided kwargs. If history is provided, it\nis sent\
      \ to the model along with the prompt.\n\nArgs:\n    prompt (str): The prompt\
      \ to chat with.\n    history (list | None): The conversation history to include\
      \ in the chat, or None for no history.\n    kwargs (dict[str, Any]): Additional\
      \ keyword arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The\
      \ response from the Model.\n\nRaises:\n    Exception: Exceptions raised by the\
      \ underlying model call may propagate."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.achat_stream
    name: achat_stream
    signature: "def achat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> AsyncGenerator[str, None]"
    docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
      \ Additional arguments to pass to the Model.\n\nReturns:\n    An asynchronous\
      \ generator that yields non-None strings representing the response.\n\nRaises:\n\
      \    Propagates exceptions raised by the underlying model call or streaming\
      \ response."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat_stream
    name: chat_stream
    signature: "def chat_stream(\n        self, prompt: str, history: list | None\
      \ = None, **kwargs\n    ) -> Generator[str, None]"
    docstring: "Stream Chat with the Model using the given prompt.\n\nArgs:\n    prompt:\
      \ The prompt to chat with.\n    history: The conversation history.\n    kwargs:\
      \ Additional arguments to pass to the Model.\n\nReturns:\n    Generator[str,\
      \ None]: A generator that yields strings representing the response.\n\nRaises:\n\
      \    NotImplementedError: chat_stream is not supported for synchronous execution."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.chat
    name: chat
    signature: 'def chat(self, prompt: str, history: list | None = None, **kwargs)
      -> ModelResponse'
    docstring: "Chat with the Model using the given prompt.\n\nArgs:\n    prompt (str):\
      \ The prompt to chat with.\n    history (list | None): The conversation history\
      \ to include in the chat, or None for no history.\n    kwargs: Additional keyword\
      \ arguments to pass to the Model.\n\nReturns:\n    ModelResponse: The response\
      \ from the Model.\n\nRaises:\n    Exception: Exceptions raised by the underlying\
      \ model call are propagated."
  - node_id: graphrag/language_model/providers/fnllm/models.py::AzureOpenAIChatFNLLM.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        name: str,\n   \
      \     config: LanguageModelConfig,\n        callbacks: WorkflowCallbacks | None\
      \ = None,\n        cache: PipelineCache | None = None,\n    ) -> None"
    docstring: "Initialize an Azure OpenAI Chat LLM provider instance.\n\nArgs:\n\
      \    name: str\n        The name to assign to the internal cache provider and\
      \ model instance.\n    config: LanguageModelConfig\n        The configuration\
      \ used to derive the OpenAI configuration.\n    callbacks: WorkflowCallbacks\
      \ | None\n        Optional WorkflowCallbacks; if provided, an error handler\
      \ will be created to log issues.\n    cache: PipelineCache | None\n        Optional\
      \ cache to wrap for the underlying FNLLM cache provider; if None, no caching\
      \ is used.\n\nReturns:\n    None\n\nRaises:\n    Exception: If initialization\
      \ of the OpenAI config, client, or FNLLM components fails due to underlying\
      \ library errors."
- class_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding
  file: graphrag/language_model/providers/litellm/types.py
  name: AFixedModelEmbedding
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::AFixedModelEmbedding.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    docstring: 'Embedding function.


      Args:

      - request_id: Optional request identifier

      - input: List input to embed

      - dimensions: Optional embedding dimensions

      - encoding_format: Optional encoding format

      - timeout: Timeout in seconds for the request (default 600)

      - api_base: Optional API base

      - api_version: Optional API version

      - api_key: Optional API key

      - api_type: Optional API type

      - caching: Whether to enable caching

      - user: Optional user identifier

      - kwargs: Additional keyword arguments that will be forwarded to the underlying
      request


      Returns:

      - EmbeddingResponse: The embedding result


      Raises:

      - None'
- class_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks
  file: graphrag/callbacks/noop_workflow_callbacks.py
  name: NoopWorkflowCallbacks
  methods:
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.progress
    name: progress
    signature: 'def progress(self, progress: Progress) -> None'
    docstring: "Handle when progress occurs.\n\nArgs:\n    progress: Progress object\
      \ representing the current progress event.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_end
    name: pipeline_end
    signature: 'def pipeline_end(self, results: list[PipelineRunResult]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline ends.\n\n\
      Args:\n    results: A list of PipelineRunResult objects representing the results\
      \ of the pipeline runs.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_end
    name: workflow_end
    signature: 'def workflow_end(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow ends.\n\nArgs:\n    name: str\n\
      \        The name of the workflow.\n    instance: object\n        The workflow\
      \ instance object.\n\nReturns:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.pipeline_start
    name: pipeline_start
    signature: 'def pipeline_start(self, names: list[str]) -> None'
    docstring: "Execute this callback to signal when the entire pipeline starts.\n\
      \nArgs:\n    names: list[str] The names of the pipelines that started.\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/callbacks/noop_workflow_callbacks.py::NoopWorkflowCallbacks.workflow_start
    name: workflow_start
    signature: 'def workflow_start(self, name: str, instance: object) -> None'
    docstring: "Execute this callback when a workflow starts.\n\nArgs:\n    name (str):\
      \ The name of the workflow starting.\n    instance (object): The workflow instance\
      \ object associated with this start event.\n\nReturns:\n    None\n\nRaises:\n\
      \    None"
- class_id: graphrag/language_model/events/base.py::ModelEventHandler
  file: graphrag/language_model/events/base.py
  name: ModelEventHandler
  methods:
  - node_id: graphrag/language_model/events/base.py::ModelEventHandler.on_error
    name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    docstring: "Handle an model error.\n\nArgs:\n    error: BaseException | None:\
      \ The error that occurred, or None if no error is provided.\n    traceback:\
      \ str | None: The traceback string associated with the error, or None if not\
      \ available.\n    arguments: dict[str, Any] | None: Additional contextual arguments\
      \ related to the error, or None.\nReturns:\n    None: The function does not\
      \ return a value."
- class_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory
  file: graphrag/index/operations/build_noun_graph/np_extractors/factory.py
  name: NounPhraseExtractorFactory
  methods:
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.get_np_extractor
    name: get_np_extractor
    signature: 'def get_np_extractor(cls, config: TextAnalyzerConfig) -> BaseNounPhraseExtractor'
    docstring: "Get the noun phrase extractor instance based on the configured type.\n\
      \nArgs:\n    cls: The class (used as a classmethod parameter).\n    config:\
      \ TextAnalyzerConfig containing extractor_type and related options such as model_name,\
      \ max_word_length, include_named_entities, exclude_entity_tags, exclude_pos_tags,\
      \ exclude_nouns, word_delimiter, noun_phrase_grammars, and noun_phrase_tags.\n\
      \nReturns:\n    BaseNounPhraseExtractor: An instance of the selected noun phrase\
      \ extractor (Syntactic, CFG, or RegexEnglish)."
  - node_id: graphrag/index/operations/build_noun_graph/np_extractors/factory.py::NounPhraseExtractorFactory.register
    name: register
    signature: 'def register(cls, np_extractor_type: str, np_extractor: type)'
    docstring: "Register a noun phrase extractor in NounPhraseExtractorFactory by\
      \ adding it to the class-level registry np_extractor_types. This is a classmethod\
      \ that updates the mapping from extractor type identifiers to extractor classes,\
      \ enabling get_np_extractor to instantiate the correct extractor based on configuration.\n\
      \nArgs:\n    cls: The class on which this classmethod is invoked.\n    np_extractor_type:\
      \ str The identifier for the noun phrase extractor type.\n    np_extractor:\
      \ type The extractor class/type to register for the given type.\n\nReturns:\n\
      \    None"
- class_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType
  file: graphrag/index/operations/extract_graph/typing.py
  name: ExtractEntityStrategyType
  methods:
  - node_id: graphrag/index/operations/extract_graph/typing.py::ExtractEntityStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this ExtractEntityStrategyType enum\
      \ member.\n\nArgs:\n    self: ExtractEntityStrategyType, the enum member to\
      \ represent as a string.\n\nReturns:\n    str: The enum member's value enclosed\
      \ in double quotes."
- class_id: graphrag/query/context_builder/builders.py::BasicContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: BasicContextBuilder
  methods:
  - node_id: graphrag/query/context_builder/builders.py::BasicContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "Build the context for the basic search mode.\n\nArgs:\n    query:\
      \ The user query to build context for.\n    conversation_history: Optional conversation\
      \ history to consider while constructing the context.\n    **kwargs: Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \    ContextBuilderResult: The result containing the built context for the basic\
      \ search mode."
- class_id: graphrag/data_model/community_report.py::CommunityReport
  file: graphrag/data_model/community_report.py
  name: CommunityReport
  methods:
  - node_id: graphrag/data_model/community_report.py::CommunityReport.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        community_id_key:\
      \ str = \"community\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        summary_key: str = \"summary\",\n        full_content_key: str = \"\
      full_content\",\n        rank_key: str = \"rank\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"CommunityReport\""
    docstring: "Create a CommunityReport instance from the dict data.\n\nArgs:\n \
      \ cls (type): The class.\n  d (dict[str, Any]): The source dictionary containing\
      \ the values for the CommunityReport fields.\n  id_key (str): Key in d for the\
      \ report's identifier. Defaults to \"id\".\n  title_key (str): Key in d for\
      \ the report title. Defaults to \"title\".\n  community_id_key (str): Key in\
      \ d for the associated community's id. Defaults to \"community\".\n  short_id_key\
      \ (str): Key in d for the optional short identifier. Defaults to \"human_readable_id\"\
      .\n  summary_key (str): Key in d for the summary. Defaults to \"summary\".\n\
      \  full_content_key (str): Key in d for the full content. Defaults to \"full_content\"\
      .\n  rank_key (str): Key in d for the rank value. Defaults to \"rank\".\n  attributes_key\
      \ (str): Key in d for optional attributes dictionary. Defaults to \"attributes\"\
      .\n  size_key (str): Key in d for the size value. Defaults to \"size\".\n  period_key\
      \ (str): Key in d for the period. Defaults to \"period\".\n\nReturns:\n  CommunityReport:\
      \ The constructed CommunityReport instance.\n\nRaises:\n  KeyError: If a required\
      \ key is missing from d (e.g., id_key, title_key, community_id_key, summary_key,\
      \ full_content_key, rank_key)."
- class_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig
  file: graphrag/config/models/extract_claims_config.py
  name: ClaimExtractionConfig
  methods:
  - node_id: graphrag/config/models/extract_claims_config.py::ClaimExtractionConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved claim extraction strategy.\n\nArgs:\n    root_dir:\
      \ The root directory used to resolve the graph and text prompt file paths.\n\
      \    model_config: The LanguageModelConfig instance containing the model configuration;\
      \ its model_dump() result is included in the strategy as llm.\n\nReturns:\n\
      \    dict: The resolved strategy. If self.strategy is provided, it is returned\
      \ as-is; otherwise, a dict with the following keys:\n        llm: The result\
      \ of model_config.model_dump().\n        extraction_prompt: The contents of\
      \ the prompt file located at Path(root_dir) / self.prompt read as UTF-8, or\
      \ None if no prompt is configured.\n        claim_description: The description\
      \ from self.description.\n        max_gleanings: The max_gleanings value from\
      \ self.max_gleanings.\n\nRaises:\n    FileNotFoundError: If a prompt is configured\
      \ and the prompt file cannot be read."
- class_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry
  file: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py
  name: NativeRetry
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function until it succeeds or the maximum number\
      \ of retries is reached.\n\nArgs:\n    func: The asynchronous function to retry.\n\
      \    kwargs: Additional keyword arguments to pass to the function.\n\nReturns:\n\
      \    Any: The result of the awaited function.\n\nRaises:\n    Exception: If\
      \ the wrapped function keeps raising and the maximum number of retries is exceeded."
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function until it succeeds or max_retries is reached.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs) and its result will be returned on success.\n    kwargs:\
      \ Any - Keyword arguments to pass to func.\n\nReturns:\n    Any - The value\
      \ returned by func on a successful invocation.\n\nRaises:\n    Exception - The\
      \ last exception raised by func after exhausting max_retries."
  - node_id: graphrag/language_model/providers/litellm/services/retry/native_wait_retry.py::NativeRetry.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        max_retries: int\
      \ = 5,\n        **kwargs: Any,\n    )"
    docstring: "Initialize NativeRetry with retry configuration.\n\nArgs:\n  max_retries:\
      \ The maximum number of retry attempts (int). Must be greater than 0.\n  kwargs:\
      \ Additional keyword arguments (Any).\n\nReturns:\n  None\n\nRaises:\n  ValueError:\
      \ max_retries must be greater than 0."
- class_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: GlobalContextBuilder
  methods:
  - node_id: graphrag/query/context_builder/builders.py::GlobalContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "Build the context for the global search mode.\n\nArgs:\n  query: The\
      \ user query to build context for.\n  conversation_history: Optional conversation\
      \ history to consider while constructing the context.\n  **kwargs: Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \  ContextBuilderResult: The result containing the built context."
- class_id: graphrag/prompt_tune/types.py::DocSelectionType
  file: graphrag/prompt_tune/types.py
  name: DocSelectionType
  methods:
  - node_id: graphrag/prompt_tune/types.py::DocSelectionType.__str__
    name: __str__
    signature: def __str__(self)
    docstring: "Return the string representation of the enum value.\n\nArgs:\n   \
      \ self: The enum member.\n\nReturns:\n    str: The string representation of\
      \ the enum value."
- class_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback
  file: graphrag/callbacks/llm_callbacks.py
  name: BaseLLMCallback
  methods:
  - node_id: graphrag/callbacks/llm_callbacks.py::BaseLLMCallback.on_llm_new_token
    name: on_llm_new_token
    signature: 'def on_llm_new_token(self, token: str)'
    docstring: "Handle when a new token is generated.\n\nArgs:\n    token: str\n \
      \       The new token generated by the LLM.\n\nReturns:\n    None"
- class_id: graphrag/config/enums.py::ChunkStrategyType
  file: graphrag/config/enums.py
  name: ChunkStrategyType
  methods:
  - node_id: graphrag/config/enums.py::ChunkStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this ChunkStrategyType enum member.\n\
      \nArgs:\n    self: ChunkStrategyType, the enum member to represent as a string.\n\
      \nReturns:\n    str: The enum member's value enclosed in double quotes."
- class_id: graphrag/data_model/entity.py::Entity
  file: graphrag/data_model/entity.py
  name: Entity
  methods:
  - node_id: graphrag/data_model/entity.py::Entity.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        short_id_key: str = \"human_readable_id\",\n\
      \        title_key: str = \"title\",\n        type_key: str = \"type\",\n  \
      \      description_key: str = \"description\",\n        description_embedding_key:\
      \ str = \"description_embedding\",\n        name_embedding_key: str = \"name_embedding\"\
      ,\n        community_key: str = \"community\",\n        text_unit_ids_key: str\
      \ = \"text_unit_ids\",\n        rank_key: str = \"degree\",\n        attributes_key:\
      \ str = \"attributes\",\n    ) -> \"Entity\""
    docstring: "Create a new entity from the dict data.\n\nArgs:\n  cls (type): The\
      \ class.\n  d (dict[str, Any]): The source dictionary containing the values\
      \ for the Entity fields.\n  id_key (str): Key in d for the entity's identifier.\
      \ Defaults to \"id\".\n  short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n  title_key (str): Key in\
      \ d for the title. Defaults to \"title\".\n  type_key (str): Key in d for the\
      \ type. Defaults to \"type\".\n  description_key (str): Key in d for the description.\
      \ Defaults to \"description\".\n  description_embedding_key (str): Key in d\
      \ for the description embedding. Defaults to \"description_embedding\".\n  name_embedding_key\
      \ (str): Key in d for the name embedding. Defaults to \"name_embedding\".\n\
      \  community_key (str): Key in d for the community IDs. Defaults to \"community\"\
      .\n  text_unit_ids_key (str): Key in d for the text unit IDs. Defaults to \"\
      text_unit_ids\".\n  rank_key (str): Key in d for the rank. Defaults to \"degree\"\
      .\n  attributes_key (str): Key in d for the attributes. Defaults to \"attributes\"\
      .\n\nReturns:\n  Entity: The newly created Entity instance.\n\nRaises:\n  KeyError:\
      \ If the dictionary does not contain the keys specified by id_key and title_key."
- class_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents
  file: graphrag/language_model/providers/fnllm/events.py
  name: FNLLMEvents
  methods:
  - node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.__init__
    name: __init__
    signature: 'def __init__(self, on_error: ErrorHandlerFn)'
    docstring: "\"\"\"Initialize FNLLMEvents with an error handler to be called on\
      \ errors.\n\nArgs:\n    on_error: ErrorHandlerFn to be invoked on errors.\n\n\
      Returns:\n    None\n\"\"\""
  - node_id: graphrag/language_model/providers/fnllm/events.py::FNLLMEvents.on_error
    name: on_error
    signature: "def on_error(\n        self,\n        error: BaseException | None,\n\
      \        traceback: str | None = None,\n        arguments: dict[str, Any] |\
      \ None = None,\n    ) -> None"
    docstring: "Handle an fnllm error.\n\nArgs:\n    error (BaseException | None):\
      \ The error to handle, or None if no error is available.\n    traceback (str\
      \ | None): The traceback string, or None if not provided.\n    arguments (dict[str,\
      \ Any] | None): Additional arguments related to the error, or None.\n\nReturns:\n\
      \    None\n\nRaises:\n    Exception: If the configured error handler raises\
      \ an exception."
- class_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer
  file: graphrag/tokenizer/litellm_tokenizer.py
  name: LitellmTokenizer
  methods:
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.decode
    name: decode
    signature: 'def decode(self, tokens: list[int]) -> str'
    docstring: "Decode a list of tokens back into a string.\n\nArgs:\n    tokens (list[int]):\
      \ A list of tokens to decode.\n\nReturns:\n    str: The decoded string from\
      \ the list of tokens.\n\nRaises:\n    Exception: If decoding fails due to an\
      \ underlying error in the decoding process."
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.__init__
    name: __init__
    signature: 'def __init__(self, model_name: str) -> None'
    docstring: "\"\"\"Initialize the LiteLLM Tokenizer.\n\nArgs:\n    model_name (str):\
      \ The name of the LiteLLM model to use for tokenization.\n\nReturns:\n    None:\
      \ This initializer does not return a value.\n\"\"\""
  - node_id: graphrag/tokenizer/litellm_tokenizer.py::LitellmTokenizer.encode
    name: encode
    signature: 'def encode(self, text: str) -> list[int]'
    docstring: "Encode the given text into a list of tokens using the configured Litellm\
      \ model.\n\nArgs:\n    self (LitellmTokenizer): The instance of LitellmTokenizer.\
      \ The encoding model is determined by the model_name attribute.\n    text (str):\
      \ The input text to encode.\n\nReturns:\n    list[int]: A list of tokens representing\
      \ the encoded text.\n\nRaises:\n    Exception: If encoding fails due to underlying\
      \ encoder errors or model issues."
- class_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor
  file: graphrag/query/structured_search/drift_search/primer.py
  name: PrimerQueryProcessor
  methods:
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__call__
    name: __call__
    signature: 'def __call__(self, query: str) -> tuple[list[float], dict[str, int]]'
    docstring: "Call method to process the query by expanding it and embedding the\
      \ result.\n\nThis asynchronous method delegates to expand_query to produce an\
      \ expanded\nquery text and a token-count dictionary, then computes the embedding\
      \ for the\nexpanded text using the text embedding model.\n\nArgs:\n    query\
      \ (str): The original search query.\n\nReturns:\n    tuple[list[float], dict[str,\
      \ int]]: A tuple containing\n        - the embedding vector (as a list of floats)\
      \ for the expanded query, and\n        - a token-count dictionary produced by\
      \ expand_query (e.g., containing\n          keys such as \"llm_calls\", \"prompt_tokens\"\
      , and \"output_tokens\").\n\nRaises:\n    Propagates exceptions raised by expand_query\
      \ or by the embedding model."
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.expand_query
    name: expand_query
    signature: 'def expand_query(self, query: str) -> tuple[str, dict[str, int]]'
    docstring: "Expand the query using a random community report template.\n\nArgs:\n\
      \    query (str): The original search query.\n\nReturns:\n    tuple[str, dict[str,\
      \ int]]: Expanded query text and a dictionary with token usage details:\n  \
      \      llm_calls: number of language model calls made (usually 1)\n        prompt_tokens:\
      \ number of tokens in the prompt\n        output_tokens: number of tokens in\
      \ the model output"
  - node_id: graphrag/query/structured_search/drift_search/primer.py::PrimerQueryProcessor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        chat_model: ChatModel,\n   \
      \     text_embedder: EmbeddingModel,\n        reports: list[CommunityReport],\n\
      \        tokenizer: Tokenizer | None = None,\n    )"
    docstring: "Initialize the PrimerQueryProcessor.\n\nArgs:\n    chat_model (ChatModel):\
      \ The language model used to process the query.\n    text_embedder (EmbeddingModel):\
      \ The text embedding model.\n    reports (list[CommunityReport]): List of community\
      \ reports.\n    tokenizer (Tokenizer | None, optional): Token encoder for token\
      \ counting.\n\nReturns:\n    None\n\nRaises:\n    None"
- class_id: graphrag/index/typing/pipeline.py::Pipeline
  file: graphrag/index/typing/pipeline.py
  name: Pipeline
  methods:
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.remove
    name: remove
    signature: 'def remove(self, name: str) -> None'
    docstring: "Remove all workflows from the pipeline that have the given name.\n\
      \nThis method mutates the Pipeline's internal state by removing every workflow\n\
      whose first element (the name) equals the provided value. All matching workflows\n\
      are removed; not just the first match.\n\nTime complexity: O(n), where n is\
      \ the number of workflows in the pipeline.\n\nIf no workflows match, the pipeline\
      \ remains unchanged and no exception is raised.\n\nArgs:\n    name: The name\
      \ of the workflows to remove.\n\nReturns:\n    None"
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.__init__
    name: __init__
    signature: 'def __init__(self, workflows: list[Workflow])'
    docstring: "Initializes the Pipeline with the provided workflows.\n\nArgs:\n \
      \   workflows: list[Workflow] The workflows to include in the pipeline.\n\n\
      Returns:\n    None"
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.run
    name: run
    signature: def run(self) -> Generator[Workflow]
    docstring: "\"\"\"Yield a generator of (name, workflow) pairs from the pipeline.\n\
      \nThe items yielded come from self.workflows and are tuples of (name, Workflow),\n\
      i.e., each yield is a pair containing the workflow's name (str) and the\ncorresponding\
      \ Workflow object.\n\nArgs:\n    self: The Pipeline instance.\n\nReturns:\n\
      \    Generator[tuple[str, Workflow]]: A generator that yields (name, workflow)\
      \ pairs in the pipeline.\n\"\"\""
  - node_id: graphrag/index/typing/pipeline.py::Pipeline.names
    name: names
    signature: def names(self) -> list[str]
    docstring: "Return the names of the workflows in the pipeline.\n\nArgs:\n    self:\
      \ The Pipeline instance.\n\nReturns:\n    list[str]: The names of the workflows\
      \ in the pipeline, extracted from the first element\n        of each workflow\
      \ in self.workflows."
- class_id: graphrag/config/errors.py::LanguageModelConfigMissingError
  file: graphrag/config/errors.py
  name: LanguageModelConfigMissingError
  methods:
  - node_id: graphrag/config/errors.py::LanguageModelConfigMissingError.__init__
    name: __init__
    signature: 'def __init__(self, key: str = "") -> None'
    docstring: "Initialize LanguageModelConfigMissingError with provided key.\n\n\
      Args:\n    key: The key of the missing model configuration. Used to customize\
      \ the error message in settings.yaml.\n\nReturns:\n    None"
- class_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen
  file: graphrag/query/question_gen/local_gen.py
  name: LocalQuestionGen
  methods:
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.agenerate
    name: agenerate
    signature: "def agenerate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate a question based on the question history and context data.\n\
      \nIf context data is not provided, it will be generated by the local context\
      \ builder using the current question and conversation history, along with any\
      \ additional keyword arguments and the configured context_builder_params.\n\n\
      Args:\n    question_history: list[str] - History of previously asked questions.\n\
      \    context_data: str | None - Optional context data used to influence generation;\
      \ None to generate automatically.\n    question_count: int - Number of questions\
      \ to generate.\n    **kwargs: Additional keyword arguments passed to the local\
      \ context builder when constructing context data.\n\nReturns:\n    QuestionResult:\
      \ The generated results including:\n        response: list[str] - The generated\
      \ response split by newline.\n        context_data: dict[str, Any] - The context\
      \ data associated with this question. If context_data was provided, contains\
      \ {\"context_data\": <value>}; otherwise contains keys from the context builder\
      \ result (e.g., \"question_context\" and context_records).\n        completion_time:\
      \ float - Time elapsed for generation in seconds.\n        llm_calls: int -\
      \ Number of LLM API calls performed (typically 1).\n        prompt_tokens: int\
      \ - Number of tokens in the system prompt.\n\nRaises:\n    None: Exceptions\
      \ are caught within the function; on error, a QuestionResult with an empty response\
      \ is returned along with the current context data."
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.generate
    name: generate
    signature: "def generate(\n        self,\n        question_history: list[str],\n\
      \        context_data: str | None,\n        question_count: int,\n        **kwargs,\n\
      \    ) -> QuestionResult"
    docstring: "Generate a question based on the question history and context data.\n\
      \nIf context data is not provided, it will be generated by the local context\
      \ builder.\n\nArgs:\n    question_history (list[str]): History of previously\
      \ asked questions.\n    context_data (str | None): Optional context data; if\
      \ None, context data will be generated by the local context builder.\n    question_count\
      \ (int): Number of questions to generate.\n    kwargs: Additional keyword arguments\
      \ for extensibility (passed to the context builder and model configuration).\n\
      \nReturns:\n    QuestionResult: The generated results including the response\
      \ as a list of strings, context_data containing the question_context and context_records,\
      \ completion_time, llm_calls, and prompt_tokens.\n\nRaises:\n    Exception:\
      \ Exceptions are caught internally and do not propagate to callers; if an unexpected\
      \ error occurs during generation, it will be logged and a default QuestionResult\
      \ will be returned."
  - node_id: graphrag/query/question_gen/local_gen.py::LocalQuestionGen.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model: ChatModel,\n        context_builder:\
      \ LocalContextBuilder,\n        tokenizer: Tokenizer | None = None,\n      \
      \  system_prompt: str = QUESTION_SYSTEM_PROMPT,\n        callbacks: list[BaseLLMCallback]\
      \ | None = None,\n        model_params: dict[str, Any] | None = None,\n    \
      \    context_builder_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize a LocalQuestionGen instance.\n\nArgs:\n    model: ChatModel\
      \ - The language model interface to use.\n    context_builder: LocalContextBuilder\
      \ - The builder that constructs the context for local question generation.\n\
      \    tokenizer: Tokenizer | None - Optional tokenizer to use.\n    system_prompt:\
      \ str - System prompt for question generation. Defaults to QUESTION_SYSTEM_PROMPT.\n\
      \    callbacks: list[BaseLLMCallback] | None - Optional callbacks for LLM events.\
      \ If None, an empty list is used.\n    model_params: dict[str, Any] | None -\
      \ Optional parameters to pass to the model.\n    context_builder_params: dict[str,\
      \ Any] | None - Optional parameters to pass to the context builder.\n\nReturns:\n\
      \    None"
- class_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry
  file: graphrag/language_model/providers/litellm/services/retry/retry.py
  name: Retry
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.retry
    name: retry
    signature: 'def retry(self, func: Callable[..., Any], **kwargs: Any) -> Any'
    docstring: "Retry a synchronous function.\n\nAbstract method that subclasses must\
      \ implement to retry the provided function using their configured retry strategy.\n\
      \nArgs:\n    func: Callable[..., Any] - The function to invoke. It will be called\
      \ as func(**kwargs).\n    kwargs: Any - Additional keyword arguments to pass\
      \ to the function being retried or to control the retry behavior.\n\nReturns:\n\
      \    Any - The result of the retried function invocation.\n\nRaises:\n    NotImplementedError\
      \ - Subclasses must implement this method."
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.__init__
    name: __init__
    signature: 'def __init__(self, /, **kwargs: Any)'
    docstring: "Initialize a Retry subclass.\n\nArgs:\n  kwargs (Any): Arbitrary keyword\
      \ arguments for subclass initialization.\n\nReturns:\n  None\n\nRaises:\n  NotImplementedError:\
      \ If subclass does not implement __init__."
  - node_id: graphrag/language_model/providers/litellm/services/retry/retry.py::Retry.aretry
    name: aretry
    signature: "def aretry(\n        self,\n        func: Callable[..., Awaitable[Any]],\n\
      \        **kwargs: Any,\n    ) -> Any"
    docstring: "Retry an asynchronous function.\n\nArgs:\n  func (Callable[..., Awaitable[Any]]):\
      \ The asynchronous function to retry.\n  kwargs (Any): Additional keyword arguments\
      \ to pass to the function.\n\nReturns:\n  Any: The result of the awaited function.\n\
      \nRaises:\n  NotImplementedError: Subclasses must implement this method"
- class_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection
  file: graphrag/query/context_builder/dynamic_community_selection.py
  name: DynamicCommunitySelection
  methods:
  - node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        model: ChatModel,\n        tokenizer:\
      \ Tokenizer,\n        rate_query: str = RATE_QUERY,\n        use_summary: bool\
      \ = False,\n        threshold: int = 1,\n        keep_parent: bool = False,\n\
      \        num_repeats: int = 1,\n        max_level: int = 2,\n        concurrent_coroutines:\
      \ int = 8,\n        model_params: dict[str, Any] | None = None,\n    )"
    docstring: "Initialize the DynamicCommunitySelection with the provided data and\
      \ prepare internal state for dynamic community selection.\n\nArgs:\n    community_reports\
      \ (list[CommunityReport]): Reports for communities to consider, mapped by community_id.\n\
      \    communities (list[Community]): Community objects used to build the hierarchy\
      \ and starting points.\n    model (ChatModel): Language model instance used\
      \ to rate relevance of communities.\n    tokenizer (Tokenizer): Tokenizer used\
      \ for text processing.\n    rate_query (str): Query string used for rate prompting.\
      \ Defaults to RATE_QUERY.\n    use_summary (bool): If True, use the summary\
      \ content when rating; otherwise use full_content.\n    threshold (int): Minimum\
      \ rating threshold for a report to be considered relevant.\n    keep_parent\
      \ (bool): Whether to preserve parent relationships during selection.\n    num_repeats\
      \ (int): Number of times to repeat the relevance rating process.\n    max_level\
      \ (int): Maximum depth of levels to explore.\n    concurrent_coroutines (int):\
      \ Maximum number of concurrent asynchronous tasks.\n    model_params (dict[str,\
      \ Any] | None): Optional additional parameters to pass to the model; if None,\
      \ an empty dict is used.\n\nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/query/context_builder/dynamic_community_selection.py::DynamicCommunitySelection.select
    name: select
    signature: 'def select(self, query: str) -> tuple[list[CommunityReport], dict[str,
      Any]]'
    docstring: "Asynchronously select relevant communities with respect to the query.\n\
      \nArgs:\n    query (str): The query to rate against.\n\nReturns:\n    tuple[list[CommunityReport],\
      \ dict[str, Any]]: A tuple containing a list of CommunityReport objects representing\
      \ the relevant communities and a dictionary with additional information including\
      \ llm usage metrics (llm_calls, prompt_tokens, output_tokens) and the ratings\
      \ mapping under the key \"ratings\".\n\nRaises:\n    Exceptions raised by rate_relevancy\
      \ or asyncio.gather may propagate to the caller."
- class_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: TextSplitter
  methods:
  - node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    docstring: "Split text into chunks according to the concrete implementation.\n\
      \nArgs:\n    text: str | list[str]\n        The input text to split. Can be\
      \ a single string or a list of strings.\n\nReturns:\n    Iterable[str]\n   \
      \     An iterable of text chunks produced by the split operation."
  - node_id: graphrag/index/text_splitting/text_splitting.py::TextSplitter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        # based on text-ada-002-embedding\
      \ max input buffer length\n        # https://platform.openai.com/docs/guides/embeddings/second-generation-models\n\
      \        chunk_size: int = 8191,\n        chunk_overlap: int = 100,\n      \
      \  length_function: LengthFn = len,\n        keep_separator: bool = False,\n\
      \        add_start_index: bool = False,\n        strip_whitespace: bool = True,\n\
      \    )"
    docstring: "Init method for TextSplitter.\n\nInitialize the text splitter with\
      \ the given configuration.\n\nArgs:\n    chunk_size (int): Maximum length of\
      \ a chunk as measured by length_function. This follows the OpenAI embedding\
      \ model's max input buffer length guidance.\n    chunk_overlap (int): Overlap\
      \ between consecutive chunks, measured using length_function.\n    length_function\
      \ (LengthFn): Function to compute the length of text; defaults to len.\n   \
      \ keep_separator (bool): If True, keep separators when splitting text.\n   \
      \ add_start_index (bool): If True, add the starting index to chunks.\n    strip_whitespace\
      \ (bool): If True, strip leading and trailing whitespace from text.\n\nReturns:\n\
      \    None"
- class_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor
  file: graphrag/index/operations/summarize_communities/community_reports_extractor.py
  name: CommunityReportsExtractor
  methods:
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__call__
    name: __call__
    signature: 'def __call__(self, input_text: str)'
    docstring: "Generate a community report for the given input text using the configured\
      \ model and return both structured and text outputs.\n\nArgs:\n  input_text:\
      \ str - The input text to generate the report from.\n\nReturns:\n  CommunityReportsResult\
      \ - The result containing:\n    structured_output: CommunityReportResponse |\
      \ None - The parsed structured report from the model.\n    output: str - The\
      \ human-readable text representation of the report.\n\nRaises:\n  None"
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor._get_text_output
    name: _get_text_output
    signature: 'def _get_text_output(self, report: CommunityReportResponse) -> str'
    docstring: "\"\"\"Get the text output for a CommunityReportResponse.\n\nArgs:\n\
      \    report: CommunityReportResponse\n        The report object containing a\
      \ title, a summary, and a list of findings. Each finding provides a summary\
      \ and an explanation.\n\nReturns:\n    str\n        A markdown-formatted string.\
      \ It starts with a top-level header using the report title, includes the report\
      \ summary, and appends a section for each finding using its summary as a subheader\
      \ and its explanation as content.\n\nRaises:\n    None\n\"\"\""
  - node_id: graphrag/index/operations/summarize_communities/community_reports_extractor.py::CommunityReportsExtractor.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        model_invoker: ChatModel,\n\
      \        extraction_prompt: str | None = None,\n        on_error: ErrorHandlerFn\
      \ | None = None,\n        max_report_length: int | None = None,\n    )"
    docstring: "Initialize a CommunityReportsExtractor with the provided configuration.\n\
      \nArgs:\n  model_invoker: ChatModel\n    The model invoker used to run prompts.\n\
      \  extraction_prompt: str | None\n    Custom prompt to use for extraction. If\
      \ None, defaults to COMMUNITY_REPORT_PROMPT.\n  on_error: ErrorHandlerFn | None\n\
      \    Function to handle errors. If None, a no-op is used.\n  max_report_length:\
      \ int | None\n    Maximum length of the generated report. If None, defaults\
      \ to 1500.\n\nReturns:\n  None"
- class_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter
  file: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py
  name: StaticRateLimiter
  methods:
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        *,\n        rpm: int | None\
      \ = None,\n        tpm: int | None = None,\n        default_stagger: float =\
      \ 0.0,\n        period_in_seconds: int = 60,\n        **kwargs: Any,\n    )"
    docstring: "Initialize the static rate limiter with optional RPM/TPM limits and\
      \ configuration.\n\nArgs:\n    rpm: int | None\n        RPM limit; positive\
      \ integer or None to disable.\n    tpm: int | None\n        TPM limit; positive\
      \ integer or None to disable.\n    default_stagger: float\n        Default stagger\
      \ between requests; must be >= 0.\n    period_in_seconds: int\n        Length\
      \ of the period in seconds; must be a positive integer.\n    kwargs: Any\n \
      \       Additional keyword arguments (ignored).\n\nReturns:\n    None\n    \
      \    This initializer does not return a value.\n\nRaises:\n    ValueError\n\
      \        If both rpm and tpm are None (disabled), or if rpm/tpm are non-positive,\
      \ or if default_stagger is negative, or if period_in_seconds is not positive."
  - node_id: graphrag/language_model/providers/litellm/services/rate_limiter/static_rate_limiter.py::StaticRateLimiter.acquire
    name: acquire
    signature: 'def acquire(self, *, token_count: int) -> Iterator[None]'
    docstring: "Acquire Rate Limiter.\n\nArgs:\n    token_count: The estimated number\
      \ of tokens for the current request.\n\nReturns:\n    None: This context manager\
      \ yields None and does not return any value."
- class_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion
  file: graphrag/language_model/providers/litellm/types.py
  name: FixedModelCompletion
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::FixedModelCompletion.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        messages: list =\
      \ [],  # type: ignore  # noqa: B006\n        stream: bool | None = None,\n \
      \       stream_options: dict | None = None,  # type: ignore\n        stop=None,\
      \  # type: ignore\n        max_completion_tokens: int | None = None,\n     \
      \   max_tokens: int | None = None,\n        modalities: list[ChatCompletionModality]\
      \ | None = None,\n        prediction: ChatCompletionPredictionContentParam |\
      \ None = None,\n        audio: ChatCompletionAudioParam | None = None,\n   \
      \     logit_bias: dict | None = None,  # type: ignore\n        user: str | None\
      \ = None,\n        # openai v1.0+ new params\n        response_format: dict\
      \ | type[BaseModel] | None = None,  # type: ignore\n        seed: int | None\
      \ = None,\n        tools: list | None = None,  # type: ignore\n        tool_choice:\
      \ str | dict | None = None,  # type: ignore\n        logprobs: bool | None =\
      \ None,\n        top_logprobs: int | None = None,\n        parallel_tool_calls:\
      \ bool | None = None,\n        web_search_options: OpenAIWebSearchOptions |\
      \ None = None,\n        deployment_id=None,  # type: ignore\n        extra_headers:\
      \ dict | None = None,  # type: ignore\n        # soon to be deprecated params\
      \ by OpenAI\n        functions: list | None = None,  # type: ignore\n      \
      \  function_call: str | None = None,\n        # Optional liteLLM function params\n\
      \        thinking: AnthropicThinkingParam | None = None,\n        **kwargs:\
      \ Any,\n    ) -> ModelResponse | CustomStreamWrapper"
    docstring: "Compute a chat completion using a language model and return the model's\
      \ response (or a streaming wrapper).\n\nArgs:\n    messages: list = []\n   \
      \     List of ChatCompletion messages to include in the request.\n    stream:\
      \ bool | None\n        If True, stream partial responses as they arrive.\n \
      \   stream_options: dict | None\n        Options for streaming.\n    stop: Any\n\
      \        Stop sequence or token.\n    max_completion_tokens: int | None\n  \
      \      Maximum number of tokens for the completion.\n    max_tokens: int | None\n\
      \        Maximum number of tokens to generate.\n    modalities: list[ChatCompletionModality]\
      \ | None\n        Modalities for the chat completion.\n    prediction: ChatCompletionPredictionContentParam\
      \ | None\n        Prediction content parameter for the chat completion.\n  \
      \  audio: ChatCompletionAudioParam | None\n        Audio parameters for the\
      \ chat completion.\n    logit_bias: dict | None\n        Biases to apply to\
      \ token logits.\n    user: str | None\n        User identifier.\n    response_format:\
      \ dict | type[BaseModel] | None\n        Response format specification.\n  \
      \  seed: int | None\n        Random seed for deterministic sampling.\n    tools:\
      \ list | None\n        Tools to use during the chat completion.\n    tool_choice:\
      \ str | dict | None\n        Tool selection to use for the request.\n    logprobs:\
      \ bool | None\n        Include log probabilities in the response.\n    top_logprobs:\
      \ int | None\n        Number of top log probabilities to return.\n    parallel_tool_calls:\
      \ bool | None\n        Enable parallel tool calls during processing.\n    web_search_options:\
      \ OpenAIWebSearchOptions | None\n        Web search options used during retrieval.\n\
      \    deployment_id: Any\n        Deployment identifier.\n    extra_headers:\
      \ dict | None\n        Extra HTTP headers to include in the request.\n    functions:\
      \ list | None\n        Deprecated OpenAI functions parameter.\n    function_call:\
      \ str | None\n        Function call specification.\n    thinking: AnthropicThinkingParam\
      \ | None\n        Optional liteLLM thinking parameter.\n    kwargs: Any\n  \
      \      Additional keyword arguments.\n\nReturns:\n    ModelResponse | CustomStreamWrapper\n\
      \        The model response object or a streaming wrapper for streaming responses.\n\
      \nRaises:\n    NotImplementedError\n        If invoked on the base protocol\
      \ without an implementing class.\n\nExample:\n    # Synchronous usage\n    response\
      \ = model(messages=[{'role': 'user', 'content': 'Hello'}], max_tokens=50)\n\n\
      \    # Streaming usage\n    stream = model(messages=[{'role': 'user', 'content':\
      \ 'Explain this concept'}], stream=True)\n    for chunk in stream:\n       \
      \ pass  # handle streaming chunks"
- class_id: graphrag/language_model/response/base.py::ModelOutput
  file: graphrag/language_model/response/base.py
  name: ModelOutput
  methods:
  - node_id: graphrag/language_model/response/base.py::ModelOutput.content
    name: content
    signature: def content(self) -> str
    docstring: "Return the textual content of the output.\n\nReturns:\n    str: The\
      \ textual content of the output."
  - node_id: graphrag/language_model/response/base.py::ModelOutput.full_response
    name: full_response
    signature: def full_response(self) -> dict[str, Any] | None
    docstring: "\"\"\"Return the complete JSON response returned by the model.\n\n\
      Args:\n    self: The instance from which the full_response is accessed.\n\n\
      Returns:\n    dict[str, Any] | None: The complete JSON response returned by\
      \ the model.\n\n\"\"\""
- class_id: graphrag/data_model/covariate.py::Covariate
  file: graphrag/data_model/covariate.py
  name: Covariate
  methods:
  - node_id: graphrag/data_model/covariate.py::Covariate.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        subject_id_key: str = \"subject_id\",\n   \
      \     covariate_type_key: str = \"covariate_type\",\n        short_id_key: str\
      \ = \"human_readable_id\",\n        text_unit_ids_key: str = \"text_unit_ids\"\
      ,\n        attributes_key: str = \"attributes\",\n    ) -> \"Covariate\""
    docstring: "Create a new Covariate from the dict data.\n\nArgs:\n    cls (type):\
      \ The Covariate class; this is a classmethod.\n    d (dict[str, Any]): The dictionary\
      \ containing covariate fields. The function reads keys including id_key, subject_id_key,\
      \ covariate_type_key, short_id_key, text_unit_ids_key, and attributes_key to\
      \ construct the Covariate.\n    id_key (str): The key in d that corresponds\
      \ to the covariate's id.\n    subject_id_key (str): The key in d that corresponds\
      \ to the subject's id.\n    covariate_type_key (str): The key in d for the covariate\
      \ type.\n    short_id_key (str): The key in d that corresponds to the covariate's\
      \ short id (human readable).\n    text_unit_ids_key (str): The key in d that\
      \ contains text unit ids (optional).\n    attributes_key (str): The key in d\
      \ that contains additional attributes (optional).\n\nReturns:\n    Covariate:\
      \ The Covariate instance created from the dictionary.\n\nRaises:\n    KeyError:\
      \ If d does not contain the required keys identified by id_key or subject_id_key."
- class_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter
  file: graphrag/index/text_splitting/text_splitting.py
  name: NoopTextSplitter
  methods:
  - node_id: graphrag/index/text_splitting/text_splitting.py::NoopTextSplitter.split_text
    name: split_text
    signature: 'def split_text(self, text: str | list[str]) -> Iterable[str]'
    docstring: "Split text into chunks.\n\nArgs:\n    text: str | list[str]\n    \
      \    The input text to split. A single string or a list of strings.\n\nReturns:\n\
      \    Iterable[str]\n        An iterable of text chunks. If a string is provided,\
      \ returns a single-element list containing the string; if a list of strings\
      \ is provided, returns that list as-is.\n\nRaises:\n    None"
- class_id: graphrag/config/enums.py::InputFileType
  file: graphrag/config/enums.py
  name: InputFileType
  methods:
  - node_id: graphrag/config/enums.py::InputFileType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enum member.\n\nArgs:\n \
      \   self: The enum member instance.\n\nReturns:\n    str: The string representation\
      \ of the enum member, with its value enclosed in double quotes.\n\"\"\""
- class_id: graphrag/query/context_builder/conversation_history.py::ConversationRole
  file: graphrag/query/context_builder/conversation_history.py
  name: ConversationRole
  methods:
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.__str__
    name: __str__
    signature: def __str__(self) -> str
    docstring: "Return string representation of the enum value.\n\nArgs:\n    self:\
      \ The enum member.\n\nReturns:\n    str: The string representation of the enum\
      \ value."
  - node_id: graphrag/query/context_builder/conversation_history.py::ConversationRole.from_string
    name: from_string
    signature: 'def from_string(value: str) -> "ConversationRole"'
    docstring: "\"\"\"Convert string to ConversationRole.\n\nArgs:\n    value: str.\
      \ The role as a string. Expected values are \"system\", \"user\", or \"assistant\"\
      .\n\nReturns:\n    ConversationRole. The corresponding ConversationRole enum\
      \ member.\n\nRaises:\n    ValueError: If value is not one of the supported roles.\n\
      \"\"\""
- class_id: graphrag/config/enums.py::CacheType
  file: graphrag/config/enums.py
  name: CacheType
  methods:
  - node_id: graphrag/config/enums.py::CacheType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes.\n\"\"\""
- class_id: graphrag/config/enums.py::ReportingType
  file: graphrag/config/enums.py
  name: ReportingType
  methods:
  - node_id: graphrag/config/enums.py::ReportingType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes."
- class_id: graphrag/language_model/response/base.pyi::BaseModelOutput
  file: graphrag/language_model/response/base.pyi
  name: BaseModelOutput
  methods:
  - node_id: graphrag/language_model/response/base.pyi::BaseModelOutput.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        content: str,\n        full_response:\
      \ dict[str, Any] | None = None,\n    ) -> None"
    docstring: "BaseModelOutput initialization.\n\nInitializes a BaseModelOutput with\
      \ the given content and optional full_response.\n\nArgs:\n    content: The output\
      \ content as a string.\n    full_response: Optional dict[str, Any] representing\
      \ the full response; defaults to None.\n\nReturns:\n    None"
- class_id: graphrag/config/errors.py::ConflictingSettingsError
  file: graphrag/config/errors.py
  name: ConflictingSettingsError
  methods:
  - node_id: graphrag/config/errors.py::ConflictingSettingsError.__init__
    name: __init__
    signature: 'def __init__(self, msg: str) -> None'
    docstring: "Initialize error with the provided message.\n\nArgs:\n    msg: The\
      \ error message to pass to the base ValueError constructor.\n\nReturns:\n  \
      \  None"
- class_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext
  file: graphrag/query/structured_search/local_search/mixed_context.py
  name: LocalSearchMixedContext
  methods:
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.filter_by_entity_keys
    name: filter_by_entity_keys
    signature: 'def filter_by_entity_keys(self, entity_keys: list[int] | list[str])'
    docstring: "Filter entity text embeddings by entity keys.\n\nArgs:\n    entity_keys:\
      \ List of entity keys to filter by. May be a list of integers or a list of strings.\n\
      \nReturns:\n    None"
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        entities: list[Entity],\n  \
      \      entity_text_embeddings: BaseVectorStore,\n        text_embedder: EmbeddingModel,\n\
      \        text_units: list[TextUnit] | None = None,\n        community_reports:\
      \ list[CommunityReport] | None = None,\n        relationships: list[Relationship]\
      \ | None = None,\n        covariates: dict[str, list[Covariate]] | None = None,\n\
      \        tokenizer: Tokenizer | None = None,\n        embedding_vectorstore_key:\
      \ str = EntityVectorStoreKey.ID,\n    )"
    docstring: "Initialize a LocalSearchMixedContext with the provided data and optional\
      \ configuration.\n\nArgs:\n    entities: list[Entity] The list of entities to\
      \ include.\n    entity_text_embeddings: BaseVectorStore The vector store containing\
      \ embeddings for entity text.\n    text_embedder: EmbeddingModel The embedding\
      \ model used to embed text for similarity search.\n    text_units: list[TextUnit]\
      \ | None Optional list of TextUnit objects to include.\n    community_reports:\
      \ list[CommunityReport] | None Optional list of CommunityReport objects to include.\n\
      \    relationships: list[Relationship] | None Optional list of Relationship\
      \ objects to include.\n    covariates: dict[str, list[Covariate]] | None Optional\
      \ mapping of covariates by key.\n    tokenizer: Tokenizer | None Optional tokenizer\
      \ to use; if None, get_tokenizer() will be used.\n    embedding_vectorstore_key:\
      \ str The key for the embedding vector store; defaults to EntityVectorStoreKey.ID.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_text_unit_context
    name: _build_text_unit_context
    signature: "def _build_text_unit_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        return_candidate_context:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        context_name:\
      \ str = \"Sources\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Rank matching text units and add them to the context window until\
      \ it hits the max_context_tokens limit.\n\nArgs:\n    selected_entities (list[Entity]):\
      \ Entities for which to collect and rank associated text units.\n    max_context_tokens\
      \ (int): Maximum number of tokens to include in the context.\n    return_candidate_context\
      \ (bool): If True, also compute and include candidate text units context data.\n\
      \    column_delimiter (str): Delimiter used to separate fields in the context\
      \ rows.\n    context_name (str): Name of the context section to populate (default\
      \ \"Sources\").\n\nReturns:\n    tuple[str, dict[str, pd.DataFrame]]: The textual\
      \ context and a mapping from context names to DataFrames containing the context\
      \ data."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        include_entity_names: list[str]\
      \ | None = None,\n        exclude_entity_names: list[str] | None = None,\n \
      \       conversation_history_max_turns: int | None = 5,\n        conversation_history_user_turns_only:\
      \ bool = True,\n        max_context_tokens: int = 8000,\n        text_unit_prop:\
      \ float = 0.5,\n        community_prop: float = 0.25,\n        top_k_mapped_entities:\
      \ int = 10,\n        top_k_relationships: int = 10,\n        include_community_rank:\
      \ bool = False,\n        include_entity_rank: bool = False,\n        rank_description:\
      \ str = \"number of relationships\",\n        include_relationship_weight: bool\
      \ = False,\n        relationship_ranking_attribute: str = \"rank\",\n      \
      \  return_candidate_context: bool = False,\n        use_community_summary: bool\
      \ = False,\n        min_community_rank: int = 0,\n        community_context_name:\
      \ str = \"Reports\",\n        column_delimiter: str = \"|\",\n        **kwargs:\
      \ dict[str, Any],\n    ) -> ContextBuilderResult"
    docstring: "Build data context for local search prompt.\n\nBuild a context by\
      \ combining community reports and entity/relationship/covariate tables, and\
      \ text units using a predefined ratio set by text_unit_prop and community_prop.\n\
      \nArgs:\n  query (str): The user query to build context for.\n  conversation_history\
      \ (ConversationHistory | None): Optional conversation history to consider while\
      \ constructing the context.\n  include_entity_names (list[str] | None): Entity\
      \ names to explicitly include in the mapping.\n  exclude_entity_names (list[str]\
      \ | None): Entity names to exclude from consideration.\n  conversation_history_max_turns\
      \ (int | None): Maximum number of user turns from conversation history to include.\n\
      \  conversation_history_user_turns_only (bool): If True, only user turns from\
      \ the conversation history are used.\n  max_context_tokens (int): Maximum token\
      \ budget for the constructed context.\n  text_unit_prop (float): Proportion\
      \ of the context allocated to text units.\n  community_prop (float): Proportion\
      \ of the context allocated to community context.\n  top_k_mapped_entities (int):\
      \ Number of top entities to map from the query.\n  top_k_relationships (int):\
      \ Number of top relationships to include for local context.\n  include_community_rank\
      \ (bool): Whether to include community ranking information in the community\
      \ context.\n  include_entity_rank (bool): Whether to include entity ranking\
      \ information in the local context.\n  rank_description (str): Description used\
      \ when presenting rankings.\n  include_relationship_weight (bool): Whether to\
      \ include relationship weights in the local context.\n  relationship_ranking_attribute\
      \ (str): Attribute name used for ranking relationships.\n  return_candidate_context\
      \ (bool): If True, return candidate context alongside the main context data.\n\
      \  use_community_summary (bool): If True, use a summarized representation of\
      \ community context when available.\n  min_community_rank (int): Minimum rank\
      \ threshold for including a community.\n  community_context_name (str): Label/name\
      \ for the community context section.\n  column_delimiter (str): Delimiter used\
      \ to separate fields in generated context.\n  **kwargs (dict[str, Any]): Additional\
      \ keyword arguments that may influence how the context is built.\n\nReturns:\n\
      \  ContextBuilderResult: The result containing the built context and associated\
      \ data.\n\nRaises:\n  ValueError: If the sum of community_prop and text_unit_prop\
      \ exceeds 1."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_community_context
    name: _build_community_context
    signature: "def _build_community_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 4000,\n        use_community_summary:\
      \ bool = False,\n        column_delimiter: str = \"|\",\n        include_community_rank:\
      \ bool = False,\n        min_community_rank: int = 0,\n        return_candidate_context:\
      \ bool = False,\n        context_name: str = \"Reports\",\n    ) -> tuple[str,\
      \ dict[str, pd.DataFrame]]"
    docstring: "Add community data to the context window until it hits the max_context_tokens\
      \ limit.\n\nArgs:\n  selected_entities (list[Entity]): Entities selected for\
      \ which related communities should be added to the context.\n  max_context_tokens\
      \ (int): Maximum number of tokens to include for the community context.\n  use_community_summary\
      \ (bool): Whether to utilize a summarized representation of communities.\n \
      \ column_delimiter (str): Delimiter used between columns in the generated context.\n\
      \  include_community_rank (bool): Whether to include the community rank in the\
      \ context.\n  min_community_rank (int): Minimum community rank to include in\
      \ the context.\n  return_candidate_context (bool): If True, also compute and\
      \ return candidate context data.\n  context_name (str): Name of the context\
      \ section (default \"Reports\").\n\nReturns:\n  tuple[str, dict[str, pd.DataFrame]]:\
      \ A tuple consisting of\n    - context_text: String containing the assembled\
      \ community context; may be empty.\n    - context_data: Mapping from a lowercase\
      \ context name to a DataFrame with additional context information.\n\nRaises:\n\
      \  Exceptions raised by underlying calls (e.g., build_community_context, get_candidate_communities)\n\
      \  may propagate to the caller."
  - node_id: graphrag/query/structured_search/local_search/mixed_context.py::LocalSearchMixedContext._build_local_context
    name: _build_local_context
    signature: "def _build_local_context(\n        self,\n        selected_entities:\
      \ list[Entity],\n        max_context_tokens: int = 8000,\n        include_entity_rank:\
      \ bool = False,\n        rank_description: str = \"relationship count\",\n \
      \       include_relationship_weight: bool = False,\n        top_k_relationships:\
      \ int = 10,\n        relationship_ranking_attribute: str = \"rank\",\n     \
      \   return_candidate_context: bool = False,\n        column_delimiter: str =\
      \ \"|\",\n    ) -> tuple[str, dict[str, pd.DataFrame]]"
    docstring: "Build data context for local search prompt by combining entity/relationship/covariate\
      \ tables.\n\nArgs:\n  selected_entities (list[Entity]): Entities to include\
      \ in the context.\n  max_context_tokens (int): Maximum allowed tokens for the\
      \ generated context text. Parsing stops when adding a new entity would exceed\
      \ this limit.\n  include_entity_rank (bool): Whether to include the entity's\
      \ rank in the context rows.\n  rank_description (str): Description of the ranking\
      \ used in the entity context (default: 'relationship count').\n  include_relationship_weight\
      \ (bool): Whether to include the relationship weight in the generated context.\n\
      \  top_k_relationships (int): Number of top relationships to include for each\
      \ entity.\n  relationship_ranking_attribute (str): Attribute name used to rank\
      \ relationships (default: 'rank').\n  return_candidate_context (bool): If True,\
      \ return all candidate entities/relationships/covariates (not only those fitted\
      \ into the context window) and tag them accordingly.\n  column_delimiter (str):\
      \ Delimiter used to separate fields in the generated context.\n\nReturns:\n\
      \  tuple[str, dict[str, pd.DataFrame]]: The final context text and a mapping\
      \ from context section names to their corresponding DataFrames."
- class_id: unified-search-app/app/state/session_variables.py::SessionVariables
  file: unified-search-app/app/state/session_variables.py
  name: SessionVariables
  methods:
  - node_id: unified-search-app/app/state/session_variables.py::SessionVariables.__init__
    name: __init__
    signature: def __init__(self)
    docstring: 'Initialize all SessionVariables for the unified search app.


      SessionVariables.__init__ creates and initializes every session attribute used
      to track the

      state of a unified search session. Each attribute is assigned a default value
      to ensure a

      consistent, predictable initial state.


      Attributes initialized (with defaults):

      - dataset: QueryVariable("dataset", "")

      - datasets: SessionVariable([])

      - dataset_config: SessionVariable()

      - datasource: SessionVariable()

      - graphrag_config: SessionVariable()

      - question: QueryVariable("question", "")

      - suggested_questions: SessionVariable(default_suggested_questions)

      - entities: SessionVariable([])

      - relationships: SessionVariable([])

      - covariates: SessionVariable({})

      - communities: SessionVariable([])

      - community_reports: SessionVariable([])

      - text_units: SessionVariable([])

      - question_in_progress: SessionVariable("")

      - include_global_search: QueryVariable("include_global_search", True)

      - include_local_search: QueryVariable("include_local_search", True)

      - include_drift_search: QueryVariable("include_drift_search", False)

      - include_basic_rag: QueryVariable("include_basic_rag", False)

      - selected_report: SessionVariable()

      - graph_community_level: SessionVariable(0)

      - selected_question: SessionVariable("")

      - generated_questions: SessionVariable([])

      - show_text_input: SessionVariable(True)


      Returns:

      None'
- class_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource
  file: unified-search-app/app/knowledge_loader/data_sources/local_source.py
  name: LocalDatasource
  methods:
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read
    name: read
    signature: "def read(\n        self,\n        table: str,\n        throw_on_missing:\
      \ bool = False,\n        columns: list[str] | None = None,\n    ) -> pd.DataFrame"
    docstring: "Read file from local source.\n\nArgs:\n    table: The table name to\
      \ read (without the .parquet extension).\n    throw_on_missing: If True, raise\
      \ FileNotFoundError when the table file does not exist.\n    columns: Optional\
      \ list of column names to read from the parquet file. If None, all columns are\
      \ read.\n\nReturns:\n    A pandas DataFrame containing the data from the parquet\
      \ file. If the table file does not exist\n    and throw_on_missing is False,\
      \ returns an empty DataFrame; if columns is provided, the DataFrame\n    will\
      \ contain those columns.\n\nRaises:\n    FileNotFoundError: If the table file\
      \ does not exist and throw_on_missing is True."
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.__init__
    name: __init__
    signature: 'def __init__(self, base_path: str)'
    docstring: "Initialize LocalDatasource with the provided base path.\n\nArgs:\n\
      \    base_path: The base directory path for local data sources. Type: str.\n\
      \nReturns:\n    None\n\nRaises:\n    None"
  - node_id: unified-search-app/app/knowledge_loader/data_sources/local_source.py::LocalDatasource.read_settings
    name: read_settings
    signature: "def read_settings(\n        self,\n        file: str,\n        throw_on_missing:\
      \ bool = False,\n    ) -> GraphRagConfig | None"
    docstring: "Read settings file from local source.\n\nNote: The 'file' parameter\
      \ is unused. Settings are loaded by invoking load_config with root_dir derived\
      \ from the datasource's base_path.\n\nArgs:\n    self: The LocalDatasource instance.\n\
      \    file: str. Unused. Path to the settings file; present for API compatibility.\n\
      \    throw_on_missing: bool. Ignored by this implementation.\n\nReturns:\n \
      \   GraphRagConfig | None: The GraphRagConfig produced by load_config, or None\
      \ if no configuration could be loaded.\n\nRaises:\n    None: This method does\
      \ not raise exceptions; any loading errors originate from load_config."
- class_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig
  file: graphrag/config/models/extract_graph_config.py
  name: ExtractGraphConfig
  methods:
  - node_id: graphrag/config/models/extract_graph_config.py::ExtractGraphConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved entity extraction strategy.\n\nArgs:\n    root_dir\
      \ (str): The root directory used to resolve the graph and text prompt file paths.\n\
      \    model_config (LanguageModelConfig): The LanguageModelConfig instance containing\
      \ the model configuration; its model_dump() result is included in the strategy\
      \ as llm.\n\nReturns:\n    dict: The resolved strategy. If self.strategy is\
      \ provided, it is returned as-is; otherwise, a default strategy dictionary is\
      \ returned with the following keys:\n        type: The strategy type (ExtractEntityStrategyType.graph_intelligence).\n\
      \        llm: model_config.model_dump().\n        extraction_prompt: The contents\
      \ of the prompt file located at Path(root_dir) / self.prompt, read as UTF-8,\
      \ if self.prompt is set; otherwise None.\n        max_gleanings: self.max_gleanings."
- class_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig
  file: graphrag/config/models/summarize_descriptions_config.py
  name: SummarizeDescriptionsConfig
  methods:
  - node_id: graphrag/config/models/summarize_descriptions_config.py::SummarizeDescriptionsConfig.resolved_strategy
    name: resolved_strategy
    signature: "def resolved_strategy(\n        self, root_dir: str, model_config:\
      \ LanguageModelConfig\n    ) -> dict"
    docstring: "Get the resolved description summarization strategy.\n\nArgs:\n  \
      \  root_dir: The root directory used to resolve the graph and text prompt file\
      \ paths.\n    model_config: The LanguageModelConfig instance containing the\
      \ model configuration; its model_dump() result is included in the strategy as\
      \ llm.\n\nReturns:\n    dict: The resolved strategy. If a custom strategy is\
      \ provided via self.strategy, that is returned; otherwise, a default strategy\
      \ dictionary is returned with the following keys:\n        type: The strategy\
      \ type (graph_intelligence)\n        llm: The serialized language model configuration\
      \ from model_config.model_dump()\n        summarize_prompt: The contents of\
      \ the prompt file located at Path(root_dir) / self.prompt when self.prompt is\
      \ set; otherwise None\n        max_summary_length: The maximum length for the\
      \ summary from self.max_length\n        max_input_tokens: The maximum input\
      \ tokens from self.max_input_tokens\n\nRaises:\n    IOError or OSError: If reading\
      \ the summarize_prompt file fails (e.g., prompt file is missing or unreadable)\
      \ when self.prompt is provided."
- class_id: unified-search-app/app/state/query_variable.py::QueryVariable
  file: unified-search-app/app/state/query_variable.py
  name: QueryVariable
  methods:
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.__init__
    name: __init__
    signature: 'def __init__(self, key: str, default: Any | None)'
    docstring: "Initialize a QueryVariable to manage a single URL query parameter\
      \ and its corresponding session_state entry.\n\nThis constructor reads the value\
      \ for the given key from the URL query parameters when available; if the key\
      \ is not present, it uses the provided default. When reading from the query\
      \ string, the value is normalized to lowercase to support case-insensitive URLs.\
      \ If the resulting value equals \"true\" or \"false\" (after normalization),\
      \ it is converted to the corresponding Python boolean True or False. If the\
      \ key is not already present in Streamlit's session_state, the derived value\
      \ is written to session_state under that key. If the key already exists in session_state,\
      \ its existing value is preserved and not overwritten during initialization.\n\
      \nArgs:\n    key (str): The key of the query parameter to manage.\n    default\
      \ (Any | None): The default value to use if the key is not present in the query\
      \ parameters.\n\nReturns:\n    None"
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.key
    name: key
    signature: def key(self) -> str
    docstring: "Key property that returns the session_state key for this variable.\n\
      \nArgs:\n    self (QueryVariable): The instance of QueryVariable.\n\nReturns:\n\
      \    str: The key used to access the session_state dictionary for this variable."
  - node_id: unified-search-app/app/state/query_variable.py::QueryVariable.value
    name: value
    signature: 'def value(self, value: Any) -> None'
    docstring: "Value setter for the QueryVariable. Sets the session state value and\
      \ updates the corresponding URL query parameter with the lowercase string representation\
      \ of the value.\n\nArgs:\n    value: The new value to assign to the session\
      \ variable. Can be of any type.\n\nReturns:\n    None"
- class_id: graphrag/config/enums.py::StorageType
  file: graphrag/config/enums.py
  name: StorageType
  methods:
  - node_id: graphrag/config/enums.py::StorageType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of the enumeration member.\n\nArgs:\n\
      \    self (Enum): The enumeration member.\n\nReturns:\n    str: The member's\
      \ value wrapped in double quotes.\n\"\"\""
- class_id: graphrag/query/context_builder/builders.py::LocalContextBuilder
  file: graphrag/query/context_builder/builders.py
  name: LocalContextBuilder
  methods:
  - node_id: graphrag/query/context_builder/builders.py::LocalContextBuilder.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        **kwargs,\n    ) -> ContextBuilderResult"
    docstring: "\"\"\"Build the context for the local search mode.\n\nArgs:\n  query\
      \ (str): The user query to build context for.\n  conversation_history (ConversationHistory\
      \ | None): Optional conversation history to consider while constructing the\
      \ context.\n  **kwargs: Additional keyword arguments that may influence how\
      \ the context is built.\n\nReturns:\n  ContextBuilderResult: The result containing\
      \ the built context for the local search mode.\n\nRaises:\n  NotImplementedError:\
      \ If invoked on the abstract base class.\n\"\"\""
- class_id: graphrag/data_model/community.py::Community
  file: graphrag/data_model/community.py
  name: Community
  methods:
  - node_id: graphrag/data_model/community.py::Community.from_dict
    name: from_dict
    signature: "def from_dict(\n        cls,\n        d: dict[str, Any],\n       \
      \ id_key: str = \"id\",\n        title_key: str = \"title\",\n        short_id_key:\
      \ str = \"human_readable_id\",\n        level_key: str = \"level\",\n      \
      \  entities_key: str = \"entity_ids\",\n        relationships_key: str = \"\
      relationship_ids\",\n        text_units_key: str = \"text_unit_ids\",\n    \
      \    covariates_key: str = \"covariate_ids\",\n        parent_key: str = \"\
      parent\",\n        children_key: str = \"children\",\n        attributes_key:\
      \ str = \"attributes\",\n        size_key: str = \"size\",\n        period_key:\
      \ str = \"period\",\n    ) -> \"Community\""
    docstring: "Create a new Community from the dict data.\n\nArgs:\n  cls (type):\
      \ The class.\n  d (dict[str, Any]): The source dictionary containing the values\
      \ for the Community fields.\n  id_key (str): Key in d for the community's identifier.\
      \ Defaults to \"id\".\n  title_key (str): Key in d for the community title.\
      \ Defaults to \"title\".\n  short_id_key (str): Key in d for the optional short\
      \ identifier. Defaults to \"human_readable_id\".\n  level_key (str): Key in\
      \ d for the community level. Defaults to \"level\".\n  entities_key (str): Key\
      \ in d for the related entity IDs. Defaults to \"entity_ids\".\n  relationships_key\
      \ (str): Key in d for the related relationship IDs. Defaults to \"relationship_ids\"\
      .\n  text_units_key (str): Key in d for the related text unit IDs. Defaults\
      \ to \"text_unit_ids\".\n  covariates_key (str): Key in d for covariate IDs.\
      \ Defaults to \"covariate_ids\".\n  parent_key (str): Key in d for the parent\
      \ community's ID. Defaults to \"parent\".\n  children_key (str): Key in d for\
      \ the child community IDs. Defaults to \"children\".\n  attributes_key (str):\
      \ Key in d for additional attributes. Defaults to \"attributes\".\n  size_key\
      \ (str): Key in d for the size of the community. Defaults to \"size\".\n  period_key\
      \ (str): Key in d for the period. Defaults to \"period\".\n\nReturns:\n  Community:\
      \ The newly created Community instance.\n\nRaises:\n  KeyError: If required\
      \ keys are missing from the input dictionary."
- class_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc
  file: graphrag/language_model/providers/litellm/types.py
  name: AsyncLitellmRequestFunc
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::AsyncLitellmRequestFunc.__call__
    name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    docstring: "Asynchronous request function.\n\nRepresents an asynchronous call\
      \ to either a chat completion or embedding function. The implementation forwards\
      \ all provided keyword arguments to the underlying request function, enabling\
      \ flexible use with different backends.\n\nArgs:\n    kwargs: Arbitrary keyword\
      \ arguments forwarded to the underlying request function. Specific accepted\
      \ keys depend on the concrete implementation (e.g., chat completion or embedding).\n\
      \nReturns:\n    Any: The result produced by the underlying request function.\
      \ The exact type depends on the concrete function being invoked (e.g., a chat\
      \ completion response or an embedding).\n\nRaises:\n    Exception: Exceptions\
      \ raised by the underlying request function are propagated to the caller (e.g.,\
      \ API or network errors)."
- class_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType
  file: graphrag/index/operations/summarize_descriptions/typing.py
  name: SummarizeStrategyType
  methods:
  - node_id: graphrag/index/operations/summarize_descriptions/typing.py::SummarizeStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "Get a string representation of this SummarizeStrategyType enum member.\n\
      \nArgs:\n    self: SummarizeStrategyType, the enum member to represent as a\
      \ string.\n\nReturns:\n    str: The enum member's value enclosed in double quotes."
- class_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType
  file: graphrag/index/operations/embed_text/embed_text.py
  name: TextEmbedStrategyType
  methods:
  - node_id: graphrag/index/operations/embed_text/embed_text.py::TextEmbedStrategyType.__repr__
    name: __repr__
    signature: def __repr__(self)
    docstring: "\"\"\"Get a string representation of this TextEmbedStrategyType enum\
      \ member.\n\nArgs:\n    self: TextEmbedStrategyType, the enum member to represent\
      \ as a string.\n\nReturns:\n    str: The enum member's value enclosed in double\
      \ quotes.\n\"\"\""
- class_id: graphrag/language_model/response/base.pyi::BaseModelResponse
  file: graphrag/language_model/response/base.pyi
  name: BaseModelResponse
  methods:
  - node_id: graphrag/language_model/response/base.pyi::BaseModelResponse.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        output: BaseModelOutput,\n \
      \       parsed_response: _T | None = None,\n        history: list[Any] = ...,\
      \  # default provided by Pydantic\n        tool_calls: list[Any] = ...,  # default\
      \ provided by Pydantic\n        metrics: Any | None = None,\n        cache_hit:\
      \ bool | None = None,\n    ) -> None"
    docstring: "Initializes a BaseModelResponse with the given output, parsed_response,\
      \ and optional metadata.\n\nArgs:\n    output: BaseModelOutput\n        BaseModelOutput\
      \ instance containing the content and full_response.\n    parsed_response: _T\
      \ | None\n        The parsed response of type _T, or None.\n    history: list[Any]\n\
      \        History list; default provided by Pydantic.\n    tool_calls: list[Any]\n\
      \        Tool calls list; default provided by Pydantic.\n    metrics: Any |\
      \ None\n        Metrics associated with the response; may be None.\n    cache_hit:\
      \ bool | None\n        Indicates whether a cache hit occurred; may be None.\n\
      \nReturns:\n    None"
- class_id: graphrag/config/errors.py::AzureApiVersionMissingError
  file: graphrag/config/errors.py
  name: AzureApiVersionMissingError
  methods:
  - node_id: graphrag/config/errors.py::AzureApiVersionMissingError.__init__
    name: __init__
    signature: 'def __init__(self, llm_type: str) -> None'
    docstring: "Init method for AzureApiVersionMissingError (internal API).\n\nThis\
      \ constructor formats the error message using the provided llm_type as: \"API\
      \ Version is required for {llm_type}. Please rerun graphrag init and set the\
      \ api_version.\" It initializes the base ValueError with that message. It returns\
      \ None and does not raise the exception by itself.\n\nArgs:\n    llm_type: The\
      \ LLM type for which the API Version is required.\n\nReturns:\n    None\n\n\
      Raises:\n    None"
- class_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding
  file: graphrag/language_model/providers/litellm/types.py
  name: FixedModelEmbedding
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::FixedModelEmbedding.__call__
    name: __call__
    signature: "def __call__(\n        self,\n        *,\n        request_id: str\
      \ | None = None,\n        input: list = [],  # type: ignore  # noqa: B006\n\
      \        # Optional params\n        dimensions: int | None = None,\n       \
      \ encoding_format: str | None = None,\n        timeout: int = 600,  # default\
      \ to 10 minutes\n        # set api_base, api_version, api_key\n        api_base:\
      \ str | None = None,\n        api_version: str | None = None,\n        api_key:\
      \ str | None = None,\n        api_type: str | None = None,\n        caching:\
      \ bool = False,\n        user: str | None = None,\n        **kwargs: Any,\n\
      \    ) -> EmbeddingResponse"
    docstring: "Embedding function for a model configured elsewhere.\n\nCompute embeddings\
      \ for a batch of inputs using a pre-configured model (no model parameter is\
      \ required). This synchronous embedding function mirrors litellm.embedding but\
      \ omits the model argument, relying on the model configuration.\n\nArgs:\n \
      \ request_id (str | None): Optional request identifier.\n  input (list): List\
      \ of inputs to embed.\n  dimensions (int | None): Optional embedding dimensions\
      \ to request. If None, the model's default is used.\n  encoding_format (str\
      \ | None): Optional encoding format to return embeddings in. If None, the default\
      \ format is used.\n  timeout (int): Timeout in seconds for the request (default\
      \ 600).\n  api_base (str | None): Optional API base URL.\n  api_version (str\
      \ | None): Optional API version.\n  api_key (str | None): Optional API key for\
      \ authentication.\n  api_type (str | None): Optional API type.\n  caching (bool):\
      \ Enable or disable caching of embeddings. Default is False.\n  user (str |\
      \ None): Optional user identifier for the request.\n  kwargs (Any): Additional\
      \ keyword arguments passed to the underlying embedding call.\n\nReturns:\n \
      \ EmbeddingResponse: The embedding response object containing the embeddings\
      \ and related metadata.\n\nRaises:\n  ValueError: If input is not a list.\n\
      \  TimeoutError: If the embedding request times out.\n  Exception: If an error\
      \ occurs during the embedding request."
- class_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc
  file: graphrag/language_model/providers/litellm/types.py
  name: LitellmRequestFunc
  methods:
  - node_id: graphrag/language_model/providers/litellm/types.py::LitellmRequestFunc.__call__
    name: __call__
    signature: 'def __call__(self, /, **kwargs: Any) -> Any'
    docstring: "Synchronous request function.\n\nRepresents either a chat completion\
      \ or embedding function. The implementation forwards all provided keyword arguments\
      \ to the underlying request function, enabling flexible use with different backends.\n\
      \nArgs:\n    kwargs: Arbitrary keyword arguments forwarded to the underlying\
      \ request function. Specific accepted keys depend on the concrete impl...\n\n\
      Returns:\n    Any: The result of the underlying request function.\n\nRaises:\n\
      \    Exception: If the underlying request function raises an exception, it will\
      \ propagate to the caller."
- class_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain
  file: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py
  name: TestRunChain
  methods:
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_single_document_correct_entities_returned
    name: test_run_extract_graph_single_document_correct_entities_returned
    signature: def test_run_extract_graph_single_document_correct_entities_returned(self)
    docstring: 'Tests that run_extract_graph returns the expected entity titles for
      a single document.


      Args:

      - self: The test method instance (TestRunChain).


      Returns:

      - None: The test does not return a value.


      Raises:

      - AssertionError: If the assertion verifying the returned entities fails.'
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entities_returned
    name: test_run_extract_graph_multiple_documents_correct_entities_returned
    signature: "def test_run_extract_graph_multiple_documents_correct_entities_returned(\n\
      \        self,\n    )"
    docstring: "Tests that run_extract_graph returns the expected entity titles for\
      \ multiple documents.\n\nArgs:\n    self (TestRunChain): The test method instance.\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If the assertion verifying the returned entities fails."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edges_returned
    name: test_run_extract_graph_multiple_documents_correct_edges_returned
    signature: def test_run_extract_graph_multiple_documents_correct_edges_returned(self)
    docstring: "Verify that run_extract_graph returns a graph with the correct edges\
      \ when given multiple documents.\n\nArgs:\n    self (TestRunChain): The test\
      \ method instance.\n\nReturns:\n    None: The test does not return a value.\n\
      \nRaises:\n    AssertionError: If the expected edges are not found in the graph."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    name: test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_entity_source_ids_mapped(\n\
      \        self,\n    )"
    docstring: "Test that run_extract_graph maps entity source_ids across multiple\
      \ documents correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion fails."
  - node_id: tests/unit/indexing/verbs/entities/extraction/strategies/graph_intelligence/test_gi_entity_extraction.py::TestRunChain.test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    name: test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped
    signature: "def test_run_extract_graph_multiple_documents_correct_edge_source_ids_mapped(\n\
      \        self,\n    )"
    docstring: "Test that run_extract_graph maps edge source_ids across multiple documents\
      \ correctly.\n\nArgs:\n    self: The test case instance (unittest.TestCase).\n\
      \nReturns:\n    None: The test does not return a value.\n\nRaises:\n    AssertionError:\
      \ If any assertion fails."
- class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: TestRunSentences
  methods:
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.setup_method
    name: setup_method
    signature: def setup_method(self, method)
    docstring: "Set up the test environment before each test by invoking bootstrap().\n\
      \nArgs:\n    method: object\n        The test method currently being executed.\
      \ Provided by the test framework.\n\nReturns:\n    None\n        Type: None\n\
      \nRaises:\n    ImportError: If the bootstrap process fails during initialization\
      \ of required resources."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_basic_functionality
    name: test_basic_functionality
    signature: def test_basic_functionality(self)
    docstring: "Test basic sentence splitting without metadata.\n\nVerifies that a\
      \ single input document containing two sentences is split into two TextChunk\
      \ objects with the expected text_chunk values, and that each chunk references\
      \ the first document (source_doc_indices == [0]). Also ensures the progress\
      \ ticker is invoked exactly once with the value 1.\n\nArgs:\n  self (TestRunSentences):\
      \ The test case instance.\n\nReturns:\n  None\n\nRaises:\n  None\n\nExamples:\n\
      \  Input:\n    [\"This is a test. Another sentence.\"]\n  Expected:\n    - Two\
      \ chunks:\n        - chunks[0].text_chunk == \"This is a test.\"\n        -\
      \ chunks[1].text_chunk == \"Another sentence.\"\n      - All chunks have source_doc_indices\
      \ == [0]\n      - tick.assert_called_once_with(1)\n  Notes:\n    No metadata\
      \ is attached to the produced chunks in this test."
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_multiple_documents
    name: test_multiple_documents
    signature: def test_multiple_documents(self)
    docstring: "Test processing multiple input documents into separate chunks and\
      \ verify correct chunk origins and progress tick behavior.\n\nThe test provides\
      \ two input documents: \\\"First. Document.\\\" and \\\"Second. Doc.\\\" which\
      \ should be chunked into four sentences (two per document). Each resulting TextChunk\
      \ should have its source_doc_indices set to the index of its originating document\
      \ (the first two chunks originate from document 0, the last two from document\
      \ 1). The test also asserts that the progress tick is invoked once for each\
      \ input document (two total).\n\nArgs:\n    self: The test case instance.\n\n\
      Returns:\n    None\n\nRaises:\n    None"
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunSentences.test_mixed_whitespace_handling
    name: test_mixed_whitespace_handling
    signature: def test_mixed_whitespace_handling(self)
    docstring: "Chunks text into multiple parts by sentence.\n\nArgs:\n  input: list[str]\
      \ - list of input documents to chunk into sentences.\n  _config: ChunkingConfig\
      \ - chunking configuration (unused by this strategy).\n  tick: ProgressTicker\
      \ - progress reporter; invoked to indicate progress after processing each input\
      \ document.\n\nReturns:\n  Iterable[TextChunk] - yields TextChunk objects for\
      \ each sentence, with text_chunk set to the sentence and source_doc_indices\
      \ containing the index of the source document.\n\nRaises:"
- class_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext
  file: graphrag/query/structured_search/global_search/community_context.py
  name: GlobalCommunityContext
  methods:
  - node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.__init__
    name: __init__
    signature: "def __init__(\n        self,\n        community_reports: list[CommunityReport],\n\
      \        communities: list[Community],\n        entities: list[Entity] | None\
      \ = None,\n        tokenizer: Tokenizer | None = None,\n        dynamic_community_selection:\
      \ bool = False,\n        dynamic_community_selection_kwargs: dict[str, Any]\
      \ | None = None,\n        random_state: int = 86,\n    )"
    docstring: "Initialize a GlobalCommunityContext instance with the provided data\
      \ and optional configuration.\n\nArgs:\n    community_reports: Reports for communities\
      \ to consider.\n    communities: Community objects used to build the hierarchy\
      \ and starting points.\n    entities: Optional list of Entity objects to include\
      \ in the context.\n    tokenizer: Tokenizer to use; if None, a default tokenizer\
      \ is obtained via get_tokenizer().\n    dynamic_community_selection: Enable\
      \ dynamic selection of communities during processing.\n    dynamic_community_selection_kwargs:\
      \ Optional keyword arguments for configuring DynamicCommunitySelection.\n  \
      \  random_state: Seed for random number generation (default 86).\n\nReturns:\n\
      \    None\n\nRaises:\n    None"
  - node_id: graphrag/query/structured_search/global_search/community_context.py::GlobalCommunityContext.build_context
    name: build_context
    signature: "def build_context(\n        self,\n        query: str,\n        conversation_history:\
      \ ConversationHistory | None = None,\n        use_community_summary: bool =\
      \ True,\n        column_delimiter: str = \"|\",\n        shuffle_data: bool\
      \ = True,\n        include_community_rank: bool = False,\n        min_community_rank:\
      \ int = 0,\n        community_rank_name: str = \"rank\",\n        include_community_weight:\
      \ bool = True,\n        community_weight_name: str = \"occurrence\",\n     \
      \   normalize_community_weight: bool = True,\n        max_context_tokens: int\
      \ = 8000,\n        context_name: str = \"Reports\",\n        conversation_history_user_turns_only:\
      \ bool = True,\n        conversation_history_max_turns: int | None = 5,\n  \
      \      **kwargs: Any,\n    ) -> ContextBuilderResult"
    docstring: "Prepare batches of community report data table as context data for\
      \ global search.\n\nArgs:\n  query: The user query to build context for.\n \
      \ conversation_history: Optional conversation history to consider while constructing\
      \ the context.\n  use_community_summary: Whether to use the community summary\
      \ in the context data.\n  column_delimiter: Delimiter used to separate columns\
      \ in the context representation.\n  shuffle_data: Whether to shuffle the data\
      \ before context assembly.\n  include_community_rank: Whether to include per-community\
      \ rank in the context.\n  min_community_rank: Minimum rank value to include\
      \ in the context.\n  community_rank_name: Name to assign to the rank field in\
      \ the context data.\n  include_community_weight: Whether to include per-community\
      \ weight in the context.\n  community_weight_name: Name to assign to the weight\
      \ field in the context data.\n  normalize_community_weight: Whether to normalize\
      \ the community weights.\n  max_context_tokens: Maximum allowed tokens for the\
      \ generated context.\n  context_name: Descriptive name for the context data\
      \ (e.g., \"Reports\").\n  conversation_history_user_turns_only: Whether to include\
      \ only user turns from the conversation history.\n  conversation_history_max_turns:\
      \ Maximum number of turns to include from the conversation history.\n  kwargs:\
      \ Additional keyword arguments.\n\nReturns:\n  ContextBuilderResult: The result\
      \ containing context chunks, context data, and related usage statistics.\n\n\
      Raises:\n  Exceptions raised by underlying components (e.g., conversation_history,\
      \ dynamic_community_selection, and build_community_context) during context construction."
- class_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens
  file: tests/unit/indexing/operations/chunk_text/test_strategies.py
  name: TestRunTokens
  methods:
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_basic_functionality
    name: test_basic_functionality
    signature: def test_basic_functionality(self, mock_get_encoding)
    docstring: "Chunks text into chunks based on encoding tokens.\n\nArgs:\n    input:\
      \ list[str] - The input texts to be chunked.\n    config: ChunkingConfig - Chunking\
      \ configuration. Uses: size (number of tokens per chunk), overlap (number of\
      \ overlapping tokens between consecutive chunks), encoding_model (name of the\
      \ encoding model used to tokenize).\n    tick: ProgressTicker - Progress reporter;\
      \ invoked to indicate progress.\n\nReturns:\n    Iterable[TextChunk] - An iterable\
      \ of TextChunk objects.\n\nRaises:\n    None"
  - node_id: tests/unit/indexing/operations/chunk_text/test_strategies.py::TestRunTokens.test_non_string_input
    name: test_non_string_input
    signature: def test_non_string_input(self, mock_get_encoding)
    docstring: "Test handling of non-string input (e.g., numbers) when tokenizing\
      \ text.\n\nArgs:\n  - self: The test case instance.\n  - mock_get_encoding:\
      \ The patched tiktoken.get_encoding function; a Mock that returns a mock encoder\
      \ used to encode/decode tokens.\n\nReturns:\n  - None. This test does not return\
      \ a value.\n\nRaises:\n  - None. No exceptions are expected during the test\
      \ execution."
